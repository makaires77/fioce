{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Análise exploraratória para avaliar utilização dos dados dos currículos Lattes para<br /> propor modelo de Grafo para análises em PDI. </center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa – Fiocruz Ceará\n",
    "\n",
    "**Introdução**\n",
    "\n",
    "A análise de Grafos permite obter insights como produtos de análises em contextos da realidade com base em modelos capazes de lidar dados heterogêneos e relações complexas.\n",
    "\n",
    "\n",
    "Neste trabalho propomos uma análise dos dados de pesquisa acadêmica tendo como fonte de dados os currículo Lattes de servidores da unidade Fiocruz Ceará.\n",
    "\n",
    "**Objetivo geral:**\n",
    "\n",
    "    Explorar dados dos currículos de servidores da Fiocruz Ceará.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "\n",
    "    1. Extrair dados dos currículos;\n",
    "    2. Propor modelo de grafo para análises futuras;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 0: Preparar e Testar Ambiente</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "        \n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_amb():\n",
    "    ## Visualizar versões dos principais componentes\n",
    "    import os\n",
    "    import pip\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # !pip3 install shutup\n",
    "    # import shutup; shutup.please()\n",
    "    \n",
    "    pyVer      = sys.version\n",
    "    pipVer     = pip.__version__\n",
    "    \n",
    "    print('\\nVERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT')\n",
    "    print('Interpretador em uso:', sys.executable)\n",
    "    print('    Ambiente ativado:',os.environ['CONDA_DEFAULT_ENV'])\n",
    "    print('     Python: '+pyVer, '\\n        Pip:', pipVer,'\\n'\n",
    "         )\n",
    "\n",
    "    !nvcc -V\n",
    "\n",
    "def get_cpu_info_windows():\n",
    "    import subprocess\n",
    "\n",
    "    try:\n",
    "        return subprocess.check_output(\"wmic cpu get Name\", shell=True).decode('utf-8').split('\\n')[1].strip()\n",
    "    except:\n",
    "        return \"Informação não disponível\"\n",
    "\n",
    "def get_cpu_info_unix():\n",
    "    import subprocess\n",
    "    try:\n",
    "        return subprocess.check_output(\"lscpu\", shell=True).decode('utf-8')\n",
    "    except:\n",
    "        try:\n",
    "            return subprocess.check_output(\"sysctl -n machdep.cpu.brand_string\", shell=True).decode('utf-8').strip()\n",
    "        except:\n",
    "            return \"Informação não disponível\"\n",
    "\n",
    "def try_cpu():\n",
    "    import psutil\n",
    "    import platform\n",
    "\n",
    "    # Métricas da CPU\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    cpu_count_logical = psutil.cpu_count(logical=True)\n",
    "    cpu_count_physical = psutil.cpu_count(logical=False)\n",
    "    cpu_freq = psutil.cpu_freq()\n",
    "    cpu_times_percent = psutil.cpu_times_percent(interval=1)\n",
    "\n",
    "    # Informação específica do modelo do processador\n",
    "    if platform.system() == \"Windows\":\n",
    "        cpu_model = get_cpu_info_windows()\n",
    "    else:\n",
    "        cpu_model = get_cpu_info_unix()\n",
    "\n",
    "    # Informações adicionais sobre o Processador\n",
    "    cpu_brand = platform.processor()\n",
    "    cpu_architecture = platform.architecture()[0]\n",
    "    cpu_machine_type = platform.machine()\n",
    "    \n",
    "    # Métricas da Memória RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    total_ram = ram.total / (1024 ** 3)  # Em GB\n",
    "    used_ram = ram.used / (1024 ** 3)  # Em GB\n",
    "    \n",
    "    # Métricas do Espaço em Disco\n",
    "    disk = psutil.disk_usage('/')\n",
    "    total_disk = disk.total / (1024 ** 3)  # Em GB\n",
    "    used_disk = disk.used / (1024 ** 3)  # Em GB\n",
    "    free_disk = (total_disk - used_disk)\n",
    "    used_disk_percent = (used_disk / total_disk) * 100\n",
    "    free_disk_percent = (1 - (used_disk / total_disk)) * 100\n",
    "\n",
    "    # Exibição das Métricas\n",
    "    print(f\"\\nMarca do Processador: {cpu_brand}\")\n",
    "    print(f\"Modelo do Processador: {cpu_model}\")\n",
    "    print(f\"Frequência da CPU: {cpu_freq.current} MHz\")\n",
    "    # print(f\"Tipo de Máquina: {cpu_machine_type}\")\n",
    "    print(f\"Arquitetura do Processador: {cpu_architecture}\")\n",
    "    print(f\"Número de CPUs físicas: {cpu_count_physical}\")\n",
    "    print(f\"Número de CPUs lógicas: {cpu_count_logical}\")\n",
    "    print(f\"Uso atual CPU: {cpu_percent}%\")\n",
    "    print(f\"Tempos de CPU: user={cpu_times_percent.user}%, system={cpu_times_percent.system}%, idle={cpu_times_percent.idle}%\")\n",
    "    print(f\"\\nTotal de RAM: {total_ram:>5.2f} GB\")\n",
    "    print(f\"Usado em RAM: {used_ram:>5.2f} GB\")\n",
    "    print(f\"Espaço Total em disco: {total_disk:>7.2f} GB\")\n",
    "    print(f\"Espaço em disco usado: {used_disk:>7.2f} GB {used_disk_percent:>4.1f}%\")\n",
    "    print(f\"Espaço em disco livre: {free_disk:>7.2f} GB {free_disk_percent:>4.1f}%\")\n",
    "\n",
    "def try_gpu():\n",
    "    print('\\nVERSÕES DO PYTORCH E GPU DISPONÍVEIS')\n",
    "    try:\n",
    "        import torch\n",
    "        print('    PyTorch:',torch.__version__)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Dispositivo:',device)\n",
    "        print('Disponível :',device,torch.cuda.is_available(),' | Inicializado:',torch.cuda.is_initialized(),'| Capacidade:',torch.cuda.get_device_capability(device=None))\n",
    "        print('Nome GPU   :',torch.cuda.get_device_name(0),'         | Quantidade:',torch.cuda.device_count(),'\\n')\n",
    "    except Exception as e:\n",
    "        print('  ERRO!! Ao configurar a GPU:',e,'\\n')\n",
    "\n",
    "def try_folders(drives,pastas,pastasraiz):\n",
    "    import os\n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    caminho_testado = drive+i+j\n",
    "                    if os.path.isfile(caminho_testado+'/chromedriver/chromedriver.exe'):\n",
    "                        print(f\"Listando arqivos em: {caminho_testado}\")\n",
    "                        print(os.listdir(caminho_testado))\n",
    "                        caminho = caminho_testado+'/'\n",
    "                except:\n",
    "                    caminho=''\n",
    "                    print('Não foi possível encontrar uma pasta de trabalho')\n",
    "    return caminho\n",
    "\n",
    "def try_browser(raiz):\n",
    "    print('\\nVERSÕES DO BROWSER E DO CHROMEDRIVER INSTALADAS')\n",
    "    from selenium import webdriver\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    try:\n",
    "        # Caminho para o seu chromedriver\n",
    "        driver_path=raiz+'chromedriver/chromedriver.exe'\n",
    "        print(driver_path)\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        str1 = driver.capabilities['browserVersion']\n",
    "        str2 = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]\n",
    "        print(f'     Versão do browser: {str1}')\n",
    "        print(f'Versão do chromedriver: {str2}')\n",
    "        driver.quit()\n",
    "\n",
    "        if str1[0:3] != str2[0:3]: \n",
    "            print(\"Versões incompatíveis, atualizar chromedriver!\")\n",
    "            print('  Baixar versão atualizada do Chromedriver em:')\n",
    "            print('  https://googlechromelabs.github.io/chrome-for-testing/#stable')\n",
    "            print('     Ex. Versão 116 PARA WINDOWS:')\n",
    "            print('\t    https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/win64/chromedriver-win64.zip')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def try_chromedriver(caminho):\n",
    "    try:\n",
    "        import os\n",
    "        os.listdir(caminho)\n",
    "    except Exception as e:\n",
    "        raiz=caminho\n",
    "\n",
    "    finally:\n",
    "        print(raiz)\n",
    "    return raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('  ERRO!! Ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "\n",
    "    caminho = drive+':/'+usuario+pastaraiz+'/'\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    import os\n",
    "\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT\n",
      "Interpretador em uso: c:\\Users\\marco\\.conda\\envs\\beakerx\\python.exe\n",
      "    Ambiente ativado: beakerx\n",
      "     Python: 3.11.2 | packaged by Anaconda, Inc. | (main, Mar 27 2023, 23:35:04) [MSC v.1916 64 bit (AMD64)] \n",
      "        Pip: 23.2.1 \n",
      "\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Jul_11_03:10:21_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.128\n",
      "Build cuda_12.2.r12.2/compiler.33053471_0\n",
      "\n",
      "Marca do Processador: AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD\n",
      "Modelo do Processador: AMD Ryzen 7 5800X 8-Core Processor\n",
      "Frequência da CPU: 3801.0 MHz\n",
      "Arquitetura do Processador: 64bit\n",
      "Número de CPUs físicas: 8\n",
      "Número de CPUs lógicas: 16\n",
      "Uso atual CPU: 7.4%\n",
      "Tempos de CPU: user=4.9%, system=2.0%, idle=93.1%\n",
      "\n",
      "Total de RAM: 63.94 GB\n",
      "Usado em RAM: 30.15 GB\n",
      "Espaço Total em disco:  465.15 GB\n",
      "Espaço em disco usado:  454.48 GB 97.7%\n",
      "Espaço em disco livre:   10.67 GB  2.3%\n",
      "\n",
      "VERSÕES DO PYTORCH E GPU DISPONÍVEIS\n",
      "    PyTorch: 2.0.1+cu118\n",
      "Dispositivo: cuda\n",
      "Disponível : cuda True  | Inicializado: False | Capacidade: (7, 5)\n",
      "Nome GPU   : NVIDIA GeForce RTX 2060          | Quantidade: 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_amb()\n",
    "try_cpu()\n",
    "try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema operacional Windows\n",
      "Drive em uso C\n",
      "Pasta armazenagem local C:/Users/marco/fioce/\n",
      "\n",
      "Listando arqivos em: C:/Users/marco/fioce\n",
      "['.git', '.gitignore', 'assets', 'chromedriver', 'csv', 'doc', 'fig', 'json', 'output', 'scripts', 'source', 'utils', 'xml_zip']\n",
      "Pasta para os xml já existe!\n",
      "Pasta para os CSV já existe!\n",
      "Pasta para os JSON já existe!\n",
      "Pasta para figuras já existe!\n",
      "Pasta para saídas já existe!\n",
      "\n",
      "Caminho da pasta raiz C:/Users/marco/fioce/\n",
      "Caminho arquivos  XML C:/Users/marco/fioce/xml_zip/\n",
      "Caminho arquivos JSON C:/Users/marco/fioce/json/\n",
      "Caminho arquivos  CSV C:/Users/marco/fioce/csv/\n",
      "Caminho para  figuras C:/Users/marco/fioce/fig/\n",
      "Pasta arquivos saídas C:/Users/marco/fioce/output/\n"
     ]
    }
   ],
   "source": [
    "pastaraiz = 'fioce'\n",
    "caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "# try_browser(caminho)\n",
    "\n",
    "drives=['C:/Users/','E:/','./home/']\n",
    "pastas=['marcos.aires/', 'marco/']\n",
    "pastasraiz=['kgfioce','fioce']\n",
    "caminho=try_folders(drives,pastas,pastasraiz)\n",
    "\n",
    "pathzip, pathcsv, pathjson, pathfig, pathaux, pathout = preparar_pastas(caminho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 1: Implementar funções de trabalho</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações agrupadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "# !pip install py2neo --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from flask import render_template_string\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, \n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    "    TimeoutException,\n",
    "    WebDriverException\n",
    ")\n",
    "\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "delay = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções acessórias em tratar HTML e chegar ao currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "    driver_path=caminho+'chromedriver/chromedriver.exe'\n",
    "    # print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)    \n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "\n",
    "    # return browser, url, session_id\n",
    "    return driver\n",
    "\n",
    "def paginar(browser):\n",
    "    '''\n",
    "    Função auxiliar para paginar resultados na página de busca\n",
    "    '''\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('  ERRO!! Ao rodar função paginar():', e)\n",
    "    return numpaginas\n",
    "\n",
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "            \n",
    "def handle_stale_file_error(driver, max_retries=5, retry_interval=10):\n",
    "    \"\"\"\n",
    "    Detects and handles the \"Stale file handle\" error message in the webpage content.\n",
    "    \n",
    "    Parameters:\n",
    "        - driver (webdriver object): The Selenium webdriver object.\n",
    "        - max_retries (int): Maximum number of retries if the error is detected.\n",
    "        - retry_interval (int): Time interval (in seconds) between retries.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the error was resolved within the retry limit, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            error_div = driver.find_element(By.CSS_SELECTOR, 'resultado')\n",
    "            linha1 = error_div.fidChild('li')\n",
    "            if 'Stale file handle' in linha1.text:\n",
    "                time.sleep(retry_interval)\n",
    "            else:\n",
    "                return True\n",
    "        except NoSuchElementException:\n",
    "            # If the error div is not found, it's assumed the error is resolved.\n",
    "            return True\n",
    "        \n",
    "    # If the loop completes without breaking, it means the error wasn't resolved in the given retries.\n",
    "    return False\n",
    "\n",
    "def format_string(input_str):\n",
    "    # Verifica se a entrada é uma string de oito dígitos\n",
    "    if input_str and len(input_str) == 9:\n",
    "        return input_str\n",
    "    elif input_str and len(input_str) == 8:\n",
    "        # Divide a string em duas partes\n",
    "        part1 = input_str[:4]\n",
    "        part2 = input_str[4:]\n",
    "        \n",
    "        # Concatena as duas partes com um hífen\n",
    "        formatted_str = f\"{part1}-{part2}\"\n",
    "        \n",
    "        return formatted_str\n",
    "    else:\n",
    "        return input_str\n",
    "\n",
    "def fill_name(driver, delay, NOME):\n",
    "    '''\n",
    "    Função 2: move cursor para o campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('  ERRO!! Ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)\n",
    "\n",
    "def achar_busca(driver, delay):\n",
    "    '''\n",
    "    Função auxiliar para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = driver.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               #expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               #logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'  ERRO!! Durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)        \n",
    "\n",
    "def extract_data_from_cvuri(element):\n",
    "    # Obter o valor do atributo cvuri\n",
    "    cvuri = element.get_attribute('cvuri')\n",
    "    \n",
    "    # Fazer o parsing da URL para extrair os parâmetros\n",
    "    parsed_url = urlparse(cvuri)\n",
    "    params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Converter a lista de valores para valores únicos, já que parse_qs retorna listas\n",
    "    data_dict = {k: v[0] for k, v in params.items()}\n",
    "    \n",
    "    return data_dict\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função principal de extração de dados do CVLattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_terms(NOME, instituicao, unidade, termo, driver, delay, limite):\n",
    "    \"\"\"\n",
    "    Função para manipular o HTML até abir a página HTML de cada currículo   \n",
    "\n",
    "    Parâmeteros:\n",
    "        - NOME: É o nome completo de cada pesquisador\n",
    "        - Instituição, unidade e termo: Strings a buscar no currículo para reduzir duplicidades\n",
    "        - driver (webdriver object): The Selenium webdriver object.\n",
    "        - limite (int): Número máximo de tentativas em casos de erro.\n",
    "        - delay (int): tempo em milisegundos a esperar nas operações de espera.\n",
    "    \n",
    "    Retorna:\n",
    "        elm_vinculo, np.NaN, np.NaN, np.NaN, driver.\n",
    "    \n",
    "    Em caso de erro retorna:\n",
    "        np.NaN, NOME, np.NaN, e, driver\n",
    "    \"\"\"\n",
    "\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    \n",
    "    # Inicializando variáveis para evitar UnboundLocalError\n",
    "    elm_vinculo = None\n",
    "    qte_resultados = 0\n",
    "\n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        # Wait and fetch the number of results\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(driver, delay, ignored_exceptions=ignored_exceptions).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                # print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                return np.NaN, NOME, np.NaN, 'Currículo não encontrado', driver\n",
    "        except Exception as e1:\n",
    "            print('  ERRO!! Currículo não disponível no Lattes')\n",
    "            return np.NaN, NOME, np.NaN, e1, driver\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(driver)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e2:\n",
    "                print('  ERRO!! Ao encontrar o primeiro resultado da lista de nomes:', e2)\n",
    "                \n",
    "                # Call the handle_stale_file_error function\n",
    "                if handle_stale_file_error(driver):\n",
    "                    # If the function returns True, it means the error was resolved.\n",
    "                    # try to get the nome_vinculo again:\n",
    "                    try:\n",
    "                        elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                        nome_vinculo = elm_vinculo.text\n",
    "                    except Exception as e3:\n",
    "                        print('  ERRO!! Servidor CNPq indisponível no momento, tentar em alguns minutos:', e3)\n",
    "                        return np.NaN, NOME, np.NaN, e3, driver\n",
    "                else:\n",
    "                    # If the function returns False, it means the error was not resolved within the given retries.\n",
    "                    return np.NaN, NOME, np.NaN, e2, driver\n",
    "\n",
    "                print('Não foi possível extrair por falha no servidor do CNPq:',e)\n",
    "                return np.NaN, NOME, np.NaN, e2, driver\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e4:\n",
    "                print('  ERRO!! Ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e4, driver\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} homônimos de: {NOME}')\n",
    "            numpaginas = paginar(driver)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(driver)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('  ERRO!! Ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        # print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e5:\n",
    "                                    print('  ERRO!! Ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e5, driver\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e6:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e6.__traceback__))\n",
    "                            print('  ERRO!! Ao procurar vínculo com currículos achados')    \n",
    "                            print(e6,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # driver.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "    \n",
    "    except exceptions.TimeoutException:\n",
    "        print(\"  ERRO!! O tempo limite de espera foi atingido.\")\n",
    "        return np.NaN, NOME, np.NaN, \"TimeoutException\", driver\n",
    "\n",
    "    except exceptions.WebDriverException as e7:\n",
    "        print(\"  ERRO!! Problema ao interagir com o driver.\")\n",
    "        return np.NaN, NOME, np.NaN, e7, driver\n",
    "\n",
    "    except Exception as e8:\n",
    "        print(\"  ERRO!! Um erro inesperado ocorreu.\")\n",
    "        return np.NaN, NOME, np.NaN, e8, driver\n",
    "\n",
    "    # Verifica antes de retornar para garantir que elm_vinculo foi definido\n",
    "    if elm_vinculo is None:\n",
    "        print(\"Vínculo não foi definido.\")\n",
    "        return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "\n",
    "    # Retorna a saída de sucesso\n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, driver\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_curriculum(driver, elm_vinculo, verbose=False):\n",
    "    \"\"\"\n",
    "    Função principal para extrair dados de cada página de currículo.\n",
    "    \n",
    "    Parameters:\n",
    "        - driver (webdriver object): The Selenium webdriver object.\n",
    "        - elm_vinculo (WebElement): O objeto achado pelas funções anteriores.    \n",
    "    Returns:\n",
    "        Dicionário com dados extraídos do tooltip\n",
    "    \"\"\"\n",
    "    link_nome = achar_busca(driver, delay)\n",
    "    window_before = driver.current_window_handle\n",
    "    \n",
    "    if elm_vinculo == np.NaN:\n",
    "        print('Vínculo não encontrado, passando para o próximo nome...')\n",
    "        return\n",
    "    else:\n",
    "        try:\n",
    "            print('Vínculo encontrado no currículo de nome:', elm_vinculo.text)\n",
    "        except AttributeError:\n",
    "            print('  ERRO!! Ao acessar texto do vínculo, elemento extraído vazio.')\n",
    "            return \n",
    "    \n",
    "    # Clicar no botão \"Abrir Currículo\" e mudar de aba\n",
    "    try:\n",
    "        link_nome = achar_busca(driver, delay)\n",
    "    except Exception as e:\n",
    "        print('Erro')\n",
    "        print(e)\n",
    "\n",
    "    limite = 2\n",
    "    if link_nome.text == None:\n",
    "        xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "        print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            wait_ms=200,\n",
    "            limit=limite,\n",
    "            on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))    \n",
    "    try:\n",
    "        ActionChains(driver).click(link_nome).perform()\n",
    "    except:\n",
    "        print(f'Currículo não encontrado.')\n",
    "\n",
    "    retry(WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "        wait_ms=200,\n",
    "        limit=limite,\n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))\n",
    "\n",
    "    # Clicar no botão para abrir o currículo\n",
    "    btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "    # Gerenciar janelas abertas no navegador\n",
    "    WebDriverWait(driver, delay).until(EC.number_of_windows_to_be(2))\n",
    "    window_after = driver.window_handles\n",
    "    new_window = [x for x in window_after if x != window_before][0]\n",
    "    driver.switch_to.window(new_window)\n",
    "\n",
    "    # Definir soup fora do loop para que esteja acessível em todo o escopo\n",
    "    soup = None\n",
    "\n",
    "    # Extração dos dados em tooltips em <div id=\"artigos-completos\">    \n",
    "    try:\n",
    "        # Esperar para garantir que todos os elementos da seção \"#artigos-completos\" foram carregados\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_all_elements_located((\n",
    "                By.CSS_SELECTOR, \"#artigos-completos img.ajaxJCR\"))\n",
    "        )\n",
    "\n",
    "        tooltip_data_list = []\n",
    "        impact_factor = ''\n",
    "        issn = ''\n",
    "        journal = ''\n",
    "        impact_factor = ''\n",
    "        doi_link = ''\n",
    "                \n",
    "        # Localizar a div principal pelas classes de div\n",
    "        WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_all_elements_located((\n",
    "                By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5'))\n",
    "                )\n",
    "        layout_cells = driver.find_elements(By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5')\n",
    "        if verbose:\n",
    "            print(len(layout_cells), 'células principais de dados encontradas')\n",
    "\n",
    "        for cell in layout_cells:\n",
    "            cvuri_dict = {}  # Defina um valor padrão vazio para cvuri_dict\n",
    "            \n",
    "            # Extrair ISSN da classe \".citado\"\n",
    "            try:\n",
    "                WebDriverWait(driver, 60).until(\n",
    "                    EC.presence_of_all_elements_located((\n",
    "                        By.CSS_SELECTOR, '.citado'))\n",
    "                        )\n",
    "                elem_citado = cell.find_element(By.CSS_SELECTOR, '.citado')\n",
    "                cvuri_dict = extract_data_from_cvuri(elem_citado)\n",
    "                if verbose:\n",
    "                    print(f'Dados artigo em cvuri: {cvuri_dict}')\n",
    "            except ElementNotInteractableException as e:\n",
    "                print('Conteúdo do erro:')\n",
    "                print(e)\n",
    "                continue\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print('O erro é o seguinte:')\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "            # Extrair o DOI\n",
    "            try:\n",
    "                doi_elem = cell.find_element(By.CSS_SELECTOR, \"a.icone-producao.icone-doi\")\n",
    "                doi_link = doi_elem.get_attribute(\"href\")\n",
    "                issn_match = re.search(r\"issn=(\\d+)\", doi_link)\n",
    "                issn_from_doi = issn_match.group(1) if issn_match else None\n",
    "            except NoSuchElementException:\n",
    "                doi_link = None\n",
    "                issn_from_doi = None\n",
    "\n",
    "            # Extrair dados do JCR dos tooltips melhorado\n",
    "            try:\n",
    "                # Espera até que o elemento seja encontrado e esteja visível na página\n",
    "                element_present = EC.presence_of_element_located((By.CSS_SELECTOR, \"img.ajaxJCR\"))\n",
    "                WebDriverWait(driver, 10).until(element_present)  # 10 é o tempo máximo de espera em segundos\n",
    "                \n",
    "                tooltip_elem = driver.find_element(By.CSS_SELECTOR, \"img.ajaxJCR\")\n",
    "                ActionChains(driver).move_to_element(tooltip_elem).perform()\n",
    "\n",
    "                original_title = tooltip_elem.get_attribute(\"original-title\")\n",
    "                journal = original_title.split('<br />')[0].strip()\n",
    "                match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "                if match:\n",
    "                    impact_factor = match.group(1)\n",
    "                data_issn = issn_from_doi or tooltip_elem.get_attribute(\"data-issn\")\n",
    "                if not data_issn:\n",
    "                    # Código para lidar com a falta de data_issn\n",
    "                    pass\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                data_issn = issn_from_doi\n",
    "            except TimeoutException:\n",
    "                # Código para lidar com o timeout\n",
    "                pass\n",
    "\n",
    "            # Compile os dados\n",
    "            issn = format_string(data_issn)\n",
    "            tooltip_data = {\n",
    "                \"doi\": doi_link,\n",
    "                \"data-issn\": issn,\n",
    "                \"original_title\": journal,\n",
    "                \"impact-factor\": impact_factor,\n",
    "            }\n",
    "            tooltip_data.update(cvuri_dict)\n",
    "            tooltip_data_list.append(tooltip_data)\n",
    "\n",
    "        print(f'Metadados de {len(tooltip_data_list)} artigos completos extraídos')\n",
    " \n",
    "        page_source = driver.page_source\n",
    "        # Use BeautifulSoup to parse\n",
    "        if page_source is not None:\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            soup.attrs['tooltips'] = tooltip_data_list\n",
    "    \n",
    "    except TimeoutException:\n",
    "        print(\"O elemento não foi encontrado no tempo especificado.\")\n",
    "        page_source = driver.page_source\n",
    "        if page_source is not None:\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(  'ERRO!! Ao extrair dados do currículo... passando ao próximo')\n",
    "        print(e)\n",
    "        page_source = driver.page_source\n",
    "        if page_source is not None:\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    driver.quit()  \n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de extração de seções específicas a partir de dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "def is_abreviation(substring):\n",
    "    pattern = r'(?: [a-zA-Z]\\.)|(?: [a-zA-Z] \\.)'\n",
    "    if re.search(pattern, substring):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_surname(substring):\n",
    "    return not is_abreviation(substring) and substring.endswith(', ')\n",
    "\n",
    "def compose_full_name(surname, parts, marker):\n",
    "    full_name = \"\"\n",
    "    for part in parts:\n",
    "        full_name += part + marker\n",
    "    full_name += ' '+surname\n",
    "    return full_name.strip()\n",
    "\n",
    "def split_authors(string, verbose=False):\n",
    "    authors_names = []\n",
    "    \n",
    "    m1a = \"; \"\n",
    "    m1b = \", \"\n",
    "    \n",
    "    if m1a in string and (string.count(m1a) <= string.count(m1b) or m1b not in string):\n",
    "        marker = m1a\n",
    "        authors_names = string.split(marker)\n",
    "        return [x.strip() for x in authors_names]\n",
    "    else:\n",
    "        marker = m1b\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Selected marker: \"{marker}\"')\n",
    "\n",
    "    parts_list = string.split(marker)\n",
    "    name    = \"\"\n",
    "    surname = \"\"\n",
    "    \n",
    "    for part in parts_list:\n",
    "        if is_abreviation(part):\n",
    "            classification = 'Abrev'\n",
    "            name += part + marker\n",
    "        else:\n",
    "            classification = 'Name'\n",
    "            if is_surname(part) or surname == \"\":\n",
    "                classification = 'SOBRENOME'\n",
    "                if surname:\n",
    "                    full_name = compose_full_name(surname.strip(), name.split(marker), ', ')\n",
    "                    if full_name not in authors_names:\n",
    "                        authors_names.append(full_name)\n",
    "                surname = part\n",
    "                name = part + marker\n",
    "            else:\n",
    "                name = part + marker\n",
    "        if verbose:\n",
    "            # print(f'Quantidade de m1a: {string.count(m1a)} \\nQuantidade de m1b: {string.count(m1b)}')\n",
    "            print(f'Autor: {part:40} | Forma: {classification}')\n",
    "\n",
    "        authors_names.append(part.strip(marker).strip())\n",
    "     \n",
    "    return [x.strip() for x in authors_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extrair_indices(layout_celula):\n",
    "    indice = layout_celula.select_one('div.layout-cell-pad-5.text-align-right')\n",
    "    valor = layout_celula.select_one('div.layout-cell.layout-cell-9 div.layout-cell-pad-5')\n",
    "    \n",
    "    if indice and valor:\n",
    "        valores_extraidos = valor.text.split('<br class=\"clear\">') if '<br class=\"clear\">' in valor.text else valor.text.split('\\n\\t\\t\\n\\t')\n",
    "        return indice.text, valores_extraidos\n",
    "    return None, None\n",
    "\n",
    "def extrair_secao(title_wrapper):\n",
    "    tags_relevantes = ['h1', 'ul', 'b']\n",
    "    tags_encontradas = []\n",
    "    for tag in tags_relevantes:\n",
    "        elemento_encontrado = title_wrapper.select_one(tag)\n",
    "        if elemento_encontrado:\n",
    "            tags_encontradas.append((tag, elemento_encontrado))\n",
    "    tags_ordenadas = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def encontrar_subchave(title_wrapper):\n",
    "    tags_relevantes  = ['ul', 'a', 'b']\n",
    "    tags_encontradas = [(tag, title_wrapper.find(tag)) for tag in tags_relevantes]\n",
    "    tags_ordenadas   = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def extrair_titulo(title_wrapper):\n",
    "    inst_back_texts = {}\n",
    "    inst_back_elements = title_wrapper.find_all('div', class_='inst_back')\n",
    "    for index, inst_back in enumerate(inst_back_elements):\n",
    "        b_tag = inst_back.find('b')\n",
    "        if b_tag:\n",
    "            inst_back_texts[index] = b_tag.text.strip()\n",
    "    return inst_back_texts if inst_back_texts else None\n",
    "\n",
    "def extrair_wraper(soup, json_data):\n",
    "    title_wrappers = soup.select('div.layout-cell-pad-main div.title-wrapper')\n",
    "    for title_wrapper in title_wrappers:\n",
    "        section_name = extrair_secao(title_wrapper)\n",
    "        if section_name:\n",
    "            section_name = section_name.text.strip()\n",
    "            \n",
    "            titles = extrair_titulo(title_wrapper)\n",
    "            json_data[\"Properties\"][section_name] = {}\n",
    "            \n",
    "            if titles:\n",
    "                for index, title in titles.items():\n",
    "                    json_data[\"Properties\"][section_name][title] = {}\n",
    "            \n",
    "            layout_cells = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "            for layout_celula in layout_cells:\n",
    "                indice, valores_extraidos = extrair_indices(layout_celula)\n",
    "                if indice and valores_extraidos:\n",
    "                    if titles and indice in titles.values():\n",
    "                        if len(titles) > 1:\n",
    "                            for title in titles.values():\n",
    "                                if title.strip() in indice:\n",
    "                                    json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                        else:\n",
    "                            title = list(titles.values())[0]\n",
    "                            json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                    else:\n",
    "                        json_data[\"Properties\"][section_name][indice] = valores_extraidos\n",
    "    return json_data\n",
    "\n",
    "def imprimir_informacoes(dados_json, nome_no, indent=0):\n",
    "    indentation = '    ' * indent  # Calculating the current indentation level\n",
    "\n",
    "    if dados_json and nome_no and dados_json.get(nome_no):\n",
    "        if indent == 0:  # Logging node-level information only at the root\n",
    "            logging.info(f\"{indentation}Node: {nome_no}\")\n",
    "            logging.info(f\"{indentation}Total keys extracted: {len(dados_json[nome_no].keys())}\")\n",
    "        \n",
    "        for key in dados_json[nome_no].keys():\n",
    "            logging.info(f\"{indentation}{key.strip() if key else ''}\")\n",
    "\n",
    "            if isinstance(dados_json[nome_no][key], dict):  # Check for nested dictionaries\n",
    "                # Recursive call to handle nested dictionaries\n",
    "                imprimir_informacoes(dados_json[nome_no], key, indent + 1)\n",
    "            else:\n",
    "                logging.info(f\"{indentation}    Values: {dados_json[nome_no][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para montar dicionários para persistir em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(input_data, dict):\n",
    "            for key, value in input_data.items():\n",
    "                if isinstance(value, dict):  # Se um valor ainda é um dicionário, converte em string JSON\n",
    "                    input_data[key] = json.dumps(Neo4jPersister.convert_to_primitives(value), ensure_ascii=False)\n",
    "                else:\n",
    "                    input_data[key] = Neo4jPersister.convert_to_primitives(value)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, list):\n",
    "            return [Neo4jPersister.convert_to_primitives(item) for item in input_data]\n",
    "        \n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        \n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return Neo4jPersister.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def persist_data(self, data_dict, label):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MERGE (node:{label}) SET node = $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "    def update_data(self, node_id, data_dict):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MATCH (node) WHERE id(node) = {node_id} SET node += $props\"\n",
    "            session.run(query, props=data_dict_primitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit1_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "    # Títulos contendo subseções\n",
    "    tit1a = ['Identificação', 'Endereço', 'Formação acadêmica/titulação', 'Pós-doutorado', 'Formação Complementar',\n",
    "            'Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão',\n",
    "            'Projetos de desenvolvimento', 'Revisor de periódico', 'Revisor de projeto de fomento', 'Áreas de atuação',\n",
    "            'Idiomas', 'Inovação']\n",
    "\n",
    "    tit1b = ['Atuação Profissional'] # dados com subseções\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        try:\n",
    "            titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        except:\n",
    "            titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "        # Verifique se o título está na lista 'tit1'\n",
    "        if titulo in tit1a:\n",
    "            if verbose:\n",
    "                print(titulo)\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                keys = []\n",
    "                vals = []\n",
    "\n",
    "                for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "                    if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "                        key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                        key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                        keys.append(key_text)\n",
    "                        val = j.find('div', class_='layout-cell-pad-5')\n",
    "                        val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                        vals.append(val_text)\n",
    "                        if verbose:\n",
    "                            print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "                agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                data_dict[titulo] = Neo4jPersister.convert_to_primitives(agg_dict)\n",
    "\n",
    "        \n",
    "        if titulo in tit1b:\n",
    "            if verbose:\n",
    "                print(titulo)\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = []\n",
    "                    if verbose:\n",
    "                        print(section_name)\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    while sibling:\n",
    "                        classes = sibling.get('class', [])\n",
    "\n",
    "                        if 'layout-cell-3' in classes:  # Data key\n",
    "                            key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                            if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "                                val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                current_data[key] = val\n",
    "                                if verbose:\n",
    "                                    print(len(current_data.values()), key, val)\n",
    "\n",
    "                        elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                            next_sibling = sibling.find_next_sibling()\n",
    "                            if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                sibling = None\n",
    "                            else:\n",
    "                                if current_data:\n",
    "                                    data_dict[titulo][section_name].append(current_data)  # Armazenamos os dados em uma lista\n",
    "\n",
    "                        if sibling:\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit2_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    \n",
    "    database = ''\n",
    "    total_trab_text = 0\n",
    "    total_cite_text = 0\n",
    "    num_fator_h = 0\n",
    "    data_wos_text = ''\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')    \n",
    "    \n",
    "    tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        try:\n",
    "            titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        except:\n",
    "            titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "        # Verifique se o título está na lista 'tit2'\n",
    "        if titulo in tit2:\n",
    "            if verbose:\n",
    "                print(f'Título: {titulo}')\n",
    "\n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = {}\n",
    "                    if verbose:\n",
    "                        print(f'Seção: {section_name}')\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_subsection = None\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    if section_name == 'Produção bibliográfica':\n",
    "                        subsections = section.find_next_siblings('div', class_='cita-artigos')\n",
    "                        if verbose:\n",
    "                            print(len(subsections), 'subseções')                       \n",
    "                        for subsection in subsections:                            \n",
    "                            if subsection:\n",
    "                                subsection_name = subsection.find('b').get_text().strip()\n",
    "                                if verbose:\n",
    "                                    print(f'    Subseção: {subsection_name}') # nomes de subseção como ocorrências \n",
    "                                    print(f'    {len(subsection)} divs na subseção {subsection_name}')                                \n",
    "                                if subsection_name == 'Citações':\n",
    "                                    current_subsection = subsection_name\n",
    "                                    data_dict[titulo][section_name]['Citações'] = {}\n",
    "                                    sub_section_list = []\n",
    "                                        \n",
    "                                    ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "                                    next_siblings = subsection.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "                                    for sibling in next_siblings:\n",
    "                                        citation_counts = sibling.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que tem os Valores de Citações\n",
    "                                        if citation_counts:\n",
    "                                            for i in citation_counts:\n",
    "                                                database = i.get_text()\n",
    "                                                total_trab = i.find_next_sibling(\"div\", class_=\"trab\")\n",
    "                                                if total_trab:\n",
    "                                                    total_trab_text = total_trab.get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                                total_cite = i.find_next_sibling(\"div\", class_=\"cita\")\n",
    "                                                if total_cite:\n",
    "                                                    total_cite_text = total_cite.get_text().split(\"Total de citações:\")[1]\n",
    "                                                fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                                num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                                data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\")\n",
    "                                                if data_wos:\n",
    "                                                    try:\n",
    "                                                        data_wos_text = data_wos.get_text().split(\"Data:\")[1].strip()\n",
    "                                                    except:\n",
    "                                                        data_wos_text = data_wos.get_text()\n",
    "\n",
    "                                                # Converta os valores para tipos de dados adequados\n",
    "                                                total_trab = int(total_trab_text)\n",
    "                                                total_cite = int(total_cite_text)\n",
    "\n",
    "                                                citation_numbers = {\n",
    "                                                    \"Database\": database,\n",
    "                                                    \"Total de trabalhos\": total_trab,\n",
    "                                                    \"Total de citações\": total_cite,\n",
    "                                                    \"Índice_H\": num_fator_h,\n",
    "                                                    \"Data\": data_wos_text\n",
    "                                                }\n",
    "\n",
    "                                                # Verifique se a subseção atual já existe no dicionário\n",
    "                                                if 'Citações' not in data_dict[titulo][section_name]:\n",
    "                                                    data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "\n",
    "                                                data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "\n",
    "                                                if verbose:\n",
    "                                                    print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "                            \n",
    "                        ## Encontrar a div irmã de div subseção com classe layout-cell-12 com artigos\n",
    "                        vals_jcr = []\n",
    "                        div_artigo_geral = data_cell.findChild(\"div\", id=\"artigos-completos\")\n",
    "                        if verbose:\n",
    "                            print(f'Encontrada {len(div_artigo_geral)} div geral de artigos')  \n",
    "                        \n",
    "                        if div_artigo_geral:\n",
    "                            divs_artigos = div_artigo_geral.find_all('div', class_='artigo-completo')\n",
    "                            if verbose:\n",
    "                                print(len(divs_artigos), 'divs de artigos')\n",
    "                            \n",
    "                            current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                            if divs_artigos:                              \n",
    "                                for div_artigo in divs_artigos:\n",
    "                                    data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = {}                                   \n",
    "                                        ## Extrair filhos da classes de artigos completos que estão à frente\n",
    "                                    sibling = div_artigo.findChild()\n",
    "\n",
    "                                    while sibling:\n",
    "                                        classes = sibling.get('class', [])\n",
    "\n",
    "                                        if 'layout-cell-1' in classes:  # Data key\n",
    "                                            key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                            if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                                val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                                info_dict = {\n",
    "                                                    'data-issn': 'NULL',\n",
    "                                                    'impact-factor': 'NULL',  \n",
    "                                                    'jcr-year': 'NULL',\n",
    "                                                }\n",
    "                                                # Remova as tags span da div\n",
    "                                                for span in sibling.find_all('span'):\n",
    "                                                    span.extract()\n",
    "                                                \n",
    "                                                val_text = sibling.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "\n",
    "                                                current_data[key] = val_text\n",
    "                                                if verbose:\n",
    "                                                    print(len(current_data.values()), key, val)\n",
    "\n",
    "                                                sup_element = sibling.find('sup')\n",
    "\n",
    "                                                if sup_element:\n",
    "                                                    raw_jcr_data = sup_element.get_text()\n",
    "                                                    # print('sup_element:',sup_element)\n",
    "                                                    img_element = sup_element.find('img')\n",
    "                                                    # print('img_element:',img_element)                                                    \n",
    "                                                    if img_element:\n",
    "                                                        original_title = img_element.get('original-title')\n",
    "                                                        if original_title:\n",
    "                                                            info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                            if info_list != 'NULL':\n",
    "                                                                issn = format_string(img_element.get('data-issn'))\n",
    "                                                                if verbose:\n",
    "                                                                    print(f'impact-factor: {info_list[1].split(\": \")[1]}')\n",
    "                                                                info_dict = {\n",
    "                                                                    'data-issn': issn,\n",
    "                                                                    'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                    'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')',''),\n",
    "                                                                    'journal': info_list[0],\n",
    "                                                                }\n",
    "                                                        else:\n",
    "                                                            if verbose:\n",
    "                                                                print('Entrou no primeiro Else')\n",
    "                                                            issn = format_string(img_element.get('data-issn'))\n",
    "                                                            info_dict = {\n",
    "                                                                'data-issn': issn,\n",
    "                                                                'impact-factor': 'NULL',\n",
    "                                                                'jcr-year': 'NULL',\n",
    "                                                                'journal': 'NULL',\n",
    "                                                            }\n",
    "                                                else:\n",
    "                                                    if verbose:\n",
    "                                                                print('Entrou no segundo Else')\n",
    "                                                    info_dict = {\n",
    "                                                        'data-issn': 'NULL',\n",
    "                                                        'impact-factor': 'NULL',\n",
    "                                                        'jcr-year': 'NULL',\n",
    "                                                        'journal': 'NULL',\n",
    "                                                    }                                                                \n",
    "                                                    \n",
    "                                                vals_jcr.append(info_dict)\n",
    "                                                if verbose:\n",
    "                                                    print(f'         {info_dict}')\n",
    "\n",
    "                                            if 'JCR' not in data_dict:\n",
    "                                                data_dict['JCR'] = []\n",
    "                                            \n",
    "                                            if verbose:\n",
    "                                                print(len(vals_jcr))\n",
    "                                            data_dict['JCR'] = vals_jcr\n",
    "\n",
    "                                        elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                            next_sibling = sibling.find_next_sibling()\n",
    "                                            if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                                sibling = None\n",
    "                                            else:\n",
    "                                                if current_data:\n",
    "                                                    converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                                    data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = converted_data\n",
    "\n",
    "                                        if sibling:\n",
    "                                            sibling = sibling.find_next_sibling()\n",
    "                    else:\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "\n",
    "                            if 'cita-artigos' in classes:  # Subsection start\n",
    "                                subsection_name = sibling.find('b').get_text().strip()\n",
    "                                current_subsection = subsection_name\n",
    "                                if verbose:\n",
    "                                    print(f'    Subseção: {subsection_name}')\n",
    "                                data_dict[titulo][section_name][current_subsection] = {}\n",
    "                                current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "\n",
    "                            elif 'layout-cell-1' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_subsection:\n",
    "                                        data_dict[titulo][section_name][current_subsection] = Neo4jPersister.convert_to_primitives(current_data)  # Armazenamos os dados da subseção atual\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "    \n",
    "    # Verifique se os dados dos tooltips estão presentes no objeto soup\n",
    "    if 'tooltips' in soup.attrs:\n",
    "        tooltips_data = soup.attrs['tooltips']\n",
    "        agg = []\n",
    "        \n",
    "        for tooltip in tooltips_data:\n",
    "            agg_data = {}\n",
    "            \n",
    "            # Extração do ano JCR a partir do \"original_title\"\n",
    "            if tooltip.get(\"original_title\"):\n",
    "                jcr_year = tooltip[\"original_title\"].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')','')\n",
    "                agg_data[\"jcr-ano\"] = jcr_year\n",
    "            \n",
    "            # Adicionar todas as chaves e valores do tooltip ao dicionário agg_data\n",
    "            for key, value in tooltip.items():\n",
    "                agg_data[key] = value\n",
    "            \n",
    "            agg.append(agg_data)\n",
    "        \n",
    "        data_dict['JCR2'] = agg\n",
    "    else:\n",
    "        print('Não foram achados os dados de tooltip')\n",
    "        print(soup.attrs)  \n",
    "           \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit3_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "    # Títulos da seção 'Eventos'\n",
    "    tit3 = ['Eventos']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        try:\n",
    "            titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        except:\n",
    "            titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        # Verifique se o título está na lista 'tit3'\n",
    "        if titulo in tit3:\n",
    "            if verbose:\n",
    "                print(f'Título: {titulo}')\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = []\n",
    "                    if verbose:\n",
    "                        print(section_name)\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    while sibling:\n",
    "                        classes = sibling.get('class', [])\n",
    "\n",
    "                        if 'layout-cell-1' in classes:  # Data key\n",
    "                            key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                            if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                current_data[key] = val\n",
    "                                if verbose:\n",
    "                                    print(len(current_data.values()), key, val)\n",
    "\n",
    "                        elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                            next_sibling = sibling.find_next_sibling()\n",
    "                            if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                sibling = None\n",
    "                            else:\n",
    "                                if current_data:\n",
    "                                    converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                    data_dict[titulo][section_name] = converted_data\n",
    "\n",
    "                        if sibling:\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função montar data_dict com todos dados extraídos do DOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_dicts(soup):\n",
    "    \"\"\"\n",
    "    Aggregates data from various dictionary sources into a consolidated nested dictionary, \n",
    "    ensuring that all nested lists within the dictionaries are transformed into nested dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    - soup: BeautifulSoup object, representing the parsed HTML content.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: An aggregated dictionary containing the consolidated data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def convert_list_to_dict(lst):\n",
    "        \"\"\"\n",
    "        Converts a list into a dictionary with indices as keys.\n",
    "        \n",
    "        Parameters:\n",
    "        - lst: list, input list to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: Transformed dictionary.\n",
    "        \"\"\"\n",
    "        return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "    def merge_dict(d1, d2):\n",
    "        \"\"\"\n",
    "        Recursively merges two dictionaries, transforming nested lists into dictionaries.\n",
    "        \n",
    "        Parameters:\n",
    "        - d1: dict, the primary dictionary into which data is merged.\n",
    "        - d2: dict or list, the secondary dictionary or list from which data is sourced.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        # If d2 is a list, convert it to a dictionary first\n",
    "        if isinstance(d2, list):\n",
    "            d2 = convert_list_to_dict(d2)\n",
    "        \n",
    "        for key, value in d2.items():\n",
    "            if isinstance(value, list):\n",
    "                d2[key] = convert_list_to_dict(value)\n",
    "            if key in d1 and isinstance(d1[key], dict) and isinstance(value, dict):\n",
    "                merge_dict(d1[key], value)\n",
    "            else:\n",
    "                d1[key] = value\n",
    "\n",
    "\n",
    "    # Extract necessary information from soup\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "    name = info_list[0]\n",
    "\n",
    "    # Initialization of the aggregated_data dictionary\n",
    "    aggregated_data = {\"labels\": \"Person\", \"name\": name, \"InfPes\": info_list, \"Resumo\": [elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()]}\n",
    "\n",
    "    # Data extraction and merging\n",
    "    for data_extraction_func in [extract_tit1_soup, extract_tit2_soup, extract_tit3_soup]:\n",
    "        extracted_sections = data_extraction_func(soup, verbose=False)\n",
    "        for title, data in extracted_sections.items():\n",
    "            if title not in aggregated_data:\n",
    "                aggregated_data[title] = {}\n",
    "            merge_dict(aggregated_data[title], data)\n",
    "\n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_dict(lst):\n",
    "    \"\"\"\n",
    "    Converts a list into a dictionary with indices as keys.\n",
    "    \n",
    "    Parameters:\n",
    "    - lst: list, input list to be transformed.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Transformed dictionary.\n",
    "    \"\"\"\n",
    "    return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "def dict_jcr_list(data_dict):\n",
    "    # Extract JCR entries from the data dictionary\n",
    "    jcr_entries = data_dict.get('JCR', {})\n",
    "\n",
    "    # Initialize an empty list to store JCR properties\n",
    "    jcr_properties_list = []\n",
    "\n",
    "    # If jcr_entries is a dictionary\n",
    "    if isinstance(jcr_entries, dict):\n",
    "        for key, value in jcr_entries.items():\n",
    "            jcr_properties_list.append(value)\n",
    "\n",
    "    # If jcr_entries is a list\n",
    "    elif isinstance(jcr_entries, list):\n",
    "        jcr_properties_list.extend(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is a string\n",
    "    elif isinstance(jcr_entries, str):\n",
    "        jcr_properties_list.append(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is of any other unexpected type\n",
    "    else:\n",
    "        print(f\"Unexpected data type {type(jcr_entries)} for JCR entries. Expected dictionary, list, or string.\")\n",
    "\n",
    "    return jcr_properties_list\n",
    "\n",
    "\n",
    "def dict_doi_list(data_dict):\n",
    "    # Extract JCR entries from the data dictionary\n",
    "    jcr_entries = data_dict.get('JCR2', {})\n",
    "\n",
    "    # Initialize an empty list to store JCR properties\n",
    "    jcr_properties_list = []\n",
    "\n",
    "    # If jcr_entries is a dictionary\n",
    "    if isinstance(jcr_entries, dict):\n",
    "        for key, value in jcr_entries.items():\n",
    "            jcr_properties_list.append(value)\n",
    "\n",
    "    # If jcr_entries is a list\n",
    "    elif isinstance(jcr_entries, list):\n",
    "        jcr_properties_list.extend(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is a string\n",
    "    elif isinstance(jcr_entries, str):\n",
    "        jcr_properties_list.append(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is of any other unexpected type\n",
    "    else:\n",
    "        print(f\"Unexpected data type {type(jcr_entries)} for JCR entries. Expected dictionary, list, or string.\")\n",
    "\n",
    "    return jcr_properties_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Função para escolher a string com o maior comprimento, considerando NaNs\n",
    "def max_len_string(x):\n",
    "    valid_strings = x.dropna()\n",
    "    if not valid_strings.empty:\n",
    "        return valid_strings.loc[valid_strings.str.len().idxmax()]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "# Função para escolher a string com o menor comprimento, considerando NaNs\n",
    "def min_len_string(x):\n",
    "    valid_strings = x.dropna()\n",
    "    if not valid_strings.empty:\n",
    "        return valid_strings.loc[valid_strings.str.len().idxmin()]\n",
    "    else:\n",
    "        return np.nan  \n",
    "\n",
    "def break_text_words_into_lines(text, max_lines=3):\n",
    "    if not isinstance(text, str):  # Handle non-string values\n",
    "        return text\n",
    "\n",
    "    words = text.split()\n",
    "    lines = ['']\n",
    "    line_index = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if len(lines[line_index]) + len(word) + 1 > len(text) // max_lines and line_index < max_lines - 1:\n",
    "            lines.append('')\n",
    "            line_index += 1\n",
    "        lines[line_index] += (word + ' ')\n",
    "    \n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "def break_text(text):\n",
    "    if not isinstance(text, str):  # Handle non-string values\n",
    "        return text\n",
    "\n",
    "    # Split at the ' (' character\n",
    "    parts = text.split(' (', 1)\n",
    "    \n",
    "    # If the text was split, then join the parts with a newline in between\n",
    "    if len(parts) > 1:\n",
    "        return parts[0] + \"\\n(\" + parts[1]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def compute_color(value, min_val, max_val):\n",
    "    if pd.isna(value):\n",
    "        return '#808080'  # return a shade of grey for NaN values\n",
    "\n",
    "    position = (value - min_val) / (max_val - min_val)\n",
    "    colors = np.array([[1, 1, 0], [0, 0.5, 0]])  # Yellow to Dark Green\n",
    "    color = colors[0] + position * (colors[1] - colors[0])\n",
    "    return mcolors.to_hex(color)\n",
    "\n",
    "def plot_vertbar(jcr_properties_list):\n",
    "    data = pd.DataFrame(jcr_properties_list)\n",
    "    # Trocar 'Ieee' por 'IEEE' nos nomes de periódicos\n",
    "    data['journal'] = data['journal'].str.replace('Ieee', 'IEEE', case=False)\n",
    "\n",
    "    # Trocar os valores do NULL para tratamento adequado na plotagem\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "\n",
    "    grouped = data.groupby('issn', dropna=False).agg(\n",
    "        count=('issn', 'size'),\n",
    "        impact_factor_max=('impact-factor', 'max'),\n",
    "        journal=('journal', max_len_string),\n",
    "        jcr_year = ('jcr-ano', min_len_string)\n",
    "    ).reset_index()\n",
    "\n",
    "    grouped.rename(columns={\"count\": \"issn_count\", \"impact_factor_max\": \"impact-factor\"}, inplace=True)\n",
    "\n",
    "    count_nan_issn  = data['issn'].isna().sum()\n",
    "    null_row        = pd.DataFrame({'issn': ['MISSING'], 'issn_count': [count_nan_issn], 'impact-factor': [np.nan], 'journal': [np.nan]})\n",
    "    \n",
    "    data_all_counts = pd.concat([grouped, null_row], ignore_index=True)\n",
    "    data_all_counts = data_all_counts.sort_values(by=['impact-factor'], ascending=True)\n",
    "\n",
    "    min_impact = data_all_counts['impact-factor'].min()\n",
    "    max_impact = data_all_counts['impact-factor'].max()\n",
    "    data_all_counts['color'] = data_all_counts['impact-factor'].apply(lambda x: compute_color(x, min_impact, max_impact))\n",
    "\n",
    "    fig = px.bar(data_all_counts, x='issn', y='issn_count', color='color',\n",
    "                 title='Frequência de publicação acumulada no período completo versus fator de impacto de cada periódico',\n",
    "                 hover_data=['impact-factor', 'issn_count', 'jcr_year'],\n",
    "                 text=data_all_counts['impact-factor'].apply(lambda x: round(x, 2) if not pd.isna(x) else x),\n",
    "                 color_discrete_map='identity'\n",
    "                )\n",
    "    fig.update_xaxes(tickangle=315,\n",
    "                     tickvals=data_all_counts['issn'].tolist(), \n",
    "                     ticktext=data_all_counts['journal'].apply(break_text).tolist(), \n",
    "                    #  categoryorder='total ascending',\n",
    "                     )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        yaxis=dict(autorange=True),\n",
    "        yaxis_title=\"Frequência de publicação no periódico\",\n",
    "        xaxis_title=\"Periódicos\",\n",
    "        xaxis_title_standoff=50,\n",
    "        height=800,\n",
    "        margin=dict(l=50, r=50, b=50, t=75),\n",
    "        xaxis=dict(\n",
    "            automargin=True,\n",
    "        )    \n",
    "    )\n",
    "    \n",
    "    fig.update_traces(texttemplate='%{text}', textposition='outside', textfont_size=10)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "\n",
    "    data = pd.DataFrame(jcr_properties_list)\n",
    "    # Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "\n",
    "    # Agrupar pelo 'issn', contar ocorrências, maior valor de 'impact-factor' e pegar o 'journal'\n",
    "    grouped = data.groupby('issn', dropna=False).agg(\n",
    "        count=('issn', 'size'),\n",
    "        impact_factor_max=('impact-factor', 'max'),\n",
    "        journal=('journal', max_len_string)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Renomear as colunas para corresponder às suas descrições\n",
    "    grouped.rename(columns={\"count\": \"issn_count\", \"impact_factor_max\": \"impact-factor\"}, inplace=True)\n",
    "    data_all_counts = grouped.sort_values(by=['impact-factor'], ascending=False)\n",
    "\n",
    "    # Criar o gráfico de dispersão com Plotly Express\n",
    "    fig = px.scatter(data_all_counts, x='issn', y='impact-factor', color='issn_count',\n",
    "                     size='issn_count',\n",
    "                     title='Gráfico de Dispersão de Fator de Impacto versus ISSN no período completo',\n",
    "                     labels={'issn': 'ISSN', 'impact-factor': 'Fator de Impacto', 'issn_count': 'Quantidade'},\n",
    "                     hover_data=['journal'],\n",
    "                     color_continuous_scale=\"YlOrRd\")\n",
    "\n",
    "    # Personalizar layout\n",
    "    fig.update_traces(marker=dict(size=data_all_counts['issn_count'] * 2))\n",
    "    fig.update_xaxes(tickangle=315, tickvals=data_all_counts['issn'].tolist(), categoryorder='total ascending')\n",
    "    fig.update_traces(marker=dict(opacity=1))\n",
    "    max_impact = int(data_all_counts['impact-factor'].max()) + 1  # máximo impact-factor arredondado para cima\n",
    "    fig.update_layout(yaxis=dict(autorange=True, tickvals=list(range(max_impact))))\n",
    "\n",
    "    # Exibir o gráfico\n",
    "    fig.show()\n",
    "\n",
    "    return data_all_counts\n",
    "\n",
    "def dataframe_to_list_of_dicts(df):\n",
    "    records = df.to_dict(orient='records')\n",
    "    formatted_records = []\n",
    "    \n",
    "    for record in records:\n",
    "        formatted_record = {}\n",
    "        for key, value in record.items():\n",
    "            if pd.isna(value):\n",
    "                formatted_record[key] = 'NULL'\n",
    "            else:\n",
    "                formatted_record[key] = str(value)\n",
    "        formatted_records.append(formatted_record)\n",
    "    \n",
    "    return formatted_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função integração com CrossRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install crossrefapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from crossref.restful import Works\n",
    "\n",
    "def get_issn(doi):\n",
    "    works = Works()\n",
    "    all_data = works.doi(doi.replace('http://dx.doi.org/','').strip())\n",
    "    issn=all_data['ISSN'][0]\n",
    "    journal=all_data['container-title'][0]\n",
    "    return issn, journal\n",
    "\n",
    "def crossref_complement(data):\n",
    "    ## Substituir NULL por None\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "    data = data.sort_values(by=['impact-factor','data-issn','doi'], ascending=False)\n",
    "\n",
    "    issn_crossref=[]\n",
    "    journal_crossref=[]\n",
    "    for i in data['doi']:\n",
    "        try:\n",
    "            issn, journal = get_issn(i)\n",
    "            issn_crossref.append(issn)\n",
    "            journal_crossref.append(journal.replace('amp;',''))\n",
    "        except:\n",
    "            issn_crossref.append(np.nan)\n",
    "            journal_crossref.append(np.nan)\n",
    "\n",
    "    data['issn'] = issn_crossref\n",
    "    data['journal'] = journal_crossref\n",
    "\n",
    "    count_jcrissn = data['data-issn'].notna().sum()\n",
    "    count_jci = data['impact-factor'].notna().sum()\n",
    "    count_doi = data['doi'].notna().sum()\n",
    "    print(f'{count_jcrissn} ISSN recuperados, {len(data.index)-count_jcrissn} ISSN ausentes')\n",
    "    print(f'{count_doi}  DOI recuperados, {len(data.index)-count_doi}  DOI ausentes')\n",
    "    print(f'{count_jci}  JCI recuperados, {len(data.index)-count_jci}  JCI ausentes')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções buscar em CSVs dados de ISSN do JCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir conteúdo em linhas, pegar linha de rótulos até a penúltima linha (ignorar informações de direitos autorais)\n",
    "def org_lines(file_path):\n",
    "    from io import StringIO\n",
    "\n",
    "    # Acessar para ler rótulos de colunas que estão desencontrados dos dados\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_content = file.read()\n",
    "\n",
    "    # Guardar rótulos de colunas\n",
    "    lines = raw_content.splitlines()[1:3]\n",
    "    csv_content = \"\\n\".join(lines)\n",
    "    columns_labels = pd.read_csv(StringIO(csv_content))\n",
    "    new_columns = list(columns_labels.columns)\n",
    "\n",
    "    # Ler novamente somente os dados e acoplar as colunas adequadamente\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_content = file.read()\n",
    "\n",
    "    lines = raw_content.splitlines()[3:-2]\n",
    "    csv_content = \"\\n\".join(lines)\n",
    "    data_local = pd.read_csv(StringIO(csv_content), header=None)\n",
    "    data_local.drop(data_local.columns[-1], axis=1, inplace=True)\n",
    "    data_local.columns=new_columns\n",
    "    \n",
    "    return data_local\n",
    "\n",
    "def load_and_concatenate(file_paths):\n",
    "    \"\"\"\n",
    "    Given a list of file paths, load each file, organize it using org_lines function,\n",
    "    and concatenate them into a single dataframe.\n",
    "    \"\"\"\n",
    "    return pd.concat([org_lines(file_path) for file_path in file_paths], ignore_index=True)\n",
    "\n",
    "def get_most_recent_data(df_aggregated):\n",
    "    \"\"\"\n",
    "    Given an aggregated dataframe, this function returns the most recent (max JIF) \n",
    "    entry for each unique ISSN or eISSN.\n",
    "    \"\"\"\n",
    "    # Determine the most recent year in the dataframe\n",
    "    most_recent_year = max([int(col.split(' ')[0]) for col in df_aggregated.columns if ' JIF' in col])\n",
    "\n",
    "    # Combining the data based on ISSN and eISSN, taking the max of JIF\n",
    "    df_grouped_by_issn = df_aggregated.groupby('ISSN', as_index=False).apply(lambda x: x.nlargest(1, f'{most_recent_year} JIF')).reset_index(drop=True)\n",
    "    df_grouped_by_eissn = df_aggregated.groupby('eISSN', as_index=False).apply(lambda x: x.nlargest(1, f'{most_recent_year} JIF')).reset_index(drop=True)\n",
    "    \n",
    "    # Combining the two dataframes and dropping any duplicates\n",
    "    df_most_recent = pd.concat([df_grouped_by_issn, df_grouped_by_eissn]).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_most_recent, most_recent_year\n",
    "\n",
    "def standardize_issn_format(issn):\n",
    "    \"\"\"Convert a given ISSN to the format '0000-0000'.\"\"\"\n",
    "    if isinstance(issn, str) and len(issn) == 8 and \"-\" not in issn:\n",
    "        return issn[:4] + '-' + issn[4:]\n",
    "    return issn\n",
    "\n",
    "def format_issn_list(issn_list):\n",
    "    return [f\"{issn[:4]}-{issn[4:]}\" if issn != np.nan else issn for issn in issn_list]\n",
    "\n",
    "def montar_lista_issn(data):\n",
    "    issn_list = data['data-issn'].to_list()\n",
    "    str_list=''\n",
    "    for n,i in enumerate(issn_list):\n",
    "        issn = standardize_issn_format(str(i))\n",
    "        if n==0:\n",
    "            str_list = issn\n",
    "        elif issn == 'NULL':\n",
    "            pass\n",
    "        elif issn == 'nan':\n",
    "            pass        \n",
    "        else:\n",
    "            str_list = str_list+(', ')+issn\n",
    "    \n",
    "    print(len(str_list.split(', ')),'ISSN extraídos do currículo')\n",
    "    return str_list\n",
    "\n",
    "def populate_missing_data(df_start, df_most_recent, most_recent_year):\n",
    "    \"\"\"\n",
    "    Given a starting dataframe and a dataframe with the most recent data for each ISSN or eISSN,\n",
    "    this function populates the starting dataframe with missing values.\n",
    "    \"\"\"\n",
    "    for index, row in df_start.iterrows():\n",
    "        formatted_issn = standardize_issn_format(row['data-issn'])\n",
    "        if pd.isna(row['impact-factor']) or pd.isna(row['jcr-ano']):\n",
    "            # Search in df_most_recent using ISSN or eISSN\n",
    "            matching_row = df_most_recent[(df_most_recent['ISSN'] == formatted_issn) | (df_most_recent['eISSN'] == formatted_issn)]\n",
    "            \n",
    "            if not matching_row.empty:\n",
    "                most_recent_entry = matching_row.iloc[0]\n",
    "                df_start.at[index, 'impact-factor'] = most_recent_entry[f'{most_recent_year} JIF']\n",
    "                df_start.at[index, 'jcr-ano'] = f'(JCR {most_recent_year})'\n",
    "                df_start.at[index, 'journal'] = most_recent_entry['Journal name']\n",
    "\n",
    "    return df_start\n",
    "\n",
    "def fill_na_from_df_aggregated(sorted_df, df_aggregated):\n",
    "    df_aggregated['ISSN'] = df_aggregated['ISSN'].apply(standardize_issn_format)\n",
    "    dt_trab = sorted_df['data-issn'].copy()\n",
    "    sorted_df['issn'] = dt_trab.apply(standardize_issn_format)\n",
    "    \n",
    "    df_nan_eissn = df_aggregated[df_aggregated['eISSN'].isna()]\n",
    "    \n",
    "    for idx, row in df_nan_eissn.iterrows():\n",
    "        issn_value = row['ISSN']\n",
    "        mask = (sorted_df['issn'] == issn_value) & sorted_df['impact-factor'].isna()\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            sorted_df.loc[mask, 'impact-factor'] = row['2022 JIF']\n",
    "            sorted_df.loc[mask, 'journal'] = row['Journal name']\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "def update_sorted_df_with_jcr(sorted_df, df_aggregated):\n",
    "    years = list(range(2018, 2023))\n",
    "    \n",
    "    # Standardize ISSN formats\n",
    "    df_aggregated['ISSN'] = df_aggregated['ISSN'].apply(standardize_issn_format)\n",
    "    df_aggregated['eISSN'] = df_aggregated['eISSN'].apply(standardize_issn_format)\n",
    "    sorted_df['data-issn'] = sorted_df['data-issn'].apply(standardize_issn_format)\n",
    "    \n",
    "    for year in reversed(years):  # Loop from most recent to oldest year\n",
    "        jif_column = f\"{year} JIF\"\n",
    "        \n",
    "        # Merge based on ISSN and eISSN\n",
    "        merged_issn = pd.merge(sorted_df, df_aggregated, left_on='data-issn', right_on='ISSN', how='left')\n",
    "        merged_eissn = pd.merge(sorted_df, df_aggregated, left_on='data-issn', right_on='eISSN', how='left')\n",
    "        \n",
    "        mask_nan = merged_issn[jif_column].notna()\n",
    "        merged = pd.concat([merged_issn[~mask_nan], merged_eissn], ignore_index=True)\n",
    "        \n",
    "        update_mask = sorted_df['impact-factor'].isna()\n",
    "        update_indices = sorted_df[update_mask].index\n",
    "        \n",
    "        for idx in update_indices:\n",
    "            sorted_df.at[idx, 'impact-factor'] = merged.at[idx, jif_column]\n",
    "            jif_value = merged.at[idx, jif_column]\n",
    "            sorted_df.at[idx, 'jcr-year'] = f\"JCR {year}\" if not pd.isna(jif_value) else np.nan\n",
    "    \n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_jcr(data, df_aggregated):\n",
    "    \"\"\"\n",
    "    Atualiza os valores NaN em 'data' usando as informações de 'df_aggregated'.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame com as colunas [data-issn, original-title, impact-factor, doi, issn].\n",
    "        df_aggregated (pd.DataFrame): DataFrame com informações sobre JIFs de vários anos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame 'data' atualizado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lista de anos em ordem decrescente para procurar os JIFs mais recentes\n",
    "    years = sorted([int(col.split()[0]) for col in df_aggregated.columns if re.match(r\"\\d{4} JIF\", col)], reverse=True)\n",
    "    print(years)\n",
    "\n",
    "    for year in years:\n",
    "        column_name = f\"{year} JIF\"\n",
    "        # Usando o método 'map' para atualizar os NaNs\n",
    "        data.loc[data['impact-factor'].isna(), 'impact-factor'] = data['issn'].map(df_aggregated.set_index('ISSN')[column_name])\n",
    "        \n",
    "        # # Se não houver mais NaNs na coluna 'impact-factor', podemos interromper o loop\n",
    "        # if data['impact-factor'].notna().all():\n",
    "        #     break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 3: Extrair lista de Currículos para dicionários</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os pacotes picroscopy e pyusid são bibliotecas especializadas destinadas principalmente à comunidade científica, especificamente àquelas que trabalham com dados científicos complexos, muitas vezes na forma de grandes conjuntos de dados multidimensionais.\n",
    "\n",
    "**Picroscopy**\n",
    "A picroscopia é fundamentalmente projetada para a análise de uma variedade de dados microscópicos. Isso pode incluir, mas não está limitado a, dados de Microscopia de Varredura de Túnel (STM), Microscopia de Força Atômica (AFM) e técnicas de microscopia eletrônica. O pacote oferece uma variedade de algoritmos e utilitários para processar e analisar dados de microscopia de maneira eficiente e padronizada.\n",
    "\n",
    "**PyUSID**\n",
    "PyUSID (Dados Espectroscópicos e de Imagem Universais baseados em Python) serve como um utilitário de uso mais geral para simplificar o processo de trabalho com grandes conjuntos de dados complexos. Oferece um formato de dados unificado para incentivar a padronização, juntamente com utilitários para visualização, manipulação e análise. O pacote visa oferecer uma abordagem coerente às tarefas de ciência de dados no domínio da ciência dos materiais, mas é aplicável a uma variedade de campos.\n",
    "\n",
    "**Aplicabilidade a este Projeto**\n",
    "Este projeto ainda não envolve análise de dados em microscopia ou espectropia. Portanto, é provável que não exijamos picroscopia e pyusid. Dado que esses pacotes foram mencionados como não causando conflitos durante a instalação do h5py, e assumindo que não precisamos deles especificamente por enquanto, podemos ignorá-los ou desinstalá-los para simplificar seu ambiente de desenvolvimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar Lista de Pessoal para extrair currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 nomes de colaboradores no total, todos vínculos e status\n",
      " 10 tipos de vínculos\n",
      "Tipos de vínculos ['SERVIDOR', 'COORENAÇÃO GERAL', 'TERCEIRIZADO', 'BOLSISTA', 'ESTÁGIO PEC', 'UNADIG', 'NORMATEL', 'SERVIDOR-CEDIDA PARA CORREGEDORIA ', 'SERVIDOR-CEDIDA PARA FIOCRUZ PE', 'SERVIDOR-CEDIDO PARA AUDITORIA INTERNA']\n",
      "  Tipos de status ['ATIVO', 'AFASTADO', 'EXONERADO', 'CONTRATO ENCERRADO', 'APOSENTADA', 'REMOÇÃO']\n",
      "57 nomes para extrair currículos\n",
      " 1. Alice Paula Di Sabatino Guimaraes\n",
      " 2. Ana Claudia De Araújo Teixeira\n",
      " 3. Ana Camila Oliveira Alves\n",
      " 4. Angela Christina De Moraes Ostritz\n",
      " 5. Adriana Costa Bacelo\n",
      " 6. Anna Carolina Machado Marinho\n",
      " 7. Antonio Marcos Aires Barbosa\n",
      " 8. Anya Pimentel Gomes Fernandes Vieira Meyer\n",
      " 9. Bruno Bezerra Carvalho\n",
      "10. Carla Freire Celedônio Fernandes\n",
      "11. Carlos Jose Araujo Pinheiro\n",
      "12. Claudia Stutz Zubieta\n",
      "13. Charles Cerqueira De Abreu\n",
      "14. Clarice Gomes E Souza Dabés\n",
      "15. Clarissa Romero Teixeira\n",
      "16. Dayane Alves Costa\n",
      "17. Donat Alexander De Chapeaurouge\n",
      "18. Eduardo Ruback Dos Santos\n",
      "19. Ezequiel Valentim De Melo\n",
      "20. Fabio Miyajima\n",
      "21. Fernando Braga Stehling Dias\n",
      "22. Fernando Ferreira Carneiro\n",
      "23. Galba Freire Moita\n",
      "24. Giovanny Augusto Camacho Antevere Mazzarotto\n",
      "25. Gilvan Pessoa Furtado\n",
      "26. Ivana Cristina De Holanda Cunha Barrêto\n",
      "27. Ivanildo Lopes Farias\n",
      "28. Jaime Ribeiro Filho\n",
      "29. João Baptista Estabile Neto\n",
      "30. João Hermínio Martins Da Silva\n",
      "31. José Luis Passos Cordeiro\n",
      "32. Kamila Matos Albuquerque \n",
      "33. Luciana Coelho Serafim\n",
      "34. Luciana Pereira Lindenmeyer\n",
      "35. Luciana Silvério Alleluia Higino Da Silva\n",
      "36. Luciano Pinto Zorzanelli\n",
      "37. Luis Fernando Pessoa De Andrade\n",
      "38. Luiz Odorico Monteiro De Andrade\n",
      "39. Marcela Helena Gambim Fonseca\n",
      "40. Marcelo Jorge Lopes Coutinho\n",
      "41. Marcos Roberto Lourenzoni\n",
      "42. Marcio Flavio Moura De Araujo \n",
      "43. Margareth Borges Coutinho Gallo\n",
      "44. Marlos De Medeiros Chaves\n",
      "45. Maximiliano Loiola Ponte De Souza\n",
      "46. Nilton Luiz Costa Machado\n",
      "47. Patricia Maria Ferreira da Silva\n",
      "48. Raphael Trevizani\n",
      "49. Regis Bernardo Brandim Gomes\n",
      "50. Renato Caldeira De Souza\n",
      "51. Roberto Nicolete\n",
      "52. Roberto Wagner Junior Freire De Freitas\n",
      "53. Rodrigo Carvalho Nogueira \n",
      "54. Sergio Dos Santos Reis\n",
      "55. Sharmenia De Araujo Soares Nuto\n",
      "56. Vanira Matos Pessoa\n",
      "57. Venúcia Bruna Magalhães Pereira\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler dados do arquivo Excel do Setor de Recursos Humanos\n",
    "pathdata = './../data/'\n",
    "file_persons = 'fioce_colaboradores-2023.xls'\n",
    "\n",
    "# Ler apenas os cabeçalhos do arquivo Excel\n",
    "headers = pd.read_excel(pathdata+file_persons, skiprows=3, header=0, nrows=0).columns\n",
    "# headers\n",
    "\n",
    "# Usar função para indicar quais colunas devem ser eliminadas na leitura\n",
    "def cols_to_keep(col_name):\n",
    "    return col_name not in ['QUANT','Unnamed: 3','Unnamed: 6','Unnamed: 9','ADICIONAL OCUPACIONAL',\n",
    "                            'EMPRESA/BOLSA/PROGRAMA','GESTOR','ADI','POSSE NA FIOCRUZ',\n",
    "                            'VIGÊNCIA BOLSA/ENCERRAMENTO DO CONTRATO','Unnamed: 17',\n",
    "                            'EMAIL INSTITUCIONAL','EMAIL PESSOAL','GENERO','DATA NASCIMENTO',\n",
    "                            'Unnamed: 22','FORMAÇÃO','ENDEREÇO RESIDENCIAL']\n",
    "\n",
    "# Filtrar cabeçalhos com base na função\n",
    "selected_columns = [col for col in headers if cols_to_keep(col)]\n",
    "\n",
    "# Ler dados do arquivo Excel do Setor de Recursos Humanos\n",
    "fioce_pessoal = pd.read_excel(pathdata+file_persons, skiprows=3, header=0, usecols=selected_columns)\n",
    "print(f'{len(fioce_pessoal.index)} nomes de colaboradores no total, todos vínculos e status')\n",
    "print(f'{len(fioce_pessoal[\"VÍNCULO\"].unique()):3} tipos de vínculos')\n",
    "print('Tipos de vínculos',list(fioce_pessoal['VÍNCULO'].unique()))\n",
    "print('  Tipos de status',list(fioce_pessoal['STATUS'].unique()))\n",
    "filtro1 = fioce_pessoal.VÍNCULO == 'SERVIDOR'\n",
    "filtro2 = fioce_pessoal['STATUS'].isin(['ATIVO', 'AFASTADO'])\n",
    "lista_nomes = fioce_pessoal[(filtro1) & (filtro2)]['NOME'].tolist()\n",
    "print(f'{len(lista_nomes)} nomes para extrair currículos')\n",
    "for i,nome in enumerate(lista_nomes):\n",
    "    print(f'{i+1:2}. {nome}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes: Extração e Conversão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "# !pip install py2neo --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "# !pip install h5py\n",
    "\n",
    "import time\n",
    "import json\n",
    "import h5py\n",
    "import logging\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from typing import List, Optional, Dict, Union\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from pyjarowinkler.distance import get_jaro_distance\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from neo4j import GraphDatabase\n",
    "from flask import render_template_string\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common import exceptions\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, \n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    "    TimeoutException,\n",
    "    WebDriverException\n",
    ")\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "logging.basicConfig(filename='lattes_scraper.log', level=logging.INFO)\n",
    "delay = 10\n",
    "\n",
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "    t=end-start\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57\n",
    "\n",
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "    except Exception as e:\n",
    "        print('  ERRO!! Ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "    caminho = drive+':/'+usuario+pastaraiz+'/'\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    import os\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    print()\n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout\n",
    "\n",
    "def try_folders(drives, pastas, pastasraiz):\n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    tested_path = drive + i + j\n",
    "                    if os.path.isfile(tested_path + '/chromedriver/chromedriver.exe'):\n",
    "                        logging.info(f\"Listing files in: {tested_path}\")\n",
    "                        logging.info(os.listdir(tested_path))\n",
    "                        return tested_path + '/'\n",
    "                except:\n",
    "                    logging.error('Could not locate a working folder.')\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictToHDF5:\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def create_dataset(self, filename, directory=None):\n",
    "        with h5py.File(f\"{directory or ''}{filename}\", \"w\") as f:\n",
    "            null_group = f.create_group(\"0000\")\n",
    "            for person_dict in self.data_list:  # Corrigido de self.data para self.data_list\n",
    "                if 'curriculo' not in person_dict:\n",
    "                    name = person_dict.get('name', 'Unknown')  # Uso de get() para evitar KeyError\n",
    "                    null_group.attrs[name] = \"No curriculum\"  # Adicionando como atributos ao grupo '0000'\n",
    "                    continue\n",
    "                person_group = f.create_group(person_dict['id'])\n",
    "                for key, value in person_dict.items():\n",
    "                    if value is None:\n",
    "                        continue\n",
    "                    if isinstance(value, list):\n",
    "                        if not value:  # Skip empty lists\n",
    "                            continue\n",
    "\n",
    "                        dtype = type(value[0])\n",
    "                        if dtype == str:\n",
    "                            dt = h5py.string_dtype(encoding='utf-8')\n",
    "                            person_group.create_dataset(key, (len(value),), dtype=dt, data=value)\n",
    "                        else:\n",
    "                            value = np.array(value, dtype=dtype)\n",
    "                            person_group.create_dataset(key, data=value)\n",
    "                    elif isinstance(value, str):\n",
    "                        dt = h5py.string_dtype(encoding='utf-8')\n",
    "                        person_group.create_dataset(key, (1,), dtype=dt, data=value)\n",
    "                    elif isinstance(value, dict) or isinstance(value, list):\n",
    "                        json_str = json.dumps(value)\n",
    "                        dt = h5py.string_dtype(encoding='utf-8')\n",
    "                        person_group.create_dataset(key, (1,), dtype=dt, data=json_str)\n",
    "                    else:\n",
    "                        person_group.create_dataset(key, data=value)\n",
    "\n",
    "    def extract_id_lattes(self, data_dict):\n",
    "        inf_pes = data_dict.get('InfPes', [])\n",
    "        for item in inf_pes:\n",
    "            if 'ID Lattes:' in item:\n",
    "                return item.split('ID Lattes: ')[-1]\n",
    "        return \"0000\" + str(data_dict.get(\"name\"))\n",
    "\n",
    "    # Para visualização\n",
    "    @staticmethod\n",
    "    def print_hdf5_structure(hdf5_file_path: str, indent=0):\n",
    "        \"\"\"\n",
    "        Imprime a estrutura do arquivo HDF5 com informações adicionais.\n",
    "        \n",
    "        Parâmetros:\n",
    "        - hdf5_file_path: str, caminho para o arquivo HDF5.\n",
    "        - indent: int, nível de recuo para representação hierárquica (default=0).\n",
    "        \"\"\"\n",
    "        with h5py.File(hdf5_file_path, 'r') as file:\n",
    "            DictToHDF5._print_hdf5_group_structure(file, indent)\n",
    "            \n",
    "    @staticmethod\n",
    "    def _print_hdf5_group_structure(group, indent=0):\n",
    "        \"\"\"\n",
    "        Auxilia na impressão da estrutura hierárquica do grupo HDF5.\n",
    "        \n",
    "        Parâmetros:\n",
    "        - group: h5py.Group, grupo HDF5 para imprimir.\n",
    "        - indent: int, nível de recuo para representação hierárquica.\n",
    "        \"\"\"\n",
    "        for key, value in group.items():\n",
    "            print(\" \" * indent + f\"{key} : {str(value)}\")\n",
    "\n",
    "            if isinstance(value, h5py.Group):\n",
    "                DictToHDF5._print_hdf5_group_structure(value, indent + 4)\n",
    "                \n",
    "            elif isinstance(value, h5py.Dataset):\n",
    "                # Informações adicionais sobre o conjunto de dados\n",
    "                print(\" \" * (indent + 4) + f\"Shape: {value.shape}, Dtype: {value.dtype}\")\n",
    "                \n",
    "                # Visualização do conteúdo do conjunto de dados\n",
    "                if value.size < 10:  # imprimir todo o conjunto de dados se ele for pequeno\n",
    "                    print(\" \" * (indent + 4) + f\"Data: {value[...]}\")\n",
    "                else:  # imprimir uma amostra dos dados se o conjunto de dados for grande\n",
    "                    sample = value[...]\n",
    "                    if value.ndim > 1:\n",
    "                        sample = sample[:min(3, value.shape[0]), :min(3, value.shape[1])]\n",
    "                    else:\n",
    "                        sample = sample[:min(3, value.size)]\n",
    "                    print(\" \" * (indent + 4) + f\"Sample Data: {sample}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def print_json_structure(json_file_path: str, indent: int = 0, max_sample_size: int = 10):\n",
    "        \"\"\"\n",
    "        Imprime a estrutura e os dados do arquivo JSON.\n",
    "\n",
    "        Parâmetros:\n",
    "        - json_file_path: str, caminho para o arquivo JSON.\n",
    "        - indent: int, nível de recuo para representação hierárquica (default=0).\n",
    "        - max_sample_size: int, tamanho máximo da amostra para visualização (default=10).\n",
    "        \"\"\"\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "            DictToHDF5._print_json_data_structure(json_data, indent, max_sample_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _print_json_data_structure(json_data, indent: int, max_sample_size: int):\n",
    "        \"\"\"\n",
    "        Auxilia na impressão da estrutura e dos dados de um objeto JSON.\n",
    "\n",
    "        Parâmetros:\n",
    "        - json_data: qualquer tipo de dado serializável em JSON.\n",
    "        - indent: int, nível de recuo para representação hierárquica.\n",
    "        - max_sample_size: int, tamanho máximo da amostra para visualização.\n",
    "        \"\"\"\n",
    "        if isinstance(json_data, dict):\n",
    "            for key, value in json_data.items():\n",
    "                print(\" \" * indent + f\"{key}: {type(value).__name__}\")\n",
    "                DictToHDF5._print_json_data_structure(value, indent + 4, max_sample_size)\n",
    "\n",
    "        elif isinstance(json_data, list):\n",
    "            print(\" \" * indent + f\"List of length {len(json_data)}\")\n",
    "            sample = json_data[:min(len(json_data), max_sample_size)]\n",
    "            for index, value in enumerate(sample):\n",
    "                print(\" \" * (indent + 4) + f\"[{index}]: {type(value).__name__}\")\n",
    "                DictToHDF5._print_json_data_structure(value, indent + 8, max_sample_size)\n",
    "\n",
    "        else:\n",
    "            print(\" \" * indent + f\"{json_data}\")\n",
    "\n",
    "    # Para persisistir em N4j\n",
    "    def persist_to_neo4j(self, filepath, neo4j_url, username, password):\n",
    "        graph = Graph(neo4j_url, auth=(username, password))\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            for key in f.keys():\n",
    "                group = f[key]\n",
    "                properties = {}\n",
    "                if key == '0000':  # Tratar grupo \"0000\" diferentemente\n",
    "                    for attr_name, attr_value in group.attrs.items():\n",
    "                        properties[attr_name] = attr_value\n",
    "                    node = Node(\"NoCurriculumGroup\", **properties)  # Criação de um nó específico para o grupo\n",
    "                else:\n",
    "                    for ds_key in group.keys():\n",
    "                        dataset = group[ds_key]\n",
    "                        properties[ds_key] = dataset[()]\n",
    "                    node = Node(\"Person\", **properties)  # Assumindo que o nó seja do tipo \"Person\"\n",
    "                graph.create(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from neo4j import GraphDatabase\n",
    "from typing import Any, List, Dict, Optional\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(value: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Converts a given value to its most primitive form suitable for storage.\n",
    "        Parameters:\n",
    "        - value: Any, the value to be converted.\n",
    "        Returns:\n",
    "        - Any: The converted value in its most primitive form.\n",
    "        \"\"\"\n",
    "        if isinstance(value, str):\n",
    "            return value  # Directly return the string, ensuring it remains a simple string.\n",
    "        elif isinstance(value, list):\n",
    "            return [Neo4jPersister.convert_to_primitives(elem) for elem in value]\n",
    "        elif isinstance(value, dict):\n",
    "            return {key: Neo4jPersister.convert_to_primitives(val) for key, val in value.items()}\n",
    "        elif isinstance(value, set):\n",
    "            return list(value)  # Convert set to list for JSON serializability.\n",
    "        # ... (handle other types if necessary)\n",
    "        else:\n",
    "            return value  # Return the value as-is if it's already in a primitive form.\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return Neo4jPersister.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def persist_data(self, data_dict, label):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MERGE (node:{label}) SET node = $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "    def update_data(self, node_id, data_dict):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MATCH (node) WHERE id(node) = {node_id} SET node += $props\"\n",
    "            session.run(query, props=data_dict_primitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseSoup:\n",
    "    def __init__(self, driver):\n",
    "        self.base_url = 'http://buscatextual.cnpq.br'\n",
    "        self.session = requests.Session()\n",
    "        self.failed_extractions = []\n",
    "        self.driver = driver\n",
    "        self.delay = 10\n",
    "        self.soup = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self  # the object to bind to the variable in the `as` clause\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.driver.quit()\n",
    "        self.driver = None\n",
    "\n",
    "    def to_json(self, data_dict: Dict, filename: str) -> None:\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(data_dict, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to JSON: {e}\")\n",
    "\n",
    "    def to_hdf5(self, processed_data: List[Dict], hdf5_filename: str) -> None:\n",
    "        try:\n",
    "            with h5py.File(hdf5_filename, 'w') as hdf5_file:\n",
    "                for i, data in enumerate(processed_data):\n",
    "                    # Serializa o dicionário como uma string JSON antes de armazená-lo.\n",
    "                    serialized_data = json.dumps(data)\n",
    "                    hdf5_file.create_dataset(str(i), data=serialized_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to HDF5: {e}\")\n",
    "\n",
    "    def dictlist_to_json(self, data_list: List[Dict], filename: str) -> None:\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(data_list, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to JSON: {e}\")\n",
    "\n",
    "    def dictlist_to_hdf5(self, data_list: List[Dict], filename: str, directory=None) -> None:\n",
    "        try:\n",
    "            converter = DictToHDF5(data_list)\n",
    "            converter.create_dataset(filename, directory)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to HDF5: {e}\")\n",
    "    \n",
    "    def format_string(self, input_str):\n",
    "        # Verifica se a entrada é uma string de oito dígitos\n",
    "        if input_str and len(input_str) == 9:\n",
    "            return input_str\n",
    "        elif input_str and len(input_str) == 8:\n",
    "            # Divide a string em duas partes\n",
    "            part1 = input_str[:4]\n",
    "            part2 = input_str[4:]\n",
    "            # Concatena as duas partes com um hífen\n",
    "            formatted_str = f\"{part1}-{part2}\"\n",
    "            return formatted_str\n",
    "        else:\n",
    "            return input_str\n",
    "            \n",
    "    def extract_tit1_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        # Títulos contendo subseções\n",
    "        tit1a = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar',\n",
    "                'Linhas de pesquisa','Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento', 'Revisor de periódico','Revisor de projeto de fomento','Áreas de atuação','Idiomas','Inovação']\n",
    "        tit1b = ['Atuação Profissional'] # dados com subseções\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit1'\n",
    "            if titulo in tit1a:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                    divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                    keys = []\n",
    "                    vals = []\n",
    "                    for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "                        if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "                            key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                            key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            keys.append(key_text)\n",
    "                            val = j.find('div', class_='layout-cell-pad-5')\n",
    "                            val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            vals.append(val_text)\n",
    "                            if verbose:\n",
    "                                print(f'      {key_text:>3}: {val_text}')\n",
    "                    agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                    data_dict[titulo] = Neo4jPersister.convert_to_primitives(agg_dict)\n",
    "            if titulo in tit1b:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "                            if 'layout-cell-3' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        data_dict[titulo][section_name].append(current_data)  # Armazenamos os dados em uma lista\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit2_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        database = ''\n",
    "        total_trab_text = 0\n",
    "        total_cite_text = 0\n",
    "        num_fator_h = 0\n",
    "        data_wos_text = ''\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit2'\n",
    "            if titulo in tit2:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = {}\n",
    "                        if verbose:\n",
    "                            print(f'Seção: {section_name}')\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_subsection = None\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        if section_name == 'Produção bibliográfica':\n",
    "                            subsections = section.find_next_siblings('div', class_='cita-artigos')\n",
    "                            if verbose:\n",
    "                                print(len(subsections), 'subseções')                       \n",
    "                            for subsection in subsections:                            \n",
    "                                if subsection:\n",
    "                                    subsection_name = subsection.find('b').get_text().strip()\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}') # nomes de subseção como ocorrências \n",
    "                                        print(f'    {len(subsection)} divs na subseção {subsection_name}')                                \n",
    "                                    if subsection_name == 'Citações':\n",
    "                                        current_subsection = subsection_name\n",
    "                                        data_dict[titulo][section_name]['Citações'] = {}\n",
    "                                        sub_section_list = []  \n",
    "                                        ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "                                        next_siblings = subsection.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "                                        for sibling in next_siblings:\n",
    "                                            citation_counts = sibling.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que tem os Valores de Citações\n",
    "                                            if citation_counts:\n",
    "                                                for i in citation_counts:\n",
    "                                                    database = i.get_text()\n",
    "                                                    total_trab = i.find_next_sibling(\"div\", class_=\"trab\")\n",
    "                                                    if total_trab:\n",
    "                                                        total_trab_text = total_trab.get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                                    total_cite = i.find_next_sibling(\"div\", class_=\"cita\")\n",
    "                                                    if total_cite:\n",
    "                                                        total_cite_text = total_cite.get_text().split(\"Total de citações:\")[1]\n",
    "                                                    fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                                    num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                                    data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\")\n",
    "                                                    if data_wos:\n",
    "                                                        try:\n",
    "                                                            data_wos_text = data_wos.get_text().split(\"Data:\")[1].strip()\n",
    "                                                        except:\n",
    "                                                            data_wos_text = data_wos.get_text()\n",
    "                                                    # Converta os valores para tipos de dados adequados\n",
    "                                                    total_trab = int(total_trab_text)\n",
    "                                                    total_cite = int(total_cite_text)\n",
    "                                                    citation_numbers = {\n",
    "                                                        \"Database\": database,\n",
    "                                                        \"Total de trabalhos\": total_trab,\n",
    "                                                        \"Total de citações\": total_cite,\n",
    "                                                        \"Índice_H\": num_fator_h,\n",
    "                                                        \"Data\": data_wos_text\n",
    "                                                    }\n",
    "                                                    # Verifique se a subseção atual já existe no dicionário\n",
    "                                                    if 'Citações' not in data_dict[titulo][section_name]:\n",
    "                                                        data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "                                                    data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "                                                    if verbose:\n",
    "                                                        print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')    \n",
    "                            ## Encontrar a div irmã de div subseção com classe layout-cell-12 com artigos\n",
    "                            vals_jcr = []\n",
    "                            div_artigo_geral = data_cell.findChild(\"div\", id=\"artigos-completos\")\n",
    "                            if verbose:\n",
    "                                print(f'Encontrada {len(div_artigo_geral)} div geral de artigos')\n",
    "                            if div_artigo_geral:\n",
    "                                divs_artigos = div_artigo_geral.find_all('div', class_='artigo-completo')\n",
    "                                if verbose:\n",
    "                                    print(len(divs_artigos), 'divs de artigos')\n",
    "                                current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                                if divs_artigos:                              \n",
    "                                    for div_artigo in divs_artigos:\n",
    "                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = {}\n",
    "                                        ## Extrair filhos da classes de artigos completos que estão à frente\n",
    "                                        sibling = div_artigo.findChild()\n",
    "                                        while sibling:\n",
    "                                            classes = sibling.get('class', [])\n",
    "                                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "                                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                                    info_dict = {\n",
    "                                                        'data-issn': 'NULL',\n",
    "                                                        'impact-factor': 'NULL',  \n",
    "                                                        'jcr-year': 'NULL',\n",
    "                                                    }\n",
    "                                                    # Remova as tags span da div\n",
    "                                                    for span in sibling.find_all('span'):\n",
    "                                                        span.extract()\n",
    "                                                    val_text = sibling.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "                                                    current_data[key] = val_text\n",
    "                                                    if verbose:\n",
    "                                                        print(len(current_data.values()), key, val)\n",
    "                                                    sup_element = sibling.find('sup')\n",
    "                                                    if sup_element:\n",
    "                                                        raw_jcr_data = sup_element.get_text()\n",
    "                                                        # print('sup_element:',sup_element)\n",
    "                                                        img_element = sup_element.find('img')\n",
    "                                                        # print('img_element:',img_element)\n",
    "                                                        if img_element:\n",
    "                                                            original_title = img_element.get('original-title')\n",
    "                                                            if original_title:\n",
    "                                                                info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                                if info_list != 'NULL':\n",
    "                                                                    issn = self.format_string(img_element.get('data-issn'))\n",
    "                                                                    if verbose:\n",
    "                                                                        print(f'impact-factor: {info_list[1].split(\": \")[1]}')\n",
    "                                                                    info_dict = {\n",
    "                                                                        'data-issn': issn,\n",
    "                                                                        'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                        'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')',''),\n",
    "                                                                        'journal': info_list[0],\n",
    "                                                                    }\n",
    "                                                            else:\n",
    "                                                                if verbose:\n",
    "                                                                    print('Entrou no primeiro Else')\n",
    "                                                                issn = self.format_string(img_element.get('data-issn'))\n",
    "                                                                info_dict = {\n",
    "                                                                    'data-issn': issn,\n",
    "                                                                    'impact-factor': 'NULL',\n",
    "                                                                    'jcr-year': 'NULL',\n",
    "                                                                    'journal': 'NULL',\n",
    "                                                                }\n",
    "                                                    else:\n",
    "                                                        if verbose:\n",
    "                                                                    print('Entrou no segundo Else')\n",
    "                                                        info_dict = {\n",
    "                                                            'data-issn': 'NULL',\n",
    "                                                            'impact-factor': 'NULL',\n",
    "                                                            'jcr-year': 'NULL',\n",
    "                                                            'journal': 'NULL',\n",
    "                                                        }\n",
    "                                                    vals_jcr.append(info_dict)\n",
    "                                                    if verbose:\n",
    "                                                        print(f'         {info_dict}')\n",
    "                                                if 'JCR' not in data_dict:\n",
    "                                                    data_dict['JCR'] = []\n",
    "                                                if verbose:\n",
    "                                                    print(len(vals_jcr))\n",
    "                                                data_dict['JCR'] = vals_jcr\n",
    "                                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                                next_sibling = sibling.find_next_sibling()\n",
    "                                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                                    sibling = None\n",
    "                                                else:\n",
    "                                                    if current_data:\n",
    "                                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = converted_data\n",
    "                                            if sibling:\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "                        else:\n",
    "                            while sibling:\n",
    "                                classes = sibling.get('class', [])\n",
    "                                if 'cita-artigos' in classes:  # Subsection start\n",
    "                                    subsection_name = sibling.find('b').get_text().strip()\n",
    "                                    current_subsection = subsection_name\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}')\n",
    "                                    data_dict[titulo][section_name][current_subsection] = {}\n",
    "                                    current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "                                elif 'layout-cell-1' in classes:  # Data key\n",
    "                                    key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "                                    if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                        val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                        current_data[key] = val\n",
    "                                elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "                                    next_sibling = sibling.find_next_sibling()\n",
    "                                    if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                        sibling = None\n",
    "                                    else:\n",
    "                                        if current_subsection:\n",
    "                                            data_dict[titulo][section_name][current_subsection] = Neo4jPersister.convert_to_primitives(current_data)  # Armazenamos os dados da subseção atual\n",
    "                                if sibling:\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "        \n",
    "        # Verifique se os dados dos tooltips estão presentes no objeto soup\n",
    "        if 'tooltips' in soup.attrs:\n",
    "            tooltips_data = soup.attrs['tooltips']\n",
    "            agg = []\n",
    "            for tooltip in tooltips_data:\n",
    "                agg_data = {}\n",
    "                # Extração do ano JCR a partir do \"original_title\"\n",
    "                if tooltip.get(\"original_title\"):\n",
    "                    jcr_year = tooltip[\"original_title\"].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')','')\n",
    "                    agg_data[\"jcr-ano\"] = jcr_year\n",
    "                # Adicionar todas as chaves e valores do tooltip ao dicionário agg_data\n",
    "                for key, value in tooltip.items():\n",
    "                    agg_data[key] = value\n",
    "                agg.append(agg_data)\n",
    "            data_dict['JCR2'] = agg\n",
    "        else:\n",
    "            print('Não foram achados os dados de tooltip')\n",
    "            print(soup.attrs)\n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit3_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        # Títulos da seção 'Eventos'\n",
    "        tit3 = ['Eventos']\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit3'\n",
    "            if titulo in tit3:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                        data_dict[titulo][section_name] = converted_data\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "        return data_dict\n",
    "\n",
    "    def extract_data(self, soup):\n",
    "        \"\"\"\n",
    "        Aggregates data from various dictionary sources into a consolidated nested dictionary, \n",
    "        ensuring that all nested lists within the dictionaries are transformed into nested dictionaries.\n",
    "        Parameters:\n",
    "        - soup: BeautifulSoup object, representing the parsed HTML content.\n",
    "        Returns:\n",
    "        - dict: An aggregated dictionary containing the consolidated data.\n",
    "        \"\"\"\n",
    "        self.soup = soup\n",
    "        \n",
    "        def convert_list_to_dict(lst):\n",
    "            \"\"\"\n",
    "            Converts a list into a dictionary with indices as keys.\n",
    "            \n",
    "            Parameters:\n",
    "            - lst: list, input list to be transformed.\n",
    "            \n",
    "            Returns:\n",
    "            - dict: Transformed dictionary.\n",
    "            \"\"\"\n",
    "            return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "        def merge_dict(d1, d2):\n",
    "            \"\"\"\n",
    "            Recursively merges two dictionaries, transforming nested lists into dictionaries.\n",
    "            Parameters:\n",
    "            - d1: dict, the primary dictionary into which data is merged.\n",
    "            - d2: dict or list, the secondary dictionary or list from which data is sourced.\n",
    "            Returns:\n",
    "            - None\n",
    "            \"\"\"\n",
    "            # If d2 is a list, convert it to a dictionary first\n",
    "            if isinstance(d2, list):\n",
    "                d2 = convert_list_to_dict(d2)\n",
    "            \n",
    "            for key, value in d2.items():\n",
    "                if isinstance(value, list):\n",
    "                    d2[key] = convert_list_to_dict(value)\n",
    "                if key in d1 and isinstance(d1[key], dict) and isinstance(value, dict):\n",
    "                    merge_dict(d1[key], value)\n",
    "                else:\n",
    "                    d1[key] = value\n",
    "\n",
    "        # Extract necessary information from soup\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "        name = info_list[0]\n",
    "\n",
    "        # Initialization of the aggregated_data dictionary\n",
    "        aggregated_data = {\"labels\": \"Person\", \"name\": name, \"InfPes\": info_list, \"Resumo\": [elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()]}\n",
    "\n",
    "        # Data extraction and merging\n",
    "        for data_extraction_func in [self.extract_tit1_soup, self.extract_tit2_soup, self.extract_tit3_soup]:\n",
    "            extracted_sections = data_extraction_func(soup)\n",
    "            for title, data in extracted_sections.items():\n",
    "                if title not in aggregated_data:\n",
    "                    aggregated_data[title] = {}\n",
    "                merge_dict(aggregated_data[title], data)\n",
    "        return aggregated_data\n",
    "\n",
    "    def convert_list_to_dict(self, lst: List[Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Converts a list into a dictionary with indices as keys.\n",
    "        Parameters:\n",
    "        - lst: List[Any], input list to be transformed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: Transformed dictionary.\n",
    "        \"\"\"\n",
    "        return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "    def preprocess_data(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Preprocesses the extracted data to ensure that it is in the desired format.\n",
    "        Parameters:\n",
    "        - extracted_data: Dict[str, Any], the data to be preprocessed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: Preprocessed data.\n",
    "        \"\"\"\n",
    "        return self._recursive_preprocessing(extracted_data)\n",
    "    \n",
    "    def _recursive_preprocessing(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Recursively preprocesses a dictionary, applying conversion methods to its elements.\n",
    "        Parameters:\n",
    "        - data: Dict[str, Any], the data dictionary that needs to be preprocessed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: The preprocessed data dictionary.\n",
    "        \"\"\"\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str):\n",
    "                data[key] = self._handle_string_value(value)\n",
    "            elif isinstance(value, list):\n",
    "                data[key] = self._handle_list_value(value)\n",
    "            elif isinstance(value, dict):\n",
    "                data[key] = self._recursive_preprocessing(value)\n",
    "            elif isinstance(value, set):\n",
    "                data[key] = self._handle_set_value(value)\n",
    "        return data\n",
    "\n",
    "    def _handle_set_value(self, value: set) -> list:\n",
    "        \"\"\"\n",
    "        Handles set type values during preprocessing by converting them to lists.\n",
    "        Parameters:\n",
    "        - value: set, the set value to be preprocessed.\n",
    "        Returns:\n",
    "        - list: The preprocessed value as a list.\n",
    "        \"\"\"\n",
    "        return list(value)\n",
    "    \n",
    "    def _handle_string_value(self, value: str) -> Any:\n",
    "        \"\"\"\n",
    "        Handles string type values during preprocessing.\n",
    "        Parameters:\n",
    "        - value: str, the string value to be preprocessed.\n",
    "        Returns:\n",
    "        - Any: The preprocessed value.\n",
    "        \"\"\"\n",
    "        return Neo4jPersister.convert_to_primitives(value)\n",
    "    \n",
    "    def _handle_list_value(self, value: list) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Handles list type values during preprocessing.\n",
    "        Parameters:\n",
    "        - value: list, the list value to be preprocessed.\n",
    "        Returns:\n",
    "        - Dict[str, Any]: The preprocessed value as a dictionary.\n",
    "        \"\"\"\n",
    "        return self.convert_list_to_dict(Neo4jPersister.convert_to_primitives(value))\n",
    "    \n",
    "    def process_single_result(self, extracted_data: Dict, json_filename: str, hdf5_filename: str) -> Optional[Dict]:\n",
    "        try:\n",
    "            # Pre-process data to convert lists and strings to dictionaries\n",
    "            preprocessed_data = self.preprocess_data(extracted_data)\n",
    "            \n",
    "            processed_data = {}\n",
    "            processed_data['name'] = preprocessed_data.get('name', 'N/A')\n",
    "            processed_data['Áreas de atuação'] = preprocessed_data.get('Áreas de atuação', 'N/A')\n",
    "            # processed_data['publications'] = int(preprocessed_data.get('publications', 0))\n",
    "            \n",
    "            self.to_json([processed_data], json_filename)\n",
    "            self.to_hdf5([processed_data], hdf5_filename)\n",
    "            print(processed_data)\n",
    "            return preprocessed_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during single result processing:\\n {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_all_results(self, \n",
    "                            all_extracted_data: List[Dict], \n",
    "                            json_filename: str, \n",
    "                            hdf5_filename: str) -> List[Dict]:\n",
    "        successful_processed_data = []\n",
    "        for extracted_data in all_extracted_data:\n",
    "            processed_data = self.process_single_result(extracted_data, json_filename, hdf5_filename)\n",
    "            if processed_data is not None:\n",
    "                successful_processed_data.append(processed_data)\n",
    "            else:\n",
    "                self.failed_extractions.append(extracted_data)\n",
    "        if self.failed_extractions:\n",
    "            logging.info(\"Retrying failed extractions...\")\n",
    "            for failed_data in self.failed_extractions:\n",
    "                processed_data = self.process_single_result(failed_data, json_filename, hdf5_filename)\n",
    "                if processed_data is not None:\n",
    "                    successful_processed_data.append(processed_data)\n",
    "        self.to_json(successful_processed_data, json_filename)\n",
    "        self.to_hdf5(successful_processed_data, hdf5_filename)\n",
    "        return successful_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "    driver_path=caminho+'chromedriver/chromedriver.exe'\n",
    "    # print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)    \n",
    "    url_busca = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_busca) # acessa a url de busca do CNPQ   \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    return driver\n",
    "\n",
    "class LattesScraper:\n",
    "    def __init__(self, driver, institution, unit, term):\n",
    "        self.base_url = 'http://buscatextual.cnpq.br'\n",
    "        self.session = requests.Session()\n",
    "        self.driver = driver\n",
    "        self.delay = 10\n",
    "\n",
    "    def wait_for_element(self, css_selector: str, ignored_exceptions=None):\n",
    "        \"\"\"\n",
    "        Waits for the element specified by the CSS selector to load.\n",
    "        :param css_selector: CSS selector of the element to wait for\n",
    "        :param ignored_exceptions: List of exceptions to ignore\n",
    "        \"\"\"\n",
    "        WebDriverWait(self.driver, self.delay, ignored_exceptions=ignored_exceptions).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "    def paginar(self, driver):\n",
    "        '''\n",
    "        Helper function to page results on the search page\n",
    "        '''\n",
    "        numpaginas = []\n",
    "        css_paginacao = \"div.paginacao:nth-child(2)\"\n",
    "        try:\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "            paginacao = self.driver.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "            paginas = paginacao.text.split(' ')\n",
    "            remover = ['', 'anterior', '...']\n",
    "            numpaginas = [x for x in paginas if x not in remover]\n",
    "        except Exception as e:\n",
    "            print('  ERRO!! Ao rodar função paginar():', e)\n",
    "        return numpaginas\n",
    "\n",
    "    def retry(self, func, expected_ex_type=Exception, limit=0, wait_ms=200,\n",
    "              wait_increase_ratio=2, on_exhaust=\"throw\"):\n",
    "        attempt = 1\n",
    "        while True:\n",
    "            try:\n",
    "                return func()\n",
    "            except Exception as ex:\n",
    "                if not isinstance(ex, expected_ex_type):\n",
    "                    raise ex\n",
    "                if 0 < limit <= attempt:\n",
    "                    if on_exhaust == \"throw\":\n",
    "                        raise ex\n",
    "                    return on_exhaust\n",
    "                attempt += 1\n",
    "                time.sleep(wait_ms / 1000)\n",
    "                wait_ms *= wait_increase_ratio\n",
    "\n",
    "    def find_terms(self, NOME, instituicao, unidade, termo, delay, limite):\n",
    "        \"\"\"\n",
    "        Função para manipular o HTML até abir a página HTML de cada currículo   \n",
    "        Parâmeteros:\n",
    "            - NOME: É o nome completo de cada pesquisador\n",
    "            - Instituição, unidade e termo: Strings a buscar no currículo para reduzir duplicidades\n",
    "            - driver (webdriver object): The Selenium webdriver object.\n",
    "            - limite (int): Número máximo de tentativas em casos de erro.\n",
    "            - delay (int): tempo em milisegundos a esperar nas operações de espera.\n",
    "        Retorna:\n",
    "            elm_vinculo, np.NaN, np.NaN, np.NaN, driver.\n",
    "        Em caso de erro retorna:\n",
    "            None, NOME, np.NaN, e, driver\n",
    "        \"\"\"\n",
    "        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "        # Inicializando variáveis para evitar UnboundLocalError\n",
    "        elm_vinculo = None\n",
    "        qte_resultados = 0\n",
    "        ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "        duvidas   = []\n",
    "        force_break_loop = False\n",
    "        try:\n",
    "            # Wait and fetch the number of results\n",
    "            css_resultados = \".resultado\"\n",
    "            WebDriverWait(driver, delay, ignored_exceptions=ignored_exceptions).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "            resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "            ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "            try:\n",
    "                css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                div_element = soup.find('div', {'class': 'tit_form'})\n",
    "                match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "                if match:\n",
    "                    qte_resultados = int(match.group(1))\n",
    "                    # print(f'{qte_resultados} resultados para {NOME}')\n",
    "                else:\n",
    "                    return None, NOME, np.NaN, 'Currículo não encontrado', driver\n",
    "            except Exception as e1:\n",
    "                print('  ERRO!! Currículo não disponível no Lattes')\n",
    "                return None, NOME, np.NaN, e1, driver\n",
    "            \n",
    "            ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "            ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "            numpaginas = self.paginar(driver)\n",
    "            if numpaginas == [] and qte_resultados==1:\n",
    "                # capturar link para o primeiro nome resultado da busca\n",
    "                try:\n",
    "                    css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                    nome_vinculo = elm_vinculo.text\n",
    "                except Exception as e2:\n",
    "                    print('  ERRO!! Ao encontrar o primeiro resultado da lista de nomes:', e2)\n",
    "                    \n",
    "                    # Call the handle stale file_error function\n",
    "                    if self.handle_stale_file_error(driver):\n",
    "                        # If the function returns True, it means the error was resolved.\n",
    "                        # try to get the nome_vinculo again:\n",
    "                        try:\n",
    "                            elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                            nome_vinculo = elm_vinculo.text\n",
    "                        except Exception as e3:\n",
    "                            print('  ERRO!! Servidor CNPq indisponível no momento, tentar em alguns minutos:', e3)\n",
    "                            return None, NOME, np.NaN, e3, driver\n",
    "                    else:\n",
    "                        # If the function returns False, it means the error was not resolved within the given retries.\n",
    "                        return None, NOME, np.NaN, e2, driver\n",
    "\n",
    "                    print('  Não foi possível extrair por falha no servidor do CNPq:',e)\n",
    "                    return None, NOME, np.NaN, e2, driver\n",
    "                # print('Clicar no nome único:', nome_vinculo)\n",
    "                try:\n",
    "                    self.retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                        wait_ms=20,\n",
    "                        limit=limite,\n",
    "                        on_exhaust=(f'  Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "                except Exception as e4:\n",
    "                    print('  ERRO!! Ao clicar no único nome encontrado anteriormente',e)\n",
    "                    return None, NOME, np.NaN, e4, driver\n",
    "            \n",
    "            ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "            else:\n",
    "                print(f'       {qte_resultados:>3} homônimos de: {NOME}')\n",
    "                numpaginas = self.paginar(driver)\n",
    "                numpaginas.append('próximo')\n",
    "                iteracoes=0\n",
    "                ## iterar em cada página de resultados\n",
    "                pagin = qte_resultados//10+1\n",
    "                count = None\n",
    "                found = None\n",
    "                for i in range(pagin+1):\n",
    "                    # print(i,'/',pagin)\n",
    "                    iteracoes+=1\n",
    "                    try:\n",
    "                        numpaginas = self.paginar(driver)\n",
    "                        # print(f'       Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                        css_resultados = \".resultado\"\n",
    "                        WebDriverWait(driver, delay).until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                    except Exception as e:\n",
    "                        print('  ERRO!! Ao paginar:',e)\n",
    "                    ## iterar em cada resultado\n",
    "                    for n,i in enumerate(resultados):\n",
    "                        linhas = i.text.split('\\n\\n')\n",
    "                        # print(linhas)\n",
    "                        if 'Stale file handle' in str(linhas):\n",
    "                            return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                        for m,linha in enumerate(linhas):\n",
    "                            # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                            # print('Conteúdo da linha:',linha.lower())\n",
    "                            # print(linha)\n",
    "                            try:\n",
    "                                if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                    # print('Vínculo encontrado!')\n",
    "                                    count=m\n",
    "                                    while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                        count-=1\n",
    "                                    # print('       Identificado vínculo no resultado:', m+1)\n",
    "                                    found = m+1\n",
    "                                    # nome_vinculo = linhas[count].replace('\\n','\\n       ').strip()\n",
    "                                    # print(f'       Achado: {nome_vinculo}')\n",
    "                                    try:\n",
    "                                        css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                        # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                        WebDriverWait(driver, delay).until(\n",
    "                                            EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                        elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                        nome_vinculo = elm_vinculo.text\n",
    "                                        # print('Elemento retornado:',nome_vinculo)\n",
    "                                        self.retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                            wait_ms=100,\n",
    "                                            limit=limite,\n",
    "                                            on_exhaust=(f'  Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                    except Exception as e5:\n",
    "                                        print('  ERRO!! Ao achar o link do nome com múltiplos resultados')\n",
    "                                        return np.NaN, NOME, np.NaN, e5, driver\n",
    "                                    force_break_loop = True\n",
    "                                    break\n",
    "                            except Exception as e6:\n",
    "                                traceback_str = ''.join(traceback.format_tb(e6.__traceback__))\n",
    "                                print('  ERRO!! Ao procurar vínculo com currículos achados')    \n",
    "                                print(e6,traceback_str)\n",
    "                            ## Caso percorra toda lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                            if m==(qte_resultados):\n",
    "                                print(f'Nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                                duvidas.append(NOME)\n",
    "                                # clear_output(wait=True)\n",
    "                                # driver.quit()\n",
    "                                continue\n",
    "                        if force_break_loop:\n",
    "                            break\n",
    "                    try:\n",
    "                        prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                        prox.click()\n",
    "                    except:\n",
    "                        continue\n",
    "                if count:\n",
    "                    nome_vinculo = linhas[count].replace('\\n','\\n       ').strip()\n",
    "                    print(f'       Escolhido homônimo {found}: {nome_vinculo}')\n",
    "                else:\n",
    "                    print(f'       Não foi possível identificar o vínculo de: {NOME}')\n",
    "                    duvidas.append(NOME)\n",
    "            try:\n",
    "                elm_vinculo.text\n",
    "                # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "            except:\n",
    "                return None, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "        except exceptions.TimeoutException:\n",
    "            print(\"  ERRO!! O tempo limite de espera foi atingido.\")\n",
    "            return None, NOME, np.NaN, \"TimeoutException\", driver\n",
    "        except exceptions.WebDriverException as e7:\n",
    "            print(\"  ERRO!! Problema ao interagir com o driver.\")\n",
    "            return None, NOME, np.NaN, e7, driver\n",
    "        except Exception as e8:\n",
    "            print(\"  ERRO 8!! Um erro inesperado ocorreu.\")\n",
    "            print(f'  {e8}')\n",
    "            return None, NOME, np.NaN, e8, driver\n",
    "        # Verifica antes de retornar para garantir que elm_vinculo foi definido\n",
    "        if elm_vinculo is None:\n",
    "            print(\"Vínculo não foi definido.\")\n",
    "            return None, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "        # Retorna a saída de sucesso\n",
    "        return elm_vinculo, np.NaN, np.NaN, np.NaN, driver\n",
    "\n",
    "    def handle_stale_file_error(self, max_retries=5, retry_interval=10):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                error_div = self.driver.find_element(By.CSS_SELECTOR, 'resultado')\n",
    "                linha1 = error_div.fidChild('li')\n",
    "                if 'Stale file handle' in linha1.text:\n",
    "                    time.sleep(retry_interval)\n",
    "                else:\n",
    "                    return True\n",
    "            except NoSuchElementException:\n",
    "                return True\n",
    "        return False\n",
    "       \n",
    "    def extract_data_from_cvuri(self, element) -> dict:\n",
    "        \"\"\"\n",
    "        Extracts data from the cvuri attribute of the given element.\n",
    "        :param element: WebElement object\n",
    "        :return: Dictionary of extracted data\n",
    "        \"\"\"\n",
    "        cvuri = element.get_attribute('cvuri')\n",
    "        parsed_url = urlparse(cvuri)\n",
    "        params = parse_qs(parsed_url.query)\n",
    "        data_dict = {k: v[0] for k, v in params.items()}\n",
    "        return data_dict\n",
    "\n",
    "    def fill_name(self, NOME):\n",
    "        '''\n",
    "        Move cursor to the search field and fill in the specified name.\n",
    "        '''\n",
    "        if self.driver is None:\n",
    "            logging.error(\"O driver não foi inicializado corretamente.\")\n",
    "            return\n",
    "        try:\n",
    "            nome = lambda: self.driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "            nome().send_keys(Keys.CONTROL + \"a\")\n",
    "            nome().send_keys(NOME)\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(f'  ERRO!! Ao colar nome para buscar.') #, {traceback_str}\n",
    "        try:            \n",
    "            seletorcss = 'div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "            \n",
    "            seletorcss = \"#botaoBuscaFiltros\"\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(f'  ERRO!! Ao clicar no botão Buscar.\\n{e}, {traceback_str}')\n",
    "\n",
    "    def return_search_page(self):\n",
    "        url_busca = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "        driver.get(url_busca) # acessa a url de busca do CNPQ        \n",
    "\n",
    "    def check_and_click_vinculo(self, elm_vinculo):\n",
    "        if elm_vinculo is None:\n",
    "            self.return_search_page()\n",
    "            logging.error(\"Vínculo não encontrado, passando para o próximo nome...\")\n",
    "        try:\n",
    "            logging.info(f'Vínculo encontrado no currículo de nome: {elm_vinculo.text}')\n",
    "        except AttributeError:\n",
    "            self.return_search_page()\n",
    "            logging.error(\"Vínculo não encontrado, passando para o próximo nome...\")\n",
    "        try:\n",
    "            # Clicar no botão para abrir o currículo\n",
    "            btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "            time.sleep(0.2)\n",
    "            ActionChains(driver).click(btn_abrir_curriculo).perform()            \n",
    "            # logging.info('Successfully clicked on the vínculo.')\n",
    "        except WebDriverException:\n",
    "            self.return_search_page()\n",
    "            logging.error('Falha ao clicar no link do nome.')\n",
    "\n",
    "    def switch_to_new_window(self):\n",
    "        window_before = self.driver.current_window_handle\n",
    "        WebDriverWait(self.driver, self.delay).until(EC.number_of_windows_to_be(2))\n",
    "        window_after = self.driver.window_handles\n",
    "        new_window = [x for x in window_after if x != window_before][0]\n",
    "        self.driver.switch_to.window(new_window)\n",
    "\n",
    "    def switch_back_to_original_window(self):\n",
    "        current_window = self.driver.current_window_handle\n",
    "        original_window = [x for x in self.driver.window_handles if x != current_window][0]\n",
    "        self.driver.close() # Close the current window\n",
    "        self.driver.switch_to.window(original_window) # Switch back to the original window\n",
    "\n",
    "    def extract_tooltip_data(self) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Extracts tooltip data from articles section using Selenium.\n",
    "        :return: List of dictionaries containing the extracted tooltip data\n",
    "        \"\"\"\n",
    "        tooltip_data_list = []\n",
    "        try:\n",
    "            self.wait_for_element(\"#artigos-completos img.ajaxJCR\", [TimeoutException])\n",
    "            layout_cells = self.driver.find_elements(By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5')\n",
    "            for cell in layout_cells:\n",
    "                tooltip_data = {}\n",
    "                try:\n",
    "                    elem_citado = cell.find_element(By.CSS_SELECTOR, '.citado')\n",
    "                    tooltip_data.update(self.extract_data_from_cvuri(elem_citado))\n",
    "                except (ElementNotInteractableException, NoSuchElementException):\n",
    "                    pass\n",
    "                try:\n",
    "                    doi_elem = cell.find_element(By.CSS_SELECTOR, \"a.icone-producao.icone-doi\")\n",
    "                    tooltip_data[\"doi\"] = doi_elem.get_attribute(\"href\")\n",
    "                except NoSuchElementException:\n",
    "                    tooltip_data[\"doi\"] = None\n",
    "                try:\n",
    "                    self.wait_for_element(\"img.ajaxJCR\", [TimeoutException])\n",
    "                    tooltip_elem = self.driver.find_element(By.CSS_SELECTOR, \"img.ajaxJCR\")\n",
    "                    ActionChains(self.driver).move_to_element(tooltip_elem).perform()\n",
    "                    original_title = tooltip_elem.get_attribute(\"original-title\")\n",
    "                    match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "                    tooltip_data[\"impact-factor\"] = match.group(1) if match else None\n",
    "                    tooltip_data[\"original_title\"] = original_title.split('<br />')[0].strip()\n",
    "                except (NoSuchElementException, TimeoutException):\n",
    "                    pass\n",
    "                tooltip_data_list.append(tooltip_data)\n",
    "            print(f'       {len(tooltip_data_list):>003} artigos extraídos')\n",
    "            logging.info(f'{len(tooltip_data_list):>003} artigos extraídos')\n",
    "\n",
    "        except TimeoutException as e:\n",
    "            logging.error(f\"Sem respota antes do timeout\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro inesperado ao extrair tooltips: {e}\")\n",
    "        return tooltip_data_list\n",
    "            \n",
    "    def search_profile(self, name, instituicao, unidade, termo):\n",
    "        try:\n",
    "            # Find terms to interact with the web page and extract the profile\n",
    "            profile_element, _, _, _, _ = self.find_terms(\n",
    "                name, \n",
    "                instituicao,  \n",
    "                unidade,  \n",
    "                termo,  \n",
    "                10,  \n",
    "                3  \n",
    "            )\n",
    "            # print('Elemento encontrado:', profile_element)\n",
    "            if profile_element:\n",
    "                return profile_element\n",
    "            else:\n",
    "                self.return_search_page()\n",
    "                logging.info(f'Currículo não encontrado: {name}')\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            logging.error(f\"HTTPError occurred: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro inesperado ao buscar: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    def scrape(self, driver, name_list, instituicao, unidade, termo, json_filename, hdf5_filename):\n",
    "        dict_list=[]\n",
    "        for k, name in enumerate(name_list):\n",
    "            try:\n",
    "                print(f'{k+1:>2}/{len(name_list)}: {name}')\n",
    "                self.fill_name(name)\n",
    "                elm_vinculo = self.search_profile(name, instituicao, unidade, termo)\n",
    "                self.check_and_click_vinculo(elm_vinculo)\n",
    "                self.switch_to_new_window()\n",
    "                \n",
    "                if elm_vinculo:\n",
    "                    tooltip_data_list = self.extract_tooltip_data()\n",
    "                    page_source = driver.page_source\n",
    "                    if page_source is not None:\n",
    "                        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                        soup.attrs['tooltips'] = tooltip_data_list                 \n",
    "                        if soup:\n",
    "                            # print('Extraindo dados do objeto Soup...')\n",
    "                            parse_soup_instance = ParseSoup(driver)\n",
    "                            data = parse_soup_instance.extract_data(soup)\n",
    "                            # Chama métodos de conversão de dicionário individual\n",
    "                            # parse_soup_instance.to_json(data, json_filename)\n",
    "                            # parse_soup_instance.to_hdf5(data, hdf5_filename)\n",
    "                            dict_list.append(data)\n",
    "                    else:\n",
    "                        logging.error(f\"Could not get soup for profile: {name}\")\n",
    "                else:\n",
    "                    logging.error(f\"Currículo não encontrado para: {name}\")\n",
    "\n",
    "                # Fechar janela do currículo e voltar para página de busca\n",
    "                self.switch_back_to_original_window()\n",
    "\n",
    "                # Clicar no botão para fechar janela pop-up\n",
    "                btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnfechar\")))\n",
    "                ActionChains(driver).click(btn_abrir_curriculo).perform()    \n",
    "                # logging.info('Successfully closed pop-up.')       \n",
    "                # # Clicar no botão para fazer nova consulta\n",
    "                # btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "                #     EC.element_to_be_clickable((By.CSS_SELECTOR, \"#botaoBuscaFiltros\")))\n",
    "                # ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "                self.return_search_page()\n",
    "                # logging.info('Successfully restarded extraction.')\n",
    "            except TimeoutException as e:\n",
    "                logging.error(f\"Sem resposta antes do timeout para: {name}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erro inesperado ao extrair para: {name}: {str(e)}\")\n",
    "        driver.quit()\n",
    "        return dict_list\n",
    "    \n",
    "# if __name__ == \"__main__\":   \n",
    "#     drives=['C:/Users/','E:/','./home/']\n",
    "#     pastas=['marcos.aires/', 'marco/']\n",
    "#     pastasraiz=['kgfioce','fioce']\n",
    "#     pasta_dados = './../data/'\n",
    "#     pastaraiz = 'fioce'\n",
    "#     caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "#     pathzip, pathcsv, pathjson, pathfig, pathaux, pathout = preparar_pastas(caminho)\n",
    "\n",
    "#     instituicao = 'Fundação Oswaldo Cruz'\n",
    "#     unidade = 'Fiocruz Ceará'\n",
    "#     termo = 'Ministerio da Saude'\n",
    "#     driver = connect_driver(caminho)\n",
    "#     t0 = time.time()\n",
    "#     scraper = LattesScraper(driver, instituicao, unidade, termo)\n",
    "#     dict_list = scraper.scrape(driver, lista_nomes, instituicao, unidade, termo,\n",
    "#                                pasta_dados+\"output.json\", pasta_dados+\"output.hdf5\")\n",
    "    \n",
    "#     print(f'{tempo(t0,time.time())} para busca e extração de dados de {len(lista_nomes)} nomes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processar usando a classe de extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema operacional Windows\n",
      "Drive em uso C\n",
      "Pasta armazenagem local C:/Users/marco/fioce/\n",
      "\n",
      "Pasta para os xml já existe!\n",
      "Pasta para os CSV já existe!\n",
      "Pasta para os JSON já existe!\n",
      "Pasta para figuras já existe!\n",
      "Pasta para saídas já existe!\n",
      "\n",
      "Caminho da pasta raiz C:/Users/marco/fioce/\n",
      "Caminho arquivos  XML C:/Users/marco/fioce/xml_zip/\n",
      "Caminho arquivos JSON C:/Users/marco/fioce/json/\n",
      "Caminho arquivos  CSV C:/Users/marco/fioce/csv/\n",
      "Caminho para  figuras C:/Users/marco/fioce/fig/\n",
      "Pasta arquivos saídas C:/Users/marco/fioce/output/\n",
      "\n",
      "Conectando com o servidor do CNPq...\n",
      " 1/57: Alice Paula Di Sabatino Guimaraes\n",
      "       002 artigos extraídos\n",
      " 2/57: Ana Claudia De Araújo Teixeira\n",
      "         2 homônimos de: Ana Claudia De Araújo Teixeira\n",
      "       Escolhido homônimo 2: Ana Cláudia de Araújo Teixeira \n",
      "       Doutorado em Educação Brasileira pela Universidade Federal do Ceará, Brasil(2008)\n",
      "       Pesquisadora em Saúde Pública da Fundação Oswaldo Cruz - Fiocruz Ceará , Brasil\n",
      "       008 artigos extraídos\n",
      " 3/57: Ana Camila Oliveira Alves\n",
      "       002 artigos extraídos\n",
      " 4/57: Angela Christina De Moraes Ostritz\n",
      " 5/57: Adriana Costa Bacelo\n",
      "       023 artigos extraídos\n",
      " 6/57: Anna Carolina Machado Marinho\n",
      "       006 artigos extraídos\n",
      " 7/57: Antonio Marcos Aires Barbosa\n",
      " 8/57: Anya Pimentel Gomes Fernandes Vieira Meyer\n",
      "       100 artigos extraídos\n",
      " 9/57: Bruno Bezerra Carvalho\n",
      "         7 homônimos de: Bruno Bezerra Carvalho\n",
      "       Escolhido homônimo 3: Bruno Bezerra Carvalho \n",
      "       Especialização em Gestão de Organizações de Ciência e Tecnologia em Saúde pela Fundação Oswaldo Cruz, Brasil(2014)\n",
      "       Analista de Gestão em Saúde Pública - TI da Fundação Oswaldo Cruz (CE) , Brasil\n",
      "10/57: Carla Freire Celedônio Fernandes\n",
      "       041 artigos extraídos\n",
      "11/57: Carlos Jose Araujo Pinheiro\n",
      "12/57: Claudia Stutz Zubieta\n",
      "       003 artigos extraídos\n",
      "13/57: Charles Cerqueira De Abreu\n",
      "14/57: Clarice Gomes E Souza Dabés\n",
      "       002 artigos extraídos\n",
      "15/57: Clarissa Romero Teixeira\n",
      "       043 artigos extraídos\n",
      "16/57: Dayane Alves Costa\n",
      "         7 homônimos de: Dayane Alves Costa\n",
      "       Não foi possível identificar o vínculo de: Dayane Alves Costa\n",
      "17/57: Donat Alexander De Chapeaurouge\n",
      "       043 artigos extraídos\n",
      "18/57: Eduardo Ruback Dos Santos\n",
      "       005 artigos extraídos\n",
      "19/57: Ezequiel Valentim De Melo\n",
      "20/57: Fabio Miyajima\n",
      "       064 artigos extraídos\n",
      "21/57: Fernando Braga Stehling Dias\n",
      "       023 artigos extraídos\n",
      "22/57: Fernando Ferreira Carneiro\n",
      "         3 homônimos de: Fernando Ferreira Carneiro\n",
      "       Escolhido homônimo 2: Fernando Ferreira Carneiro \n",
      "       Doutorado em Ciência Animal - Med. Vet. Prev. e Epidemiologia pela Escola de Veterinária da UFMG, Brasil(2007)\n",
      "       Pesquisador da Fundação Oswaldo Cruz , Brasil\n",
      "       062 artigos extraídos\n",
      "23/57: Galba Freire Moita\n",
      "         2 homônimos de: Galba Freire Moita\n",
      "       Escolhido homônimo 2: Galba Freire Moita \n",
      "       Doutorado em Doutoramento em Gestão-Ciencia Aplicada à Decisão pela Universidade de Coimbra, Portugal(2019)\n",
      "       Coordenador Geral Monitoramento e Avaliação do Ministerio da Saude do Brasil , Brasil\n",
      "       011 artigos extraídos\n",
      "24/57: Giovanny Augusto Camacho Antevere Mazzarotto\n",
      "       011 artigos extraídos\n",
      "25/57: Gilvan Pessoa Furtado\n",
      "       024 artigos extraídos\n",
      "26/57: Ivana Cristina De Holanda Cunha Barrêto\n",
      "27/57: Ivanildo Lopes Farias\n",
      "28/57: Jaime Ribeiro Filho\n",
      "         2 homônimos de: Jaime Ribeiro Filho\n",
      "       Escolhido homônimo 2: Jaime Ribeiro Filho \n",
      "       Bolsista de Produtividade em Pesquisa 2\n",
      "       Doutorado em Biologia Celular e Molecular pela Fundação Oswaldo Cruz, Brasil(2013)\n",
      "       Pesquisador em Saúde Pública da Fundação Oswaldo Cruz , Brasil\n",
      "       104 artigos extraídos\n",
      "29/57: João Baptista Estabile Neto\n",
      "30/57: João Hermínio Martins Da Silva\n",
      "       029 artigos extraídos\n",
      "31/57: José Luis Passos Cordeiro\n",
      "       042 artigos extraídos\n",
      "32/57: Kamila Matos Albuquerque \n",
      "       001 artigos extraídos\n",
      "33/57: Luciana Coelho Serafim\n",
      "34/57: Luciana Pereira Lindenmeyer\n",
      "       001 artigos extraídos\n",
      "35/57: Luciana Silvério Alleluia Higino Da Silva\n",
      "36/57: Luciano Pinto Zorzanelli\n",
      "37/57: Luis Fernando Pessoa De Andrade\n",
      "38/57: Luiz Odorico Monteiro De Andrade\n",
      "       084 artigos extraídos\n",
      "39/57: Marcela Helena Gambim Fonseca\n",
      "       017 artigos extraídos\n",
      "40/57: Marcelo Jorge Lopes Coutinho\n",
      "41/57: Marcos Roberto Lourenzoni\n",
      "       026 artigos extraídos\n",
      "42/57: Marcio Flavio Moura De Araujo \n",
      "43/57: Margareth Borges Coutinho Gallo\n",
      "       018 artigos extraídos\n",
      "44/57: Marlos De Medeiros Chaves\n",
      "       007 artigos extraídos\n",
      "45/57: Maximiliano Loiola Ponte De Souza\n",
      "       044 artigos extraídos\n",
      "46/57: Nilton Luiz Costa Machado\n",
      "47/57: Patricia Maria Ferreira da Silva\n",
      "         7 homônimos de: Patricia Maria Ferreira da Silva\n",
      "       Escolhido homônimo 4: Patricia Maria Ferreira da Silva \n",
      "       Especialização em Gestão de Sistemas e Serviços de Saúde pela Escola Nacional de Saúde Pública, Brasil(2007)\n",
      "       Analista de Gestão - Gestão de Projetos da Fundação Oswaldo Cruz , Brasil\n",
      "48/57: Raphael Trevizani\n",
      "       004 artigos extraídos\n",
      "49/57: Regis Bernardo Brandim Gomes\n",
      "       038 artigos extraídos\n",
      "50/57: Renato Caldeira De Souza\n",
      "         2 homônimos de: Renato Caldeira De Souza\n",
      "       Não foi possível identificar o vínculo de: Renato Caldeira De Souza\n",
      "51/57: Roberto Nicolete\n",
      "       056 artigos extraídos\n",
      "52/57: Roberto Wagner Junior Freire De Freitas\n",
      "       092 artigos extraídos\n",
      "53/57: Rodrigo Carvalho Nogueira \n",
      "         8 homônimos de: Rodrigo Carvalho Nogueira \n",
      "       Escolhido homônimo 5: Rodrigo Carvalho Nogueira \n",
      "       Mestrado Profissional em Saúde da Criança e do Adolescente pela Universidade Estadual do Ceará, Brasil(2009)\n",
      "       Analista de Gestão em saúde - Gestão e Desenv da Fundação Oswaldo Cruz , Brasil\n",
      "54/57: Sergio Dos Santos Reis\n",
      "        10 homônimos de: Sergio Dos Santos Reis\n",
      "       Não foi possível identificar o vínculo de: Sergio Dos Santos Reis\n",
      "55/57: Sharmenia De Araujo Soares Nuto\n",
      "       050 artigos extraídos\n",
      "56/57: Vanira Matos Pessoa\n",
      "       029 artigos extraídos\n",
      "57/57: Venúcia Bruna Magalhães Pereira\n",
      "       006 artigos extraídos\n",
      "00h 28m 31s para busca e extração de dados de 57 nomes\n"
     ]
    }
   ],
   "source": [
    "drives=['C:/Users/','E:/','./home/']\n",
    "pastas=['marcos.aires/', 'marco/']\n",
    "pastasraiz=['kgfioce','fioce']\n",
    "pasta_dados = './../data/'\n",
    "pastaraiz = 'fioce'\n",
    "caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "pathzip, pathcsv, pathjson, pathfig, pathaux, pathout = preparar_pastas(caminho)\n",
    "\n",
    "instituicao = 'Fundação Oswaldo Cruz'\n",
    "unidade = 'Fiocruz Ceará'\n",
    "termo = 'Ministerio da Saude'\n",
    "driver = connect_driver(caminho)\n",
    "t0 = time.time()\n",
    "scraper = LattesScraper(driver, instituicao, unidade, termo)\n",
    "dict_list = scraper.scrape(driver, lista_nomes, instituicao, unidade, termo,\n",
    "                            pasta_dados+\"output.json\", pasta_dados+\"output.hdf5\")\n",
    "\n",
    "print(f'{tempo(t0,time.time())} para busca e extração de dados de {len(lista_nomes)} nomes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Vínculo encontrado no currículo de nome: Alice Paula Di Sabatino Guimarães\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ana Cláudia de Araújo Teixeira\n",
      "INFO:root:008 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ana Camila Oliveira Alves\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Angela Christina de Moraes Ostritz\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Adriana Costa Bacelo\n",
      "INFO:root:023 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Anna Carolina Machado Marinho\n",
      "INFO:root:006 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Antonio Marcos Aires Barbosa\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Anya Pimentel Gomes Fernandes Vieira Meyer\n",
      "INFO:root:100 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Bruno Bezerra Carvalho\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Carla Freire Celedonio Fernandes\n",
      "INFO:root:041 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Carlos Jose Araujo Pinheiro\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Carlos Jose Araujo Pinheiro: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Claudia Stutz Zubieta\n",
      "INFO:root:003 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Charles Cerqueira De Abreu\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Charles Cerqueira De Abreu: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Clarice Gomes e Souza Dabés\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Clarissa Romero Teixeira\n",
      "INFO:root:043 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Dayane Alves Costa\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Dayane Alves Costa: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Donat Alexander de Chapeaurouge\n",
      "INFO:root:043 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Eduardo Ruback dos Santos\n",
      "INFO:root:005 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ezequiel Valentim de Melo\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Fabio Miyajima\n",
      "INFO:root:064 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Fernando Braga Stehling Dias\n",
      "INFO:root:023 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Fernando Ferreira Carneiro\n",
      "INFO:root:062 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Galba Freire Moita\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Galba Freire Moita: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Giovanny Augusto Camacho Antevere Mazzarotto\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Gilvan Pessoa Furtado\n",
      "INFO:root:024 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ivana Cristina de Holanda Cunha Barreto\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ivanildo Lopes Farias\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Jaime Ribeiro Filho\n",
      "INFO:root:104 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: João Baptista Estabile Neto\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: João Hermínio Martins da Silva\n",
      "INFO:root:029 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: José Luís Passos Cordeiro\n",
      "INFO:root:042 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Kamila Matos de Albuquerque\n",
      "INFO:root:001 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luciana Coelho Serafim\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luciana Pereira Lindenmeyer\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luciana Silvério Alleluia Higino da Silva\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Currículo não encontrado: Luciano Pinto Zorzanelli\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Luciano Pinto Zorzanelli: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luis Fernando Pessoa de Andrade\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luiz Odorico Monteiro de Andrade\n",
      "INFO:root:084 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marcela Helena Gambim Fonseca\n",
      "INFO:root:017 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marcelo Jorge Lopes Coutinho\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marcos Roberto Lourenzoni\n",
      "INFO:root:026 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Márcio Flávio Moura de Araújo\n",
      "INFO:root:142 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Margareth Borges Coutinho Gallo\n",
      "INFO:root:018 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marlos de Medeiros Chaves\n",
      "INFO:root:007 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Maximiliano Loiola Ponte de Souza\n",
      "INFO:root:044 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Nilton Luiz Costa Machado\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Nilton Luiz Costa Machado: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Patricia Maria Ferreira da Silva\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Raphael Trevizani\n",
      "INFO:root:004 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Regis Bernardo Brandim Gomes\n",
      "INFO:root:038 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Renato Caldeira De Souza\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Renato Caldeira De Souza: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Roberto Nicolete\n",
      "INFO:root:056 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Roberto Wagner Júnior Freire de Freitas\n",
      "INFO:root:092 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Rodrigo Carvalho Nogueira\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Currículo não encontrado: Sergio Dos Santos Reis\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Sergio Dos Santos Reis: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Sharmênia de Araújo Soares Nuto\n",
      "INFO:root:050 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Vanira Matos Pessoa\n",
      "INFO:root:029 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Venúcia Bruna Magalhães Pereira\n",
      "INFO:root:006 artigos extraídos\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001DF59E16550>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente')': /session/6063b01acded29d7120f483b4bf254a8\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001DF5344ABD0>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente')': /session/6063b01acded29d7120f483b4bf254a8\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001DF5895AE90>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente')': /session/6063b01acded29d7120f483b4bf254a8\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Alice Paula Di Sabatino Guimarães\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ana Cláudia de Araújo Teixeira\n",
      "INFO:root:008 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ana Camila Oliveira Alves\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Angela Christina de Moraes Ostritz\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Adriana Costa Bacelo\n",
      "INFO:root:023 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Anna Carolina Machado Marinho\n",
      "INFO:root:006 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Antonio Marcos Aires Barbosa\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Anya Pimentel Gomes Fernandes Vieira Meyer\n",
      "INFO:root:100 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Bruno Bezerra Carvalho\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Carla Freire Celedonio Fernandes\n",
      "INFO:root:041 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Carlos Jose Araujo Pinheiro\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Carlos Jose Araujo Pinheiro: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Claudia Stutz Zubieta\n",
      "INFO:root:003 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Charles Cerqueira De Abreu\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Charles Cerqueira De Abreu: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Clarice Gomes e Souza Dabés\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Clarissa Romero Teixeira\n",
      "INFO:root:043 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Dayane Alves Costa\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Dayane Alves Costa: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Donat Alexander de Chapeaurouge\n",
      "INFO:root:043 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Eduardo Ruback dos Santos\n",
      "INFO:root:005 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ezequiel Valentim de Melo\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Fabio Miyajima\n",
      "INFO:root:064 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Fernando Braga Stehling Dias\n",
      "INFO:root:023 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Fernando Ferreira Carneiro\n",
      "INFO:root:062 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Galba Freire Moita\n",
      "INFO:root:011 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Giovanny Augusto Camacho Antevere Mazzarotto\n",
      "INFO:root:011 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Gilvan Pessoa Furtado\n",
      "INFO:root:024 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ivana Cristina de Holanda Cunha Barreto\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ivanildo Lopes Farias\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Jaime Ribeiro Filho\n",
      "INFO:root:104 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: João Baptista Estabile Neto\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: João Hermínio Martins da Silva\n",
      "INFO:root:029 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: José Luís Passos Cordeiro\n",
      "INFO:root:042 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Kamila Matos de Albuquerque\n",
      "INFO:root:001 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luciana Coelho Serafim\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luciana Pereira Lindenmeyer\n",
      "INFO:root:001 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luciana Silvério Alleluia Higino da Silva\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Currículo não encontrado: Luciano Pinto Zorzanelli\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Luciano Pinto Zorzanelli: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luis Fernando Pessoa de Andrade\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luiz Odorico Monteiro de Andrade\n",
      "INFO:root:084 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marcela Helena Gambim Fonseca\n",
      "INFO:root:017 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marcelo Jorge Lopes Coutinho\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marcos Roberto Lourenzoni\n",
      "INFO:root:026 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Márcio Flávio Moura de Araújo\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Margareth Borges Coutinho Gallo\n",
      "INFO:root:018 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marlos de Medeiros Chaves\n",
      "INFO:root:007 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Maximiliano Loiola Ponte de Souza\n",
      "INFO:root:044 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Nilton Luiz Costa Machado\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Nilton Luiz Costa Machado: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Patricia Maria Ferreira da Silva\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Raphael Trevizani\n",
      "INFO:root:004 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Regis Bernardo Brandim Gomes\n",
      "INFO:root:038 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Renato Caldeira De Souza\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Renato Caldeira De Souza: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Roberto Nicolete\n",
      "INFO:root:056 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Roberto Wagner Júnior Freire de Freitas\n",
      "INFO:root:092 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Rodrigo Carvalho Nogueira\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Currículo não encontrado: Sergio Dos Santos Reis\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Sergio Dos Santos Reis: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Sharmênia de Araújo Soares Nuto\n",
      "INFO:root:050 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Vanira Matos Pessoa\n",
      "INFO:root:029 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Venúcia Bruna Magalhães Pereira\n",
      "INFO:root:006 artigos extraídos\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001DF58F57650>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente')': /session/6e6777e2456502a41b119545b3f84d0b\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001DF587E1E10>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente')': /session/6e6777e2456502a41b119545b3f84d0b\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001DF587E0BD0>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente')': /session/6e6777e2456502a41b119545b3f84d0b\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Alice Paula Di Sabatino Guimarães\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ana Cláudia de Araújo Teixeira\n",
      "INFO:root:008 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ana Camila Oliveira Alves\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Angela Christina de Moraes Ostritz\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Adriana Costa Bacelo\n",
      "INFO:root:023 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Anna Carolina Machado Marinho\n",
      "INFO:root:006 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Antonio Marcos Aires Barbosa\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Anya Pimentel Gomes Fernandes Vieira Meyer\n",
      "INFO:root:100 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Bruno Bezerra Carvalho\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Carla Freire Celedonio Fernandes\n",
      "INFO:root:041 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Carlos Jose Araujo Pinheiro\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Carlos Jose Araujo Pinheiro: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Claudia Stutz Zubieta\n",
      "INFO:root:003 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Charles Cerqueira De Abreu\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Charles Cerqueira De Abreu: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Clarice Gomes e Souza Dabés\n",
      "INFO:root:002 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Clarissa Romero Teixeira\n",
      "INFO:root:043 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Dayane Alves Costa\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Dayane Alves Costa: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Donat Alexander de Chapeaurouge\n",
      "INFO:root:043 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Eduardo Ruback dos Santos\n",
      "INFO:root:005 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ezequiel Valentim de Melo\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Fabio Miyajima\n",
      "INFO:root:064 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Fernando Braga Stehling Dias\n",
      "INFO:root:023 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Fernando Ferreira Carneiro\n",
      "INFO:root:062 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Galba Freire Moita\n",
      "INFO:root:011 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Giovanny Augusto Camacho Antevere Mazzarotto\n",
      "INFO:root:011 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Gilvan Pessoa Furtado\n",
      "INFO:root:024 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ivana Cristina de Holanda Cunha Barreto\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Ivanildo Lopes Farias\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Jaime Ribeiro Filho\n",
      "INFO:root:104 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: João Baptista Estabile Neto\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: João Hermínio Martins da Silva\n",
      "INFO:root:029 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: José Luís Passos Cordeiro\n",
      "INFO:root:042 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Kamila Matos de Albuquerque\n",
      "INFO:root:001 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luciana Coelho Serafim\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luciana Pereira Lindenmeyer\n",
      "INFO:root:001 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luciana Silvério Alleluia Higino da Silva\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Currículo não encontrado: Luciano Pinto Zorzanelli\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Luciano Pinto Zorzanelli: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luis Fernando Pessoa de Andrade\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Luiz Odorico Monteiro de Andrade\n",
      "INFO:root:084 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marcela Helena Gambim Fonseca\n",
      "INFO:root:017 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marcelo Jorge Lopes Coutinho\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marcos Roberto Lourenzoni\n",
      "INFO:root:026 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Márcio Flávio Moura de Araújo\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Margareth Borges Coutinho Gallo\n",
      "INFO:root:018 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Marlos de Medeiros Chaves\n",
      "INFO:root:007 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Maximiliano Loiola Ponte de Souza\n",
      "INFO:root:044 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Nilton Luiz Costa Machado\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Nilton Luiz Costa Machado: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Patricia Maria Ferreira da Silva\n",
      "ERROR:root:Sem respota antes do timeout\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Raphael Trevizani\n",
      "INFO:root:004 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Regis Bernardo Brandim Gomes\n",
      "INFO:root:038 artigos extraídos\n",
      "INFO:root:Currículo não encontrado: Renato Caldeira De Souza\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Renato Caldeira De Souza: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Roberto Nicolete\n",
      "INFO:root:056 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Roberto Wagner Júnior Freire de Freitas\n",
      "INFO:root:092 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Rodrigo Carvalho Nogueira\n",
      "ERROR:root:Erro inesperado ao extrair tooltips: Message: element not interactable: [object HTMLImageElement] has no size and location\n",
      "  (Session info: chrome=117.0.5938.150)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7F38978A2+54818]\n",
      "\t(No symbol) [0x00007FF7F3806AD2]\n",
      "\t(No symbol) [0x00007FF7F36BDA3B]\n",
      "\t(No symbol) [0x00007FF7F36C246D]\n",
      "\t(No symbol) [0x00007FF7F36C3F5F]\n",
      "\t(No symbol) [0x00007FF7F36C4050]\n",
      "\t(No symbol) [0x00007FF7F3703DB1]\n",
      "\t(No symbol) [0x00007FF7F37034C6]\n",
      "\t(No symbol) [0x00007FF7F373F958]\n",
      "\t(No symbol) [0x00007FF7F371EAAA]\n",
      "\t(No symbol) [0x00007FF7F37375A2]\n",
      "\t(No symbol) [0x00007FF7F371E883]\n",
      "\t(No symbol) [0x00007FF7F36F3691]\n",
      "\t(No symbol) [0x00007FF7F36F48D4]\n",
      "\tGetHandleVerifier [0x00007FF7F3BFB9A2+3610402]\n",
      "\tGetHandleVerifier [0x00007FF7F3C51870+3962352]\n",
      "\tGetHandleVerifier [0x00007FF7F3C49D5F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF7F3933656+693206]\n",
      "\t(No symbol) [0x00007FF7F3811638]\n",
      "\t(No symbol) [0x00007FF7F380D944]\n",
      "\t(No symbol) [0x00007FF7F380DA72]\n",
      "\t(No symbol) [0x00007FF7F37FE123]\n",
      "\tBaseThreadInitThunk [0x00007FFBA3187344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBA36426B1+33]\n",
      "\n",
      "INFO:root:Currículo não encontrado: Sergio Dos Santos Reis\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Vínculo não encontrado, passando para o próximo nome...\n",
      "ERROR:root:Falha ao clicar no link do nome.\n",
      "ERROR:root:Sem resposta antes do timeout para: Sergio Dos Santos Reis: Message: \n",
      "\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Sharmênia de Araújo Soares Nuto\n",
      "INFO:root:050 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Vanira Matos Pessoa\n",
      "INFO:root:029 artigos extraídos\n",
      "INFO:root:Vínculo encontrado no currículo de nome: Venúcia Bruna Magalhães Pereira\n",
      "INFO:root:006 artigos extraídos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('lattes_scraper.log', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 dicionários montados\n",
      " 0 002 002 Alice Paula Di Sabatino Guimarães \n",
      " 1 008 008 Ana Cláudia de Araújo Teixeira \n",
      " 2 002 002 Ana Camila Oliveira Alves \n",
      " 3 000 000 Angela Christina de Moraes Ostritz \n",
      " 4 023 023 Adriana Costa Bacelo \n",
      " 5 006 006 Anna Carolina Machado Marinho \n",
      " 6 002 000 Antonio Marcos Aires Barbosa \n",
      " 7 100 100 Anya Pimentel Gomes Fernandes Vieira Meyer \n",
      " 8 000 000 Bruno Bezerra Carvalho \n",
      " 9 041 041 Carla Freire Celedonio Fernandes \n",
      "10 003 003 Claudia Stutz Zubieta \n",
      "11 002 002 Clarice Gomes e Souza Dabés \n",
      "12 043 043 Clarissa Romero Teixeira \n",
      "13 043 043 Donat Alexander de Chapeaurouge \n",
      "14 005 005 Eduardo Ruback dos Santos \n",
      "15 000 000 Ezequiel Valentim de Melo \n",
      "16 064 064 Fabio Miyajima \n",
      "17 023 023 Fernando Braga Stehling Dias \n",
      "18 062 062 Fernando Ferreira Carneiro \n",
      "19 011 011 Galba Freire Moita \n",
      "20 011 011 Giovanny Augusto Camacho Antevere Mazzarotto \n",
      "21 024 024 Gilvan Pessoa Furtado \n",
      "22 072 000 Ivana Cristina de Holanda Cunha Barreto \n",
      "23 000 000 Ivanildo Lopes Farias \n",
      "24 104 104 Jaime Ribeiro Filho \n",
      "25 000 000 João Baptista Estabile Neto \n",
      "26 029 029 João Hermínio Martins da Silva \n",
      "27 042 042 José Luís Passos Cordeiro \n",
      "28 001 001 Kamila Matos de Albuquerque \n",
      "29 000 000 Luciana Coelho Serafim \n",
      "30 001 001 Luciana Pereira Lindenmeyer \n",
      "31 010 000 Luciana Silvério Alleluia Higino da Silva \n",
      "32 000 000 Luis Fernando Pessoa de Andrade \n",
      "33 084 084 Luiz Odorico Monteiro de Andrade \n",
      "34 017 017 Marcela Helena Gambim Fonseca \n",
      "35 000 000 Marcelo Jorge Lopes Coutinho \n",
      "36 026 026 Marcos Roberto Lourenzoni \n",
      "37 142 000 Márcio Flávio Moura de Araújo \n",
      "38 018 018 Margareth Borges Coutinho Gallo \n",
      "39 007 007 Marlos de Medeiros Chaves \n",
      "40 044 044 Maximiliano Loiola Ponte de Souza \n",
      "41 000 000 Patricia Maria Ferreira da Silva \n",
      "42 004 004 Raphael Trevizani \n",
      "43 038 038 Regis Bernardo Brandim Gomes \n",
      "44 056 056 Roberto Nicolete \n",
      "45 092 092 Roberto Wagner Júnior Freire de Freitas \n",
      "46 003 000 Rodrigo Carvalho Nogueira \n",
      "47 050 050 Sharmênia de Araújo Soares Nuto \n",
      "48 029 029 Vanira Matos Pessoa \n",
      "49 006 006 Venúcia Bruna Magalhães Pereira \n",
      "\n",
      "Total de artigos em todos períodos: 1350\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(dict_list)} dicionários montados')\n",
    "qte_artigos=0\n",
    "for k,i in enumerate(dict_list):\n",
    "    try:\n",
    "        qte_jcr = len(i['JCR'])\n",
    "    except:\n",
    "        qte_jcr = 0\n",
    "    try:\n",
    "       qte_jcr2 = len(i['JCR2'])\n",
    "    except:\n",
    "       qte_jcr2 = 0\n",
    "    qte_artigos+=qte_jcr\n",
    "    print(f\"{k:>2} {qte_jcr:>03} {qte_jcr2:>03} {i['name']} \")\n",
    "\n",
    "print(f'\\nTotal de artigos em todos períodos: {qte_artigos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converter Dicionários em Datasets JSON e HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Alice Paula Di Sabatino Guimarães', 'Áreas de atuação': {'1.': 'Grande área: Ciências Sociais Aplicadas / Área: Administração.', '2.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Biologia Molecular.', '3.': 'Grande área: Ciências Biológicas / Área: Biologia Geral.'}}\n",
      "{'name': 'Ana Cláudia de Araújo Teixeira', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Produção, Ambiente e Saúde.'}}\n",
      "{'name': 'Ana Camila Oliveira Alves', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Farmácia / Subárea: ANÁLISES CLÍNICAS.', '2.': 'Grande área: Ciências da Saúde / Área: Farmácia / Subárea: Farmacotécnica e tecnologia farmacêutica.', '3.': 'Grande área: Ciências da Saúde / Área: Farmácia / Subárea: Garantia e controle de qualidade farmacêuticos.'}}\n",
      "{'name': 'Angela Christina de Moraes Ostritz', 'Áreas de atuação': {'1.': 'Grande área: Ciências Humanas / Área: Educação.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Política, Planejamento e Gestão Pública.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Modelos Assistenciais na Rede SUS.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Elaboração de Projetos.', '5.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Implantação de Políticas Públicas.', '6.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Atenção/Internação Domiciliar.'}}\n",
      "{'name': 'Adriana Costa Bacelo', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: saúde digital.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Coletiva.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Nutrição Clinica aplicada a doenças infecciosas.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Atendimento ambulatorial de nutrição clinica com foco em má nutrição.'}}\n",
      "{'name': 'Anna Carolina Machado Marinho', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Química de Macromoléculas.', '2.': 'Grande área: Ciências Biológicas / Área: Biotecnologia / Subárea: Proteômica.', '3.': 'Grande área: Ciências da Saúde / Área: Farmácia / Subárea: Garantia e controle de qualidade farmacêuticos.', '4.': 'Grande área: Ciências Biológicas / Área: Biotecnologia / Subárea: Biologia Geral.', '5.': 'Grande área: Ciências Biológicas / Área: Biotecnologia / Subárea: BOAS PRÁTICAS DE FABRICAÇÃO - BPF.', '6.': 'Grande área: Ciências Biológicas / Área: Biotecnologia / Subárea: Farmácia.'}}\n",
      "{'name': 'Antonio Marcos Aires Barbosa', 'Áreas de atuação': {'1.': 'Grande área: Engenharias / Área: Engenharia de Produção / Subárea: Inovação.', '2.': 'Grande área: Engenharias / Área: Engenharia de Produção / Subárea: Propriedade Intelectual.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Transferência de Tecnologia.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Gestão de Processos.', '5.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Pública.', '6.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Engenharia de Produção.'}}\n",
      "{'name': 'Anya Pimentel Gomes Fernandes Vieira Meyer', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Pública.', '2.': 'Grande área: Ciências da Saúde / Área: Odontologia / Subárea: Odontopediatria.', '3.': 'Grande área: Ciências da Saúde / Área: Odontologia / Subárea: Biomateriais/Materiais Odontologicos.'}}\n",
      "{'name': 'Bruno Bezerra Carvalho', 'Áreas de atuação': {'1.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Sistemas de Computação/Especialidade: Gestão de Tecnologia da Informação e Comunicação.', '2.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação.', '3.': 'Grande área: Ciências Sociais Aplicadas / Área: Ciência da Informação.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Pública.'}}\n",
      "{'name': 'Carla Freire Celedonio Fernandes', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Farmacologia / Subárea: Farmacologia Bioquímica e Molecular.', '2.': 'Grande área: Ciências da Saúde / Área: Farmácia.', '3.': 'Grande área: Ciências Biológicas / Área: Bioquímica.'}}\n",
      "{'name': 'Claudia Stutz Zubieta', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Histologia.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Citologia e Biologia Celular.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Farmacologia Geral.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Biologia Molecular.'}}\n",
      "{'name': 'Clarice Gomes e Souza Dabés', 'Áreas de atuação': 'N/A'}\n",
      "{'name': 'Clarissa Romero Teixeira', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Imunologia.', '2.': 'Grande área: Ciências Biológicas / Área: Parasitologia.'}}\n",
      "{'name': 'Donat Alexander de Chapeaurouge', 'Áreas de atuação': 'N/A'}\n",
      "{'name': 'Eduardo Ruback dos Santos', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Biologia Geral / Subárea: Biotecnologia/Especialidade: Biotecnologia Aplicada.', '2.': 'Grande área: Ciências Biológicas / Área: Microbiologia / Subárea: Microbiologia e Imunoilogia/Especialidade: Microbiologia e Imunologia.', '3.': 'Grande área: Ciências Biológicas / Área: Morfologia / Subárea: Citologia e Biologia Celular.', '4.': 'Grande área: Ciências Biológicas / Área: Microbiologia / Subárea: Biologia e Fisiologia dos Microorganismos/Especialidade: Virologia.', '5.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Sistemas de Computação/Especialidade: Bioinformática.', '6.': 'Grande área: Ciências Biológicas / Área: Biologia Geral / Subárea: Biossegurança.'}}\n",
      "{'name': 'Ezequiel Valentim de Melo', 'Áreas de atuação': {}}\n",
      "{'name': 'Fabio Miyajima', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Epidemiologia Molecular.', '2.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Medicina Translacional.', '3.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Doenças Emergentes, Reemergentes e Negligenciadas.', '4.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Genética Humana e Médica.', '5.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Neuropsiquiatria.', '6.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Genética Molecular Forense.'}}\n",
      "{'name': 'Fernando Braga Stehling Dias', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Parasitologia.', '2.': 'Grande área: Ciências Biológicas / Área: Parasitologia / Subárea: Entomologia e Malacologia de Parasitos e Vetores.', '3.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Biologia Molecular.'}}\n",
      "{'name': 'Fernando Ferreira Carneiro', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde e Ambiente.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Coletiva.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde das Populações do Campo, Floresta e das Águas.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Ciências Ambientais.', '5.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Desenvolvimento Sustentável.'}}\n",
      "{'name': 'Galba Freire Moita', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Avaliação e Gestão de Saúde Pública.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Gestâo de Organizações de Saúde (Hospitais e Sistemas de Saúde).', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Organizações Públicas.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Gestão Organizacional.', '5.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Tecnologia e Inovação Cientifica.', '6.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Educação Profissional.'}}\n",
      "{'name': 'Giovanny Augusto Camacho Antevere Mazzarotto', 'Áreas de atuação': {'1.': 'Grande área: Ciências Agrárias / Área: Zootecnia / Subárea: Produção Animal/Especialidade: Instalações para Produção Animal.', '2.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Virologia.', '3.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: anestesiologia veterinária.', '4.': 'Grande área: Ciências Biológicas / Área: Zoologia / Subárea: Biologia de animais silvestres.', '5.': 'Grande área: Ciências Agrárias / Área: Medicina Veterinária / Subárea: Medicina veterinária de animais silvestres.', '6.': 'Grande área: Engenharias / Área: Engenharia Civil / Subárea: Instalação e monitoramento de áreas biocontidas de nível 2 e 3 de biossegurança.'}}\n",
      "{'name': 'Gilvan Pessoa Furtado', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Bioquímica.', '2.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Química de Macromoléculas.', '3.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Enzimologia.', '4.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Biologia Molecular.'}}\n",
      "{'name': 'Ivana Cristina de Holanda Cunha Barreto', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde da Família.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Pública.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Ensino-Aprendizagem.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Epidemiologia.', '5.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Materno-Infantil.', '6.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Digital.'}}\n",
      "{'name': 'Ivanildo Lopes Farias', 'Áreas de atuação': {'1.': 'Grande área: Engenharias / Área: Engenharia Mecânica.', '2.': 'Grande área: Engenharias / Área: Engenharia Mecânica / Subárea: Processos de Fabricação/Especialidade: soldagem.', '3.': 'Grande área: Engenharias / Área: Engenharia Mecânica / Subárea: Engenharia Térmica/Especialidade: Termodinâmica.', '4.': 'Grande área: Engenharias / Área: Engenharia Mecânica / Subárea: Fenômenos de Transporte/Especialidade: Mecânica dos Fluídos.', '5.': 'Grande área: Engenharias / Área: Engenharia Mecânica / Subárea: Projetos de Máquinas/Especialidade: Máquinas, Motores e Equipamentos.'}}\n",
      "{'name': 'Jaime Ribeiro Filho', 'Áreas de atuação': 'N/A'}\n",
      "{'name': 'João Baptista Estabile Neto', 'Áreas de atuação': {'1.': 'Grande área: Ciências Sociais Aplicadas / Área: Desenho Industrial / Subárea: Programação Visual.'}}\n",
      "{'name': 'João Hermínio Martins da Silva', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Biotecnologia / Subárea: Biotecnologia em Saúde Humana e Animal.', '2.': 'Grande área: Ciências Biológicas / Área: Biofísica / Subárea: Biofísica Molecular/Especialidade: Modelagem Molecular.', '3.': 'Grande área: Ciências Biológicas / Área: Biofísica / Subárea: Biofísica Molecular.', '4.': 'Grande área: Ciências Biológicas / Área: Imunologia.'}}\n",
      "{'name': 'José Luís Passos Cordeiro', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Ecologia.', '2.': 'Grande área: Ciências Biológicas / Área: Ecologia / Subárea: Ecologia de Paisagem.', '3.': 'Grande área: Ciências Exatas e da Terra / Área: Geociências / Subárea: Geografia Física/Especialidade: geoprocessamento.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Zoonoses.', '5.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Mastozoologia.', '6.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Modelos de distribuição.'}}\n",
      "{'name': 'Kamila Matos de Albuquerque', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Coletiva.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Atenção Básica.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Avaliação de programas e serviços de saúde.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Gestão do Trabalho e Educação na Saúde.', '5.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Planejamento e Gestão.'}}\n",
      "{'name': 'Luciana Coelho Serafim', 'Áreas de atuação': {'1.': 'Grande área: Ciências Agrárias / Área: Ciência e Tecnologia de Alimentos.'}}\n",
      "{'name': 'Luciana Pereira Lindenmeyer', 'Áreas de atuação': {'1.': 'Grande área: Ciências Sociais Aplicadas / Área: Serviço Social / Subárea: Recursos Humanos/Especialidade: Desenvolvimento de Recursos Humanos.', '2.': 'Grande área: Ciências Sociais Aplicadas / Área: Serviço Social / Subárea: Recursos Humanos/Especialidade: Qualidade de Vida no Trabalho.', '3.': 'Grande área: Ciências Sociais Aplicadas / Área: Administração / Subárea: Administração Pública/Especialidade: Administração e Desenvolvimento de Pessoal.'}}\n",
      "{'name': 'Luciana Silvério Alleluia Higino da Silva', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Enfermagem / Subárea: Enfermagem em Saúde Mental.', '2.': 'Grande área: Ciências da Saúde / Área: Enfermagem / Subárea: Enfermagem em Saúde Mental.', '3.': 'Grande área: Ciências da Saúde / Área: Enfermagem / Subárea: Enfermagem em Saúde Coletiva.', '4.': 'Grande área: Ciências da Saúde / Área: Enfermagem / Subárea: Enfermagem em Doenças Emergentes, Reemergentes e Negligenciadas.', '5.': 'Grande área: Ciências da Saúde / Área: Enfermagem.', '6.': 'Grande área: Ciências Humanas / Área: Educação / Subárea: Docente em enfermgem.'}}\n",
      "{'name': 'Luis Fernando Pessoa de Andrade', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Pública.', '2.': 'Grande área: Ciências Humanas / Área: Psicologia / Subárea: Psicologia do Ensino e da Aprendizagem/Especialidade: Planejamento Institucional.', '3.': 'Grande área: Ciências Sociais Aplicadas / Área: Administração / Subárea: Administração Pública/Especialidade: Política e Planejamento Governamentais.', '4.': 'Grande área: Engenharias / Área: Engenharia de Produção / Subárea: Engenharia Econômica/Especialidade: Avaliação de Projetos.', '5.': 'Grande área: Ciências Sociais Aplicadas / Área: Economia / Subárea: Economia Industrial/Especialidade: Gestão Tecnológica e da Inovação.', '6.': 'Grande área: Ciências Sociais Aplicadas / Área: Administração / Subárea: Responsabilidade Social Corporativa.'}}\n",
      "{'name': 'Luiz Odorico Monteiro de Andrade', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Saúde Coletiva/Especialidade: Saúde Pública.', '2.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Saúde Coletiva/Especialidade: Saúde Coletiva.', '3.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Saúde Materno-Infantil.'}}\n",
      "{'name': 'Marcela Helena Gambim Fonseca', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Imunologia.'}}\n",
      "{'name': 'Marcelo Jorge Lopes Coutinho', 'Áreas de atuação': 'N/A'}\n",
      "{'name': 'Marcos Roberto Lourenzoni', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Biotecnologia.', '2.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Biofísica Molecular.', '3.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Proteínas.', '4.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Modelagem de Sistemas Biológicos.', '5.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Farmacologia Bioquímica e Molecular.', '6.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Desenvolvimento de Enzimas para biorrefinarias.'}}\n",
      "{'name': 'Márcio Flávio Moura de Araújo', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Enfermagem / Subárea: Enfermagem em Saúde Coletiva/Especialidade: Diabetes Mellitus.', '2.': 'Grande área: Ciências da Saúde / Área: Nutrição / Subárea: Nutrição e alimentação humana.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Pesquisa clínica.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Condições crônicas não transmissíveis.', '5.': 'Grande área: Ciências da Saúde / Área: Enfermagem / Subárea: Condições crônicas transmissíveis.'}}\n",
      "{'name': 'Margareth Borges Coutinho Gallo', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: metabolômica.', '2.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Proteômica.', '3.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Ciências Ambientais.', '4.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: validação e qualificação (processos, métodos e equipamentos).', '5.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Garantia e controle de qualidade farmacêuticos.', '6.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Probabilidade e Estatística Aplicadas.'}}\n",
      "{'name': 'Marlos de Medeiros Chaves', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Biotecnologia / Subárea: Biotecnologia Ambiental e Recursos Naturais.', '2.': 'Grande área: Ciências Biológicas / Área: Biotecnologia / Subárea: Biotecnologia em Saúde Humana e Animal.', '3.': 'Grande área: Ciências Biológicas / Área: Parasitologia.'}}\n",
      "{'name': 'Maximiliano Loiola Ponte de Souza', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Medicina / Subárea: Psiquiatria/Especialidade: Psiquiatria Transcultural.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva.'}}\n",
      "{'name': 'Patricia Maria Ferreira da Silva', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Pública/Especialidade: Políticas Públicas de Saúde.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva.', '3.': 'Grande área: Ciências Humanas / Área: Psicologia / Subárea: Psicologia do Desenvolvimento Humano/Especialidade: Desenvolvimento Social e da Personalidade.', '4.': 'Grande área: Ciências Humanas / Área: Psicologia / Subárea: Tratamento e Prevenção Psicológica/Especialidade: Programas de Atendimento Comunitário.', '5.': 'Grande área: Ciências Humanas / Área: Psicologia / Subárea: Tratamento e Prevenção Psicológica/Especialidade: Intervenção Terapêutica.'}}\n",
      "{'name': 'Raphael Trevizani', 'Áreas de atuação': {'1.': 'Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Metodologia e Técnicas da Computação/Especialidade: Modelagem Computacional.', '2.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Química de Macromoléculas/Especialidade: Proteínas.'}}\n",
      "{'name': 'Regis Bernardo Brandim Gomes', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Imunologia / Subárea: Imunoparasitologia.'}}\n",
      "{'name': 'Roberto Nicolete', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Imunologia.', '2.': 'Grande área: Ciências da Saúde / Área: Farmácia.', '3.': 'Grande área: Ciências da Saúde / Área: Farmácia / Subárea: Farmacotécnica e tecnologia farmacêutica.', '4.': 'Grande área: Ciências Biológicas / Área: Imunologia / Subárea: Imunofarmacologia.'}}\n",
      "{'name': 'Roberto Wagner Júnior Freire de Freitas', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Enfermagem.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Pública.'}}\n",
      "{'name': 'Rodrigo Carvalho Nogueira', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Odontologia.', '2.': 'Grande área: Ciências da Saúde / Área: Odontologia / Subárea: Odontologia Social e Preventiva.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Pública.'}}\n",
      "{'name': 'Sharmênia de Araújo Soares Nuto', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva.', '2.': 'Grande área: Ciências da Saúde / Área: Odontologia.'}}\n",
      "{'name': 'Vanira Matos Pessoa', 'Áreas de atuação': {'1.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva.', '2.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Pública/Especialidade: Atenção Primária à Saúde.', '3.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde da Família.', '4.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde do Trabalhador.', '5.': 'Grande área: Ciências da Saúde / Área: Saúde Coletiva / Subárea: Saúde Ambiental.', '6.': 'Grande área: Ciências da Saúde / Área: Enfermagem / Subárea: Enfermagem em Saúde Coletiva.'}}\n",
      "{'name': 'Venúcia Bruna Magalhães Pereira', 'Áreas de atuação': {'1.': 'Grande área: Ciências Biológicas / Área: Farmacologia.', '2.': 'Grande área: Ciências Biológicas / Área: Imunologia.', '3.': 'Grande área: Ciências Biológicas / Área: Bioquímica / Subárea: Biologia Molecular.', '4.': 'Grande área: Ciências da Saúde / Área: Farmácia / Subárea: Farmácia Hospitalar.', '5.': 'Grande área: Ciências Biológicas / Área: Farmacologia / Subárea: Toxicologia.'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Caminhos onde os arquivos JSON e HDF5 serão armazenados\n",
    "folder = './../data/'\n",
    "json_filename = folder+\"processed_data.json\"\n",
    "hdf5_filename = folder+\"processed_data.hdf5\"\n",
    "\n",
    "# Processamento da lista completa e armazenamento dos dados processados\n",
    "with ParseSoup(driver) as parse_soup_instance:\n",
    "    processed_data = parse_soup_instance.process_all_results(dict_list, json_filename, hdf5_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exibir conteúdo dos JSONs\n",
    "# converter = DictToHDF5(dict_list)\n",
    "# directory=\"./../data/\"\n",
    "# converter.print_json_structure(directory+\"processed_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exibir conteúdo dos HDF5\n",
    "# converter = DictToHDF5(dict_list)\n",
    "# directory=\"./../data/\"\n",
    "# converter.print_hdf5_structure(directory+\"processed_data.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistir dicionários no Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe persistir classificaçaõ do CNPq no Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "import logging\n",
    "\n",
    "# Configuration for logging operations\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filename='database_operations.log',\n",
    "                    filemode='w')\n",
    "\n",
    "class CNPqClassifier:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    # Methods for node creation\n",
    "    def _merge_nodes(self, node_type, code, name):\n",
    "        with self._driver.session() as session:\n",
    "            session.run(f\"MERGE (n:{node_type} {{code: $code, name: $name}})\", code=code, name=name)\n",
    "\n",
    "    def _create_relationship(self, parent_type, parent_code, child_type, child_code, relation_name):\n",
    "        with self._driver.session() as session:\n",
    "            session.run(f\"\"\"\n",
    "                MATCH (p:`{parent_type}` {{code: $parent_code}})\n",
    "                MATCH (c:`{child_type}` {{code: $child_code}})\n",
    "                MERGE (p)-[:{relation_name}]->(c)\n",
    "            \"\"\", parent_code=parent_code, child_code=child_code)\n",
    "\n",
    "    def process(self, classifications):\n",
    "        # First, create nodes for each classification\n",
    "        for code, name in classifications:\n",
    "            # Remove the verification digit\n",
    "            code = code.rsplit('-', 1)[0]\n",
    "            \n",
    "            # Define regex patterns\n",
    "            grande_area_pattern   = r\"^\\d\\.00\\.00\\.00$\"\n",
    "            area_pattern          = r\"^\\d\\.\\d{2}\\.00\\.00$\"  \n",
    "            subarea_pattern       = r\"^\\d\\.\\d{2}\\.\\d{2}\\.00$\"  \n",
    "            especialidade_pattern = r\"^\\d\\.\\d{2}\\.\\d{2}\\.\\d{2}$\"\n",
    "\n",
    "            if re.match(grande_area_pattern, code):\n",
    "                self._merge_nodes('GrandeÁrea', code, name)\n",
    "            elif re.match(area_pattern, code):\n",
    "                self._merge_nodes('Área', code, name)\n",
    "            elif re.match(subarea_pattern, code):\n",
    "                self._merge_nodes('Subárea', code, name)\n",
    "            elif re.match(especialidade_pattern, code):\n",
    "                self._merge_nodes('Especialidade', code, name)\n",
    "            else:\n",
    "                logging.error(f\"Invalid code format: {code}\")\n",
    "                raise ValueError(f\"Invalid code format: {code}\")\n",
    "\n",
    "        # Next, establish relationships\n",
    "        for code, _ in classifications:\n",
    "            # Remove the verification digit\n",
    "            code = code.rsplit('-', 1)[0]\n",
    "\n",
    "            # For GrandeÁrea to Área\n",
    "            if re.match(area_pattern, code):\n",
    "                match = re.match(r\"^(\\d)\\.\\d{2}\\.00\\.00$\", code)\n",
    "                if match:\n",
    "                    parent_code = match.group(1) + \".00.00.00\"\n",
    "                    self._create_relationship('GrandeÁrea', parent_code, 'Área', code, 'CONTÉM_ÁREA')\n",
    "                else:\n",
    "                    logging.error(f\"Unexpected code format for Área: {code}\")\n",
    "                    raise ValueError(f\"Unexpected code format for Área: {code}\")\n",
    "\n",
    "            # For Área to Subárea\n",
    "            elif re.match(subarea_pattern, code):\n",
    "                match = re.match(r\"(\\d\\.\\d{2})\\.\\d{2}\\.00$\", code)\n",
    "                if match:\n",
    "                    parent_code = match.group(1) + \".00.00\"\n",
    "                    self._create_relationship('Área', parent_code, 'Subárea', code, 'CONTÉM_SUBÁREA')\n",
    "                else:\n",
    "                    logging.error(f\"Unexpected code format for Subárea: {code}\")\n",
    "                    raise ValueError(f\"Unexpected code format for Subárea: {code}\")\n",
    "\n",
    "            # For Subárea to Especialidade\n",
    "            elif re.match(especialidade_pattern, code):\n",
    "                match = re.match(r\"(\\d\\.\\d{2}\\.\\d{2})\\.\\d{2}$\", code)\n",
    "                if match:\n",
    "                    parent_code = match.group(1) + \".00\"\n",
    "                    self._create_relationship('Subárea', parent_code, 'Especialidade', code, 'CONTÉM_ESPECIALIDADE')\n",
    "                else:\n",
    "                    logging.error(f\"Unexpected code format for Especialidade: {code}\")\n",
    "                    raise ValueError(f\"Unexpected code format for Especialidade: {code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes para criar nós secundários de propriedades no Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JcrHandler:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def consultar_nome(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            query = \"MATCH (p:Person) WHERE p.name = $name RETURN p.name AS nome\"\n",
    "            results = session.run(query, name=name)\n",
    "            for record in results:\n",
    "                print(record['nome'])\n",
    "\n",
    "    def consultar_propriedade(self, check_property_query, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(check_property_query, name=name)\n",
    "            if result:\n",
    "                print(\"Consulta retornou um resultado\")\n",
    "                count = result.single()[0]\n",
    "                print(f\"Número de registros encontrados com o nome '{name}': {count}\")\n",
    "                return count\n",
    "            else:\n",
    "                print(\"Consulta não retornou nenhum resultado\")\n",
    "\n",
    "    def format_string(input_str):\n",
    "        # Verifica se a entrada é uma string de oito dígitos\n",
    "        if input_str and len(input_str) == 8:\n",
    "            # Divide a string em duas partes\n",
    "            part1 = input_str[:4]\n",
    "            part2 = input_str[4:]\n",
    "            \n",
    "            # Concatena as duas partes com um hífen\n",
    "            formatted_str = f\"{part1}-{part2}\"\n",
    "            \n",
    "            return formatted_str\n",
    "        else:\n",
    "            return input_str\n",
    "    \n",
    "    def consultar_propriedades_jcr(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            query = \"MATCH (p:Person {name: $name}) RETURN p.JCR2 AS jcr\"\n",
    "            result = session.run(query, name=name)\n",
    "            # result = session.run(query, name=name.encode('utf-8'))\n",
    "            \n",
    "            result_data = result.single()\n",
    "            if result_data:\n",
    "                print(f'Processando resultado de {len(result_data)} query...')\n",
    "                jcr_data = result_data[\"jcr\"]\n",
    "                jcr_properties_list = json.loads(jcr_data)\n",
    "                return jcr_properties_list\n",
    "            else:\n",
    "                print(f\"Não foram encontrados dados JCR2 para {name}.\")\n",
    "                return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_list_to_dict(lst):\n",
    "        \"\"\"\n",
    "        Converts a list into a dictionary with indices as keys.\n",
    "        \n",
    "        Parameters:\n",
    "        - lst: list, input list to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: Transformed dictionary.\n",
    "        \"\"\"\n",
    "        return {str(i): item for i, item in enumerate(lst)}\n",
    "    \n",
    "    def create_person_with_jcr(self, name, jcr_properties):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._create_person_with_jcr, name, jcr_properties)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_person_with_jcr(tx, name, jcr_properties):\n",
    "        # Cria o nó Person\n",
    "        person_query = (\n",
    "            \"CREATE (p:Person {name: $name}) \"\n",
    "            \"RETURN p\"\n",
    "        )\n",
    "        person_result = tx.run(person_query, name=name)\n",
    "        person_node = person_result.single()[0]\n",
    "\n",
    "        # Cria os nós secundários para cada valor único de issn\n",
    "        issn_values = set(prop.get(\"issn\") for prop in jcr_properties)\n",
    "        for issn in issn_values:\n",
    "            # print(f'ISSN formatado a persistir: {issn}')\n",
    "            if issn:\n",
    "                secondary_node_query = (\n",
    "                    \"CREATE (s:SecondaryNode {issn: $issn}) \"\n",
    "                    \"RETURN s\"\n",
    "                )\n",
    "                tx.run(secondary_node_query, issn=issn)\n",
    "\n",
    "                # Cria a relação entre o nó Person e o nó secundário\n",
    "                relation_query = (\n",
    "                    \"MATCH (p:Person {name: $name}), (s:SecondaryNode {issn: $issn}) \"\n",
    "                    \"CREATE (p)-[:HAS_JCR]->(s)\"\n",
    "                )\n",
    "                tx.run(relation_query, name=name, issn=issn)\n",
    "\n",
    "    def createJournalsNodes(self, name):\n",
    "        # Get JCR properties\n",
    "        jcr_properties = self.consultar_propriedades_jcr(name)\n",
    "\n",
    "        if jcr_properties is None:\n",
    "            print(f\"  ERRO!! Ao executar função createJournalsNodes(name):\")\n",
    "            print(f\"  consultar_propriedades_jcr({name}) retournou valor 'None'\")\n",
    "            return\n",
    "        \n",
    "        # Convert the serialized JSON strings back into dictionaries\n",
    "        deserialized_jcr_properties = [json.loads(prop) for prop in jcr_properties.values()]\n",
    "\n",
    "        # Inform the user about the total number of JCR property entries\n",
    "        total_entries = len(deserialized_jcr_properties)\n",
    "        print(f\"Read {total_entries} entries from JCR properties of Person '{name}'.\")\n",
    "\n",
    "        # Extract relevant journal properties and their count\n",
    "        journal_counts = Counter(prop.get(\"issn\") for prop in deserialized_jcr_properties)\n",
    "        \n",
    "        # Number of unique ISSNs\n",
    "        unique_issns = len(journal_counts)\n",
    "        print(f\"Identified {unique_issns} unique ISSN values.\")\n",
    "\n",
    "        null_count = journal_counts.pop(None, 0)  # Remove None (null) ISSN and get its count\n",
    "        null_count += journal_counts.pop(\"NULL\", 0)  # Also account for \"NULL\" as a string\n",
    "\n",
    "        # Counters for journals\n",
    "        successful_journal_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for issn, count in journal_counts.items():\n",
    "                if issn and issn != \"NULL\":\n",
    "                    issn = format_string(issn)\n",
    "                    # print(f'ISSN formatado a persistir: {issn} de tipo {type(issn)}')\n",
    "                    try:\n",
    "                        representative_entry = next(prop for prop in deserialized_jcr_properties if prop.get(\"data-issn\") == issn)\n",
    "                    except:\n",
    "                        continue\n",
    "                    journal_name = representative_entry.get(\"original_title\")\n",
    "                    fator_impacto = representative_entry.get(\"impact-factor\")\n",
    "                    jcr_year = representative_entry.get(\"jcr-ano\")\n",
    "\n",
    "                    # Create or merge the Journal node\n",
    "                    journal_node_query = (\n",
    "                        \"MERGE (j:Revistas {ISSN: $issn}) \"\n",
    "                        \"ON CREATE SET j.name = $journal_name, j.FatorImpacto = $impact_factor, j.JCRYear = $jcr_year \"  # Corrected this line\n",
    "                        \"RETURN j\"\n",
    "                    )\n",
    "                    session.run(journal_node_query, issn=issn, journal_name=journal_name, impact_factor=fator_impacto, jcr_year=jcr_year)  # And this line\n",
    "\n",
    "                    # Create or update the \"PUBLICOU_EM\" relationship\n",
    "                    relation_query = (\n",
    "                        \"MATCH (p:Person {name: $name}), (j:Revistas {ISSN: $issn}) \"\n",
    "                        \"MERGE (p)-[r:PUBLICOU_EM]->(j) \"\n",
    "                        \"ON CREATE SET r.QuantidadePublicações = $count \"\n",
    "                        \"ON MATCH SET r.QuantidadePublicações = r.QuantidadePublicações + $count\"\n",
    "                    )\n",
    "                    session.run(relation_query, name=name, issn=issn, count=count)\n",
    "                    \n",
    "                    successful_journal_creations += 1\n",
    "                \n",
    "                if null_count:\n",
    "                    pass\n",
    "        \n",
    "        # Inform the user about journals\n",
    "        print(f\"{successful_journal_creations} Revistas adicionadas com sucesso.\")\n",
    "        print(f\"{null_count} Revistas não foram criadas por terem valor NULL de ISSN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teste buscar nome com acentuação gráfica (funcionou)\n",
    "# journals_analysis_obj = JcrHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "# journals_analysis_obj.consultar_nome(\"João Hermínio Martins da Silva\")\n",
    "# journals_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teste procurar por Propriedades (falhou)\n",
    "# print(name)\n",
    "# journals_analysis_obj = JcrHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "# # check_property_query = \"MATCH (p:Person {name: $name}) RETURN properties(p)\"\n",
    "# check_property_query = \"MATCH (p:Person) WHERE p.name =~ '(?i)^João Hermínio Martins da Silva$' RETURN count(p)\"\n",
    "# result = journals_analysis_obj.consultar_propriedade(check_property_query, name=name)\n",
    "# journals_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import urllib.parse\n",
    "import json\n",
    "import re\n",
    "\n",
    "class AdvisorHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        if isinstance(input_data, dict):\n",
    "            for key, value in input_data.items():\n",
    "                if isinstance(value, dict):  \n",
    "                    input_data[key] = json.dumps(AdvisorHandler.convert_to_primitives(value), ensure_ascii=False)\n",
    "                else:\n",
    "                    input_data[key] = AdvisorHandler.convert_to_primitives(value)\n",
    "            return input_data\n",
    "        elif isinstance(input_data, list):\n",
    "            return [AdvisorHandler.convert_to_primitives(item) for item in input_data]\n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return AdvisorHandler.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise     \n",
    "            \n",
    "    def consult_orientacoes(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            # Query to fetch data from the Neo4j database\n",
    "            query = (\n",
    "                \"MATCH (p:Person {name: $name})\"\n",
    "                \"RETURN p.Orientações AS orientacoes_data\"\n",
    "            )\n",
    "            try:\n",
    "                neo4j_result = session.run(query, name=name)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERRO!! Ao executar a query para Person '{name}':\\n {e}\")\n",
    "                return None\n",
    "            \n",
    "            # Fetch a single result. If no result, return an appropriate message\n",
    "            try:\n",
    "                single_result = neo4j_result.single()\n",
    "                print(f'Processando resultado de {len(single_result)} query...')\n",
    "                if not single_result:\n",
    "                    print(f\"  ERRO!! Ao consultar 'Orientações' em Person '{name}'.\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"  ERRO!! Ao rodar consult_orientacoes() 'Orientações' em Person '{name}':\\n. {e}\")\n",
    "            \n",
    "            orientacoes_data = single_result[\"orientacoes_data\"]\n",
    "            if orientacoes_data is None:\n",
    "                print(f\"O atributo 'Orientações' está vazio para Person '{name}'\")\n",
    "                return None\n",
    "            \n",
    "            # Convert the serialized JSON string back into a dictionary\n",
    "            try:\n",
    "                orientacoes_properties_list = json.loads(orientacoes_data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"  ERRO!! Ao decodificar JSON em Person '{name}'\")\n",
    "                return None\n",
    "\n",
    "            return orientacoes_properties_list\n",
    "\n",
    "    def create_advisor_relations(self, name):\n",
    "        # Consultar propriedade Orientações\n",
    "        orient_properties = self.consult_orientacoes(name)\n",
    "\n",
    "        # Se orient_properties for None, retorne da função\n",
    "        if orient_properties is None:\n",
    "            print(f\"  Não foi encontrada propriedade 'Orientações' para Person '{name}'.\")\n",
    "            return\n",
    "\n",
    "        # Converter strings JSON serializadas de volta em dicionários\n",
    "        try:\n",
    "            deserialized_orient_properties = self.debug_and_convert(orient_properties)\n",
    "        except Exception as e:\n",
    "            print(f\"  ERRO!! Ao deserializar propriedade Orientações:/n {e}\")\n",
    "            return\n",
    "\n",
    "        # Advisory relationship mapping\n",
    "        advisory_types = {\n",
    "            \"Dissertação de mestrado\": \"ORIENTOU_MESTRADO\",\n",
    "            \"Tese de doutorado\": \"ORIENTOU_DOUTORADO\",\n",
    "            \"Trabalho de conclusão de curso de graduação\": \"ORIENTOU_GRADUAÇÃO\"\n",
    "        }\n",
    "\n",
    "        successful_advisory_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for orientacao_category, advisories in deserialized_orient_properties.items():\n",
    "                if isinstance(advisories, str):\n",
    "                    try:\n",
    "                        advisories = json.loads(advisories)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"  Falha ao deserializar JSON em 'Orientações' para '{orientacao_category}': {advisories}\")\n",
    "                        continue  # Se não for possível decodificar, vá para a próxima iteração\n",
    "\n",
    "                if not isinstance(advisories, dict):\n",
    "                    print(f\"  Tipo indevido de categorias em 'Orientações/n' '{orientacao_category}': {advisories}\")\n",
    "                    continue\n",
    "\n",
    "                for advisory_type, relationships in advisories.items():\n",
    "                    relation_label = advisory_types.get(advisory_type)\n",
    "                    if not relation_label:\n",
    "                        continue  # skip if the advisory type is not one of the specified ones\n",
    "\n",
    "                    for _, detail in json.loads(relationships).items():\n",
    "                        try:\n",
    "                            student_name = detail.split(\".\")[0]\n",
    "                            title = detail.split(\".\")[1]\n",
    "                            \n",
    "                            # Extract the year from the detail string\n",
    "                            year_match = re.search(r'(\\d{4})', detail)\n",
    "                            year = year_match.group(1) if year_match else None\n",
    "\n",
    "                            # Create or merge the Orientações node\n",
    "                            node_query = (\n",
    "                                \"MERGE (a:Orientações {Title: $title}) \"\n",
    "                                \"ON CREATE SET a.StudentName = $student_name, a.Tipo = $advisory_type, a.Year = $year \"\n",
    "                                \"ON MATCH SET a.Tipo = $advisory_type, a.Year = $year \"\n",
    "                                \"RETURN a\"\n",
    "                            )\n",
    "                            session.run(node_query, title=title, student_name=student_name, advisory_type=advisory_type, year=year)\n",
    "\n",
    "                            # Create or update the advisory relationship\n",
    "                            relation_query = (\n",
    "                                f\"MATCH (p:Person {{name: $name}}), (a:Orientações {{Title: $title}}) \"\n",
    "                                f\"MERGE (p)-[r:{relation_label}]->(a) \"\n",
    "                            )\n",
    "                            session.run(relation_query, name=name, title=title)\n",
    "\n",
    "                            successful_advisory_creations += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"  ERRO!! Ao processar orientações '{detail}': {e}\")\n",
    "\n",
    "        # Inform the user about advisories\n",
    "        print(f\"{successful_advisory_creations} orientações atualizadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_areas(area_detail):\n",
    "    fields = ['Grande área', 'Área', 'Subárea', 'Especialidade']\n",
    "    extracted = {}\n",
    "    \n",
    "    for field in fields:\n",
    "        match = re.search(f\"{field}: (.*?)(?:/|$)\", area_detail)\n",
    "        if match:\n",
    "            extracted[field] = match.group(1).strip()\n",
    "            \n",
    "    return extracted\n",
    "\n",
    "# Example usage\n",
    "area_details = [\n",
    "    \"Grande área: Ciências Biológicas / Área: Biotecnologia / Subárea: Biotecnologia em Saúde Humana e Animal.\",\n",
    "    \"Grande área: Ciências Biológicas / Área: Biofísica / Subárea: Biofísica Molecular/Especialidade: Modelagem Molecular.\",\n",
    "    \"Grande área: Ciências Biológicas / Área: Biofísica / Subárea: Biofísica Molecular.\",\n",
    "    \"Grande área: Ciências Biológicas / Área: Imunologia.\"\n",
    "]\n",
    "\n",
    "for detail in area_details:\n",
    "    print(extract_areas(detail))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# from neo4j import GraphDatabase\n",
    "\n",
    "# class AreasHandler:\n",
    "\n",
    "#     def __init__(self, uri, user, password):\n",
    "#         self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "#     def close(self):\n",
    "#         self._driver.close()\n",
    "        \n",
    "#     def consult_areas_atuacao(self, name):\n",
    "#         with self._driver.session() as session:\n",
    "#             result = session.run(\"MATCH (p:Person {name: $name}) RETURN p.`Áreas de atuação` as areas_atuacao\", name=name)\n",
    "#             record = result.single()\n",
    "#             if record:\n",
    "#                 return record['areas_atuacao']\n",
    "#             return None\n",
    "\n",
    "#     def debug_and_convert(self, areas_str):\n",
    "#         try:\n",
    "#             return json.loads(areas_str)\n",
    "#         except json.JSONDecodeError:\n",
    "#             print(f\"Failed to deserialize JSON string: {areas_str}\")\n",
    "#             return None\n",
    "\n",
    "#     def extract_subarea(self, area_detail):\n",
    "#         # Extract the 'Subárea' content from the area detail\n",
    "#         match = re.search(r'Subárea: ([^/]+)', area_detail)\n",
    "#         if match:\n",
    "#             return match.group(1).strip()\n",
    "#         return None\n",
    "\n",
    "#     def extract_areas(self, area_detail):\n",
    "#         # Extract the 'Grande Área', 'Área', and 'Subárea' contents from the area detail\n",
    "#         grande_area_match = re.search(r'Grande área: ([^/]+)', area_detail)\n",
    "#         area_match = re.search(r'Área: ([^/]+)', area_detail)\n",
    "#         subarea_match = re.search(r'Subárea: ([^/]+)', area_detail)\n",
    "        \n",
    "#         grande_area = grande_area_match.group(1).strip() if grande_area_match else None\n",
    "#         area = area_match.group(1).strip() if area_match else None\n",
    "#         subarea = subarea_match.group(1).strip() if subarea_match else None\n",
    "        \n",
    "#         return grande_area, area, subarea\n",
    "\n",
    "#     def create_areas_relations(self, name):\n",
    "#         # Get 'Áreas de atuação' properties\n",
    "#         areas_properties = self.consult_areas_atuacao(name)\n",
    "\n",
    "#         # Convert the serialized JSON strings back into dictionaries\n",
    "#         try:\n",
    "#             deserialized_areas_properties = self.debug_and_convert(areas_properties)\n",
    "#         except Exception as e:\n",
    "#             print(f\"  ERRO!! Ao deserializar propriedade 'Áreas de atuação':\\n {e}\")\n",
    "#             return\n",
    "\n",
    "#         successful_areas_creations = 0\n",
    "\n",
    "#         with self._driver.session() as session:\n",
    "#             for _, area_detail in deserialized_areas_properties.items():\n",
    "#                 try:\n",
    "#                     # Extracting Grande Área, Área, and Subárea from the details\n",
    "#                     match = re.match(r'Grande área: (.*?) / Área: (.*?) / Subárea: (.*?)(?:/Especialidade: (.*?))?\\.?$', area_detail)\n",
    "#                     if not match:\n",
    "#                         print(f\"Unexpected format for 'Áreas de atuação' detail: {area_detail}\")\n",
    "#                         continue\n",
    "#                     grande_area, area, subarea = match.groups()[:3]\n",
    "\n",
    "#                     # Creating or merging nodes for Subárea, Área, and Grande Área\n",
    "#                     session.run(\"MERGE (s:Subárea {name: $subarea})\", subarea=subarea)\n",
    "#                     session.run(\"MERGE (a:Área {name: $area})\", area=area)\n",
    "#                     session.run(\"MERGE (ga:GrandeÁrea {name: $grande_area})\", grande_area=grande_area)\n",
    "\n",
    "#                     # Creating or merging relationships. Using MERGE ensures no duplicate relationships are created.\n",
    "#                     session.run(\"MATCH (p:Person {name: $name}), (s:Subárea {name: $subarea}) MERGE (p)-[r:ATUA_EM]->(s)\", name=name, subarea=subarea)\n",
    "#                     session.run(\"MATCH (ga:GrandeÁrea {name: $grande_area}), (a:Área {name: $area}) MERGE (ga)-[r:CONTÉM_ÁREA]->(a)\", grande_area=grande_area, area=area)\n",
    "#                     session.run(\"MATCH (a:Área {name: $area}), (s:Subárea {name: $subarea}) MERGE (a)-[r:CONTÉM_SUBÁREA]->(s)\", area=area, subarea=subarea)\n",
    "\n",
    "#                     successful_areas_creations += 1\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing 'Áreas de atuação' detail '{area_detail}': {e}\")\n",
    "\n",
    "#             # Inform the user about areas\n",
    "#             print(f\"{successful_areas_creations} 'Áreas de atuação' relations successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class AreasHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "        \n",
    "    def consult_areas_atuacao(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"MATCH (p:Person {name: $name}) RETURN p.`Áreas de atuação` as areas_atuacao\", name=name)\n",
    "            record = result.single()\n",
    "            if record:\n",
    "                return record['areas_atuacao']\n",
    "            return None\n",
    "\n",
    "    def debug_and_convert(self, areas_str):\n",
    "        try:\n",
    "            return json.loads(areas_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to deserialize JSON string: {areas_str}\")\n",
    "            return None\n",
    "\n",
    "    def extract_areas(self, area_detail):\n",
    "        fields = ['Grande área', 'Área', 'Subárea', 'Especialidade']\n",
    "        extracted = {}\n",
    "        \n",
    "        for field in fields:\n",
    "            match = re.search(f\"{field}: (.*?)(?:/|$)\", area_detail)\n",
    "            if match:\n",
    "                extracted[field] = match.group(1).strip().rstrip('.')\n",
    "                \n",
    "        return extracted\n",
    "\n",
    "    def create_areas_relations(self, name):\n",
    "        # Get 'Áreas de atuação' properties\n",
    "        areas_properties = self.consult_areas_atuacao(name)\n",
    "        \n",
    "        # Convert the serialized JSON strings back into dictionaries\n",
    "        try:\n",
    "            deserialized_areas_properties = self.debug_and_convert(areas_properties)\n",
    "        except Exception as e:\n",
    "            print(f\"  ERRO!! Ao deserializar propriedade 'Áreas de atuação':\\n {e}\")\n",
    "            return\n",
    "\n",
    "        successful_areas_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for _, area_detail in deserialized_areas_properties.items():\n",
    "                try:\n",
    "                    # Extracting fields from the details using the extract_areas function\n",
    "                    extracted = self.extract_areas(area_detail)\n",
    "                    \n",
    "                    # Skip if no information is extracted\n",
    "                    if not extracted:\n",
    "                        continue\n",
    "\n",
    "                    last_node = None\n",
    "                    last_label = None\n",
    "                    most_specific_node = None\n",
    "                    most_specific_label = None\n",
    "\n",
    "                    # Iterating over each field-label pair\n",
    "                    for field, label in [('Grande área', 'GrandeÁrea'), ('Área', 'Área'), ('Subárea', 'Subárea'), ('Especialidade', 'Especialidade')]:\n",
    "                        if field in extracted:\n",
    "                            current_node = session.run(f\"MERGE (n:{label} {{name: $name}}) RETURN n\", name=extracted[field]).single()['n']\n",
    "\n",
    "                            # Save the most specific node details\n",
    "                            most_specific_node = current_node\n",
    "                            most_specific_label = label\n",
    "\n",
    "                            # Create relationship if there's a prior node to connect to\n",
    "                            if last_node:\n",
    "                                relation_name = f\"CONTÉM_{label.upper()}\"\n",
    "                                session.run(f\"MATCH (a:{last_label} {{name: $name_a}}), (b:{label} {{name: $name_b}}) MERGE (a)-[:{relation_name}]->(b)\", name_a=last_node['name'], name_b=current_node['name'])\n",
    "\n",
    "                            last_node = current_node\n",
    "                            last_label = label\n",
    "\n",
    "                    # After processing each line, connect the Person node to the most specific node available\n",
    "                    if most_specific_node:\n",
    "                        session.run(f\"MATCH (p:Person {{name: $person_name}}), (n:{most_specific_label} {{name: $node_name}}) MERGE (p)-[:ATUA_EM]->(n)\", person_name=name, node_name=most_specific_node['name'])\n",
    "\n",
    "                    successful_areas_creations += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing 'Áreas de atuação' detail '{area_detail}': {e}\")\n",
    "\n",
    "            print(f\"{successful_areas_creations} 'Áreas de atuação' relations successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectsHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def consult_data_by_property(self, name, property_name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(f\"MATCH (p:Person {{name: $name}}) RETURN p.`{property_name}` as data\", name=name)\n",
    "            record = result.single()\n",
    "            return record['data'] if record else None\n",
    "\n",
    "    def create_projects_relations(self, name):\n",
    "        successful_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            # Process 'Atuação Profissional' data\n",
    "            professional_data = self.consult_data_by_property(name, 'Atuação Profissional')\n",
    "            if professional_data:\n",
    "                for institution_name, _ in json.loads(professional_data).items():\n",
    "                    session.run(\"MERGE (i:Instituição {name: $institution_name})\", institution_name=institution_name)\n",
    "                    print(f\"Institution node created/merged for: {institution_name}\")\n",
    "\n",
    "                    session.run(\"MATCH (p:Person {name: $name}), (i:Instituição {name: $institution_name}) MERGE (p)-[:TEM]->(i)\", name=name, institution_name=institution_name)\n",
    "                    print(f\"Relationship established between {name} and {institution_name}.\")\n",
    "\n",
    "            # Process other dynamic nodes\n",
    "            key_labels_to_check = ['Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento']\n",
    "            for key in key_labels_to_check:\n",
    "                formatted_key = f\"`{key}`\"  # Wrap the key with backticks\n",
    "                project_data = self.consult_data_by_property(name, key)\n",
    "                if project_data:\n",
    "                    for project_time, project_name in json.loads(project_data).items():\n",
    "                        if project_name:  # to avoid empty names\n",
    "                            session.run(f\"MERGE (p:{formatted_key} {{name: $project_name}})\", project_name=project_name)\n",
    "                            print(f\"{key} node created/merged for: {project_name}\")\n",
    "\n",
    "                            session.run(f\"MATCH (a:Person {{name: $name}}), (p:{formatted_key} {{name: $project_name}}) MERGE (a)-[:TEM]->(p)\", name=name, project_name=project_name)\n",
    "                            print(f\"Relationship established between {name} and {project_name} ({key}).\")\n",
    "                            successful_creations += 1\n",
    "                else:\n",
    "                    print(f\"'{key}' data not found for {name}\")\n",
    "\n",
    "        print(f\"{successful_creations} projetos atualizados com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class ArticleHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def fetch_person_productions(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"MATCH (p:Person {name: $name}) RETURN p.Produções as produções\", name=name)\n",
    "            record = result.single()\n",
    "            return record['produções'] if record else None\n",
    "\n",
    "    def extract_article_info(self, input_str):\n",
    "        # Encontre todas as abreviaturas de iniciais em maiúsculas e seus índices\n",
    "        abbreviations = [(match.group(), match.start()) for match in re.finditer(r'\\b[A-Z]\\.', input_str)]\n",
    "\n",
    "        # Encontre a posição da maior ocorrência de abreviaturas de iniciais, se houver\n",
    "        if abbreviations:\n",
    "            max_abbr_position = max(abbreviations, key=lambda x: x[1])\n",
    "\n",
    "            # Encontre a primeira ocorrência de '. ' ou ' . ' após a maior ocorrência de abreviaturas de iniciais\n",
    "            first_separator_candidates = [\n",
    "                input_str.find('. ', max_abbr_position[1] + 3),\n",
    "                input_str.find(' . ', max_abbr_position[1] + 3),\n",
    "                input_str.find('.. ')\n",
    "            ]\n",
    "            first_separator_candidates = [pos for pos in first_separator_candidates if pos != -1]\n",
    "\n",
    "            if first_separator_candidates:\n",
    "                first_separator = min(first_separator_candidates)\n",
    "\n",
    "                # Encontre a primeira ocorrência de '. ' após o primeiro separador\n",
    "                second_separator = input_str.find('. ', first_separator + 2)\n",
    "\n",
    "                # Encontre a primeira ocorrência de ', ' após o segundo separador\n",
    "                third_separator = input_str.find(', ', second_separator + 2)\n",
    "            else:\n",
    "                first_separator = second_separator = third_separator = -1\n",
    "        else:\n",
    "            first_separator = second_separator = third_separator = -1\n",
    "\n",
    "        # Defina o padrão para encontrar \"p.\" e o conteúdo até a próxima vírgula\n",
    "        pages_match = re.search(r' p\\.\\s*(.*?),', input_str)\n",
    "        pages = pages_match.group(1) if pages_match else \"\"\n",
    "\n",
    "        # Defina o padrão para encontrar \"v.\" e o conteúdo até a próxima vírgula\n",
    "        volume_match = re.search(r' v\\.\\s*(.*?),', input_str)\n",
    "        volume = volume_match.group(1) if volume_match else \"\"\n",
    "\n",
    "        # Encontre a primeira ocorrência de um ano de quatro dígitos seguido de ponto final após o terceiro separador\n",
    "        year_match = re.search(r' \\d{4}\\.', input_str[third_separator + 2:])\n",
    "        year = year_match.group().strip('.').strip() if year_match else \"\"\n",
    "\n",
    "        # Extraia os dados com base nas posições dos separadores\n",
    "        authors = input_str[:first_separator].strip()\n",
    "        title = input_str[first_separator + 2:second_separator].strip()\n",
    "        journal = input_str[second_separator + 2:third_separator].strip()\n",
    "\n",
    "        # Verifique se a lista de autores e o título não estão vazios\n",
    "        if not authors or not title:\n",
    "            return None  # Retorna None para indicar falha\n",
    "\n",
    "        # Crie um dicionário com os dados extraídos\n",
    "        article_info = {\n",
    "            \"authors\": authors,\n",
    "            \"title\": title,\n",
    "            \"original_title\": journal,\n",
    "            \"pages\": pages,\n",
    "            \"volume\": volume,\n",
    "            \"year\": year\n",
    "        }\n",
    "\n",
    "        return article_info\n",
    "    \n",
    "    def deserialize_and_create_nodes(self, name):\n",
    "        print(f\"Fetching 'Produções' data for {name}...\")\n",
    "        productions_data = self.fetch_person_productions(name)\n",
    "        \n",
    "        if not productions_data:\n",
    "            print(f\"'Produções' data not found or empty for {name}.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Attempting to deserialize 'Produções' data for {name}...\")\n",
    "        try:\n",
    "            productions_data = json.loads(productions_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to deserialize 'Produções' data for {name}: {e}\")\n",
    "            return\n",
    "\n",
    "        successful_articles = 0\n",
    "        unsuccessful_articles = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            print(f\"Processing 'Produção bibliográfica' for {name}...\")\n",
    "            bibliographic_production = productions_data.get(\"Produção bibliográfica\", {})\n",
    "            \n",
    "            if isinstance(bibliographic_production, str):\n",
    "                print(f\"Attempting to deserialize 'Produção bibliográfica' for {name}...\")\n",
    "                try:\n",
    "                    bibliographic_production = json.loads(bibliographic_production)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to deserialize 'Produção bibliográfica' for {name}: {e}\")\n",
    "                    return\n",
    "\n",
    "            articles = json.loads(bibliographic_production.get(\"Artigos completos publicados em periódicos\", \"{}\"))\n",
    "\n",
    "            for _, article_str in articles.items():\n",
    "                article_details = self.extract_article_info(article_str)\n",
    "\n",
    "                # Vamos imprimir os detalhes de cada artigo e verificar se os autores estão presentes.\n",
    "                print(f\"Original Article: {article_str}\")\n",
    "                print(f\"Extracted Details: {article_details}\")\n",
    "\n",
    "                if article_details:\n",
    "                    article_details[\"title\"] = article_details[\"title\"].strip()\n",
    "                    article_details[\"original_title\"] = article_details[\"original_title\"].strip()\n",
    "\n",
    "                    session.run(f\"MERGE (a:Artigo {{title: $title}}) SET a += $details\", title=article_details[\"title\"], details=article_details)\n",
    "                    session.run(f\"MATCH (p:Person {{name: $name}}), (a:Artigo {{title: $title}}) MERGE (p)-[:PUBLICOU]->(a)\", name=name, title=article_details[\"title\"])\n",
    "                    successful_articles += 1\n",
    "                else:\n",
    "                    unsuccessful_articles.append(article_str)\n",
    "\n",
    "        print(f\"Processed {successful_articles} articles successfully for {name}.\")\n",
    "\n",
    "        if unsuccessful_articles:\n",
    "            print(\"Failed to process the following articles:\")\n",
    "            for article in unsuccessful_articles:\n",
    "                print(article)\n",
    "\n",
    "    def process_articles(self, name):\n",
    "        self.deserialize_and_create_nodes(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class DataRemovalHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        \"\"\"\n",
    "        Inicializa a classe DataRemovalHandler com informações de conexão ao banco de dados Neo4j.\n",
    "\n",
    "        Parâmetros:\n",
    "        - uri (str): URI de conexão ao Neo4j.\n",
    "        - user (str): Nome de usuário para autenticação.\n",
    "        - password (str): Senha para autenticação.\n",
    "        \"\"\"\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Fecha a conexão com o banco de dados Neo4j.\n",
    "        \"\"\"\n",
    "        self._driver.close()\n",
    "\n",
    "    def delete_nodes_by_label(self, label):\n",
    "        \"\"\"\n",
    "        Deleta todos os nós associados a um label específico no Neo4j.\n",
    "\n",
    "        Parâmetro:\n",
    "        - label (str): O label dos nós a serem deletados.\n",
    "\n",
    "        Retorna:\n",
    "        - int: O número de nós deletados.\n",
    "        \"\"\"\n",
    "        with self._driver.session() as session:\n",
    "            # Esta consulta combina com todos os nós do label especificado e os deleta\n",
    "            result = session.run(f\"MATCH (n:{label}) DETACH DELETE n RETURN count(n) as deleted_count\")\n",
    "            deleted_count = result.single()[\"deleted_count\"]\n",
    "            return deleted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para extrair Lista de Currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  #seletorcss=\"div.paginacao:nth-child(4) > a:nth-child(2)\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('  ERRO!! Ao utilizar função paginar():', e)\n",
    "    return numpaginas\n",
    "\n",
    "def achar_busca(browser, delay):\n",
    "    '''\n",
    "    Função para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = browser.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               # expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               # logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'  ERRO!! Ao conectar com achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)\n",
    "\n",
    "def preencher_busca(browser, delay, NOME):\n",
    "    '''\n",
    "    Função para passar o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('  ERRO!! Ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)\n",
    "\n",
    "def definir_filtros(browser, delay, mestres=True, assunto=False):\n",
    "    '''\n",
    "    Clica nos check-boxes para definir os filtros de buscas\n",
    "    Para buscar por Assuntos usar parâmetro True, caso omitido fará busca por Nome por default\n",
    "    '''\n",
    "    from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import traceback   \n",
    "    \n",
    "    ## Aguardar carregar e clicar em checkbox de Assunto\n",
    "    try:\n",
    "        if mestres == True:\n",
    "            limite=2\n",
    "            ## Aguardar opção dropdown ser carregada e clicar em sua checkbox\n",
    "            css_buscar_demais = '#buscarDemais'\n",
    "            checkbox_buscar_demais = browser.find_element(By.CSS_SELECTOR, css_buscar_demais)\n",
    "            \n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, css_buscar_demais))),\n",
    "                   wait_ms=150,\n",
    "                   limit=limite, \n",
    "                   on_exhaust=(f'Problema clicar em {checkbox_buscar_demais}, {limite} tentativas sem sucesso.'))\n",
    "            \n",
    "            time.sleep(1)\n",
    "            checkbox_buscar_demais.click()\n",
    "            print(f'Clique efetuado em {checkbox_buscar_demais}')\n",
    "\n",
    "        # css_estrangeiros  = '#buscarEstrangeiros'\n",
    "\n",
    "        if assunto == True:\n",
    "            ## Implementar número de retentativas para casos de conexão muito instável\n",
    "            limite=5\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#buscaAssunto\"))).click(),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "        \n",
    "            ## Aguardar opção Atuação Profissional ser carregada e clicar em sua checkbox\n",
    "            xpath_atuacaoprofissional = \".//*[contains(text(), 'Atuação profissional')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_atuacaoprofissional))).click()\n",
    "\n",
    "            ## Aguardar opção Ciências da Saúde ser carregada e clicar em sua checkbox\n",
    "            xpath_cienciassaude = \".//*[contains(text(), 'Ciências da Saúde')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_cienciassaude))).click()\n",
    "            #browser.find_element_by_xpath(xpath_cienciassaude).click()        \n",
    "\n",
    "            ## Aguardar opção Enfermagem ser carregada e clicar em sua checkbox\n",
    "            xpath_enfermagem    = \".//*[contains(text(), 'Enfermagem')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_enfermagem))).click()\n",
    "            #browser.find_element_by_xpath(xpath_enfermagem).click()\n",
    "            aplicar_link  = browser.find_element(By.LINK_TEXT, 'Aplicar')\n",
    "            aplicar_link.click()\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f'  ERRO!! Ao definir_filtros()')\n",
    "        print(e)\n",
    "        # traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        # print(e, traceback_str) \n",
    "\n",
    "def procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "\n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('  ERRO!! Ao ler a quantidade de resultados:')\n",
    "            return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(browser)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('  ERRO!! Ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('  ERRO!! Ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} homônimos de: {NOME}')\n",
    "            numpaginas = paginar(browser)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(browser)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(browser, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('  ERRO!! Ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', browser\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('  ERRO!! Ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, browser\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('  ERRO!! Ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # browser.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = browser.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', browser\n",
    "    \n",
    "    except Exception as err:\n",
    "        print('  ERRO!! Ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, browser\n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, browser\n",
    "\n",
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    title = soup.title.string if soup.title else \"Unknown\"\n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    return header_node\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_personinfo(soup):\n",
    "    # Step 1: Identify Node Name\n",
    "    properties = {}\n",
    "\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    # div_informacoesautor = soup.find(\"ul\", {\"class\": \"informacoes-autor\"})\n",
    "    if node_name:\n",
    "        properties['name'] = node_name\n",
    "    for li in soup.find_all('li'):\n",
    "        text_content = li.text  # Extract the text content of the 'li' element\n",
    "        span_class = li.span['class'][0] if li.span else 'Unknown'  # Extract the class of the span within the 'li', if present\n",
    "\n",
    "        # Populate dictionary based on span class\n",
    "        if span_class == 'img_link':\n",
    "            properties['CV_URL'] = text_content.replace('Endereço para acessar este CV: ', '').strip()\n",
    "        elif span_class == 'img_identy':\n",
    "            properties['ID_Lattes'] = li.find('span', {'style': 'font-weight: bold; color: #326C99;'}).text.strip() if li.find('span', {'style': 'font-weight: bold; color: #326C99;'}) else 'Unknown'\n",
    "        elif span_class == 'img_cert':\n",
    "            date_str = text_content.replace('Última atualização do currículo em ', '').strip()\n",
    "            \n",
    "            # Assuming that the date string is in the format 'DD/MM/YYYY', parse it into a datetime object\n",
    "            # Modify the format string according to the actual format of date_str\n",
    "            parsed_date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "            # Convert the datetime object to ISO 8601 format (YYYY-MM-DD)\n",
    "            iso_date = parsed_date.strftime('%Y-%m-%d')\n",
    "            properties['Last_Updated'] = iso_date\n",
    "\n",
    "    return properties\n",
    "\n",
    "\n",
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_infpes_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_infpes_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_infpes_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_infpes_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_infpes_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_infpes_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_infpes_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_infpes_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_formation_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_formation_data[key] = value\n",
    "    \n",
    "    return extracted_formation_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that with the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de teste e alternativa para extrair todas seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_chaves_em_arvore(dicionario, nivel=0):\n",
    "    if not isinstance(dicionario, dict):\n",
    "        return\n",
    "    for chave, valor in dicionario.items():\n",
    "        print(\"  \" * nivel + str(chave))  # Utiliza espaços em vez de tabulações para maior visibilidade\n",
    "        if isinstance(valor, dict):\n",
    "            mostrar_chaves_em_arvore(valor, nivel + 1)\n",
    "        elif isinstance(valor, list):\n",
    "            for i, item in enumerate(valor):\n",
    "                if isinstance(item, dict):\n",
    "                    print(\"  \" * (nivel + 1) + f\"Item {i+1}\")\n",
    "                    mostrar_chaves_em_arvore(item, nivel + 2)\n",
    "\n",
    "import json\n",
    "\n",
    "def mostrar_chaves_avancado(E, nivel=0):\n",
    "    if E is None or E == []:\n",
    "        return\n",
    "\n",
    "    # Tenta converter strings JSON para dicionários ou listas\n",
    "    if isinstance(E, str):\n",
    "        try:\n",
    "            E = json.loads(E)\n",
    "        except json.JSONDecodeError:\n",
    "            return\n",
    "\n",
    "    if isinstance(E, dict):\n",
    "        for chave, valor in E.items():\n",
    "            print(\"  \" * nivel + str(chave))\n",
    "            mostrar_chaves_avancado(valor, nivel + 1)\n",
    "    elif isinstance(E, list):\n",
    "        for i, item in enumerate(E):\n",
    "            print(\"  \" * nivel + f\"Item {i+1}\")\n",
    "            mostrar_chaves_avancado(item, nivel + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionario_aninhado = {\n",
    "    \"InfPes\": {\n",
    "        \"Resumo\": \"Texto resumo\",\n",
    "        \"Identificação\": {\n",
    "            \"nome\": \"João\",\n",
    "            \"idade\": 30\n",
    "        },\n",
    "        \"Endereço\": {\n",
    "            \"rua\": \"123 Rua Principal\",\n",
    "            \"cidade\": \"Lisboa\"\n",
    "        }\n",
    "    },\n",
    "    \"Formação acadêmica/titulação\": [\n",
    "        {\"curso\": \"Engenharia\", \"ano\": 2000},\n",
    "        {\"curso\": \"Matemática\", \"ano\": 2004}\n",
    "    ]\n",
    "}\n",
    "\n",
    "estrutura_complexa = {\n",
    "    \"chave1\": {\n",
    "        \"subchave1\": \"valor1\",\n",
    "        \"subchave2\": \"{\\\"subsubchave1\\\": \\\"valor2\\\"}\"\n",
    "    },\n",
    "    \"chave2\": \"[{\\\"subchave3\\\": \\\"valor3\\\"}, {\\\"subchave4\\\": \\\"valor4\\\"}]\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostrar_chaves_em_arvore(estrutura_complexa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostrar_chaves_avancado(estrutura_complexa, nivel=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        data_dict = {}\n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "\n",
    "            higher_order_key = None  # Inicializa variável para armazenar a chave de ordem superior\n",
    "            data_list = []  # Inicializa lista para armazenar entradas de índices de dataframe\n",
    "            \n",
    "            parag_elements = parent_div.find_all('p')\n",
    "            if parag_elements:\n",
    "                for idx, elem in enumerate(parag_elements):\n",
    "                    class_name = elem.get('class', [None])[0]  # Assumes only one class; otherwise, join them into a single string\n",
    "                    higher_order_key = class_name\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "                    data_entry = {'rotulos': class_name, 'conteudos': elem.text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "\n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                kdict_elements = cell.find_all('b')\n",
    "                # print(len(kdict_elements))\n",
    "\n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "\n",
    "                index_elems   = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for key_elem, details_elem in zip(index_elems, details_elems):\n",
    "                    key_text     = key_elem.text.strip() if key_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    data_entry = {'rotulos': key_text, 'conteudos': details_text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "                \n",
    "            if higher_order_key is None:\n",
    "                # Se nenhuma chave de ordem superior for encontrada, associa a lista de dados diretamente ao título\n",
    "                result_dict[title_text] = data_list\n",
    "            else:\n",
    "                # Caso contrário, associa o data_dict contendo chaves de ordem superior ao título\n",
    "                result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def generate_dataframe_and_neo4j_dict(data_dict):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame and a dictionary for Neo4j persistence, incorporating section and subsection names.\n",
    "\n",
    "    Parameters:\n",
    "        data_dict (dict): A nested dictionary containing section and subsection data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame aggregating all sections and subsections, with additional columns specifying their names.\n",
    "        dict: A dictionary intended for Neo4j persistence, formatted according to the Neo4j data model.\n",
    "    \"\"\"\n",
    "\n",
    "    all_frames = []  # List to store DataFrames corresponding to each section and subsection\n",
    "    neo4j_dict = {}  # Dictionary for Neo4j persistence\n",
    "\n",
    "    for section, items in data_dict.items():\n",
    "        if section:  # Exclude empty sections\n",
    "            neo4j_dict[section] = {}\n",
    "            if isinstance(items, list):  # If items is a list, convert to DataFrame\n",
    "                df = pd.DataFrame(items)\n",
    "                df['Section'] = section  # Append a column for the section name\n",
    "                all_frames.append(df)\n",
    "                neo4j_dict[section] = items  # For list items, add them as they are\n",
    "\n",
    "            elif isinstance(items, dict):  # If items is a dictionary, explore subsections\n",
    "                for subsection, subitems in items.items():\n",
    "                    if subitems:  # Exclude empty subsections\n",
    "                        df = pd.DataFrame(subitems)\n",
    "                        df['Subsection'] = subsection  # Append a column for the subsection name\n",
    "                        df['Section'] = section  # Append a column for the section name\n",
    "                        all_frames.append(df)\n",
    "                        neo4j_dict[section][subsection] = subitems  # Store subsection data\n",
    "\n",
    "    # Concatenate all DataFrames vertically to form one unified DataFrame\n",
    "    dataframe = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "    return dataframe, neo4j_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções alternativas diversas para interagir com Neo4j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database credentials and URI\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    try:\n",
    "        graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    except:\n",
    "        print('  ERRO!! Ao conectar com Neo4j')\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'].split('(')[1].strip(')'), meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    print(type(header_node))\n",
    "\n",
    "    return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict, List\n",
    "\n",
    "def create_or_update_publications(graph: Graph, publications_dict: Dict[str, Dict[str, List[Dict[str, str]]]]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its associated publications in the Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        publications_dict (Dict[str, Dict[str, List[Dict[str, str]]]]): Dictionary containing publication information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    publications_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Search for existing researcher node by name\n",
    "    existing_node = matcher.match(\"Researcher\", name=publications_dict['Node Name']).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding publications.\")\n",
    "\n",
    "    # Process publications\n",
    "    for publication in publications_dict['Properties']['Produções']:\n",
    "        # Check if a similar publication already exists\n",
    "        existing_pub_node = matcher.match(\"Publication\", doi=publication['doi']).first()\n",
    "\n",
    "        if not existing_pub_node:\n",
    "            pub_node = Node(\"Publication\", **publication)\n",
    "            graph.create(pub_node)\n",
    "            publications_created += 1\n",
    "        else:\n",
    "            for key, value in publication.items():\n",
    "                if key not in existing_pub_node or existing_pub_node[key] != value:\n",
    "                    existing_pub_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            pub_node = existing_pub_node\n",
    "            graph.push(pub_node)\n",
    "\n",
    "        # Create or update relationship between researcher and publication\n",
    "        rel = Relationship(existing_node, \"PUBLICOU\", pub_node)\n",
    "        graph.merge(rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Publications created: {publications_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    if 'nome' in researcher_dict and 'resumo' in researcher_dict:\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['nome'][0] if isinstance(researcher_dict['nome'], list) else researcher_dict['nome']\n",
    "        summary = researcher_dict['resumo'][0] if isinstance(researcher_dict['resumo'], list) else researcher_dict['resumo']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, resumo=summary)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'nome' and 'resumo'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'nome': ['John Doe'], 'resumo': ['This is a summary.']}\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "# create_researcher_node(graph, researcher_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "\n",
    "def generate_lattes_dict(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a dictionary from a BeautifulSoup object to be persisted in Neo4j.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML/XML data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the relevant information for Neo4j persistence.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold the extracted data\n",
    "    lattes_data = {}\n",
    "    \n",
    "    # Extracting researcher's name as an example\n",
    "    name_section = soup.find('div', {'id': 'name-section'})\n",
    "    if name_section:\n",
    "        lattes_data['researcher_name'] = name_section.text.strip()\n",
    "    \n",
    "    # Extracting list of publications as an example\n",
    "    publications = []\n",
    "    for pub in soup.find_all('div', {'class': 'publication'}):\n",
    "        publication_data = {}\n",
    "        title = pub.find('span', {'class': 'title'})\n",
    "        authors = pub.find('span', {'class': 'authors'})\n",
    "        \n",
    "        if title:\n",
    "            publication_data['title'] = title.text.strip()\n",
    "        \n",
    "        if authors:\n",
    "            publication_data['authors'] = authors.text.strip().split(',')\n",
    "        \n",
    "        publications.append(publication_data)\n",
    "    \n",
    "    lattes_data['publications'] = publications\n",
    "    \n",
    "    # Additional extractions can be performed as per the requirements\n",
    "    \n",
    "    return lattes_data\n",
    "\n",
    "# Example usage (Assuming 'some_html_content' has the HTML content)\n",
    "# soup = BeautifulSoup(some_html_content, 'html.parser')\n",
    "# lattes_dict = generate_lattes_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node_from_dict(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "# create_researcher_node_from_dict(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    nodes_created = 0\n",
    "    nodes_updated = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "\n",
    "        # Create a NodeMatcher object\n",
    "        matcher = NodeMatcher(graph)\n",
    "\n",
    "        # Look for existing nodes with the same name\n",
    "        existing_node = matcher.match(\"Researcher\", name=name).first()\n",
    "\n",
    "        if existing_node:\n",
    "            # Update properties of existing node\n",
    "            for key, value in researcher_dict.items():\n",
    "                if key.lower() not in existing_node or existing_node[key.lower()] != value:\n",
    "                    existing_node[key.lower()] = value\n",
    "                    properties_updated += 1\n",
    "\n",
    "            # Push the changes to the database\n",
    "            graph.push(existing_node)\n",
    "            nodes_updated += 1\n",
    "\n",
    "        else:\n",
    "            # Create a new node of type 'Researcher'\n",
    "            researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "            # Add the node to the Neo4j database\n",
    "            graph.create(researcher_node)\n",
    "            nodes_created += 1\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"Nodes created: {nodes_created}\")\n",
    "        print(f\"Nodes updated: {nodes_updated}\")\n",
    "        print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'NOME': 'John Doe', 'IDLATTES': '0000000000000000', 'ATUALIZAÇÃO': '31/12/2023'}\n",
    "\n",
    "# Create or update a Researcher node in Neo4j based on the dictionary\n",
    "# create_or_update_researcher_node(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_professional_links(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its professional links in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's professional information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    relationships_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Look for existing nodes with the same name\n",
    "    existing_node = matcher.match(\"Researcher\", name=researcher_dict.get('Nome')).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding professional links.\")\n",
    "\n",
    "    # Process professional affiliations and activities\n",
    "    for key, value in researcher_dict.items():\n",
    "        if key not in ['Nome', 'Endereço Profissional']:\n",
    "            # Create or find the organization/affiliation node\n",
    "            org_node = matcher.match(\"Organization\", name=key).first()\n",
    "            if not org_node:\n",
    "                org_node = Node(\"Organization\", name=key)\n",
    "                graph.create(org_node)\n",
    "\n",
    "            # Create or update the relationship\n",
    "            rel_type = \"AFFILIATED_WITH\" if 'Atual' in value else \"HAS_BEEN_AFFILIATED_WITH\"\n",
    "            rel = Relationship(existing_node, rel_type, org_node, details=value)\n",
    "\n",
    "            # Check if a similar relationship already exists\n",
    "            existing_rel = None\n",
    "            for r in graph.match((existing_node, org_node), r_type=rel_type):\n",
    "                if r['details'] == value:\n",
    "                    existing_rel = r\n",
    "                    break\n",
    "\n",
    "            # Create new or update existing relationship\n",
    "            if not existing_rel:\n",
    "                graph.create(rel)\n",
    "                relationships_created += 1\n",
    "            else:\n",
    "                for property_name, property_value in rel.items():\n",
    "                    if property_name not in existing_rel or existing_rel[property_name] != property_value:\n",
    "                        existing_rel[property_name] = property_value\n",
    "                        properties_updated += 1\n",
    "                graph.push(existing_rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Relationships created: {relationships_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update professional links in Neo4j based on the dictionary\n",
    "# create_or_update_professional_links(graph, dict_vinculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "def persist_journals_from_xlsx(graph: Graph, xlsx_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Persist journal information into a Neo4j database from an Excel (.xlsx) file.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        xlsx_path (str): The path to the Excel (.xlsx) file containing the journal information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize counters for tracking\n",
    "    nodes_created = 0\n",
    "    properties_updated = 0\n",
    "    \n",
    "    # Create NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "    \n",
    "    # Read the Excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract journal properties\n",
    "        properties = {'ISSN': row['ISSN'],\n",
    "                      'Título': row['Título'],\n",
    "                      'Área de Avaliação': row['Área de Avaliação'],\n",
    "                      'Estrato': row['Estrato']}\n",
    "        \n",
    "        # Check if a node with the same ISSN already exists\n",
    "        existing_node = matcher.match(\"Journal\", ISSN=row['ISSN']).first()\n",
    "        \n",
    "        if existing_node:\n",
    "            for key, value in properties.items():\n",
    "                if key not in existing_node or existing_node[key] != value:\n",
    "                    existing_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            graph.push(existing_node)\n",
    "        else:\n",
    "            new_node = Node(\"Journal\", **properties)\n",
    "            graph.create(new_node)\n",
    "            nodes_created += 1\n",
    "    \n",
    "    # Print statistical data\n",
    "    print(f\"Nodes created: {nodes_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_path = './../data/classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx'\n",
    "xlsx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Persistir informações sobre ISSN e Qualis do Periódicos Capes da Plataforma Sucurira de Arquivo Excel\n",
    "# Obs.: 154.410 linhas, 30.583 em 30min, 1020 nós/min em IntelCore i5-8500 CPU @ 3.00GHz 16Gb RAM no Windows\n",
    "# Obs.: 154.410 linhas, 31.339 em 20min, 1567 nós/min em AMDRyzen 7 5800X 8-Core 3.80GHz 64Gb RAM no Windows\n",
    "# Aproximadamente 2.5 horas para persistir os dados no Neo4j, terminará 15h\n",
    "\n",
    "# persist_journals_from_xlsx(graph, xlsx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para persistir nós de Classificação do CNPq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pathdata = './../data/'\n",
    "file_cnpq = 'cnpq_tabela-areas-conhecimento.pdf'\n",
    "caminho = pathdata+file_cnpq\n",
    "\n",
    "def verifica_ponto_virgula(df):\n",
    "    return df[df['Descricao'].str.contains(';', regex=False)]\n",
    "\n",
    "def verifica_virgula(df):\n",
    "    return df[df['Descricao'].str.contains(',', regex=False)]\n",
    "\n",
    "def verifica_formato_descricao(descricao):\n",
    "    excecoes = [\"de\", \"do\", \"da\", \"dos\", \"das\", \"a\", \"o\", \"e\", \"em\", \"com\", \"para\", \"por\", \"sem\"]\n",
    "    palavras = descricao.split()\n",
    "    \n",
    "    for i, palavra in enumerate(palavras):\n",
    "        if palavra.lower() in excecoes or palavra[0]==\"(\":\n",
    "            continue\n",
    "        if not palavra[0].isupper() or (palavra==palavras[-1] and palavra in excecoes):\n",
    "            return False, i  # Retornar False e o índice da palavra problemática\n",
    "    return True, None\n",
    "\n",
    "    # for idx, word in enumerate(palavras):\n",
    "    #     # Se a palavra inicia com letra minúscula e não é uma preposição ou artigo\n",
    "    #     if word[0].islower() and word not in excecoes:\n",
    "    #         # Aqui verificamos se a palavra anterior termina com uma letra e a palavra atual é uma preposição ou artigo\n",
    "    #         if idx > 0 and palavras[idx - 1][-1].isalpha() and word in excecoes:\n",
    "    #             return (False, idx)\n",
    "    #         # Ou apenas a condição de começar com minúscula e não ser preposição ou artigo\n",
    "    #         elif idx == 0 or (idx > 0 and not palavras[idx - 1][-1].isalpha()):\n",
    "    #             return (False, idx)    \n",
    "\n",
    "def corrigir_descricao(descricao, word_index):\n",
    "    excecoes = [\"de\", \"do\", \"da\", \"dos\", \"das\", \"a\", \"o\", \"e\", \"em\", \"com\", \"para\", \"por\", \"sem\"]\n",
    "    palavras = descricao.split()\n",
    "\n",
    "    # Se o índice anterior existir e a palavra atual começa com minúscula\n",
    "    if word_index > 0 and palavras[word_index][0].islower():\n",
    "        # Checar se a palavra é uma preposição ou artigo e se a anterior termina com uma letra\n",
    "        if palavras[word_index] in excecoes:\n",
    "            palavras[word_index - 1] += palavras[word_index]\n",
    "            del palavras[word_index]\n",
    "        else:\n",
    "            # Juntar palavra atual com a palavra anterior\n",
    "            palavras[word_index - 1] += palavras[word_index]\n",
    "            del palavras[word_index]\n",
    "\n",
    "    # Após as correções, juntamos as palavras de volta em uma única string\n",
    "    nova_descricao = ' '.join(palavras)\n",
    "\n",
    "    # Imprimindo para debug\n",
    "    # print(f\"Descrição ruim: {descricao}\")\n",
    "    # print(f\"Correção feita: {palavra_anterior} + {palavra_incorreta} = {correcao}\")\n",
    "    # print(f\"Nova descrição: {nova_descricao}\\n\")\n",
    "    \n",
    "    return nova_descricao\n",
    "\n",
    "\n",
    "def extrair_areas(caminho):\n",
    "    texto_completo = \"\"\n",
    "\n",
    "    reader = PdfReader(caminho)\n",
    "    \n",
    "    for npag, p in tqdm(enumerate(reader.pages), total=len(reader.pages), desc=\"Processando páginas do PDF das Áreas de pesquisa do CNPq..\"):\n",
    "        texto_completo += p.extract_text()\n",
    "\n",
    "    texto_completo = texto_completo.replace('\\n', ' ').replace(\" -\",\"-\").replace(\" ,\",\",\").strip().replace(\"ã o\",\"ão\")\n",
    "    texto_completo = re.sub(r'\\s?(\\d)\\s?(\\.)\\s?(\\d{2})\\s?(\\.)\\s?(\\d{2})\\s?(\\.)\\s?(\\d{2})\\s?(-)\\s?(\\d)\\s?', r'\\1\\2\\3\\4\\5\\6\\7\\8\\9', texto_completo)\n",
    "\n",
    "    pattern = r'(\\d\\.\\d{2}\\.\\d{2}\\.\\d{2}-\\d)([^0-9]+)'\n",
    "    matches = re.findall(pattern, texto_completo)\n",
    "\n",
    "    codigos = [match[0] for match in matches]\n",
    "    descricoes = [match[1].strip() for match in matches]\n",
    "\n",
    "    print(f'Total dos códigos   identificados: {len(codigos)}')\n",
    "    print(f'Total de descrições identificadas: {len(descricoes)}')\n",
    "\n",
    "    df_linhas = pd.DataFrame({'Codigo': codigos, 'Descricao': descricoes})\n",
    "\n",
    "    # Verificação da divisão correta dos códigos/descrições\n",
    "    descricoes_com_numeros = df_linhas[df_linhas['Descricao'].str.contains(r'\\d')]\n",
    "    if not descricoes_com_numeros.empty:\n",
    "        print(f\"Conferência: {len(descricoes_com_numeros)} descrições contêm números!\")\n",
    "    else:\n",
    "        print(f\"Nenhum erro de códigos em descrições!\")\n",
    "\n",
    "    # Identificando e printando a quantidade de possíveis erros\n",
    "    erros = sum(1 for descricao in descricoes if not verifica_formato_descricao(descricao)[0])\n",
    "    print(f\"{erros} possíveis erros de descrição detectados.\")\n",
    "\n",
    "    # Barra de progresso para correção das descrições\n",
    "    with tqdm(total=df_linhas.shape[0], desc=\"Corrigindo descrições...\") as pbar:\n",
    "        for index, row in df_linhas.iterrows():\n",
    "            is_valid, word_index = verifica_formato_descricao(row['Descricao'])\n",
    "            loop_count = 0\n",
    "            while not is_valid and loop_count < 10:\n",
    "                row['Descricao'] = corrigir_descricao(row['Descricao'], word_index)\n",
    "                is_valid, word_index = verifica_formato_descricao(row['Descricao'])\n",
    "                loop_count += 1\n",
    "            if loop_count == 10:\n",
    "                print(f\"Problema corrigindo descrição: {row['Descricao']}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    return df_linhas\n",
    "\n",
    "df_areas = extrair_areas(caminho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Reconhecimento de Entidades Nomeadas (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_named_entities(text):\n",
    "#     import spacy\n",
    "    \n",
    "#     nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "#     nlp_pt = spacy.load(\"pt_core_news_lg\")\n",
    "#     doc = nlp_pt(text)\n",
    "#     entities = {'ORG': [], 'GPE': [], 'NORP': [], 'PERSON': [], 'PRODUCT': [], 'WORK_OF_ART': []}\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ in entities:\n",
    "#             entities[ent.label_].append(ent.text)\n",
    "#     return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GraphModel:\n",
    "#     def __init__(self, uri, user, password):\n",
    "#         self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "#     def close(self):\n",
    "#         self._driver.close()\n",
    "\n",
    "#     def add_entity(self, entity_type, entity_value):\n",
    "#         with self._driver.session() as session:\n",
    "#             session.write_transaction(self._add_entity, entity_type, entity_value)\n",
    "            \n",
    "#     @staticmethod\n",
    "#     def _add_entity(tx, entity_type, entity_value):\n",
    "#         query = f\"MERGE (a:{entity_type} {{name: $name}})\"\n",
    "#         tx.run(query, name=entity_value)\n",
    "    \n",
    "#     def add_relation(self, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "#         with self._driver.session() as session:\n",
    "#             session.write_transaction(self._add_relation, src_type, src_name, rel_type, tgt_type, tgt_name)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _add_relation(tx, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "#         query = (\n",
    "#             f\"MATCH (a:{src_type} {{name: $src_name}}), (b:{tgt_type} {{name: $tgt_name}}) \"\n",
    "#             f\"MERGE (a)-[:{rel_type}]->(b)\"\n",
    "#         )\n",
    "#         tx.run(query, src_name=src_name, tgt_name=tgt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_dict(d):\n",
    "#     def expand(key, value):\n",
    "#         if isinstance(value, dict):\n",
    "#             return [(str(key) + '.' + str(k), v) for k, v in flatten_dict(value).items()]\n",
    "#         else:\n",
    "#             return [(str(key), value)]\n",
    "        \n",
    "#     items = [item for k, v in d.items() for item in expand(k, v)]\n",
    "#     return dict(items)\n",
    "\n",
    "# def main(data_dict):\n",
    "#     graph_model = GraphModel(\"bolt://localhost:7687\", \"neo4j\", \"password\")    \n",
    "#     flattened_data = flatten_dict(data_dict['Properties']['Identificação'])\n",
    "#     person_name = flattened_data.get('Nome', None)[0]\n",
    "#     print(f'Nome: {person_name}')\n",
    "#     if person_name:\n",
    "#         graph_model.add_entity(\"Person\", person_name)\n",
    "    \n",
    "#     institutional = flatten_dict(data_dict['Properties'])\n",
    "#     for key, value in institutional.items():\n",
    "#         if key in ['Endereço.Endereço Profissional']:\n",
    "#             text = ' '.join(value)\n",
    "#             print(text)\n",
    "#             entities = extract_named_entities(text)\n",
    "#             print(f'Entidades reconhecidas: {entities}')\n",
    "#             entities\n",
    "            \n",
    "#             for org in entities.get('ORG', []):\n",
    "#                 graph_model.add_entity(\"Organization\", org)\n",
    "#                 graph_model.add_relation(\"Person\", person_name, \"AFFILIATED_WITH\", \"Organization\", org)\n",
    "                \n",
    "#             for gpe in entities.get('GPE', []):\n",
    "#                 graph_model.add_entity(\"Location\", gpe)\n",
    "#                 graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "#     projetos = flatten_dict(data_dict['Properties'])\n",
    "#     for key, value in projetos.items():\n",
    "#         if key in ['Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento']:\n",
    "#             entities = extract_named_entities(value)\n",
    "            \n",
    "#             for org in entities.get('ORG', []):\n",
    "#                 graph_model.add_entity(\"Organization\", org)\n",
    "#                 graph_model.add_relation(\"Person\", person_name, \"HAS_PROJECT_IN\", \"Organization\", org)\n",
    "                \n",
    "#             for gpe in entities.get('GPE', []):\n",
    "#                 graph_model.add_entity(\"Location\", gpe)\n",
    "#                 graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "#     graph_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests --trusted-host pypi.org --trusted-host files.pythonhosted.org --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python.exe -m pip install --upgrade pip --trusted-host pypi.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy --trusted-host pypi.org --trusted-host files.pythonhosted.org --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# # Processamento para eliminar strings duplicadas e vazias\n",
    "# def remove_duplicates(entities):\n",
    "#     for key in entities.keys():\n",
    "#         seen = set()\n",
    "#         unique_values = []\n",
    "        \n",
    "#         for value in entities[key]:\n",
    "#             lower_value = value.lower()\n",
    "#             if lower_value not in seen and value.strip():\n",
    "#                 seen.add(lower_value)\n",
    "#                 unique_values.append(value)\n",
    "        \n",
    "#         # Atualização do dicionário\n",
    "#         entities[key] = unique_values\n",
    "#     return entities\n",
    "\n",
    "# def extract_named_entities(text):\n",
    "#     nlp = spacy.load('en_core_web_lg')  # Assuming the use of the small Portuguese model\n",
    "#     doc = nlp(text)\n",
    "#     entities = {'ORG': [], 'GPE': [], 'PHONE': [], 'URL': []}\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ in entities.keys():\n",
    "#             entities[ent.label_].append(ent.text)\n",
    "    \n",
    "#     unique_entities = remove_duplicates(entities)\n",
    "    \n",
    "#     return unique_entities\n",
    "\n",
    "# text = 'Pós-Doutor em Computação pela Sorbonne Université-Pierre et Marie Curie (França - 2020). Doutor em Ciência da Computação pela Universitat Politecnica de Catalunya (Espanha - 2005). Atualmente é professor titular da Universidade de Fortaleza - UNIFOR na qual é membro permanente dos programas de pós-graduação em Informática Aplicada (mestrado e doutorado) e do mestrado profissional em administração. Possui mais de 100 trabalhos publicados em conferências e periódicos nacionais e internacionais. Tem experiência na área de Ciência da Computação, com ênfase em Ciência de Dados e Teleinformática, atuando principalmente nos seguintes temas: Redes Complexas, Grafos de Conhecimento, Aprendizagem de Máquina, Internet das Coisas, Segurança e Blockchain. (Texto informado pelo autor)\\n'\n",
    "\n",
    "# entities = extract_named_entities(text)\n",
    "# entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_dict['Properties'].keys())\n",
    "\n",
    "# main(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 2: Persistir classes CNPq em Neo4j</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar nós e relações entre classificações do CNPq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "classifier = CNPqClassifier(uri, user, password)\n",
    "classifications = list(zip(df_areas['Codigo'], df_areas['Descricao']))\n",
    "classifier.process(classifications)\n",
    "classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_dict in dict_list:\n",
    "    for k, v in data_dict.items():\n",
    "        print(f'{k:>30}| {type(v)}, {v}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(input_data, dict):\n",
    "            return {key: Neo4jPersister.convert_to_primitives(value) for key, value in input_data.items()}\n",
    "        \n",
    "        elif isinstance(input_data, list):\n",
    "            return [Neo4jPersister.convert_to_primitives(item) for item in input_data]\n",
    "        \n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        \n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return Neo4jPersister.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def extract_lattes_id(self, infpes_list):\n",
    "        \"\"\"Extracts the Lattes ID from the InfPes list.\"\"\"\n",
    "        for entry in infpes_list:\n",
    "            if 'ID Lattes:' in entry:\n",
    "                # Extracting the numeric portion of the 'ID Lattes:' entry\n",
    "                return entry.split(\":\")[1].strip()\n",
    "        return None\n",
    "\n",
    "    def persist_data(self, data_dict, label):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "\n",
    "        # Extracting the Lattes ID from the provided structure\n",
    "        lattes_id = self.extract_lattes_id(data_dict.get(\"InfPes\", []))\n",
    "        \n",
    "        if not lattes_id:\n",
    "            print(\"Lattes ID not found or invalid.\")\n",
    "            return\n",
    "        \n",
    "        # Flatten the \"Identificação\" properties into the main dictionary\n",
    "        if \"Identificação\" in data_dict_primitives:\n",
    "            id_properties = data_dict_primitives.pop(\"Identificação\")\n",
    "            \n",
    "            if isinstance(id_properties, dict):\n",
    "                for key, value in id_properties.items():\n",
    "                    # Adding a prefix to avoid potential property name conflicts\n",
    "                    data_dict_primitives[f\"Identificação_{key}\"] = value\n",
    "            else:\n",
    "                # If it's not a dictionary, then perhaps store it as a single property (optional)\n",
    "                data_dict_primitives[\"Identificação_value\"] = id_properties\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MERGE (node:{label} {{lattes_id: $lattes_id}}) SET node = $props\"\n",
    "            session.run(query, lattes_id=lattes_id, props=data_dict_primitives)\n",
    "\n",
    "    def update_data(self, node_id, data_dict):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MATCH (node) WHERE id(node) = {node_id} SET node += $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "    def parse_area(self, area_string):\n",
    "        \"\"\"Parses the area string and returns a dictionary with the parsed fields.\"\"\"\n",
    "        parts = area_string.split(\" / \")\n",
    "        area_data = {}\n",
    "        for part in parts:\n",
    "            key, _, value = part.partition(\":\")\n",
    "            area_data[key.strip()] = value.strip()\n",
    "        return area_data\n",
    "\n",
    "    # def persist_secondary_nodes(self, lattes_id, areas):\n",
    "    #     \"\"\"Persists secondary nodes linked to the main Person node.\"\"\"\n",
    "    #     with self._driver.session() as session:\n",
    "    #         for _, area_string in areas.items():\n",
    "    #             area_data = self.parse_area(area_string)\n",
    "\n",
    "    #             grande_area_name = area_data.get(\"Grande área\")\n",
    "    #             area_name = area_data.get(\"Área\")\n",
    "    #             subarea_name = area_data.get(\"Subárea\")\n",
    "    #             especialidade_name = area_data.get(\"Especialidade\")\n",
    "\n",
    "    #             # Use parameters to prevent Cypher injection\n",
    "    #             parameters = {\n",
    "    #                 'grande_area_name': grande_area_name,\n",
    "    #                 'area_name': area_name,\n",
    "    #                 'subarea_name': subarea_name,\n",
    "    #                 'especialidade_name': especialidade_name,\n",
    "    #                 'lattes_id': lattes_id\n",
    "    #             }\n",
    "\n",
    "    #             query = \"\"\n",
    "\n",
    "    #             # Build the Cypher query dynamically\n",
    "    #             if grande_area_name:\n",
    "    #                 query += \"MERGE (ga:GrandeÁrea {name: $grande_area_name}) \"\n",
    "    #             if area_name:\n",
    "    #                 query += \"MERGE (a:Área {name: $area_name}) \"\n",
    "    #                 if grande_area_name:\n",
    "    #                     query += \"MERGE (ga)-[:CONTAINS]->(a) \"\n",
    "    #             if subarea_name:\n",
    "    #                 query += \"MERGE (sa:Subárea {name: $subarea_name}) \"\n",
    "    #                 if area_name:\n",
    "    #                     query += \"MERGE (a)-[:CONTAINS]->(sa) \"\n",
    "    #             if especialidade_name:\n",
    "    #                 query += \"MERGE (e:Especialidade {name: $especialidade_name}) \"\n",
    "    #                 if subarea_name:\n",
    "    #                     query += \"MERGE (sa)-[:CONTAINS]->(e) \"\n",
    "    #                 query += \"MERGE (p:Person {lattes_id: $lattes_id}) MERGE (p)-[:HAS_EXPERTISE_IN]->(e) \"\n",
    "    #             elif subarea_name:\n",
    "    #                 query += \"MERGE (p:Person {lattes_id: $lattes_id}) MERGE (p)-[:HAS_EXPERTISE_IN]->(sa) \"\n",
    "\n",
    "    #             # Execute the query\n",
    "    #             if query:\n",
    "    #                 session.run(query, parameters)\n",
    "\n",
    "    def process_all_person_nodes(self):\n",
    "        \"\"\"Iterates over all Person nodes and persists secondary nodes and relationships.\"\"\"\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"MATCH (p:Person) RETURN p.name AS name, p.`Áreas de atuação` AS areas\")\n",
    "\n",
    "            for record in result:\n",
    "                person_name = record[\"name\"]\n",
    "                \n",
    "                # Check if name or areas is None\n",
    "                if person_name is None or record[\"areas\"] is None:\n",
    "                    print(f\"Skipping record for name {person_name} due to missing name or areas.\")\n",
    "                    continue\n",
    "\n",
    "                # Check if the areas data is already in dict form\n",
    "                if isinstance(record[\"areas\"], dict):\n",
    "                    areas = record[\"areas\"]\n",
    "                else:\n",
    "                    # Attempt to convert from a string representation (e.g., JSON)\n",
    "                    try:\n",
    "                        areas = json.loads(record[\"areas\"])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to parse areas for name {person_name}. Error: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                self.persist_secondary_nodes(person_name, areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list[0]['Áreas de atuação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri=\"bolt://localhost:7687\"\n",
    "user=\"neo4j\"\n",
    "password=\"password\"\n",
    "neo4j_persister = Neo4jPersister(uri, user, password)\n",
    "\n",
    "# Primeiro persistir todos os nós com dados de cada CV\n",
    "for data_dict in dict_list:\n",
    "    NOME = data_dict['name']\n",
    "    print(f'Persistir nó: {NOME}')\n",
    "    neo4j_persister.persist_data(data_dict, \"Person\")\n",
    "\n",
    "# Fechar a conexão após a persistência de todos os dicionários\n",
    "neo4j_persister.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistindo nós de Atuação\n",
    "neo4j_persister = Neo4jPersister(uri, user, password)\n",
    "neo4j_persister.process_all_person_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depois consultar as áreas de atuação em cada nó\n",
    "for data_dict in dict_list:    \n",
    "    NOME=data_dict['name']\n",
    "    print(f'Persistir áreas de atuação de: {NOME}')\n",
    "    areas_analysis_obj = AreasHandler(uri=uri, user=user, password=password)\n",
    "    areas_analysis_obj.create_areas_relations(NOME)\n",
    "    areas_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Análise de Currículo Individual</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalization of the DOM extraction from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digital representation of the HTML DOM (Document Object Model) in question follow a consistent class-based structure, where the division of information into various classes within 'div' elements serves as an important taxonomy for organizing and categorizing the information.\n",
    "\n",
    "The nested structure predominantly consists of HTML div elements differentiated by their CSS classes. The div elements appear in a tree-like organization, hierarchically grouped under recursive presence of the div elements within the class 'title-wrapper', followed by the div elements marked with 'layout-cell' and hierarquically organized until reaching the more detailed levels where the data of interest is, contained in the classes such 'data-cell', 'text-align-right' or 'layout-cell-pad-5' and tags like 'a', 'b'.\n",
    "\n",
    "The extraction of data from this intricate nested architecture necessitates a recursive methodology that maintains the hierarchical fidelity of the original data. Thus, one approach to transforming this data into a structured JSON object would be to employ depth-first search (DFS) algorithms to traverse through each node in this tree-like structure. Each traversal would examine the class attributes and potentially the text content within each div. \n",
    "\n",
    "**Formalization:**\n",
    "\n",
    "In the formal language of computational theory, let \\( T \\) be the DOM tree with each node \\( n \\) containing a list of attributes \\( A(n) \\) and a text content \\( C(n) \\). Let \\( JSON(n) \\) be the JSON representation of the node \\( n \\). The recursive function to extract data can be described as:\n",
    "\n",
    "\n",
    "JSON(n) = \n",
    "\\begin{cases} \n",
    "\\{ \"type\": A(n), \"content\": C(n), \"children\": \\{ JSON(c) \\,|\\, c \\in \\text{children of } n \\} \\} & \\text{if } n \\text{ has children} \\\\\n",
    "\\{ \"type\": A(n), \"content\": C(n) \\} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "**Python implementation:**\n",
    " \n",
    "In terms of practical implementation, Python's Beautiful Soup library can be particularly effective for this task, allowing for a relatively straightforward traversal of each div element to construct the JSON object.\n",
    "\n",
    "The end result would be a JSON object where each entry corresponds to a 'div' element in the original HTML structure, represented by a dictionary containing the attributes and content of the div, and potentially another dictionary (or list of dictionaries) representing any nested child div elements. This would effectively capture the data within each div while maintaining the hierarchical structure of the original HTML document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair DOM para Dicionários de um Currículo apenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = connect_driver(caminho)\n",
    "\n",
    "NOME = 'João Hermínio Martins da Silva'\n",
    "fill_name(driver, delay, NOME)\n",
    "\n",
    "limite=3\n",
    "instituicao = 'Fundação Oswaldo Cruz'\n",
    "unidade     = 'Fiocruz Ceará'\n",
    "termo       = 'Ministerio da Saude'\n",
    "\n",
    "elm_vinculo, np.NaN, np.NaN, np.NaN, driver = find_terms(NOME, instituicao, unidade, termo, driver, delay, limite)\n",
    "soup = open_curriculum(driver,elm_vinculo)\n",
    "\n",
    "caracteres = len(soup.text)\n",
    "linhas = len(soup.text.split('\\n'))\n",
    "print(f'Total de caracteres extraídos: {caracteres:6d}')\n",
    "print(f'Quantidade extraída de linhas: {linhas:6d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data_dict = aggregate_data_dicts(soup)\n",
    "aggregated_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos Dados extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar uma lista de dicionários\n",
    "data_list = dict_doi_list(aggregated_data_dict)\n",
    "pprint(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Caso não esteja como dicionário e sim listas ou strings\n",
    "# # Convert the entire string representation to a dictionary\n",
    "# outer_dict = json.loads(data_list[0])\n",
    "# # Iterate over the outer dictionary and convert the inner JSON-like strings to valid dictionaries\n",
    "# for key, value in outer_dict.items():\n",
    "#     outer_dict[key] = json.loads(value)\n",
    "\n",
    "# # Now, outer_dict holds the data in nested dictionary format\n",
    "# pprint(outer_dict)\n",
    "\n",
    "# jcr_properties_list = outer_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data_list)\n",
    "\n",
    "# Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "data.replace(['NULL', None], np.nan, inplace=True)\n",
    "data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "data = data.sort_values(by=['impact-factor','issn','doi'], ascending=False)\n",
    "cout_data_issn = data['data-issn'].notnull().sum()\n",
    "print(f'{cout_data_issn} ISSN recuperados dos  tooltips, {len(data.index)-cout_data_issn} ISSN ausentes')\n",
    "\n",
    "# Preencher \n",
    "data['data-issn'].fillna(data['issn'].apply(standardize_issn_format), inplace=True)\n",
    "\n",
    "count_jcrissn = data['issn'].notna().sum()\n",
    "count_doi = data['doi'].notna().sum()\n",
    "count_jci = data['impact-factor'].notna().sum()\n",
    "\n",
    "print(f'{count_jcrissn} ISSN recuperados de id-artigos, {len(data.index)-count_jcrissn} ISSN ausentes')\n",
    "print(f'{count_doi}  DOI recuperados, {len(data.index)-count_jci}  DOI ausentes')\n",
    "print(f'{count_jci}  JCI recuperados, {len(data.index)-count_jci}  JCI ausentes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complementar com dados do CrossRef "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_crossref = crossref_complement(data)\n",
    "# data_crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtro1 = data_crossref['impact-factor'].isna()\n",
    "# temp = data_crossref[filtro1]\n",
    "# filtro2 = temp['issn'].notna()\n",
    "# consultar = temp[filtro2]\n",
    "# consultar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complementar ISSN de arquivos de texto do JCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(montar_lista_issn(data))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Exemplo dados do Raimir\n",
    "# 30 buscados\n",
    "'1566-2535, 2327-4662, 0890-8044, 1084-8045, 2169-3536, 2169-3536, 2169-3536, 1868-5137, 2076-3417, 1074-5351, 1055-7148, 2494-2715, 2236-0700, 2236-0700, 2076-3417, 1980-086X, 1676-2789, 1676-2789, 1553-877X, 1550-1329, 1548-7709, 1448-5869, 1424-8220, 1424-8220, 1424-8220, 1424-8220, 1424-8220, 1414-5685, 0101-8191, 0101-8191'\n",
    "\n",
    "# 09 duplicados\n",
    "'1980-086X, 1676-2789, 1548-7709, 1448-5869, 2494-2715, 1424-5685, 0101-8191, 2327-4662, 2236-0700'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Em caso de muitos dados de ISSN faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # File paths\n",
    "\n",
    "# file_paths = [\n",
    "#     f'../data/{NOME}_JCR_JournalResults2018.csv',\n",
    "#     f'../data/{NOME}_JCR_JournalResults2019.csv',\n",
    "#     f'../data/{NOME}_JCR_JournalResults2020.csv',\n",
    "#     f'../data/{NOME}_JCR_JournalResults2021.csv',\n",
    "#     f'../data/{NOME}_JCR_JournalResults2022.csv'\n",
    "# ]\n",
    "\n",
    "# # Generate df_aggregated by concatenating all the annual dataframes\n",
    "# df_aggregated = load_and_concatenate(file_paths)\n",
    "# df_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_df_updated = fill_na_from_df_aggregated(df_final, df_aggregated)\n",
    "# sorted_df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch the most recent data for each ISSN or eISSN and get the most recent year\n",
    "# df_most_recent, most_recent_year = get_most_recent_data(df_aggregated)\n",
    "# df_most_recent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo entradas duplicadas\n",
    "# df_unic = df_most_recent.drop_duplicates(subset='ISSN', keep='last')\n",
    "\n",
    "# # Uso da função\n",
    "# updated_data = complement_jcr(data_crossref, df_unic)\n",
    "# updated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Populate the starting dataframe with missing data\n",
    "# updated_df = populate_missing_data(updated_data, df_most_recent, most_recent_year)\n",
    "# updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para prosseguir com extração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final  = df_to_use.sort_values(by=['impact-factor','issn','jcr-ano','doi'], ascending=False)\n",
    "df_final.columns = ['jcr-ano', 'doi', 'data-issn', 'original_title', 'impact-factor',\n",
    "       'issn', 'volume', 'paginaInicial', 'titulo', 'sequencial',\n",
    "       'journal']\n",
    "\n",
    "count_doi_non_nan = df_final['doi'].notna().sum()\n",
    "count_jci_non_nan = df_final['impact-factor'].notna().sum()\n",
    "count_issn_non_nan = df_final['issn'].notna().sum()\n",
    "print(f'{count_doi_non_nan} valores de DOI identificados')\n",
    "print(f'{count_issn_non_nan} valores de ISSN não nulos extraídos')\n",
    "print(f'{count_jci_non_nan} valores de fator de impacto não nulos')\n",
    "\n",
    "df_final.sort_values(by=['impact-factor','issn','doi'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotar dados extraídos de um currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "# data.replace(['NULL', None], np.nan, inplace=True)\n",
    "# data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "# data = data.sort_values(by=['impact-factor','issn'], ascending=False)\n",
    "\n",
    "# # Agrupar pelo 'data-issn', contar ocorrências, maior valor de 'impact-factor' e pegar o 'journal'\n",
    "# grouped = data.groupby('issn').agg(\n",
    "#     count=('issn', 'size'),\n",
    "#     impact_factor_max=('impact-factor', 'max'),\n",
    "#     journal=('journal', max_len_string)\n",
    "# ).reset_index()\n",
    "\n",
    "# # Renomear as colunas para corresponder às suas descrições\n",
    "# grouped.rename(columns={\"count\": \"issn_count\", \"impact_factor_max\": \"impact-factor\"}, inplace=True)\n",
    "\n",
    "# count_nan_issn  = data['issn'].isna().sum()\n",
    "# null_row        = pd.DataFrame({'issn': ['NULL'], 'issn_count': [count_nan_issn]})\n",
    "# data_all_counts = pd.concat([grouped, null_row], ignore_index=True)\n",
    "# data_all_counts = data_all_counts.sort_values(by=['impact-factor'], ascending=False)\n",
    "# print(data_all_counts['issn_count'].sum())\n",
    "# # data_all_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_records = dataframe_to_list_of_dicts(df_final)\n",
    "jcr_properties_list = formatted_records\n",
    "\n",
    "plot_vertbar(jcr_properties_list)\n",
    "plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preencher dados de ISSN faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultar dados do JCR a cada ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avaliar a evolução de cada revista ao longo dos anos quanto ao JCI\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2018.csv'\n",
    "# df2018 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2019.csv'\n",
    "# df2019 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2020.csv'\n",
    "# df2020 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2021.csv'\n",
    "# df2021 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2022.csv'\n",
    "# df2022 = org_lines(file_path)\n",
    "\n",
    "# df_issn = pd.concat([df2018, df2019, df2020, df2021, df2022], ignore_index=True)\n",
    "\n",
    "# df_issn = pd.merge(df1, df2, on='issn', how='inner')\n",
    "# df_issn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesclar eISSN com ISSN para achar mais dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def fill_na_from_df_aggregated(df_final, df_aggregated):\n",
    "#     # Filter df_aggregated for rows where eISSN is NaN\n",
    "#     df_nan_eissn = df_aggregated[df_aggregated['eISSN'].isna()]\n",
    "\n",
    "#     # Iterate through the rows of df_nan_eissn\n",
    "#     for idx, row in df_nan_eissn.iterrows():\n",
    "#         issn_value = row['ISSN']\n",
    "        \n",
    "#         # Find the rows in sorted_df with matching 'issn' and NaN 'impact-factor'\n",
    "#         mask = (df_final['issn'] == issn_value) & df_final['impact-factor'].isna()\n",
    "        \n",
    "#         if mask.sum() > 0:  # if there are matching rows in sorted_df\n",
    "#             df_final.loc[mask, 'impact-factor'] = row['2022 JIF']  # Assuming you want the most recent JIF\n",
    "#             df_final.loc[mask, 'journal'] = row['Journal name']\n",
    "    \n",
    "#     df_final = df_final.sort_values(by=['impact-factor','issn'], ascending=False, na_position='last')\n",
    "    \n",
    "#     return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_json = fill_na_from_df_aggregated(df_final, df_aggregated)\n",
    "# print(len(result_json))\n",
    "# result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chame a função com sua lista de dados jcr_properties_list\n",
    "# plot_vertbar(result_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscal avaliação Qualis Periódicos em arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/makaires77/fioce/main/source/adapters/input/data/sucupira_todas_as_areas_avaliacao1672761192111.csv\"\n",
    "# data_sucupira = pd.read_csv(url, delimiter=';')\n",
    "# data_sucupira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Classe persistência nó Person e nós secundários</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistir nó individual e propriedades em nós de Label: Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j_persister import Neo4jPersister  # Para quando já houver pacote neo4j_persister.py\n",
    "\n",
    "# Create a Neo4jPersister instance and persist the data\n",
    "neo4j_persister = Neo4jPersister(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "neo4j_persister.persist_data(data_dict, \"Person\")\n",
    "\n",
    "# Close the Neo4j connection\n",
    "neo4j_persister.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistir nós secundários e relacionamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AreasHandler object with the required connection details\n",
    "areas_analysis_obj = AreasHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for {name}\n",
    "areas_analysis_obj.create_areas_relations(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "areas_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the JcrHandler object with the required connection details\n",
    "journals_analysis_obj = JcrHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to create Journals nodes and establish relationships for {name}\n",
    "journals_analysis_obj.createJournalsNodes(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "journals_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AdvisorHandler object with the required connection details\n",
    "advisor_analysis_obj = AdvisorHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for {name}\n",
    "advisor_analysis_obj.create_advisor_relations(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "advisor_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ProjectsHandler object with the required connection details\n",
    "projects_analysis_obj = ProjectsHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for {name}\n",
    "projects_analysis_obj.create_projects_relations(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "projects_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ProjectsHandler object with the required connection details\n",
    "articles_analysis_obj = ArticleHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for {name}\n",
    "articles_analysis_obj.process_articles(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "articles_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletar nós por Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handler = DataRemovalHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "# # labels=\"GrandeÁrea\"\n",
    "# # labels=\"Área\"\n",
    "# # labels=\"Subárea\"\n",
    "# labels = 'Artigo'\n",
    "# deleted_count = handler.delete_nodes_by_label(labels)\n",
    "# print(f\"{deleted_count} nós deletados com rótulo {labels}.\")\n",
    "# handler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from typing import Optional\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from pyjarowinkler.distance import get_jaro_distance\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException,\n",
    "    ElementNotInteractableException,\n",
    "    TimeoutException,\n",
    "    WebDriverException\n",
    ")\n",
    "\n",
    "class CurriculumExtractor:\n",
    "    def __init__(self, driver, delay: int = 10, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the WebScraper object.\n",
    "        :param driver: Selenium WebDriver object\n",
    "        :param delay: Time delay for waiting elements to load\n",
    "        :param verbose: Flag to enable logging\n",
    "        :param failed_extractions: list for failed extraction attempts\n",
    "        \"\"\"\n",
    "        self.driver  = driver\n",
    "        self.delay   = delay\n",
    "        self.verbose = verbose\n",
    "        self.failed_extractions = []  # Lista para armazenar extrações falhas\n",
    "        logging.basicConfig(level=logging.INFO if self.verbose else logging.ERROR)\n",
    "\n",
    "    def wait_for_element(self, css_selector: str, ignored_exceptions=None):\n",
    "        \"\"\"\n",
    "        Waits for the element specified by the CSS selector to load.\n",
    "        :param css_selector: CSS selector of the element to wait for\n",
    "        :param ignored_exceptions: List of exceptions to ignore\n",
    "        \"\"\"\n",
    "        WebDriverWait(self.driver, self.delay, ignored_exceptions=ignored_exceptions).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "    def fill_name(self, NOME):\n",
    "        '''\n",
    "        Move cursor to the search field and fill in the specified name.\n",
    "        '''\n",
    "        try:\n",
    "            nome = lambda: self.driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "            nome().send_keys(Keys.CONTROL + \"a\")\n",
    "            nome().send_keys(NOME)\n",
    "            \n",
    "            seletorcss = 'div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "            \n",
    "            seletorcss = \"#botaoBuscaFiltros\"\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(f'  ERRO!! Ao preencher nome no campo de busca, pausando por 1 segundo. {e}, {traceback_str}')\n",
    "    \n",
    "    def achar_busca(self):\n",
    "        '''\n",
    "        Locate and click the Search Resume button.\n",
    "        '''\n",
    "        delay = 10\n",
    "        try:\n",
    "            limite = 5\n",
    "            xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "            self.retry(\n",
    "                lambda: WebDriverWait(self.driver, delay).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                wait_ms=20,\n",
    "                limit=limite,\n",
    "                on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função achar_busca(). {limite} tentativas sem sucesso.')\n",
    "            )\n",
    "            \n",
    "            link_nome = self.driver.find_element(By.XPATH, xpath_nome)\n",
    "            \n",
    "            if link_nome.text is None:\n",
    "                xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "                print('Sem resposta do servidor, tentando novamente...')\n",
    "                self.retry(\n",
    "                    lambda: WebDriverWait(self.driver, delay).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                    wait_ms=200,\n",
    "                    limit=limite,\n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função achar_busca(). {limite} tentativas sem sucesso.')\n",
    "                )\n",
    "                \n",
    "            return link_nome\n",
    "        except TimeoutException as t:\n",
    "            print(f'  ERRO!! Durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "            time.sleep(2)\n",
    "\n",
    "    def paginar(self):\n",
    "        '''\n",
    "        Helper function to page results on the search page\n",
    "        '''\n",
    "        numpaginas = []\n",
    "        css_paginacao = \"div.paginacao:nth-child(2)\"\n",
    "        try:\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "            paginacao = self.driver.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "            paginas = paginacao.text.split(' ')\n",
    "            remover = ['', 'anterior', '...']\n",
    "            numpaginas = [x for x in paginas if x not in remover]\n",
    "        except Exception as e:\n",
    "            print('  ERRO!! Ao rodar função paginar():', e)\n",
    "        return numpaginas\n",
    "\n",
    "    def retry(self, func, expected_ex_type=Exception, limit=0, wait_ms=100,\n",
    "              wait_increase_ratio=2, on_exhaust=\"throw\"):\n",
    "        attempt = 1\n",
    "        while True:\n",
    "            try:\n",
    "                return func()\n",
    "            except Exception as ex:\n",
    "                if not isinstance(ex, expected_ex_type):\n",
    "                    raise ex\n",
    "                if 0 < limit <= attempt:\n",
    "                    if on_exhaust == \"throw\":\n",
    "                        raise ex\n",
    "                    return on_exhaust\n",
    "                attempt += 1\n",
    "                time.sleep(wait_ms / 1000)\n",
    "                wait_ms *= wait_increase_ratio\n",
    "\n",
    "    def find_terms(self, NOME, instituicao, unidade, termo):\n",
    "        elm_vinculo = None\n",
    "        qte_resultados = 0\n",
    "        force_break_loop = False\n",
    "\n",
    "        try:\n",
    "            css_resultados = \".resultado\"\n",
    "            WebDriverWait(self.driver, self.delay, ignored_exceptions=self.ignored_exceptions).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "            resultados = self.driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "            \n",
    "            try:\n",
    "                css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "                WebDriverWait(self.driver, self.delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                div_element = soup.find('div', {'class': 'tit_form'})\n",
    "                match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "                if match:\n",
    "                    qte_resultados = int(match.group(1))\n",
    "                else:\n",
    "                    return np.NaN, NOME, np.NaN, 'Currículo não encontrado', self.driver\n",
    "            except Exception as e1:\n",
    "                print('  ERRO!! Currículo não disponível no Lattes')\n",
    "                return np.NaN, NOME, np.NaN, e1, self.driver\n",
    "\n",
    "            # Full logic for selecting a specific result based on your criteria\n",
    "            for resultado in resultados:\n",
    "                if force_break_loop:\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    nome = resultado.find_element(By.CSS_SELECTOR, \".nome\").text\n",
    "                    vinculos = resultado.find_elements(By.CSS_SELECTOR, \".instituicao\")\n",
    "\n",
    "                    for vinculo in vinculos:\n",
    "                        instituicao_text = vinculo.text.split('-')[0].strip()\n",
    "                        unidade_text = vinculo.text.split('-')[1].strip() if '-' in vinculo.text else None\n",
    "\n",
    "                        if get_jaro_distance(nome, NOME, winkler=True, scaling=0.1) > 0.85 and \\\n",
    "                        instituicao_text == instituicao and \\\n",
    "                        (unidade_text == unidade if unidade is not None else True):\n",
    "                            elm_vinculo = vinculo\n",
    "                            force_break_loop = True\n",
    "                            break\n",
    "\n",
    "                except Exception as e2:\n",
    "                    print(f\"  ERRO!! Problema ao processar o resultado: {e2}\")\n",
    "                    traceback.print_exc()\n",
    "            \n",
    "            try:\n",
    "                elm_vinculo.text\n",
    "            except:\n",
    "                return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', self.driver\n",
    "    \n",
    "        except TimeoutException:\n",
    "            print(\"  ERRO!! O tempo limite de espera foi atingido.\")\n",
    "            return np.NaN, NOME, np.NaN, \"TimeoutException\", self.driver\n",
    "\n",
    "        except WebDriverException as e7:\n",
    "            print(\"  ERRO!! Problema ao interagir com o driver.\")\n",
    "            return np.NaN, NOME, np.NaN, e7, self.driver\n",
    "\n",
    "        except Exception as e8:\n",
    "            print(\"  ERRO!! Um erro inesperado ocorreu.\")\n",
    "            return np.NaN, NOME, np.NaN, e8, self.driver\n",
    "\n",
    "        if elm_vinculo is None:\n",
    "            print(\"Vínculo não foi definido.\")\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', self.driver\n",
    "\n",
    "        return elm_vinculo, np.NaN, np.NaN, np.NaN, self.driver\n",
    "    \n",
    "    def switch_to_new_window(self):\n",
    "        window_before = self.driver.current_window_handle\n",
    "        WebDriverWait(self.driver, self.delay).until(EC.number_of_windows_to_be(2))\n",
    "        window_after = self.driver.window_handles\n",
    "        new_window = [x for x in window_after if x != window_before][0]\n",
    "        self.driver.switch_to.window(new_window)\n",
    "\n",
    "    def handle_stale_file_error(self, max_retries=5, retry_interval=10):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                error_div = self.driver.find_element(By.CSS_SELECTOR, 'resultado')\n",
    "                linha1 = error_div.fidChild('li')\n",
    "                if 'Stale file handle' in linha1.text:\n",
    "                    time.sleep(retry_interval)\n",
    "                else:\n",
    "                    return True\n",
    "            except NoSuchElementException:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def extract_data_from_cvuri(self, element) -> dict:\n",
    "        \"\"\"\n",
    "        Extracts data from the cvuri attribute of the given element.\n",
    "        :param element: WebElement object\n",
    "        :return: Dictionary of extracted data\n",
    "        \"\"\"\n",
    "        cvuri = element.get_attribute('cvuri')\n",
    "        parsed_url = urlparse(cvuri)\n",
    "        params = parse_qs(parsed_url.query)\n",
    "        data_dict = {k: v[0] for k, v in params.items()}\n",
    "        return data_dict\n",
    "\n",
    "    def check_and_click_vinculo(self, elm_vinculo):\n",
    "        if elm_vinculo is None:\n",
    "            raise NoSuchElementException(\"Vínculo não encontrado, passando para o próximo nome...\")\n",
    "        try:\n",
    "            logging.info(f'Vínculo encontrado no currículo de nome: {elm_vinculo.text}')\n",
    "        except AttributeError:\n",
    "            raise AttributeError('Erro ao acessar texto do vínculo, elemento extraído vazio.')\n",
    "        \n",
    "    def extract_tooltip_data(self) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Extracts tooltip data from articles section using Selenium.\n",
    "        :return: List of dictionaries containing the extracted tooltip data\n",
    "        \"\"\"\n",
    "        tooltip_data_list = []\n",
    "        try:\n",
    "            self.wait_for_element(\"#artigos-completos img.ajaxJCR\", [TimeoutException])\n",
    "            layout_cells = self.driver.find_elements(By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5')\n",
    "\n",
    "            for cell in layout_cells:\n",
    "                tooltip_data = {}\n",
    "                try:\n",
    "                    elem_citado = cell.find_element(By.CSS_SELECTOR, '.citado')\n",
    "                    tooltip_data.update(self.extract_data_from_cvuri(elem_citado))\n",
    "                except (ElementNotInteractableException, NoSuchElementException):\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    doi_elem = cell.find_element(By.CSS_SELECTOR, \"a.icone-producao.icone-doi\")\n",
    "                    tooltip_data[\"doi\"] = doi_elem.get_attribute(\"href\")\n",
    "                except NoSuchElementException:\n",
    "                    tooltip_data[\"doi\"] = None\n",
    "\n",
    "                try:\n",
    "                    self.wait_for_element(\"img.ajaxJCR\", [TimeoutException])\n",
    "                    tooltip_elem = self.driver.find_element(By.CSS_SELECTOR, \"img.ajaxJCR\")\n",
    "                    ActionChains(self.driver).move_to_element(tooltip_elem).perform()\n",
    "                    \n",
    "                    original_title = tooltip_elem.get_attribute(\"original-title\")\n",
    "                    match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "                    tooltip_data[\"impact-factor\"] = match.group(1) if match else None\n",
    "                    tooltip_data[\"original_title\"] = original_title.split('<br />')[0].strip()\n",
    "\n",
    "                except (NoSuchElementException, TimeoutException):\n",
    "                    pass\n",
    "                \n",
    "                tooltip_data_list.append(tooltip_data)\n",
    "\n",
    "            if self.verbose:\n",
    "                logging.info(f'Metadata of {len(tooltip_data_list)} complete articles extracted.')\n",
    "        except TimeoutException as e:\n",
    "            logging.error(f\"Element not found within specified time: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return tooltip_data_list\n",
    "    \n",
    "    def open_curriculum(self, elm_vinculo) -> dict:\n",
    "        \"\"\"\n",
    "        Opens a curriculum link and extracts data.\n",
    "        :param elm_vinculo: WebElement of the link to open\n",
    "        :return: Dictionary containing soup extracted data and tooltips\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.check_and_click_vinculo(elm_vinculo)\n",
    "            self.switch_to_new_window()\n",
    "            tooltip_data_list = self.extract_tooltip_data()\n",
    "            page_source = self.driver.page_source\n",
    "            if page_source:\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                return {\"soup\": soup, \"tooltips\": tooltip_data_list}\n",
    "            else:\n",
    "                raise Exception(\"Page source is None.\")\n",
    "        except NoSuchElementException as e:\n",
    "            logging.error(f\"Element not found: {e}\")\n",
    "        except TimeoutException as e:\n",
    "            logging.error(f\"Operation timed out: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "        return None\n",
    "\n",
    "    def extract_tit1_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        \n",
    "        # Títulos contendo subseções\n",
    "        tit1a = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar',\n",
    "                'Linhas de pesquisa','Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento', 'Revisor de periódico','Revisor de projeto de fomento','Áreas de atuação','Idiomas','Inovação']\n",
    "\n",
    "        tit1b = ['Atuação Profissional'] # dados com subseções\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            \n",
    "            # Verifique se o título está na lista 'tit1'\n",
    "            if titulo in tit1a:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                \n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                    divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                    keys = []\n",
    "                    vals = []\n",
    "\n",
    "                    for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "                        if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "                            key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                            key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            keys.append(key_text)\n",
    "                            val = j.find('div', class_='layout-cell-pad-5')\n",
    "                            val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            vals.append(val_text)\n",
    "                            if verbose:\n",
    "                                print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "                    agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                    data_dict[titulo] = Neo4jPersister.convert_to_primitives(agg_dict)\n",
    "            \n",
    "            if titulo in tit1b:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                \n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "\n",
    "                            if 'layout-cell-3' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        data_dict[titulo][section_name].append(current_data)  # Armazenamos os dados em uma lista\n",
    "\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit2_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        \n",
    "        database = ''\n",
    "        total_trab_text = 0\n",
    "        total_cite_text = 0\n",
    "        num_fator_h = 0\n",
    "        data_wos_text = ''\n",
    "\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')    \n",
    "        \n",
    "        tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            \n",
    "            # Verifique se o título está na lista 'tit2'\n",
    "            if titulo in tit2:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = {}\n",
    "                        if verbose:\n",
    "                            print(f'Seção: {section_name}')\n",
    "\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_subsection = None\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                        if section_name == 'Produção bibliográfica':\n",
    "                            subsections = section.find_next_siblings('div', class_='cita-artigos')\n",
    "                            if verbose:\n",
    "                                print(len(subsections), 'subseções')                       \n",
    "                            for subsection in subsections:                            \n",
    "                                if subsection:\n",
    "                                    subsection_name = subsection.find('b').get_text().strip()\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}') # nomes de subseção como ocorrências \n",
    "                                        print(f'    {len(subsection)} divs na subseção {subsection_name}')                                \n",
    "                                    if subsection_name == 'Citações':\n",
    "                                        current_subsection = subsection_name\n",
    "                                        data_dict[titulo][section_name]['Citações'] = {}\n",
    "                                        sub_section_list = []\n",
    "                                            \n",
    "                                        ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "                                        next_siblings = subsection.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "                                        for sibling in next_siblings:\n",
    "                                            citation_counts = sibling.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que tem os Valores de Citações\n",
    "                                            if citation_counts:\n",
    "                                                for i in citation_counts:\n",
    "                                                    database = i.get_text()\n",
    "                                                    total_trab = i.find_next_sibling(\"div\", class_=\"trab\")\n",
    "                                                    if total_trab:\n",
    "                                                        total_trab_text = total_trab.get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                                    total_cite = i.find_next_sibling(\"div\", class_=\"cita\")\n",
    "                                                    if total_cite:\n",
    "                                                        total_cite_text = total_cite.get_text().split(\"Total de citações:\")[1]\n",
    "                                                    fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                                    num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                                    data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\")\n",
    "                                                    if data_wos:\n",
    "                                                        try:\n",
    "                                                            data_wos_text = data_wos.get_text().split(\"Data:\")[1].strip()\n",
    "                                                        except:\n",
    "                                                            data_wos_text = data_wos.get_text()\n",
    "\n",
    "                                                    # Converta os valores para tipos de dados adequados\n",
    "                                                    total_trab = int(total_trab_text)\n",
    "                                                    total_cite = int(total_cite_text)\n",
    "\n",
    "                                                    citation_numbers = {\n",
    "                                                        \"Database\": database,\n",
    "                                                        \"Total de trabalhos\": total_trab,\n",
    "                                                        \"Total de citações\": total_cite,\n",
    "                                                        \"Índice_H\": num_fator_h,\n",
    "                                                        \"Data\": data_wos_text\n",
    "                                                    }\n",
    "\n",
    "                                                    # Verifique se a subseção atual já existe no dicionário\n",
    "                                                    if 'Citações' not in data_dict[titulo][section_name]:\n",
    "                                                        data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "\n",
    "                                                    data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "\n",
    "                                                    if verbose:\n",
    "                                                        print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "                                \n",
    "                            ## Encontrar a div irmã de div subseção com classe layout-cell-12 com artigos\n",
    "                            vals_jcr = []\n",
    "                            div_artigo_geral = data_cell.findChild(\"div\", id=\"artigos-completos\")\n",
    "                            if verbose:\n",
    "                                print(f'Encontrada {len(div_artigo_geral)} div geral de artigos')  \n",
    "                            \n",
    "                            if div_artigo_geral:\n",
    "                                divs_artigos = div_artigo_geral.find_all('div', class_='artigo-completo')\n",
    "                                if verbose:\n",
    "                                    print(len(divs_artigos), 'divs de artigos')\n",
    "                                \n",
    "                                current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                                if divs_artigos:                              \n",
    "                                    for div_artigo in divs_artigos:\n",
    "                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = {}                                   \n",
    "                                            ## Extrair filhos da classes de artigos completos que estão à frente\n",
    "                                        sibling = div_artigo.findChild()\n",
    "\n",
    "                                        while sibling:\n",
    "                                            classes = sibling.get('class', [])\n",
    "\n",
    "                                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                                    info_dict = {\n",
    "                                                        'data-issn': 'NULL',\n",
    "                                                        'impact-factor': 'NULL',  \n",
    "                                                        'jcr-year': 'NULL',\n",
    "                                                    }\n",
    "                                                    # Remova as tags span da div\n",
    "                                                    for span in sibling.find_all('span'):\n",
    "                                                        span.extract()\n",
    "                                                    \n",
    "                                                    val_text = sibling.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "\n",
    "                                                    current_data[key] = val_text\n",
    "                                                    if verbose:\n",
    "                                                        print(len(current_data.values()), key, val)\n",
    "\n",
    "                                                    sup_element = sibling.find('sup')\n",
    "\n",
    "                                                    if sup_element:\n",
    "                                                        raw_jcr_data = sup_element.get_text()\n",
    "                                                        # print('sup_element:',sup_element)\n",
    "                                                        img_element = sup_element.find('img')\n",
    "                                                        # print('img_element:',img_element)                                                    \n",
    "                                                        if img_element:\n",
    "                                                            original_title = img_element.get('original-title')\n",
    "                                                            if original_title:\n",
    "                                                                info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                                if info_list != 'NULL':\n",
    "                                                                    issn = format_string(img_element.get('data-issn'))\n",
    "                                                                    if verbose:\n",
    "                                                                        print(f'impact-factor: {info_list[1].split(\": \")[1]}')\n",
    "                                                                    info_dict = {\n",
    "                                                                        'data-issn': issn,\n",
    "                                                                        'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                        'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')',''),\n",
    "                                                                        'journal': info_list[0],\n",
    "                                                                    }\n",
    "                                                            else:\n",
    "                                                                if verbose:\n",
    "                                                                    print('Entrou no primeiro Else')\n",
    "                                                                issn = format_string(img_element.get('data-issn'))\n",
    "                                                                info_dict = {\n",
    "                                                                    'data-issn': issn,\n",
    "                                                                    'impact-factor': 'NULL',\n",
    "                                                                    'jcr-year': 'NULL',\n",
    "                                                                    'journal': 'NULL',\n",
    "                                                                }\n",
    "                                                    else:\n",
    "                                                        if verbose:\n",
    "                                                                    print('Entrou no segundo Else')\n",
    "                                                        info_dict = {\n",
    "                                                            'data-issn': 'NULL',\n",
    "                                                            'impact-factor': 'NULL',\n",
    "                                                            'jcr-year': 'NULL',\n",
    "                                                            'journal': 'NULL',\n",
    "                                                        }                                                                \n",
    "                                                        \n",
    "                                                    vals_jcr.append(info_dict)\n",
    "                                                    if verbose:\n",
    "                                                        print(f'         {info_dict}')\n",
    "\n",
    "                                                if 'JCR' not in data_dict:\n",
    "                                                    data_dict['JCR'] = []\n",
    "                                                \n",
    "                                                if verbose:\n",
    "                                                    print(len(vals_jcr))\n",
    "                                                data_dict['JCR'] = vals_jcr\n",
    "\n",
    "                                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                                next_sibling = sibling.find_next_sibling()\n",
    "                                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                                    sibling = None\n",
    "                                                else:\n",
    "                                                    if current_data:\n",
    "                                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = converted_data\n",
    "\n",
    "                                            if sibling:\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "                        else:\n",
    "                            while sibling:\n",
    "                                classes = sibling.get('class', [])\n",
    "\n",
    "                                if 'cita-artigos' in classes:  # Subsection start\n",
    "                                    subsection_name = sibling.find('b').get_text().strip()\n",
    "                                    current_subsection = subsection_name\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}')\n",
    "                                    data_dict[titulo][section_name][current_subsection] = {}\n",
    "                                    current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "\n",
    "                                elif 'layout-cell-1' in classes:  # Data key\n",
    "                                    key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                    if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                        val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                        current_data[key] = val\n",
    "\n",
    "                                elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "                                    next_sibling = sibling.find_next_sibling()\n",
    "                                    if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                        sibling = None\n",
    "                                    else:\n",
    "                                        if current_subsection:\n",
    "                                            data_dict[titulo][section_name][current_subsection] = Neo4jPersister.convert_to_primitives(current_data)  # Armazenamos os dados da subseção atual\n",
    "                                if sibling:\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "        \n",
    "        # Verifique se os dados dos tooltips estão presentes no objeto soup\n",
    "        if 'tooltips' in soup.attrs:\n",
    "            tooltips_data = soup.attrs['tooltips']\n",
    "            agg = []\n",
    "            \n",
    "            for tooltip in tooltips_data:\n",
    "                agg_data = {}\n",
    "                \n",
    "                # Extração do ano JCR a partir do \"original_title\"\n",
    "                if tooltip.get(\"original_title\"):\n",
    "                    jcr_year = tooltip[\"original_title\"].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')','')\n",
    "                    agg_data[\"jcr-ano\"] = jcr_year\n",
    "                \n",
    "                # Adicionar todas as chaves e valores do tooltip ao dicionário agg_data\n",
    "                for key, value in tooltip.items():\n",
    "                    agg_data[key] = value\n",
    "                \n",
    "                agg.append(agg_data)\n",
    "            \n",
    "            data_dict['JCR2'] = agg\n",
    "        else:\n",
    "            print('Não foram achados os dados de tooltip')\n",
    "            print(soup.attrs)  \n",
    "            \n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit3_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        \n",
    "        # Títulos da seção 'Eventos'\n",
    "        tit3 = ['Eventos']\n",
    "\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit3'\n",
    "            if titulo in tit3:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "                \n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "\n",
    "                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                        data_dict[titulo][section_name] = converted_data\n",
    "\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "    def aggregate_data_dicts(self, soup):\n",
    "        \"\"\"\n",
    "        Aggregates data from various dictionary sources into a consolidated nested dictionary, \n",
    "        ensuring that all nested lists within the dictionaries are transformed into nested dictionaries.\n",
    "        \n",
    "        Parameters:\n",
    "        - soup: BeautifulSoup object, representing the parsed HTML content.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: An aggregated dictionary containing the consolidated data.\n",
    "        \"\"\"\n",
    "        \n",
    "        def convert_list_to_dict(lst):\n",
    "            \"\"\"\n",
    "            Converts a list into a dictionary with indices as keys.\n",
    "            \n",
    "            Parameters:\n",
    "            - lst: list, input list to be transformed.\n",
    "            \n",
    "            Returns:\n",
    "            - dict: Transformed dictionary.\n",
    "            \"\"\"\n",
    "            return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "        def merge_dict(d1, d2):\n",
    "            \"\"\"\n",
    "            Recursively merges two dictionaries, transforming nested lists into dictionaries.\n",
    "            \n",
    "            Parameters:\n",
    "            - d1: dict, the primary dictionary into which data is merged.\n",
    "            - d2: dict or list, the secondary dictionary or list from which data is sourced.\n",
    "            \n",
    "            Returns:\n",
    "            - None\n",
    "            \"\"\"\n",
    "            # If d2 is a list, convert it to a dictionary first\n",
    "            if isinstance(d2, list):\n",
    "                d2 = convert_list_to_dict(d2)\n",
    "            \n",
    "            for key, value in d2.items():\n",
    "                if isinstance(value, list):\n",
    "                    d2[key] = convert_list_to_dict(value)\n",
    "                if key in d1 and isinstance(d1[key], dict) and isinstance(value, dict):\n",
    "                    merge_dict(d1[key], value)\n",
    "                else:\n",
    "                    d1[key] = value\n",
    "\n",
    "\n",
    "        # Extract necessary information from soup\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "        name = info_list[0]\n",
    "\n",
    "        # Initialization of the aggregated_data dictionary\n",
    "        aggregated_data = {\"labels\": \"Person\", \"name\": name, \"InfPes\": info_list, \"Resumo\": [elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()]}\n",
    "\n",
    "        # Data extraction and merging\n",
    "        for data_extraction_func in [extract_tit1_soup, extract_tit2_soup, extract_tit3_soup]:\n",
    "            extracted_sections = data_extraction_func(soup, verbose=False)\n",
    "            for title, data in extracted_sections.items():\n",
    "                if title not in aggregated_data:\n",
    "                    aggregated_data[title] = {}\n",
    "                merge_dict(aggregated_data[title], data)\n",
    "\n",
    "        return aggregated_data\n",
    "\n",
    "    def to_json(self, data_list: list[dict], filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Converts processed data to JSON.\n",
    "        :param data_list: List of dictionaries containing the processed data\n",
    "        :param filename: Name of the JSON file\n",
    "        \"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data_list, f)\n",
    "\n",
    "    def to_hdf5(self, data_list: list[dict], filename: str, directory=None) -> None:\n",
    "        \"\"\"\n",
    "        Converts processed data to HDF5.\n",
    "        :param data_list: List of dictionaries containing the processed data\n",
    "        :param filename: Name of the HDF5 file\n",
    "        :param directory: Optional directory path\n",
    "        \"\"\"\n",
    "        converter = DictToHDF5(data_list)\n",
    "        converter.create_dataset(filename, directory)\n",
    "\n",
    "    def process_single_result(self, extracted_data: dict, json_filename: str, hdf5_filename: str) -> Optional[dict]:\n",
    "        \"\"\"\n",
    "        Processes a single extracted result and saves it in JSON and HDF5 formats.\n",
    "        :param extracted_data: Dictionary containing the extracted data\n",
    "        :param json_filename: Name of the JSON file to save\n",
    "        :param hdf5_filename: Name of the HDF5 file to save\n",
    "        :return: Processed dictionary or None\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            processed_data = {}\n",
    "            processed_data['name'] = extracted_data.get('name', 'N/A')\n",
    "            processed_data['affiliation'] = extracted_data.get('affiliation', 'N/A')\n",
    "            processed_data['publications'] = int(extracted_data.get('publications', 0))\n",
    "\n",
    "            # Additional code for converting to JSON and HDF5 formats\n",
    "            self.to_json(processed_data, json_filename)\n",
    "            self.to_hdf5(processed_data, hdf5_filename)\n",
    "\n",
    "            return processed_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during single result processing: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_all_results(self, all_extracted_data, json_filename: str, hdf5_filename: str):\n",
    "        \"\"\"\n",
    "        Processes all extracted results and saves them in JSON and HDF5 formats.\n",
    "        :param all_extracted_data: List of dictionaries containing the extracted data\n",
    "        :param json_filename: Name of the JSON file to save\n",
    "        :param hdf5_filename: Name of the HDF5 file to save\n",
    "        :return: List of successfully processed data dictionaries\n",
    "        \"\"\"\n",
    "        successful_processed_data = []\n",
    "        for extracted_data in all_extracted_data:\n",
    "            processed_data = self.process_single_result(extracted_data, json_filename, hdf5_filename)\n",
    "            if processed_data is not None:\n",
    "                successful_processed_data.append(processed_data)\n",
    "            else:\n",
    "                self.failed_extractions.append(extracted_data)\n",
    "\n",
    "        # Attempt to reprocess failed extractions\n",
    "        if self.failed_extractions:\n",
    "            logging.info(\"Retrying failed extractions...\")\n",
    "            for failed_data in self.failed_extractions:\n",
    "                processed_data = self.process_single_result(failed_data, json_filename, hdf5_filename)\n",
    "                if processed_data is not None:\n",
    "                    successful_processed_data.append(processed_data)\n",
    "\n",
    "        # Performing a single write operation for optimization\n",
    "        self.to_json(successful_processed_data, json_filename)\n",
    "        self.to_hdf5(successful_processed_data, hdf5_filename)\n",
    "\n",
    "        return successful_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import logging\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# class WebScrapingEnhanced:\n",
    "#     def __init__(self, neo4j_persister):\n",
    "#         self.neo4j_persister = neo4j_persister\n",
    "#         logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#     def _make_request(self, url, params=None):\n",
    "#         try:\n",
    "#             response = requests.get(url, params=params)\n",
    "#             response.raise_for_status()\n",
    "#             return response\n",
    "#         except requests.RequestException as e:\n",
    "#             logging.error(f\"An error occurred while making a request to {url}: {e}\")\n",
    "#             return None\n",
    "\n",
    "#     def _parse_html(self, html_content):\n",
    "#         return BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     def _persist_data(self, data):\n",
    "#         try:\n",
    "#             self.neo4j_persister.persist(data)\n",
    "#             return True\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"An error occurred while persisting data: {e}\")\n",
    "#             return False\n",
    "\n",
    "#     def scrape_and_persist(self, url, scrape_function):\n",
    "#         response = self._make_request(url)\n",
    "#         if not response:\n",
    "#             return False\n",
    "\n",
    "#         soup = self._parse_html(response.text)\n",
    "#         data = scrape_function(soup)\n",
    "#         if not data:\n",
    "#             logging.info(\"No data to persist.\")\n",
    "#             return False\n",
    "\n",
    "#         return self._persist_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def open_curriculum(driver, elm_vinculo, verbose=False):\n",
    "    #     \"\"\"\n",
    "    #     Função principal para extrair dados de cada página de currículo.\n",
    "        \n",
    "    #     Parameters:\n",
    "    #         - driver (webdriver object): The Selenium webdriver object.\n",
    "    #         - elm_vinculo (WebElement): O objeto achado pelas funções anteriores.    \n",
    "    #     Returns:\n",
    "    #         Dicionário com dados extraídos do elemento soup após processado\n",
    "    #     \"\"\"\n",
    "    #     link_nome = achar_busca(driver, delay)\n",
    "    #     window_before = driver.current_window_handle\n",
    "        \n",
    "    #     if elm_vinculo == np.NaN:\n",
    "    #         print('Vínculo não encontrado, passando para o próximo nome...')\n",
    "    #         return\n",
    "    #     else:\n",
    "    #         try:\n",
    "    #             print('Vínculo encontrado no currículo de nome:', elm_vinculo.text)\n",
    "    #         except AttributeError:\n",
    "    #             print('  ERRO!! Ao acessar texto do vínculo, elemento extraído vazio.')\n",
    "    #             return \n",
    "        \n",
    "    #     # Clicar no botão \"Abrir Currículo\" e mudar de aba\n",
    "    #     try:\n",
    "    #         link_nome = achar_busca(driver, delay)\n",
    "    #     except Exception as e:\n",
    "    #         print('Erro')\n",
    "    #         print(e)\n",
    "\n",
    "    #     limite = 2\n",
    "    #     if link_nome.text == None:\n",
    "    #         xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "    #         print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "    #         retry(WebDriverWait(driver, delay).until(\n",
    "    #             EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "    #             wait_ms=200,\n",
    "    #             limit=limite,\n",
    "    #             on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))    \n",
    "    #     try:\n",
    "    #         ActionChains(driver).click(link_nome).perform()\n",
    "    #     except:\n",
    "    #         print(f'Currículo não encontrado.')\n",
    "\n",
    "    #     retry(WebDriverWait(driver, delay).until(\n",
    "    #         EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "    #         wait_ms=200,\n",
    "    #         limit=limite,\n",
    "    #         on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))\n",
    "\n",
    "    #     # Clicar no botão para abrir o currículo\n",
    "    #     btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "    #         EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "    #     time.sleep(0.2)\n",
    "\n",
    "    #     ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "    #     # Gerenciar janelas abertas no navegador\n",
    "    #     WebDriverWait(driver, delay).until(EC.number_of_windows_to_be(2))\n",
    "    #     window_after = driver.window_handles\n",
    "    #     new_window = [x for x in window_after if x != window_before][0]\n",
    "    #     driver.switch_to.window(new_window)\n",
    "\n",
    "    #     # Definir soup fora do loop para que esteja acessível em todo o escopo\n",
    "    #     soup = None\n",
    "\n",
    "    #     # Extração dos dados em tooltips em <div id=\"artigos-completos\">    \n",
    "    #     try:\n",
    "    #         # Esperar para garantir que todos os elementos da seção \"#artigos-completos\" foram carregados\n",
    "    #         WebDriverWait(driver, 60).until(\n",
    "    #             EC.presence_of_all_elements_located((\n",
    "    #                 By.CSS_SELECTOR, \"#artigos-completos img.ajaxJCR\"))\n",
    "    #         )\n",
    "\n",
    "    #         tooltip_data_list = []\n",
    "    #         impact_factor = ''\n",
    "    #         issn = ''\n",
    "    #         journal = ''\n",
    "    #         impact_factor = ''\n",
    "    #         doi_link = ''\n",
    "                    \n",
    "    #         # Localizar a div principal pelas classes de div\n",
    "    #         WebDriverWait(driver, 60).until(\n",
    "    #             EC.presence_of_all_elements_located((\n",
    "    #                 By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5'))\n",
    "    #                 )\n",
    "    #         layout_cells = driver.find_elements(By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5')\n",
    "    #         if verbose:\n",
    "    #             print(len(layout_cells), 'células principais de dados encontradas')\n",
    "\n",
    "    #         for cell in layout_cells:\n",
    "    #             cvuri_dict = {}  # Defina um valor padrão vazio para cvuri_dict\n",
    "                \n",
    "    #             # Extrair ISSN da classe \".citado\"\n",
    "    #             try:\n",
    "    #                 WebDriverWait(driver, 60).until(\n",
    "    #                     EC.presence_of_all_elements_located((\n",
    "    #                         By.CSS_SELECTOR, '.citado'))\n",
    "    #                         )\n",
    "    #                 elem_citado = cell.find_element(By.CSS_SELECTOR, '.citado')\n",
    "    #                 cvuri_dict = extract_data_from_cvuri(elem_citado)\n",
    "    #                 if verbose:\n",
    "    #                     print(f'Dados artigo em cvuri: {cvuri_dict}')\n",
    "    #             except ElementNotInteractableException as e:\n",
    "    #                 print('Conteúdo do erro:')\n",
    "    #                 print(e)\n",
    "    #                 continue\n",
    "    #             except NoSuchElementException:\n",
    "    #                 continue\n",
    "    #             except Exception as e:\n",
    "    #                 print('O erro é o seguinte:')\n",
    "    #                 print(e)\n",
    "    #                 continue\n",
    "                \n",
    "    #             # Extrair o DOI\n",
    "    #             try:\n",
    "    #                 doi_elem = cell.find_element(By.CSS_SELECTOR, \"a.icone-producao.icone-doi\")\n",
    "    #                 doi_link = doi_elem.get_attribute(\"href\")\n",
    "    #                 issn_match = re.search(r\"issn=(\\d+)\", doi_link)\n",
    "    #                 issn_from_doi = issn_match.group(1) if issn_match else None\n",
    "    #             except NoSuchElementException:\n",
    "    #                 doi_link = None\n",
    "    #                 issn_from_doi = None\n",
    "\n",
    "    #             # Extrair dados do JCR dos tooltips melhorado\n",
    "    #             try:\n",
    "    #                 # Espera até que o elemento seja encontrado e esteja visível na página\n",
    "    #                 element_present = EC.presence_of_element_located((By.CSS_SELECTOR, \"img.ajaxJCR\"))\n",
    "    #                 WebDriverWait(driver, 10).until(element_present)  # 10 é o tempo máximo de espera em segundos\n",
    "                    \n",
    "    #                 tooltip_elem = driver.find_element(By.CSS_SELECTOR, \"img.ajaxJCR\")\n",
    "    #                 ActionChains(driver).move_to_element(tooltip_elem).perform()\n",
    "\n",
    "    #                 original_title = tooltip_elem.get_attribute(\"original-title\")\n",
    "    #                 journal = original_title.split('<br />')[0].strip()\n",
    "    #                 match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "    #                 if match:\n",
    "    #                     impact_factor = match.group(1)\n",
    "    #                 data_issn = issn_from_doi or tooltip_elem.get_attribute(\"data-issn\")\n",
    "    #                 if not data_issn:\n",
    "    #                     # Código para lidar com a falta de data_issn\n",
    "    #                     pass\n",
    "\n",
    "    #             except NoSuchElementException:\n",
    "    #                 data_issn = issn_from_doi\n",
    "    #             except TimeoutException:\n",
    "    #                 # Código para lidar com o timeout\n",
    "    #                 pass\n",
    "\n",
    "    #             # Compile os dados\n",
    "    #             issn = format_string(data_issn)\n",
    "    #             tooltip_data = {\n",
    "    #                 \"doi\": doi_link,\n",
    "    #                 \"data-issn\": issn,\n",
    "    #                 \"original_title\": journal,\n",
    "    #                 \"impact-factor\": impact_factor,\n",
    "    #             }\n",
    "    #             tooltip_data.update(cvuri_dict)\n",
    "    #             tooltip_data_list.append(tooltip_data)\n",
    "\n",
    "    #         print(f'Metadados de {len(tooltip_data_list)} artigos completos extraídos')\n",
    "    \n",
    "    #         page_source = driver.page_source\n",
    "    #         # Use BeautifulSoup to parse\n",
    "    #         if page_source is not None:\n",
    "    #             soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    #             soup.attrs['tooltips'] = tooltip_data_list\n",
    "        \n",
    "    #     except TimeoutException:\n",
    "    #         print(\"O elemento não foi encontrado no tempo especificado.\")\n",
    "    #         page_source = driver.page_source\n",
    "    #         if page_source is not None:\n",
    "    #             soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         print(  'ERRO!! Ao extrair dados do currículo... passando ao próximo')\n",
    "    #         print(e)\n",
    "    #         page_source = driver.page_source\n",
    "    #         if page_source is not None:\n",
    "    #             soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    #     # driver.quit()  \n",
    "\n",
    "    #     return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes de extração de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_for_level(level: int):\n",
    "    return df_areas['Codigo'].str.split('.', expand=True)[level].nunique()\n",
    "\n",
    "# Remover o sufixo após o hífen\n",
    "def count_unique_for_last_level():\n",
    "    return df_areas['Codigo'].str.split('.', expand=True).iloc[:, -1].str.split('-').str[0].nunique()\n",
    "\n",
    "levels = df_areas['Codigo'].str.count(\"\\.\").iloc[0]  # conta a quantidade de pontos, para determinar o número de níveis\n",
    "\n",
    "unique_counts = [count_unique_for_level(i) for i in range(levels)]\n",
    "unique_counts.append(count_unique_for_last_level())\n",
    "\n",
    "# qte_grandeareas, qte_areas, qte_subareas, qte_especialidades = unique_counts\n",
    "# print(f'Quantidades de codigos:')\n",
    "# print(f'  Grande_Área: {qte_grandeareas:2}')\n",
    "# print(f'         Área: {qte_areas:2}')\n",
    "# print(f'      Subárea: {qte_subareas:2}')\n",
    "# print(f'Especialidade: {qte_especialidades:2}')\n",
    "\n",
    "# Dividir a coluna 'Codigo' em várias colunas\n",
    "df_split = df_areas['Codigo'].str.split('.', expand=True)\n",
    "\n",
    "# Remover o último hífen e dígito das colunas \n",
    "df_split.iloc[:, -1] = df_split.iloc[:, -1].str.split('-').str[0]\n",
    "\n",
    "def count_sublevels(df, level):\n",
    "    if level == 0:\n",
    "        return df[0].nunique()\n",
    "    else:\n",
    "        return df.groupby(list(range(level)))[level].nunique().reset_index(name=\"count\")[\"count\"].to_list()\n",
    "\n",
    "sublevels_counts = [count_sublevels(df_split, i) for i in range(df_split.shape[1])]\n",
    "\n",
    "# Criar uma coluna para armazenar a contagem de subníveis\n",
    "# df_areas['SublevelCount'] = df_split.apply(lambda row: [sublevels_counts[col][row[:col].astype(str).tolist().index(row[col-1]) if row[col-1] in row[:col].astype(str).tolist() else -1] if col > 0 else sublevels_counts[col] for col in df_split.columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def contar_marcadores(texto):\n",
    "    padrao = r'\\.00'\n",
    "    ocorrencias = re.findall(padrao, texto)\n",
    "    return len(ocorrencias)\n",
    "\n",
    "cat_grandeareas=[]\n",
    "cat_subareas=[]\n",
    "cat_areas=[]\n",
    "cat_especialidades=[]\n",
    "\n",
    "for cod,des in zip(df_areas['Codigo'],df_areas['Descricao']):\n",
    "    k = contar_marcadores(cod)\n",
    "    if k==3:\n",
    "        cat_grandeareas.append((cod,des))\n",
    "    elif k==2:\n",
    "        cat_areas.append((cod,des))\n",
    "    elif k==1:\n",
    "        cat_subareas.append((cod,des))\n",
    "    elif k==0:\n",
    "        cat_especialidades.append((cod,des))\n",
    "    else:\n",
    "        print('  ERRO!! Ao separar na função contar marcadores()')\n",
    "        print(f'{k} {cod}{des}')\n",
    "\n",
    "print(f'{len(cat_grandeareas)+len(cat_subareas)+len(cat_areas)+len(cat_especialidades):4} Total de nós de classificação')\n",
    "print(f'{len(cat_grandeareas):4} GrandeÁrea')\n",
    "print(f'{len(cat_areas):4} Área')\n",
    "print(f'{len(cat_subareas):4} Subárea')\n",
    "print(f'{len(cat_especialidades):4} Especialidade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_areas[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_grandeareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in cat_grandeareas:\n",
    "    print(i.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_areas[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_subareas[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in cat_subareas[:6]:\n",
    "    print('.'.join(i.split('.')[0]))\n",
    "    print('.'.join(i.split('.')[0:2]))\n",
    "    print('.'.join(i.split('.')[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, j in cat_areas[:6]:\n",
    "#     print('.'.join(i.split('.')[0]))\n",
    "#     print('.'.join(i.split('.')[0:2]))\n",
    "#     print('.'.join(i.split('.')[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_especialidades[:12]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
