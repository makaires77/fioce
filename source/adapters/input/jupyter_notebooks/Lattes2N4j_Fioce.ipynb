{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Análise exploraratória para avaliar utilização dos dados dos currículos Lattes para<br /> propor modelo de Grafo para análises em PDI. </center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa – Fiocruz Ceará\n",
    "\n",
    "**Introdução**\n",
    "\n",
    "A análise de Grafos permite obter insights como produtos de análises em contextos da realidade com base em modelos capazes de lidar dados heterogêneos e relações complexas.\n",
    "\n",
    "\n",
    "Neste trabalho propomos uma análise dos dados de pesquisa acadêmica tendo como fonte de dados os currículo Lattes de servidores da unidade Fiocruz Ceará.\n",
    "\n",
    "**Objetivo geral:**\n",
    "\n",
    "    Explorar dados dos currículos de servidores da Fiocruz Ceará.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "\n",
    "    1. Extrair dados dos currículos;\n",
    "    2. Propor modelo de grafo para análises futuras;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 0: Preparar e Testar Ambiente</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "        \n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_amb():\n",
    "    ## Visualizar versões dos principais componentes\n",
    "    import os\n",
    "    import pip\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # !pip3 install shutup\n",
    "    # import shutup; shutup.please()\n",
    "    \n",
    "    pyVer      = sys.version\n",
    "    pipVer     = pip.__version__\n",
    "    \n",
    "    print('\\nVERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT')\n",
    "    print('Interpretador em uso:', sys.executable)\n",
    "    print('    Ambiente ativado:',os.environ['CONDA_DEFAULT_ENV'])\n",
    "    print('     Python: '+pyVer, '\\n        Pip:', pipVer,'\\n'\n",
    "         )\n",
    "\n",
    "    !nvcc -V\n",
    "\n",
    "def get_cpu_info_windows():\n",
    "    import subprocess\n",
    "\n",
    "    try:\n",
    "        return subprocess.check_output(\"wmic cpu get Name\", shell=True).decode('utf-8').split('\\n')[1].strip()\n",
    "    except:\n",
    "        return \"Informação não disponível\"\n",
    "\n",
    "def get_cpu_info_unix():\n",
    "    import subprocess\n",
    "    try:\n",
    "        return subprocess.check_output(\"lscpu\", shell=True).decode('utf-8')\n",
    "    except:\n",
    "        try:\n",
    "            return subprocess.check_output(\"sysctl -n machdep.cpu.brand_string\", shell=True).decode('utf-8').strip()\n",
    "        except:\n",
    "            return \"Informação não disponível\"\n",
    "\n",
    "def try_cpu():\n",
    "    import psutil\n",
    "    import platform\n",
    "\n",
    "    # Métricas da CPU\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    cpu_count_logical = psutil.cpu_count(logical=True)\n",
    "    cpu_count_physical = psutil.cpu_count(logical=False)\n",
    "    cpu_freq = psutil.cpu_freq()\n",
    "    cpu_times_percent = psutil.cpu_times_percent(interval=1)\n",
    "\n",
    "    # Informação específica do modelo do processador\n",
    "    if platform.system() == \"Windows\":\n",
    "        cpu_model = get_cpu_info_windows()\n",
    "    else:\n",
    "        cpu_model = get_cpu_info_unix()\n",
    "\n",
    "    # Informações adicionais sobre o Processador\n",
    "    cpu_brand = platform.processor()\n",
    "    cpu_architecture = platform.architecture()[0]\n",
    "    cpu_machine_type = platform.machine()\n",
    "    \n",
    "    # Métricas da Memória RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    total_ram = ram.total / (1024 ** 3)  # Em GB\n",
    "    used_ram = ram.used / (1024 ** 3)  # Em GB\n",
    "    \n",
    "    # Métricas do Espaço em Disco\n",
    "    disk = psutil.disk_usage('/')\n",
    "    total_disk = disk.total / (1024 ** 3)  # Em GB\n",
    "    used_disk = disk.used / (1024 ** 3)  # Em GB\n",
    "    free_disk = (total_disk - used_disk)\n",
    "    used_disk_percent = (used_disk / total_disk) * 100\n",
    "    free_disk_percent = (1 - (used_disk / total_disk)) * 100\n",
    "\n",
    "    # Exibição das Métricas\n",
    "    print(f\"\\nMarca do Processador: {cpu_brand}\")\n",
    "    print(f\"Modelo do Processador: {cpu_model}\")\n",
    "    print(f\"Frequência da CPU: {cpu_freq.current} MHz\")\n",
    "    # print(f\"Tipo de Máquina: {cpu_machine_type}\")\n",
    "    print(f\"Arquitetura do Processador: {cpu_architecture}\")\n",
    "    print(f\"Número de CPUs físicas: {cpu_count_physical}\")\n",
    "    print(f\"Número de CPUs lógicas: {cpu_count_logical}\")\n",
    "    print(f\"Uso atual CPU: {cpu_percent}%\")\n",
    "    print(f\"Tempos de CPU: user={cpu_times_percent.user}%, system={cpu_times_percent.system}%, idle={cpu_times_percent.idle}%\")\n",
    "    print(f\"\\nTotal de RAM: {total_ram:>5.2f} GB\")\n",
    "    print(f\"Usado em RAM: {used_ram:>5.2f} GB\")\n",
    "    print(f\"Espaço Total em disco: {total_disk:>7.2f} GB\")\n",
    "    print(f\"Espaço em disco usado: {used_disk:>7.2f} GB {used_disk_percent:>4.1f}%\")\n",
    "    print(f\"Espaço em disco livre: {free_disk:>7.2f} GB {free_disk_percent:>4.1f}%\")\n",
    "\n",
    "def try_gpu():\n",
    "    print('\\nVERSÕES DO PYTORCH E GPU DISPONÍVEIS')\n",
    "    try:\n",
    "        import torch\n",
    "        print('    PyTorch:',torch.__version__)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Dispositivo:',device)\n",
    "        print('Disponível :',device,torch.cuda.is_available(),' | Inicializado:',torch.cuda.is_initialized(),'| Capacidade:',torch.cuda.get_device_capability(device=None))\n",
    "        print('Nome GPU   :',torch.cuda.get_device_name(0),'         | Quantidade:',torch.cuda.device_count(),'\\n')\n",
    "    except Exception as e:\n",
    "        print('Erro ao configurar a GPU:',e,'\\n')\n",
    "\n",
    "def try_folders(drives,pastas,pastasraiz):\n",
    "    import os\n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    caminho_testado = drive+i+j\n",
    "                    if os.path.isfile(caminho_testado+'/chromedriver/chromedriver.exe'):\n",
    "                        print(f\"Listando arqivos em: {caminho_testado}\")\n",
    "                        print(os.listdir(caminho_testado))\n",
    "                        caminho = caminho_testado+'/'\n",
    "                except:\n",
    "                    caminho=''\n",
    "                    print('Não foi possível encontrar uma pasta de trabalho')\n",
    "    return caminho\n",
    "\n",
    "def try_browser(raiz):\n",
    "    print('\\nVERSÕES DO BROWSER E DO CHROMEDRIVER INSTALADAS')\n",
    "    from selenium import webdriver\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    try:\n",
    "        # Caminho para o seu chromedriver\n",
    "        driver_path=raiz+'chromedriver/chromedriver.exe'\n",
    "        print(driver_path)\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        str1 = driver.capabilities['browserVersion']\n",
    "        str2 = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]\n",
    "        print(f'     Versão do browser: {str1}')\n",
    "        print(f'Versão do chromedriver: {str2}')\n",
    "        driver.quit()\n",
    "\n",
    "        if str1[0:3] != str2[0:3]: \n",
    "            print(\"Versões incompatíveis, atualizar chromedriver!\")\n",
    "            print('  Baixar versão atualizada do Chromedriver em:')\n",
    "            print('  https://googlechromelabs.github.io/chrome-for-testing/#stable')\n",
    "            print('     Ex. Versão 116 PARA WINDOWS:')\n",
    "            print('\t    https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/win64/chromedriver-win64.zip')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def try_chromedriver(caminho):\n",
    "    try:\n",
    "        import os\n",
    "        os.listdir(caminho)\n",
    "    except Exception as e:\n",
    "        raiz=caminho\n",
    "\n",
    "    finally:\n",
    "        print(raiz)\n",
    "    return raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Erro ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "\n",
    "    caminho = drive+':/'+usuario+pastaraiz+'/'\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    import os\n",
    "\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastaraiz = 'fioce'\n",
    "caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "try_amb()\n",
    "try_cpu()\n",
    "try_gpu()\n",
    "# try_browser(caminho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drives=['C:/Users/','E:/','./home/']\n",
    "pastas=['marcos.aires/', 'marco/']\n",
    "pastasraiz=['kgfioce','fioce']\n",
    "caminho=try_folders(drives,pastas,pastasraiz)\n",
    "\n",
    "preparar_pastas(caminho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 1: Implementar funções de trabalho</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções básicas importar, conectar e gerar driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "# !pip install py2neo --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from flask import render_template_string\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "delay = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Função 1: Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    \n",
    "\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "    driver_path=caminho+'chromedriver/chromedriver.exe'\n",
    "    # print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)    \n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "\n",
    "    # return browser, url, session_id\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções acessórias em tratar HTML e chegar ao currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_name(driver, delay, NOME):\n",
    "    '''\n",
    "    Função 2: move cursor para o campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    '''\n",
    "    Função auxiliar para paginar resultados na página de busca\n",
    "    '''\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def achar_busca(driver, delay):\n",
    "    '''\n",
    "    Função auxiliar para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = driver.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               #expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               #logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função principal de extração de dados do CVLattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def format_string(input_str):\n",
    "    # Verifica se a entrada é uma string de oito dígitos\n",
    "    if input_str and len(input_str) == 9:\n",
    "        return input_str\n",
    "    elif input_str and len(input_str) == 8:\n",
    "        # Divide a string em duas partes\n",
    "        part1 = input_str[:4]\n",
    "        part2 = input_str[4:]\n",
    "        \n",
    "        # Concatena as duas partes com um hífen\n",
    "        formatted_str = f\"{part1}-{part2}\"\n",
    "        \n",
    "        return formatted_str\n",
    "    else:\n",
    "        return input_str\n",
    "        \n",
    "\n",
    "def extract_data_from_cvuri(element):\n",
    "    # Obter o valor do atributo cvuri\n",
    "    cvuri = element.get_attribute('cvuri')\n",
    "    \n",
    "    # Fazer o parsing da URL para extrair os parâmetros\n",
    "    parsed_url = urlparse(cvuri)\n",
    "    params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Converter a lista de valores para valores únicos, já que parse_qs retorna listas\n",
    "    data_dict = {k: v[0] for k, v in params.items()}\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_curriculum(driver, elm_vinculo):\n",
    "    \"\"\"\n",
    "    Função principal para extrair dados de cada página de currículo.\n",
    "    \n",
    "    Parameters:\n",
    "        - driver (webdriver object): The Selenium webdriver object.\n",
    "        - elm_vinculo (WebElement): O objeto achado pelas funções anteriores.    \n",
    "    Returns:\n",
    "        Dicionário com dados extraídos do tooltip\n",
    "    \"\"\"       \n",
    "    link_nome = achar_busca(driver, delay)\n",
    "    window_before = driver.current_window_handle\n",
    "\n",
    "    limite = 5\n",
    "    if str(elm_vinculo) == 'nan':\n",
    "        print('Vínculo não encontrado, passando para o próximo nome...')\n",
    "        raise Exception\n",
    "    try:\n",
    "        print('Vínculo encontrado no currículo de nome:', elm_vinculo.text)\n",
    "    except AttributeError:\n",
    "        print('Erro ao tentar acessar o texto do vínculo. O elemento pode não ter sido localizado corretamente.')\n",
    "    \n",
    "    # Clicar no botão \"Abrir Currículo\" e mudar de aba\n",
    "    try:\n",
    "        link_nome = achar_busca(driver, delay)\n",
    "    except Exception as e:\n",
    "        print('Erro')\n",
    "        print(e)\n",
    "\n",
    "    if link_nome.text == None:\n",
    "        xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "        print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            wait_ms=200,\n",
    "            limit=limite,\n",
    "            on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))\n",
    "    try:\n",
    "        ActionChains(driver).click(link_nome).perform()\n",
    "    except:\n",
    "        print(f'Currículo não encontrado.')\n",
    "\n",
    "    retry(WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "        wait_ms=200,\n",
    "        limit=limite,\n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNPq. {limite} tentativas sem sucesso.'))\n",
    "\n",
    "    # Clicar no botão para abrir o currículo\n",
    "    btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "    # Gerenciar janelas abertas no navegador\n",
    "    WebDriverWait(driver, delay).until(EC.number_of_windows_to_be(2))\n",
    "    window_after = driver.window_handles\n",
    "    new_window = [x for x in window_after if x != window_before][0]\n",
    "    driver.switch_to.window(new_window)\n",
    "\n",
    "    # Definir soup fora do loop para que esteja acessível em todo o escopo\n",
    "    soup = None\n",
    "    tooltips_data = []\n",
    "\n",
    "    # Extração dos dados em tooltips em <div id=\"artigos-completos\">    \n",
    "    # Esperar para garantir que todos os elementos da seção \"#artigos-completos\" foram carregados\n",
    "    WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_all_elements_located((\n",
    "            By.CSS_SELECTOR, \"#artigos-completos img.ajaxJCR\"))\n",
    "    )\n",
    "\n",
    "    tooltip_data_list = []\n",
    "    impact_factor = ''\n",
    "    \n",
    "    # Localizar a div principal pelas classes de div\n",
    "    layout_cells = driver.find_elements(By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5')\n",
    "    print(len(layout_cells), 'células principais de dados encontradas')\n",
    "\n",
    "    for cell in layout_cells:\n",
    "        cvuri_dict = {}  # Defina um valor padrão vazio para cvuri_dict\n",
    "        \n",
    "        # Extrair ISSN da classe \".citado\"\n",
    "        try:\n",
    "            elem_citado = cell.find_element(By.CSS_SELECTOR, '.citado')\n",
    "            cvuri_dict = extract_data_from_cvuri(elem_citado)\n",
    "            print(f'Dados artigo em cvuri: {cvuri_dict}')\n",
    "        except NoSuchElementException:\n",
    "            print('Nenhum dado encontrado na classe \"citado\".')\n",
    "        \n",
    "        # Extrair o DOI\n",
    "        try:\n",
    "            doi_elem = cell.find_element(By.CSS_SELECTOR, \"a.icone-producao.icone-doi\")\n",
    "            doi_link = doi_elem.get_attribute(\"href\")\n",
    "            issn_match = re.search(r\"issn=(\\d+)\", doi_link)\n",
    "            issn_from_doi = issn_match.group(1) if issn_match else None\n",
    "        except NoSuchElementException:\n",
    "            doi_link = None\n",
    "            issn_from_doi = None\n",
    "\n",
    "        # Extrair dados do JCR dos tooltips\n",
    "        try:\n",
    "            tooltip_elem = cell.find_element(By.CSS_SELECTOR, \"img.ajaxJCR\")\n",
    "            \n",
    "            # Traga o tooltip à vista para extração\n",
    "            ActionChains(driver).move_to_element(tooltip_elem).perform()\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "            original_title = tooltip_elem.get_attribute(\"original-title\")\n",
    "            journal = original_title.split('<br />')[0].strip()\n",
    "            match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "            if match:\n",
    "                impact_factor = match.group(1)\n",
    "            data_issn = issn_from_doi or tooltip_elem.get_attribute(\"data-issn\")\n",
    "            if not data_issn:\n",
    "                data_issn\n",
    "        except NoSuchElementException:\n",
    "            data_issn = issn_from_doi\n",
    "\n",
    "        # Compile os dados\n",
    "        issn = format_string(data_issn)\n",
    "        tooltip_data = {\n",
    "            \"doi\": doi_link,\n",
    "            \"data-issn\": issn,\n",
    "            \"original_title\": journal,\n",
    "            \"impact-factor\": impact_factor,\n",
    "        }\n",
    "        tooltip_data.update(cvuri_dict)\n",
    "        tooltip_data_list.append(tooltip_data)\n",
    "\n",
    "    ## Extraçao com tentativas para casos de muita instabilidade no servidor CNPq\n",
    "    # index = 0\n",
    "    # max_attempts = 3\n",
    "\n",
    "    # while index < tooltip_count:\n",
    "    #     attempts = 0\n",
    "    #     while index < tooltip_count:\n",
    "    #         attempts = 0\n",
    "    #         while attempts < max_attempts:\n",
    "    #             try:\n",
    "    #                 # Refetch os elementos para evitar StaleElementReferenceException\n",
    "    #                 elems = driver.find_elements(By.CSS_SELECTOR, \"#artigos-completos img.ajaxJCR\")\n",
    "    #                 elem = elems[index]\n",
    "                    \n",
    "    #                 ActionChains(driver).move_to_element(elem).perform()\n",
    "    #                 time.sleep(0.3)\n",
    "\n",
    "    #                 data_issn = elem.get_attribute(\"data-issn\")\n",
    "    #                 original_title = elem.get_attribute(\"original-title\")\n",
    "\n",
    "    #                 if original_title:\n",
    "    #                     match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "    #                     impact_factor = match.group(1) if match else None\n",
    "                    \n",
    "    #                 # Se não tivermos um original-title, vamos buscar pelo DOI\n",
    "    #                 if not original_title:\n",
    "    #                     print('JCR não disponível, buscando ISSN pelo DOI...')\n",
    "    #                     try:\n",
    "    #                         # Localizar o elemento pai do elemento atual\n",
    "    #                         parent_elem = elem.find_element_by_xpath('..')\n",
    "                            \n",
    "    #                         # Tentando localizar o elemento que tem o DOI a partir do elemento pai\n",
    "    #                         # doi_elem = parent_elem.find_element(By.XPATH, \".//a[@class='icone-producao icone-doi']\")\n",
    "    #                         # doi_elem = elem.find_element(\n",
    "    #                         #     By.XPATH, \"preceding-sibling::a[@class='icone-producao icone-doi']\")\n",
    "    #                         doi_elem = parent_elem.find_element(\n",
    "    #                             By.XPATH, \"preceding-sibling::a[@class='icone-producao icone-doi']\")\n",
    "\n",
    "    #                         doi_link = doi_elem.get_attribute(\"href\")\n",
    "    #                         print(doi_link)\n",
    "                            \n",
    "    #                         # Usando regex para extrair o DOI e o ISSN\n",
    "    #                         issn_match = re.search(r\"issn=(\\d+)\", doi_link)\n",
    "    #                         issn_from_doi = issn_match.group(1) if issn_match else None\n",
    "                            \n",
    "    #                         # Se obtivemos o ISSN a partir do DOI, usamos ele\n",
    "    #                         if issn_from_doi:\n",
    "    #                             data_issn = issn_from_doi\n",
    "\n",
    "    #                     except NoSuchElementException:\n",
    "    #                         # Se não conseguimos encontrar o DOI, continuamos a processar\n",
    "    #                         pass\n",
    "    #                     impact_factor = None\n",
    "\n",
    "    #                 tooltip_data = {\n",
    "    #                     \"data-issn\": data_issn,\n",
    "    #                     \"original_title\": original_title,\n",
    "    #                     \"impact-factor\": impact_factor\n",
    "    #                 }\n",
    "\n",
    "    #                 tooltip_data_list.append(tooltip_data)\n",
    "    #                 print(f\"{index+1:2}. Tooltip extraído com sucesso: {data_issn}\")  # Log do tooltip extraído\n",
    "    #                 index += 1  # Incrementar o índice para mover-se para o próximo elemento\n",
    "    #                 break  # Break out of the attempts loop once successful\n",
    "\n",
    "    #             except IndexError:\n",
    "    #                 # print('Elemento faltando na extração, passando ao próximo...')\n",
    "    #                 index += 1\n",
    "    #                 break\n",
    "\n",
    "    #             except StaleElementReferenceException:\n",
    "    #                 # Increment the attempts count and try again\n",
    "    #                 attempts += 1\n",
    "\n",
    "    #         if attempts == max_attempts:\n",
    "    #             print(f\"Não foi possível processar o elemento de índice {index} após {max_attempts} tentativas. Passando ao próximo...\")\n",
    "    #             index += 1\n",
    "\n",
    "    print(f'Dados de {len(tooltip_data_list)} artigos completos extraídos')\n",
    "\n",
    "    # After adding the tooltip data, get the updated HTML content\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Use BeautifulSoup to parse\n",
    "    if page_source is not None:\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        soup.attrs['tooltips'] = tooltip_data_list\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def handle_stale_file_error(driver, max_retries=5, retry_interval=10):\n",
    "    \"\"\"\n",
    "    Detects and handles the \"Stale file handle\" error message in the webpage content.\n",
    "    \n",
    "    Parameters:\n",
    "        - driver (webdriver object): The Selenium webdriver object.\n",
    "        - max_retries (int): Maximum number of retries if the error is detected.\n",
    "        - retry_interval (int): Time interval (in seconds) between retries.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the error was resolved within the retry limit, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            error_div = driver.find_element(By.CSS_SELECTOR, 'resultado')\n",
    "            linha1 = error_div.fidChild('li')\n",
    "            if 'Stale file handle' in linha1.text:\n",
    "                time.sleep(retry_interval)\n",
    "            else:\n",
    "                return True\n",
    "        except NoSuchElementException:\n",
    "            # If the error div is not found, it's assumed the error is resolved.\n",
    "            return True\n",
    "        \n",
    "    # If the loop completes without breaking, it means the error wasn't resolved in the given retries.\n",
    "    return False\n",
    "\n",
    "def find_terms(NOME, instituicao, unidade, termo, driver, delay, limite):\n",
    "    \"\"\"\n",
    "    Função para manipular o HTML até abir a página HTML de cada currículo   \n",
    "\n",
    "    Parameters:\n",
    "        - NOME: É o nome completo de cada pesquisador\n",
    "        - Instituição, unidade e termo: Strings a buscar no currículo para reduzir duplicidades\n",
    "        - driver (webdriver object): The Selenium webdriver object.\n",
    "        - limite (int): Número máximo de tentativas em casos de erro.\n",
    "        - delay (int): tempo em milisegundos a esperar nas operações de espera.\n",
    "    \n",
    "    Returns:\n",
    "        elm_vinculo, np.NaN, np.NaN, np.NaN, driver.\n",
    "    \"\"\"\n",
    "\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    \n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(driver)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:', e)\n",
    "                \n",
    "                # Call the handle_stale_file_error function\n",
    "                if handle_stale_file_error(driver):\n",
    "                    # If the function returns True, it means the error was resolved.\n",
    "                    # try to get the nome_vinculo again:\n",
    "                    try:\n",
    "                        elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                        nome_vinculo = elm_vinculo.text\n",
    "                    except Exception as e2:\n",
    "                        print('Erro ao encontrar o primeiro resultado da lista de nomes após tratamento do erro:', e2)\n",
    "                        return np.NaN, NOME, np.NaN, e2, driver\n",
    "                else:\n",
    "                    # If the function returns False, it means the error was not resolved within the given retries.\n",
    "                    return np.NaN, NOME, np.NaN, e, driver\n",
    "\n",
    "                print('Não foi possível extrair por falha no servidor do CNPq:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(driver)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(driver)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, driver\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # driver.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função find_terms()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, driver\n",
    "    \n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "def is_abreviation(substring):\n",
    "    pattern = r'(?: [a-zA-Z]\\.)|(?: [a-zA-Z] \\.)'\n",
    "    if re.search(pattern, substring):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_surname(substring):\n",
    "    return not is_abreviation(substring) and substring.endswith(', ')\n",
    "\n",
    "def compose_full_name(surname, parts, marker):\n",
    "    full_name = \"\"\n",
    "    for part in parts:\n",
    "        full_name += part + marker\n",
    "    full_name += ' '+surname\n",
    "    return full_name.strip()\n",
    "\n",
    "def split_authors(string, verbose=False):\n",
    "    authors_names = []\n",
    "    \n",
    "    m1a = \"; \"\n",
    "    m1b = \", \"\n",
    "    \n",
    "    if m1a in string and (string.count(m1a) <= string.count(m1b) or m1b not in string):\n",
    "        marker = m1a\n",
    "        authors_names = string.split(marker)\n",
    "        return [x.strip() for x in authors_names]\n",
    "    else:\n",
    "        marker = m1b\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Selected marker: \"{marker}\"')\n",
    "\n",
    "    parts_list = string.split(marker)\n",
    "    name    = \"\"\n",
    "    surname = \"\"\n",
    "    \n",
    "    for part in parts_list:\n",
    "        if is_abreviation(part):\n",
    "            classification = 'Abrev'\n",
    "            name += part + marker\n",
    "        else:\n",
    "            classification = 'Name'\n",
    "            if is_surname(part) or surname == \"\":\n",
    "                classification = 'SOBRENOME'\n",
    "                if surname:\n",
    "                    full_name = compose_full_name(surname.strip(), name.split(marker), ', ')\n",
    "                    if full_name not in authors_names:\n",
    "                        authors_names.append(full_name)\n",
    "                surname = part\n",
    "                name = part + marker\n",
    "            else:\n",
    "                name = part + marker\n",
    "        if verbose:\n",
    "            # print(f'Quantidade de m1a: {string.count(m1a)} \\nQuantidade de m1b: {string.count(m1b)}')\n",
    "            print(f'Autor: {part:40} | Forma: {classification}')\n",
    "\n",
    "        authors_names.append(part.strip(marker).strip())\n",
    "     \n",
    "    return [x.strip() for x in authors_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extrair_indices(layout_celula):\n",
    "    indice = layout_celula.select_one('div.layout-cell-pad-5.text-align-right')\n",
    "    valor = layout_celula.select_one('div.layout-cell.layout-cell-9 div.layout-cell-pad-5')\n",
    "    \n",
    "    if indice and valor:\n",
    "        valores_extraidos = valor.text.split('<br class=\"clear\">') if '<br class=\"clear\">' in valor.text else valor.text.split('\\n\\t\\t\\n\\t')\n",
    "        return indice.text, valores_extraidos\n",
    "    return None, None\n",
    "\n",
    "def extrair_secao(title_wrapper):\n",
    "    tags_relevantes = ['h1', 'ul', 'b']\n",
    "    tags_encontradas = []\n",
    "    for tag in tags_relevantes:\n",
    "        elemento_encontrado = title_wrapper.select_one(tag)\n",
    "        if elemento_encontrado:\n",
    "            tags_encontradas.append((tag, elemento_encontrado))\n",
    "    tags_ordenadas = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def encontrar_subchave(title_wrapper):\n",
    "    tags_relevantes  = ['ul', 'a', 'b']\n",
    "    tags_encontradas = [(tag, title_wrapper.find(tag)) for tag in tags_relevantes]\n",
    "    tags_ordenadas   = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def extrair_titulo(title_wrapper):\n",
    "    inst_back_texts = {}\n",
    "    inst_back_elements = title_wrapper.find_all('div', class_='inst_back')\n",
    "    for index, inst_back in enumerate(inst_back_elements):\n",
    "        b_tag = inst_back.find('b')\n",
    "        if b_tag:\n",
    "            inst_back_texts[index] = b_tag.text.strip()\n",
    "    return inst_back_texts if inst_back_texts else None\n",
    "\n",
    "def extrair_wraper(soup, json_data):\n",
    "    title_wrappers = soup.select('div.layout-cell-pad-main div.title-wrapper')\n",
    "    for title_wrapper in title_wrappers:\n",
    "        section_name = extrair_secao(title_wrapper)\n",
    "        if section_name:\n",
    "            section_name = section_name.text.strip()\n",
    "            \n",
    "            titles = extrair_titulo(title_wrapper)\n",
    "            json_data[\"Properties\"][section_name] = {}\n",
    "            \n",
    "            if titles:\n",
    "                for index, title in titles.items():\n",
    "                    json_data[\"Properties\"][section_name][title] = {}\n",
    "            \n",
    "            layout_cells = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "            for layout_celula in layout_cells:\n",
    "                indice, valores_extraidos = extrair_indices(layout_celula)\n",
    "                if indice and valores_extraidos:\n",
    "                    if titles and indice in titles.values():\n",
    "                        if len(titles) > 1:\n",
    "                            for title in titles.values():\n",
    "                                if title.strip() in indice:\n",
    "                                    json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                        else:\n",
    "                            title = list(titles.values())[0]\n",
    "                            json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                    else:\n",
    "                        json_data[\"Properties\"][section_name][indice] = valores_extraidos\n",
    "    return json_data\n",
    "\n",
    "def imprimir_informacoes(dados_json, nome_no, indent=0):\n",
    "    indentation = '    ' * indent  # Calculating the current indentation level\n",
    "\n",
    "    if dados_json and nome_no and dados_json.get(nome_no):\n",
    "        if indent == 0:  # Logging node-level information only at the root\n",
    "            logging.info(f\"{indentation}Node: {nome_no}\")\n",
    "            logging.info(f\"{indentation}Total keys extracted: {len(dados_json[nome_no].keys())}\")\n",
    "        \n",
    "        for key in dados_json[nome_no].keys():\n",
    "            logging.info(f\"{indentation}{key.strip() if key else ''}\")\n",
    "\n",
    "            if isinstance(dados_json[nome_no][key], dict):  # Check for nested dictionaries\n",
    "                # Recursive call to handle nested dictionaries\n",
    "                imprimir_informacoes(dados_json[nome_no], key, indent + 1)\n",
    "            else:\n",
    "                logging.info(f\"{indentation}    Values: {dados_json[nome_no][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funçoes para montar dicionários para persistir em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(input_data, dict):\n",
    "            for key, value in input_data.items():\n",
    "                if isinstance(value, dict):  # Se um valor ainda é um dicionário, converte em string JSON\n",
    "                    input_data[key] = json.dumps(Neo4jPersister.convert_to_primitives(value), ensure_ascii=False)\n",
    "                else:\n",
    "                    input_data[key] = Neo4jPersister.convert_to_primitives(value)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, list):\n",
    "            return [Neo4jPersister.convert_to_primitives(item) for item in input_data]\n",
    "        \n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        \n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return Neo4jPersister.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def persist_data(self, data_dict, label):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MERGE (node:{label}) SET node = $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "    def update_data(self, node_id, data_dict):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MATCH (node) WHERE id(node) = {node_id} SET node += $props\"\n",
    "            session.run(query, props=data_dict_primitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit1_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "    # Títulos contendo subseções\n",
    "    tit1a = ['Identificação', 'Endereço', 'Formação acadêmica/titulação', 'Pós-doutorado', 'Formação Complementar',\n",
    "            'Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão',\n",
    "            'Projetos de desenvolvimento', 'Revisor de periódico', 'Revisor de projeto de fomento', 'Áreas de atuação',\n",
    "            'Idiomas', 'Inovação']\n",
    "\n",
    "    tit1b = ['Atuação Profissional'] # dados com subseções\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        try:\n",
    "            titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        except:\n",
    "            titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "        # Verifique se o título está na lista 'tit1'\n",
    "        if titulo in tit1a:\n",
    "            if verbose:\n",
    "                print(titulo)\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                keys = []\n",
    "                vals = []\n",
    "\n",
    "                for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "                    if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "                        key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                        key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                        keys.append(key_text)\n",
    "                        val = j.find('div', class_='layout-cell-pad-5')\n",
    "                        val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                        vals.append(val_text)\n",
    "                        if verbose:\n",
    "                            print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "                agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                data_dict[titulo] = Neo4jPersister.convert_to_primitives(agg_dict)\n",
    "\n",
    "        \n",
    "        if titulo in tit1b:\n",
    "            if verbose:\n",
    "                print(titulo)\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = []\n",
    "                    if verbose:\n",
    "                        print(section_name)\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    while sibling:\n",
    "                        classes = sibling.get('class', [])\n",
    "\n",
    "                        if 'layout-cell-3' in classes:  # Data key\n",
    "                            key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                            if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "                                val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                current_data[key] = val\n",
    "                                if verbose:\n",
    "                                    print(len(current_data.values()), key, val)\n",
    "\n",
    "                        elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                            next_sibling = sibling.find_next_sibling()\n",
    "                            if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                sibling = None\n",
    "                            else:\n",
    "                                if current_data:\n",
    "                                    data_dict[titulo][section_name].append(current_data)  # Armazenamos os dados em uma lista\n",
    "\n",
    "                        if sibling:\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit2_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')    \n",
    "    \n",
    "    tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        try:\n",
    "            titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        except:\n",
    "            titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "        # Verifique se o título está na lista 'tit2'\n",
    "        if titulo in tit2:\n",
    "            if verbose:\n",
    "                print(f'Título: {titulo}')\n",
    "\n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = {}\n",
    "                    if verbose:\n",
    "                        print(f'Seção: {section_name}')\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_subsection = None\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    if section_name == 'Produção bibliográfica':\n",
    "                        subsections = section.find_next_siblings('div', class_='cita-artigos')\n",
    "                        if verbose:\n",
    "                            print(len(subsections), 'subseções')                       \n",
    "                        for subsection in subsections:                            \n",
    "                            if subsection:\n",
    "                                subsection_name = subsection.find('b').get_text().strip()\n",
    "                                if verbose:\n",
    "                                    print(f'    Subseção: {subsection_name}') # nomes de subseção como ocorrências \n",
    "                                    print(f'    {len(subsection)} divs na subseção {subsection_name}')                                \n",
    "                                if subsection_name == 'Citações':\n",
    "                                    current_subsection = subsection_name\n",
    "                                    data_dict[titulo][section_name]['Citações'] = {}\n",
    "                                    sub_section_list = []\n",
    "                                        \n",
    "                                    ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "                                    next_siblings = subsection.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "                                    for sibling in next_siblings:\n",
    "                                        citation_counts = sibling.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que contém os Valores de Citações\n",
    "                                        if citation_counts:\n",
    "                                            for i in citation_counts:\n",
    "                                                database = i.get_text()\n",
    "                                                total_trab = i.find_next_sibling(\"div\", class_=\"trab\").get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                                total_cite = i.find_next_sibling(\"div\", class_=\"cita\").get_text().split(\"Total de citações:\")[1]\n",
    "                                                fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                                num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                                data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\").get_text().split(\"Data:\")[1].strip()\n",
    "\n",
    "                                                # Converta os valores para tipos de dados adequados\n",
    "                                                total_trab = int(total_trab)\n",
    "                                                total_cite = int(total_cite)\n",
    "\n",
    "                                                citation_numbers = {\n",
    "                                                    \"Database\": database,\n",
    "                                                    \"Total de trabalhos\": total_trab,\n",
    "                                                    \"Total de citações\": total_cite,\n",
    "                                                    \"Índice_H\": num_fator_h,\n",
    "                                                    \"Data\": data_wos\n",
    "                                                }\n",
    "\n",
    "                                                # Verifique se a subseção atual já existe no dicionário\n",
    "                                                if 'Citações' not in data_dict[titulo][section_name]:\n",
    "                                                    data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "\n",
    "                                                data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "\n",
    "                                                if verbose:\n",
    "                                                    print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "                            \n",
    "                        ## Encontrar a div irmã de div subseção com classe layout-cell-12 com artigos\n",
    "                        vals_jcr = []\n",
    "                        div_artigo_geral = data_cell.findChild(\"div\", id=\"artigos-completos\")\n",
    "                        if verbose:\n",
    "                            print(f'Encontrada {len(div_artigo_geral)} div geral de artigos')  \n",
    "                        divs_artigos = div_artigo_geral.find_all('div', class_='artigo-completo')\n",
    "                        if verbose:\n",
    "                            print(len(divs_artigos), 'divs de artigos')\n",
    "                        \n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                        if divs_artigos:                              \n",
    "                            for div_artigo in divs_artigos:\n",
    "                                data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = {}                                   \n",
    "                                    ## Extrair filhos da classes de artigos completos que estão à frente\n",
    "                                sibling = div_artigo.findChild()\n",
    "\n",
    "                                while sibling:\n",
    "                                    classes = sibling.get('class', [])\n",
    "\n",
    "                                    if 'layout-cell-1' in classes:  # Data key\n",
    "                                        key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                        sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                        if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                            val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                            info_dict = {\n",
    "                                                'data-issn': 'NULL',\n",
    "                                                'impact-factor': 'NULL',  \n",
    "                                                'jcr-year': 'NULL',\n",
    "                                            }\n",
    "                                            # Remova as tags span da div\n",
    "                                            for span in sibling.find_all('span'):\n",
    "                                                span.extract()\n",
    "                                            \n",
    "                                            val_text = sibling.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "\n",
    "                                            current_data[key] = val_text\n",
    "                                            if verbose:\n",
    "                                                print(len(current_data.values()), key, val)\n",
    "\n",
    "                                            sup_element = sibling.find('sup')\n",
    "                                            raw_jcr_data = sup_element.get_text()\n",
    "                                            # print('sup_element:',sup_element)\n",
    "                                            img_element = sup_element.find('img')\n",
    "                                            # print('img_element:',img_element)\n",
    "\n",
    "                                            if sup_element:\n",
    "                                                if img_element:\n",
    "                                                    original_title = img_element.get('original-title')\n",
    "                                                    if original_title:\n",
    "                                                        info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                        if info_list != 'NULL':\n",
    "                                                            issn = format_string(img_element.get('data-issn'))\n",
    "                                                            if verbose:\n",
    "                                                                print(f'impact-factor: {info_list[1].split(\": \")[1]}')\n",
    "                                                            info_dict = {\n",
    "                                                                'data-issn': issn,\n",
    "                                                                'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')',''),\n",
    "                                                                'journal': info_list[0],\n",
    "                                                            }\n",
    "                                                    else:\n",
    "                                                        if verbose:\n",
    "                                                            print('Entrou no primeiro Else')\n",
    "                                                        issn = format_string(img_element.get('data-issn'))\n",
    "                                                        info_dict = {\n",
    "                                                            'data-issn': issn,\n",
    "                                                            'impact-factor': 'NULL',\n",
    "                                                            'jcr-year': 'NULL',\n",
    "                                                            'journal': 'NULL',\n",
    "                                                        }\n",
    "                                            else:\n",
    "                                                if verbose:\n",
    "                                                            print('Entrou no segundo Else')\n",
    "                                                info_dict = {\n",
    "                                                    'data-issn': 'NULL',\n",
    "                                                    'impact-factor': 'NULL',\n",
    "                                                    'jcr-year': 'NULL',\n",
    "                                                    'journal': 'NULL',\n",
    "                                                }                                                                \n",
    "                                                \n",
    "                                            vals_jcr.append(info_dict)\n",
    "                                            if verbose:\n",
    "                                                print(f'         {info_dict}')\n",
    "\n",
    "                                        if 'JCR' not in data_dict:\n",
    "                                            data_dict['JCR'] = []\n",
    "                                        \n",
    "                                        if verbose:\n",
    "                                            print(len(vals_jcr))\n",
    "                                        data_dict['JCR'] = vals_jcr\n",
    "\n",
    "                                    elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                        next_sibling = sibling.find_next_sibling()\n",
    "                                        if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                            sibling = None\n",
    "                                        else:\n",
    "                                            if current_data:\n",
    "                                                converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                                data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = converted_data\n",
    "\n",
    "                                    if sibling:\n",
    "                                        sibling = sibling.find_next_sibling()\n",
    "                    else:\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "\n",
    "                            if 'cita-artigos' in classes:  # Subsection start\n",
    "                                subsection_name = sibling.find('b').get_text().strip()\n",
    "                                current_subsection = subsection_name\n",
    "                                if verbose:\n",
    "                                    print(f'    Subseção: {subsection_name}')\n",
    "                                data_dict[titulo][section_name][current_subsection] = {}\n",
    "                                current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "\n",
    "                            elif 'layout-cell-1' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_subsection:\n",
    "                                        data_dict[titulo][section_name][current_subsection] = Neo4jPersister.convert_to_primitives(current_data)  # Armazenamos os dados da subseção atual\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "    \n",
    "    # Verifique se os dados dos tooltips estão presentes no objeto soup\n",
    "    if 'tooltips' in soup.attrs:\n",
    "        tooltips_data = soup.attrs['tooltips']\n",
    "        agg = []\n",
    "        \n",
    "        for tooltip in tooltips_data:\n",
    "            agg_data = {}\n",
    "            \n",
    "            # Extração do ano JCR a partir do \"original_title\"\n",
    "            if tooltip.get(\"original_title\"):\n",
    "                jcr_year = tooltip[\"original_title\"].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')','')\n",
    "                agg_data[\"jcr-ano\"] = jcr_year\n",
    "            \n",
    "            # Adicionar todas as chaves e valores do tooltip ao dicionário agg_data\n",
    "            for key, value in tooltip.items():\n",
    "                agg_data[key] = value\n",
    "            \n",
    "            agg.append(agg_data)\n",
    "        \n",
    "        data_dict['JCR2'] = agg\n",
    "    else:\n",
    "        print('Não foram achados os dados de tooltip')\n",
    "        print(soup.attrs)  \n",
    "           \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit3_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "    # Títulos da seção 'Eventos'\n",
    "    tit3 = ['Eventos']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        try:\n",
    "            titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        except:\n",
    "            titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        # Verifique se o título está na lista 'tit3'\n",
    "        if titulo in tit3:\n",
    "            if verbose:\n",
    "                print(f'Título: {titulo}')\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = []\n",
    "                    if verbose:\n",
    "                        print(section_name)\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    while sibling:\n",
    "                        classes = sibling.get('class', [])\n",
    "\n",
    "                        if 'layout-cell-1' in classes:  # Data key\n",
    "                            key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                            if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                current_data[key] = val\n",
    "                                if verbose:\n",
    "                                    print(len(current_data.values()), key, val)\n",
    "\n",
    "                        elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                            next_sibling = sibling.find_next_sibling()\n",
    "                            if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                sibling = None\n",
    "                            else:\n",
    "                                if current_data:\n",
    "                                    converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                    data_dict[titulo][section_name] = converted_data\n",
    "\n",
    "                        if sibling:\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tit2_soup(soup, data_dict=None, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função montar data_dict com todos dados extraídos do DOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_dicts(soup):\n",
    "    \"\"\"\n",
    "    Aggregates data from various dictionary sources into a consolidated nested dictionary, \n",
    "    ensuring that all nested lists within the dictionaries are transformed into nested dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    - soup: BeautifulSoup object, representing the parsed HTML content.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: An aggregated dictionary containing the consolidated data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def convert_list_to_dict(lst):\n",
    "        \"\"\"\n",
    "        Converts a list into a dictionary with indices as keys.\n",
    "        \n",
    "        Parameters:\n",
    "        - lst: list, input list to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: Transformed dictionary.\n",
    "        \"\"\"\n",
    "        return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "    def merge_dict(d1, d2):\n",
    "        \"\"\"\n",
    "        Recursively merges two dictionaries, transforming nested lists into dictionaries.\n",
    "        \n",
    "        Parameters:\n",
    "        - d1: dict, the primary dictionary into which data is merged.\n",
    "        - d2: dict or list, the secondary dictionary or list from which data is sourced.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        # If d2 is a list, convert it to a dictionary first\n",
    "        if isinstance(d2, list):\n",
    "            d2 = convert_list_to_dict(d2)\n",
    "        \n",
    "        for key, value in d2.items():\n",
    "            if isinstance(value, list):\n",
    "                d2[key] = convert_list_to_dict(value)\n",
    "            if key in d1 and isinstance(d1[key], dict) and isinstance(value, dict):\n",
    "                merge_dict(d1[key], value)\n",
    "            else:\n",
    "                d1[key] = value\n",
    "\n",
    "\n",
    "    # Extract necessary information from soup\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "    name = info_list[0]\n",
    "\n",
    "    # Initialization of the aggregated_data dictionary\n",
    "    aggregated_data = {\"labels\": \"Person\", \"name\": name, \"InfPes\": info_list, \"Resumo\": [elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()]}\n",
    "\n",
    "    # Data extraction and merging\n",
    "    for data_extraction_func in [extract_tit1_soup, extract_tit2_soup, extract_tit3_soup]:\n",
    "        extracted_data = data_extraction_func(soup, verbose=False)\n",
    "        for title, data in extracted_data.items():\n",
    "            if title not in aggregated_data:\n",
    "                aggregated_data[title] = {}\n",
    "            merge_dict(aggregated_data[title], data)\n",
    "\n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funçoes de plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_dict(lst):\n",
    "    \"\"\"\n",
    "    Converts a list into a dictionary with indices as keys.\n",
    "    \n",
    "    Parameters:\n",
    "    - lst: list, input list to be transformed.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Transformed dictionary.\n",
    "    \"\"\"\n",
    "    return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "def dict_jcr_list(data_dict):\n",
    "    # Extract JCR entries from the data dictionary\n",
    "    jcr_entries = data_dict.get('JCR', {})\n",
    "\n",
    "    # Initialize an empty list to store JCR properties\n",
    "    jcr_properties_list = []\n",
    "\n",
    "    # If jcr_entries is a dictionary\n",
    "    if isinstance(jcr_entries, dict):\n",
    "        for key, value in jcr_entries.items():\n",
    "            jcr_properties_list.append(value)\n",
    "\n",
    "    # If jcr_entries is a list\n",
    "    elif isinstance(jcr_entries, list):\n",
    "        jcr_properties_list.extend(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is a string\n",
    "    elif isinstance(jcr_entries, str):\n",
    "        jcr_properties_list.append(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is of any other unexpected type\n",
    "    else:\n",
    "        print(f\"Unexpected data type {type(jcr_entries)} for JCR entries. Expected dictionary, list, or string.\")\n",
    "\n",
    "    return jcr_properties_list\n",
    "\n",
    "\n",
    "def dict_doi_list(data_dict):\n",
    "    # Extract JCR entries from the data dictionary\n",
    "    jcr_entries = data_dict.get('JCR2', {})\n",
    "\n",
    "    # Initialize an empty list to store JCR properties\n",
    "    jcr_properties_list = []\n",
    "\n",
    "    # If jcr_entries is a dictionary\n",
    "    if isinstance(jcr_entries, dict):\n",
    "        for key, value in jcr_entries.items():\n",
    "            jcr_properties_list.append(value)\n",
    "\n",
    "    # If jcr_entries is a list\n",
    "    elif isinstance(jcr_entries, list):\n",
    "        jcr_properties_list.extend(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is a string\n",
    "    elif isinstance(jcr_entries, str):\n",
    "        jcr_properties_list.append(jcr_entries)\n",
    "\n",
    "    # If jcr_entries is of any other unexpected type\n",
    "    else:\n",
    "        print(f\"Unexpected data type {type(jcr_entries)} for JCR entries. Expected dictionary, list, or string.\")\n",
    "\n",
    "    return jcr_properties_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Função para escolher a string com o maior comprimento, considerando NaNs\n",
    "def max_len_string(x):\n",
    "    valid_strings = x.dropna()\n",
    "    if not valid_strings.empty:\n",
    "        return valid_strings.loc[valid_strings.str.len().idxmax()]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "# Função para escolher a string com o menor comprimento, considerando NaNs\n",
    "def min_len_string(x):\n",
    "    valid_strings = x.dropna()\n",
    "    if not valid_strings.empty:\n",
    "        return valid_strings.loc[valid_strings.str.len().idxmin()]\n",
    "    else:\n",
    "        return np.nan  \n",
    "\n",
    "def break_text_words_into_lines(text, max_lines=3):\n",
    "    if not isinstance(text, str):  # Handle non-string values\n",
    "        return text\n",
    "\n",
    "    words = text.split()\n",
    "    lines = ['']\n",
    "    line_index = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if len(lines[line_index]) + len(word) + 1 > len(text) // max_lines and line_index < max_lines - 1:\n",
    "            lines.append('')\n",
    "            line_index += 1\n",
    "        lines[line_index] += (word + ' ')\n",
    "    \n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "def break_text(text):\n",
    "    if not isinstance(text, str):  # Handle non-string values\n",
    "        return text\n",
    "\n",
    "    # Split at the ' (' character\n",
    "    parts = text.split(' (', 1)\n",
    "    \n",
    "    # If the text was split, then join the parts with a newline in between\n",
    "    if len(parts) > 1:\n",
    "        return parts[0] + \"\\n(\" + parts[1]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def compute_color(value, min_val, max_val):\n",
    "    if pd.isna(value):\n",
    "        return '#808080'  # return a shade of grey for NaN values\n",
    "\n",
    "    position = (value - min_val) / (max_val - min_val)\n",
    "    colors = np.array([[1, 1, 0], [0, 0.5, 0]])  # Yellow to Dark Green\n",
    "    color = colors[0] + position * (colors[1] - colors[0])\n",
    "    return mcolors.to_hex(color)\n",
    "\n",
    "def plot_vertbar(jcr_properties_list):\n",
    "    data = pd.DataFrame(jcr_properties_list)\n",
    "    # Trocar 'Ieee' por 'IEEE' nos nomes de periódicos\n",
    "    data['journal'] = data['journal'].str.replace('Ieee', 'IEEE', case=False)\n",
    "\n",
    "    # Trocar os valores do NULL para tratamento adequado na plotagem\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "\n",
    "    grouped = data.groupby('issn', dropna=False).agg(\n",
    "        count=('issn', 'size'),\n",
    "        impact_factor_max=('impact-factor', 'max'),\n",
    "        journal=('journal', max_len_string),\n",
    "        jcr_year = ('jcr-ano', min_len_string)\n",
    "    ).reset_index()\n",
    "\n",
    "    grouped.rename(columns={\"count\": \"issn_count\", \"impact_factor_max\": \"impact-factor\"}, inplace=True)\n",
    "\n",
    "    count_nan_issn  = data['issn'].isna().sum()\n",
    "    null_row        = pd.DataFrame({'issn': ['MISSING'], 'issn_count': [count_nan_issn], 'impact-factor': [np.nan], 'journal': [np.nan]})\n",
    "    \n",
    "    data_all_counts = pd.concat([grouped, null_row], ignore_index=True)\n",
    "    data_all_counts = data_all_counts.sort_values(by=['impact-factor'], ascending=True)\n",
    "\n",
    "    min_impact = data_all_counts['impact-factor'].min()\n",
    "    max_impact = data_all_counts['impact-factor'].max()\n",
    "    data_all_counts['color'] = data_all_counts['impact-factor'].apply(lambda x: compute_color(x, min_impact, max_impact))\n",
    "\n",
    "    fig = px.bar(data_all_counts, x='issn', y='issn_count', color='color',\n",
    "                 title='Frequência de publicação acumulada no período completo versus fator de impacto de cada periódico',\n",
    "                 hover_data=['impact-factor', 'issn_count', 'jcr_year'],\n",
    "                 text=data_all_counts['impact-factor'].apply(lambda x: round(x, 2) if not pd.isna(x) else x),\n",
    "                 color_discrete_map='identity'\n",
    "                )\n",
    "    fig.update_xaxes(tickangle=315,\n",
    "                     tickvals=data_all_counts['issn'].tolist(), \n",
    "                     ticktext=data_all_counts['journal'].apply(break_text).tolist(), \n",
    "                    #  categoryorder='total ascending',\n",
    "                     )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        yaxis=dict(autorange=True),\n",
    "        yaxis_title=\"Frequência de publicação no periódico\",\n",
    "        xaxis_title=\"Periódicos\",\n",
    "        xaxis_title_standoff=50,\n",
    "        height=800,\n",
    "        margin=dict(l=50, r=50, b=50, t=75),\n",
    "        xaxis=dict(\n",
    "            automargin=True,\n",
    "        )    \n",
    "    )\n",
    "    \n",
    "    fig.update_traces(texttemplate='%{text}', textposition='outside', textfont_size=10)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "\n",
    "    data = pd.DataFrame(jcr_properties_list)\n",
    "    # Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "\n",
    "    # Agrupar pelo 'issn', contar ocorrências, maior valor de 'impact-factor' e pegar o 'journal'\n",
    "    grouped = data.groupby('issn', dropna=False).agg(\n",
    "        count=('issn', 'size'),\n",
    "        impact_factor_max=('impact-factor', 'max'),\n",
    "        journal=('journal', max_len_string)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Renomear as colunas para corresponder às suas descrições\n",
    "    grouped.rename(columns={\"count\": \"issn_count\", \"impact_factor_max\": \"impact-factor\"}, inplace=True)\n",
    "    data_all_counts = grouped.sort_values(by=['impact-factor'], ascending=False)\n",
    "\n",
    "    # Criar o gráfico de dispersão com Plotly Express\n",
    "    fig = px.scatter(data_all_counts, x='issn', y='impact-factor', color='issn_count',\n",
    "                     size='issn_count',\n",
    "                     title='Gráfico de Dispersão de Fator de Impacto versus ISSN no período completo',\n",
    "                     labels={'issn': 'ISSN', 'impact-factor': 'Fator de Impacto', 'issn_count': 'Quantidade'},\n",
    "                     hover_data=['journal'],\n",
    "                     color_continuous_scale=\"YlOrRd\")\n",
    "\n",
    "    # Personalizar layout\n",
    "    fig.update_traces(marker=dict(size=data_all_counts['issn_count'] * 2))\n",
    "    fig.update_xaxes(tickangle=315, tickvals=data_all_counts['issn'].tolist(), categoryorder='total ascending')\n",
    "    fig.update_traces(marker=dict(opacity=1))\n",
    "    max_impact = int(data_all_counts['impact-factor'].max()) + 1  # máximo impact-factor arredondado para cima\n",
    "    fig.update_layout(yaxis=dict(autorange=True, tickvals=list(range(max_impact))))\n",
    "\n",
    "    # Exibir o gráfico\n",
    "    fig.show()\n",
    "\n",
    "    return data_all_counts\n",
    "\n",
    "def dataframe_to_list_of_dicts(df):\n",
    "    records = df.to_dict(orient='records')\n",
    "    formatted_records = []\n",
    "    \n",
    "    for record in records:\n",
    "        formatted_record = {}\n",
    "        for key, value in record.items():\n",
    "            if pd.isna(value):\n",
    "                formatted_record[key] = 'NULL'\n",
    "            else:\n",
    "                formatted_record[key] = str(value)\n",
    "        formatted_records.append(formatted_record)\n",
    "    \n",
    "    return formatted_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função integração com CrossRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install crossrefapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from crossref.restful import Works\n",
    "\n",
    "def get_issn(doi):\n",
    "    works = Works()\n",
    "    all_data = works.doi(doi.replace('http://dx.doi.org/','').strip())\n",
    "    issn=all_data['ISSN'][0]\n",
    "    journal=all_data['container-title'][0]\n",
    "    return issn, journal\n",
    "\n",
    "def crossref_complement(data):\n",
    "    ## Substituir NULL por None\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "    data = data.sort_values(by=['impact-factor','data-issn','doi'], ascending=False)\n",
    "\n",
    "    issn_crossref=[]\n",
    "    journal_crossref=[]\n",
    "    for i in data['doi']:\n",
    "        try:\n",
    "            issn, journal = get_issn(i)\n",
    "            issn_crossref.append(issn)\n",
    "            journal_crossref.append(journal.replace('amp;',''))\n",
    "        except:\n",
    "            issn_crossref.append(np.nan)\n",
    "            journal_crossref.append(np.nan)\n",
    "\n",
    "    data['issn'] = issn_crossref\n",
    "    data['journal'] = journal_crossref\n",
    "\n",
    "    count_jcrissn = data['data-issn'].notna().sum()\n",
    "    count_jci = data['impact-factor'].notna().sum()\n",
    "    count_doi = data['doi'].notna().sum()\n",
    "    print(f'{count_jcrissn} ISSN recuperados, {len(data.index)-count_jcrissn} ISSN ausentes')\n",
    "    print(f'{count_doi}  DOI recuperados, {len(data.index)-count_doi}  DOI ausentes')\n",
    "    print(f'{count_jci}  JCI recuperados, {len(data.index)-count_jci}  JCI ausentes')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções buscar em CSVs dados de ISSN do JCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir conteúdo em linhas, pegar linha de rótulos até a penúltima linha (ignorar informações de direitos autorais)\n",
    "def org_lines(file_path):\n",
    "    from io import StringIO\n",
    "\n",
    "    # Acessar para ler rótulos de colunas que estão desencontrados dos dados\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_content = file.read()\n",
    "\n",
    "    # Guardar rótulos de colunas\n",
    "    lines = raw_content.splitlines()[1:3]\n",
    "    csv_content = \"\\n\".join(lines)\n",
    "    columns_labels = pd.read_csv(StringIO(csv_content))\n",
    "    new_columns = list(columns_labels.columns)\n",
    "\n",
    "    # Ler novamente somente os dados e acoplar as colunas adequadamente\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_content = file.read()\n",
    "\n",
    "    lines = raw_content.splitlines()[3:-2]\n",
    "    csv_content = \"\\n\".join(lines)\n",
    "    data_local = pd.read_csv(StringIO(csv_content), header=None)\n",
    "    data_local.drop(data_local.columns[-1], axis=1, inplace=True)\n",
    "    data_local.columns=new_columns\n",
    "    \n",
    "    return data_local\n",
    "\n",
    "def load_and_concatenate(file_paths):\n",
    "    \"\"\"\n",
    "    Given a list of file paths, load each file, organize it using org_lines function,\n",
    "    and concatenate them into a single dataframe.\n",
    "    \"\"\"\n",
    "    return pd.concat([org_lines(file_path) for file_path in file_paths], ignore_index=True)\n",
    "\n",
    "def get_most_recent_data(df_aggregated):\n",
    "    \"\"\"\n",
    "    Given an aggregated dataframe, this function returns the most recent (max JIF) \n",
    "    entry for each unique ISSN or eISSN.\n",
    "    \"\"\"\n",
    "    # Determine the most recent year in the dataframe\n",
    "    most_recent_year = max([int(col.split(' ')[0]) for col in df_aggregated.columns if ' JIF' in col])\n",
    "\n",
    "    # Combining the data based on ISSN and eISSN, taking the max of JIF\n",
    "    df_grouped_by_issn = df_aggregated.groupby('ISSN', as_index=False).apply(lambda x: x.nlargest(1, f'{most_recent_year} JIF')).reset_index(drop=True)\n",
    "    df_grouped_by_eissn = df_aggregated.groupby('eISSN', as_index=False).apply(lambda x: x.nlargest(1, f'{most_recent_year} JIF')).reset_index(drop=True)\n",
    "    \n",
    "    # Combining the two dataframes and dropping any duplicates\n",
    "    df_most_recent = pd.concat([df_grouped_by_issn, df_grouped_by_eissn]).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_most_recent, most_recent_year\n",
    "\n",
    "def standardize_issn_format(issn):\n",
    "    \"\"\"Convert a given ISSN to the format '0000-0000'.\"\"\"\n",
    "    if isinstance(issn, str) and len(issn) == 8 and \"-\" not in issn:\n",
    "        return issn[:4] + '-' + issn[4:]\n",
    "    return issn\n",
    "\n",
    "def format_issn_list(issn_list):\n",
    "    return [f\"{issn[:4]}-{issn[4:]}\" if issn != np.nan else issn for issn in issn_list]\n",
    "\n",
    "def montar_lista_issn(data):\n",
    "    issn_list = data['data-issn'].to_list()\n",
    "    str_list=''\n",
    "    for n,i in enumerate(issn_list):\n",
    "        issn = standardize_issn_format(str(i))\n",
    "        if n==0:\n",
    "            str_list = issn\n",
    "        elif issn == 'NULL':\n",
    "            pass\n",
    "        elif issn == 'nan':\n",
    "            pass        \n",
    "        else:\n",
    "            str_list = str_list+(', ')+issn\n",
    "    \n",
    "    print(len(str_list.split(', ')),'ISSN extraídos do currículo')\n",
    "    return str_list\n",
    "\n",
    "def populate_missing_data(df_start, df_most_recent, most_recent_year):\n",
    "    \"\"\"\n",
    "    Given a starting dataframe and a dataframe with the most recent data for each ISSN or eISSN,\n",
    "    this function populates the starting dataframe with missing values.\n",
    "    \"\"\"\n",
    "    for index, row in df_start.iterrows():\n",
    "        formatted_issn = standardize_issn_format(row['data-issn'])\n",
    "        if pd.isna(row['impact-factor']) or pd.isna(row['jcr-ano']):\n",
    "            # Search in df_most_recent using ISSN or eISSN\n",
    "            matching_row = df_most_recent[(df_most_recent['ISSN'] == formatted_issn) | (df_most_recent['eISSN'] == formatted_issn)]\n",
    "            \n",
    "            if not matching_row.empty:\n",
    "                most_recent_entry = matching_row.iloc[0]\n",
    "                df_start.at[index, 'impact-factor'] = most_recent_entry[f'{most_recent_year} JIF']\n",
    "                df_start.at[index, 'jcr-ano'] = f'(JCR {most_recent_year})'\n",
    "                df_start.at[index, 'journal'] = most_recent_entry['Journal name']\n",
    "\n",
    "    return df_start\n",
    "\n",
    "def fill_na_from_df_aggregated(sorted_df, df_aggregated):\n",
    "    df_aggregated['ISSN'] = df_aggregated['ISSN'].apply(standardize_issn_format)\n",
    "    dt_trab = sorted_df['data-issn'].copy()\n",
    "    sorted_df['issn'] = dt_trab.apply(standardize_issn_format)\n",
    "    \n",
    "    df_nan_eissn = df_aggregated[df_aggregated['eISSN'].isna()]\n",
    "    \n",
    "    for idx, row in df_nan_eissn.iterrows():\n",
    "        issn_value = row['ISSN']\n",
    "        mask = (sorted_df['issn'] == issn_value) & sorted_df['impact-factor'].isna()\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            sorted_df.loc[mask, 'impact-factor'] = row['2022 JIF']\n",
    "            sorted_df.loc[mask, 'journal'] = row['Journal name']\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "def update_sorted_df_with_jcr(sorted_df, df_aggregated):\n",
    "    years = list(range(2018, 2023))\n",
    "    \n",
    "    # Standardize ISSN formats\n",
    "    df_aggregated['ISSN'] = df_aggregated['ISSN'].apply(standardize_issn_format)\n",
    "    df_aggregated['eISSN'] = df_aggregated['eISSN'].apply(standardize_issn_format)\n",
    "    sorted_df['data-issn'] = sorted_df['data-issn'].apply(standardize_issn_format)\n",
    "    \n",
    "    for year in reversed(years):  # Loop from most recent to oldest year\n",
    "        jif_column = f\"{year} JIF\"\n",
    "        \n",
    "        # Merge based on ISSN and eISSN\n",
    "        merged_issn = pd.merge(sorted_df, df_aggregated, left_on='data-issn', right_on='ISSN', how='left')\n",
    "        merged_eissn = pd.merge(sorted_df, df_aggregated, left_on='data-issn', right_on='eISSN', how='left')\n",
    "        \n",
    "        mask_nan = merged_issn[jif_column].notna()\n",
    "        merged = pd.concat([merged_issn[~mask_nan], merged_eissn], ignore_index=True)\n",
    "        \n",
    "        update_mask = sorted_df['impact-factor'].isna()\n",
    "        update_indices = sorted_df[update_mask].index\n",
    "        \n",
    "        for idx in update_indices:\n",
    "            sorted_df.at[idx, 'impact-factor'] = merged.at[idx, jif_column]\n",
    "            jif_value = merged.at[idx, jif_column]\n",
    "            sorted_df.at[idx, 'jcr-year'] = f\"JCR {year}\" if not pd.isna(jif_value) else np.nan\n",
    "    \n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_jcr(data, df_aggregated):\n",
    "    \"\"\"\n",
    "    Atualiza os valores NaN em 'data' usando as informações de 'df_aggregated'.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame com as colunas [data-issn, original-title, impact-factor, doi, issn].\n",
    "        df_aggregated (pd.DataFrame): DataFrame com informações sobre JIFs de vários anos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame 'data' atualizado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lista de anos em ordem decrescente para procurar os JIFs mais recentes\n",
    "    years = sorted([int(col.split()[0]) for col in df_aggregated.columns if re.match(r\"\\d{4} JIF\", col)], reverse=True)\n",
    "    print(years)\n",
    "\n",
    "    for year in years:\n",
    "        column_name = f\"{year} JIF\"\n",
    "        # Usando o método 'map' para atualizar os NaNs\n",
    "        data.loc[data['impact-factor'].isna(), 'impact-factor'] = data['issn'].map(df_aggregated.set_index('ISSN')[column_name])\n",
    "        \n",
    "        # # Se não houver mais NaNs na coluna 'impact-factor', podemos interromper o loop\n",
    "        # if data['impact-factor'].notna().all():\n",
    "        #     break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções Criar nós secundários de propriedades no Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teste buscar nome com acentuação gráfica (funcionou)\n",
    "# journals_analysis_obj = JcrHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "# journals_analysis_obj.consultar_nome(\"João Hermínio Martins da Silva\")\n",
    "# journals_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teste procurar por Propriedades (falhou)\n",
    "# print(name)\n",
    "# journals_analysis_obj = JcrHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "# # check_property_query = \"MATCH (p:Person {name: $name}) RETURN properties(p)\"\n",
    "# check_property_query = \"MATCH (p:Person) WHERE p.name =~ '(?i)^João Hermínio Martins da Silva$' RETURN count(p)\"\n",
    "# result = journals_analysis_obj.consultar_propriedade(check_property_query, name=name)\n",
    "# journals_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JcrHandler:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def consultar_nome(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            query = \"MATCH (p:Person) WHERE p.name = $name RETURN p.name AS nome\"\n",
    "            results = session.run(query, name=name)\n",
    "            for record in results:\n",
    "                print(record['nome'])\n",
    "\n",
    "    def consultar_propriedade(self, check_property_query, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(check_property_query, name=name)\n",
    "            if result:\n",
    "                print(\"Consulta retornou um resultado\")\n",
    "                count = result.single()[0]\n",
    "                print(f\"Número de registros encontrados com o nome '{name}': {count}\")\n",
    "                return count\n",
    "            else:\n",
    "                print(\"Consulta não retornou nenhum resultado\")\n",
    "\n",
    "    def format_string(input_str):\n",
    "        # Verifica se a entrada é uma string de oito dígitos\n",
    "        if input_str and len(input_str) == 8:\n",
    "            # Divide a string em duas partes\n",
    "            part1 = input_str[:4]\n",
    "            part2 = input_str[4:]\n",
    "            \n",
    "            # Concatena as duas partes com um hífen\n",
    "            formatted_str = f\"{part1}-{part2}\"\n",
    "            \n",
    "            return formatted_str\n",
    "        else:\n",
    "            return input_str\n",
    "    \n",
    "    def consultar_propriedades_jcr(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            query = \"MATCH (p:Person {name: $name}) RETURN p.JCR2 AS jcr\"\n",
    "            result = session.run(query, name=name)\n",
    "            # result = session.run(query, name=name.encode('utf-8'))\n",
    "            \n",
    "            result_data = result.single()\n",
    "            if result_data:\n",
    "                print(f'Processando resultado de {len(result_data)} query...')\n",
    "                jcr_data = result_data[\"jcr\"]\n",
    "                jcr_properties_list = json.loads(jcr_data)\n",
    "                return jcr_properties_list\n",
    "            else:\n",
    "                print(f\"Não foram encontrados dados JCR2 para {name}.\")\n",
    "                return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_list_to_dict(lst):\n",
    "        \"\"\"\n",
    "        Converts a list into a dictionary with indices as keys.\n",
    "        \n",
    "        Parameters:\n",
    "        - lst: list, input list to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: Transformed dictionary.\n",
    "        \"\"\"\n",
    "        return {str(i): item for i, item in enumerate(lst)}\n",
    "    \n",
    "    def create_person_with_jcr(self, name, jcr_properties):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._create_person_with_jcr, name, jcr_properties)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_person_with_jcr(tx, name, jcr_properties):\n",
    "        # Cria o nó Person\n",
    "        person_query = (\n",
    "            \"CREATE (p:Person {name: $name}) \"\n",
    "            \"RETURN p\"\n",
    "        )\n",
    "        person_result = tx.run(person_query, name=name)\n",
    "        person_node = person_result.single()[0]\n",
    "\n",
    "        # Cria os nós secundários para cada valor único de issn\n",
    "        issn_values = set(prop.get(\"issn\") for prop in jcr_properties)\n",
    "        for issn in issn_values:\n",
    "            # print(f'ISSN formatado a persistir: {issn}')\n",
    "            if issn:\n",
    "                secondary_node_query = (\n",
    "                    \"CREATE (s:SecondaryNode {issn: $issn}) \"\n",
    "                    \"RETURN s\"\n",
    "                )\n",
    "                tx.run(secondary_node_query, issn=issn)\n",
    "\n",
    "                # Cria a relação entre o nó Person e o nó secundário\n",
    "                relation_query = (\n",
    "                    \"MATCH (p:Person {name: $name}), (s:SecondaryNode {issn: $issn}) \"\n",
    "                    \"CREATE (p)-[:HAS_JCR]->(s)\"\n",
    "                )\n",
    "                tx.run(relation_query, name=name, issn=issn)\n",
    "\n",
    "    def createJournalsNodes(self, name):\n",
    "        # Get JCR properties\n",
    "        jcr_properties = self.consultar_propriedades_jcr(name)\n",
    "\n",
    "        if jcr_properties is None:\n",
    "            print(f\"  Erro ao executar função createJournalsNodes(name):\")\n",
    "            print(f\"  consultar_propriedades_jcr({name}) retournou valor 'None'\")\n",
    "            return\n",
    "        \n",
    "        # Convert the serialized JSON strings back into dictionaries\n",
    "        deserialized_jcr_properties = [json.loads(prop) for prop in jcr_properties.values()]\n",
    "\n",
    "        # Inform the user about the total number of JCR property entries\n",
    "        total_entries = len(deserialized_jcr_properties)\n",
    "        print(f\"Read {total_entries} entries from JCR properties of Person '{name}'.\")\n",
    "\n",
    "        # Extract relevant journal properties and their count\n",
    "        journal_counts = Counter(prop.get(\"issn\") for prop in deserialized_jcr_properties)\n",
    "        \n",
    "        # Number of unique ISSNs\n",
    "        unique_issns = len(journal_counts)\n",
    "        print(f\"Identified {unique_issns} unique ISSN values.\")\n",
    "\n",
    "        null_count = journal_counts.pop(None, 0)  # Remove None (null) ISSN and get its count\n",
    "        null_count += journal_counts.pop(\"NULL\", 0)  # Also account for \"NULL\" as a string\n",
    "\n",
    "        # Counters for journals\n",
    "        successful_journal_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for issn, count in journal_counts.items():\n",
    "                if issn and issn != \"NULL\":\n",
    "                    issn = format_string(issn)\n",
    "                    # print(f'ISSN formatado a persistir: {issn} de tipo {type(issn)}')\n",
    "                    try:\n",
    "                        representative_entry = next(prop for prop in deserialized_jcr_properties if prop.get(\"data-issn\") == issn)\n",
    "                    except:\n",
    "                        continue\n",
    "                    journal_name = representative_entry.get(\"original_title\")\n",
    "                    fator_impacto = representative_entry.get(\"impact-factor\")\n",
    "                    jcr_year = representative_entry.get(\"jcr-ano\")\n",
    "\n",
    "                    # Create or merge the Journal node\n",
    "                    journal_node_query = (\n",
    "                        \"MERGE (j:Revistas {ISSN: $issn}) \"\n",
    "                        \"ON CREATE SET j.name = $journal_name, j.FatorImpacto = $impact_factor, j.JCRYear = $jcr_year \"  # Corrected this line\n",
    "                        \"RETURN j\"\n",
    "                    )\n",
    "                    session.run(journal_node_query, issn=issn, journal_name=journal_name, impact_factor=fator_impacto, jcr_year=jcr_year)  # And this line\n",
    "\n",
    "                    # Create or update the \"PUBLICOU_EM\" relationship\n",
    "                    relation_query = (\n",
    "                        \"MATCH (p:Person {name: $name}), (j:Revistas {ISSN: $issn}) \"\n",
    "                        \"MERGE (p)-[r:PUBLICOU_EM]->(j) \"\n",
    "                        \"ON CREATE SET r.QuantidadePublicações = $count \"\n",
    "                        \"ON MATCH SET r.QuantidadePublicações = r.QuantidadePublicações + $count\"\n",
    "                    )\n",
    "                    session.run(relation_query, name=name, issn=issn, count=count)\n",
    "                    \n",
    "                    successful_journal_creations += 1\n",
    "                \n",
    "                if null_count:\n",
    "                    pass\n",
    "        \n",
    "        # Inform the user about journals\n",
    "        print(f\"{successful_journal_creations} Revistas adicionadas com sucesso.\")\n",
    "        print(f\"{null_count} Revistas não foram criadas por terem valor NULL de ISSN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import urllib.parse\n",
    "import json\n",
    "import re\n",
    "\n",
    "class AdvisorHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        if isinstance(input_data, dict):\n",
    "            for key, value in input_data.items():\n",
    "                if isinstance(value, dict):  \n",
    "                    input_data[key] = json.dumps(AdvisorHandler.convert_to_primitives(value), ensure_ascii=False)\n",
    "                else:\n",
    "                    input_data[key] = AdvisorHandler.convert_to_primitives(value)\n",
    "            return input_data\n",
    "        elif isinstance(input_data, list):\n",
    "            return [AdvisorHandler.convert_to_primitives(item) for item in input_data]\n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return AdvisorHandler.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise     \n",
    "            \n",
    "    def consult_orientacoes(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            # Query to fetch data from the Neo4j database\n",
    "            query = (\n",
    "                \"MATCH (p:Person {name: $name})\"\n",
    "                \"RETURN p.Orientações AS orientacoes_data\"\n",
    "            )\n",
    "            try:\n",
    "                neo4j_result = session.run(query, name=name)\n",
    "            except Exception as e:\n",
    "                print(f\"  Erro ao executar a query para Person '{name}':\\n {e}\")\n",
    "                return None\n",
    "            \n",
    "            # Fetch a single result. If no result, return an appropriate message\n",
    "            try:\n",
    "                single_result = neo4j_result.single()\n",
    "                print(f'Processando resultado de {len(single_result)} query...')\n",
    "                if not single_result:\n",
    "                    print(f\"  Erro ao consultar 'Orientações' em Person '{name}'.\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"  Erro em consult_orientacoes() 'Orientações' em Person '{name}':\\n. {e}\")\n",
    "            \n",
    "            orientacoes_data = single_result[\"orientacoes_data\"]\n",
    "            if orientacoes_data is None:\n",
    "                print(f\"O atributo 'Orientações' está vazio para Person '{name}'\")\n",
    "                return None\n",
    "            \n",
    "            # Convert the serialized JSON string back into a dictionary\n",
    "            try:\n",
    "                orientacoes_properties_list = json.loads(orientacoes_data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"  Erro ao decodificar JSON em Person '{name}'\")\n",
    "                return None\n",
    "\n",
    "            return orientacoes_properties_list\n",
    "\n",
    "    def create_advisor_relations(self, name):\n",
    "        # Consultar propriedade Orientações\n",
    "        orient_properties = self.consult_orientacoes(name)\n",
    "\n",
    "        # Se orient_properties for None, retorne da função\n",
    "        if orient_properties is None:\n",
    "            print(f\"  Não foi encontrada propriedade 'Orientações' para Person '{name}'.\")\n",
    "            return\n",
    "\n",
    "        # Converter strings JSON serializadas de volta em dicionários\n",
    "        try:\n",
    "            deserialized_orient_properties = self.debug_and_convert(orient_properties)\n",
    "        except Exception as e:\n",
    "            print(f\"  Erro ao deserializar propriedade Orientações:/n {e}\")\n",
    "            return\n",
    "\n",
    "        # Advisory relationship mapping\n",
    "        advisory_types = {\n",
    "            \"Dissertação de mestrado\": \"ORIENTOU_MESTRADO\",\n",
    "            \"Tese de doutorado\": \"ORIENTOU_DOUTORADO\",\n",
    "            \"Trabalho de conclusão de curso de graduação\": \"ORIENTOU_GRADUAÇÃO\"\n",
    "        }\n",
    "\n",
    "        successful_advisory_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for orientacao_category, advisories in deserialized_orient_properties.items():\n",
    "                if isinstance(advisories, str):\n",
    "                    try:\n",
    "                        advisories = json.loads(advisories)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"  Falha ao deserializar JSON em 'Orientações' para '{orientacao_category}': {advisories}\")\n",
    "                        continue  # Se não for possível decodificar, vá para a próxima iteração\n",
    "\n",
    "                if not isinstance(advisories, dict):\n",
    "                    print(f\"  Tipo indevido de categorias em 'Orientações/n' '{orientacao_category}': {advisories}\")\n",
    "                    continue\n",
    "\n",
    "                for advisory_type, relationships in advisories.items():\n",
    "                    relation_label = advisory_types.get(advisory_type)\n",
    "                    if not relation_label:\n",
    "                        continue  # skip if the advisory type is not one of the specified ones\n",
    "\n",
    "                    for _, detail in json.loads(relationships).items():\n",
    "                        try:\n",
    "                            student_name = detail.split(\".\")[0]\n",
    "                            title = detail.split(\".\")[1]\n",
    "                            \n",
    "                            # Extract the year from the detail string\n",
    "                            year_match = re.search(r'(\\d{4})', detail)\n",
    "                            year = year_match.group(1) if year_match else None\n",
    "\n",
    "                            # Create or merge the Orientações node\n",
    "                            node_query = (\n",
    "                                \"MERGE (a:Orientações {Title: $title}) \"\n",
    "                                \"ON CREATE SET a.StudentName = $student_name, a.Tipo = $advisory_type, a.Year = $year \"\n",
    "                                \"ON MATCH SET a.Tipo = $advisory_type, a.Year = $year \"\n",
    "                                \"RETURN a\"\n",
    "                            )\n",
    "                            session.run(node_query, title=title, student_name=student_name, advisory_type=advisory_type, year=year)\n",
    "\n",
    "                            # Create or update the advisory relationship\n",
    "                            relation_query = (\n",
    "                                f\"MATCH (p:Person {{name: $name}}), (a:Orientações {{Title: $title}}) \"\n",
    "                                f\"MERGE (p)-[r:{relation_label}]->(a) \"\n",
    "                            )\n",
    "                            session.run(relation_query, name=name, title=title)\n",
    "\n",
    "                            successful_advisory_creations += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"  Erro ao processar orientações '{detail}': {e}\")\n",
    "\n",
    "        # Inform the user about advisories\n",
    "        print(f\"{successful_advisory_creations} orientações atualizadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class AreasHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "        \n",
    "    def consult_areas_atuacao(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"MATCH (p:Person {name: $name}) RETURN p.`Áreas de atuação` as areas_atuacao\", name=name)\n",
    "            record = result.single()\n",
    "            if record:\n",
    "                return record['areas_atuacao']\n",
    "            return None\n",
    "\n",
    "    def debug_and_convert(self, areas_str):\n",
    "        try:\n",
    "            return json.loads(areas_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to deserialize JSON string: {areas_str}\")\n",
    "            return None\n",
    "\n",
    "    def extract_subarea(self, area_detail):\n",
    "        # Extract the 'Subárea' content from the area detail\n",
    "        match = re.search(r'Subárea: ([^/]+)', area_detail)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return None\n",
    "\n",
    "    def extract_areas(self, area_detail):\n",
    "        # Extract the 'Grande Área', 'Área', and 'Subárea' contents from the area detail\n",
    "        grande_area_match = re.search(r'Grande área: ([^/]+)', area_detail)\n",
    "        area_match = re.search(r'Área: ([^/]+)', area_detail)\n",
    "        subarea_match = re.search(r'Subárea: ([^/]+)', area_detail)\n",
    "        \n",
    "        grande_area = grande_area_match.group(1).strip() if grande_area_match else None\n",
    "        area = area_match.group(1).strip() if area_match else None\n",
    "        subarea = subarea_match.group(1).strip() if subarea_match else None\n",
    "        \n",
    "        return grande_area, area, subarea\n",
    "\n",
    "    def create_areas_relations(self, name):\n",
    "        # Get 'Áreas de atuação' properties\n",
    "        areas_properties = self.consult_areas_atuacao(name)\n",
    "\n",
    "        # Convert the serialized JSON strings back into dictionaries\n",
    "        try:\n",
    "            deserialized_areas_properties = self.debug_and_convert(areas_properties)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao deserializar propriedade 'Áreas de atuação':\\n {e}\")\n",
    "            return\n",
    "\n",
    "        successful_areas_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            for _, area_detail in deserialized_areas_properties.items():\n",
    "                try:\n",
    "                    # Extracting Grande Área, Área, and Subárea from the details\n",
    "                    match = re.match(r'Grande área: (.*?) / Área: (.*?) / Subárea: (.*?)(?:/Especialidade: (.*?))?\\.?$', area_detail)\n",
    "                    if not match:\n",
    "                        print(f\"Unexpected format for 'Áreas de atuação' detail: {area_detail}\")\n",
    "                        continue\n",
    "                    grande_area, area, subarea = match.groups()[:3]\n",
    "\n",
    "                    # Creating or merging nodes for Subárea, Área, and Grande Área\n",
    "                    session.run(\"MERGE (s:Subárea {name: $subarea})\", subarea=subarea)\n",
    "                    session.run(\"MERGE (a:Área {name: $area})\", area=area)\n",
    "                    session.run(\"MERGE (ga:GrandeÁrea {name: $grande_area})\", grande_area=grande_area)\n",
    "\n",
    "                    # Creating or merging relationships. Using MERGE ensures no duplicate relationships are created.\n",
    "                    session.run(\"MATCH (p:Person {name: $name}), (s:Subárea {name: $subarea}) MERGE (p)-[r:ATUA_EM]->(s)\", name=name, subarea=subarea)\n",
    "                    session.run(\"MATCH (ga:GrandeÁrea {name: $grande_area}), (a:Área {name: $area}) MERGE (ga)-[r:CONTÉM_ÁREA]->(a)\", grande_area=grande_area, area=area)\n",
    "                    session.run(\"MATCH (a:Área {name: $area}), (s:Subárea {name: $subarea}) MERGE (a)-[r:CONTEM_SUBÁREA]->(s)\", area=area, subarea=subarea)\n",
    "\n",
    "                    successful_areas_creations += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing 'Áreas de atuação' detail '{area_detail}': {e}\")\n",
    "\n",
    "            # Inform the user about areas\n",
    "            print(f\"{successful_areas_creations} 'Áreas de atuação' relations successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectsHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def consult_data_by_property(self, name, property_name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(f\"MATCH (p:Person {{name: $name}}) RETURN p.`{property_name}` as data\", name=name)\n",
    "            record = result.single()\n",
    "            return record['data'] if record else None\n",
    "\n",
    "    def create_projects_relations(self, name):\n",
    "        successful_creations = 0\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            # Process 'Atuação Profissional' data\n",
    "            professional_data = self.consult_data_by_property(name, 'Atuação Profissional')\n",
    "            if professional_data:\n",
    "                for institution_name, _ in json.loads(professional_data).items():\n",
    "                    session.run(\"MERGE (i:Instituição {name: $institution_name})\", institution_name=institution_name)\n",
    "                    print(f\"Institution node created/merged for: {institution_name}\")\n",
    "\n",
    "                    session.run(\"MATCH (p:Person {name: $name}), (i:Instituição {name: $institution_name}) MERGE (p)-[:TEM]->(i)\", name=name, institution_name=institution_name)\n",
    "                    print(f\"Relationship established between {name} and {institution_name}.\")\n",
    "\n",
    "            # Process other dynamic nodes\n",
    "            key_labels_to_check = ['Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento']\n",
    "            for key in key_labels_to_check:\n",
    "                formatted_key = f\"`{key}`\"  # Wrap the key with backticks\n",
    "                project_data = self.consult_data_by_property(name, key)\n",
    "                if project_data:\n",
    "                    for project_time, project_name in json.loads(project_data).items():\n",
    "                        if project_name:  # to avoid empty names\n",
    "                            session.run(f\"MERGE (p:{formatted_key} {{name: $project_name}})\", project_name=project_name)\n",
    "                            print(f\"{key} node created/merged for: {project_name}\")\n",
    "\n",
    "                            session.run(f\"MATCH (a:Person {{name: $name}}), (p:{formatted_key} {{name: $project_name}}) MERGE (a)-[:TEM]->(p)\", name=name, project_name=project_name)\n",
    "                            print(f\"Relationship established between {name} and {project_name} ({key}).\")\n",
    "                            successful_creations += 1\n",
    "                else:\n",
    "                    print(f\"'{key}' data not found for {name}\")\n",
    "\n",
    "        print(f\"{successful_creations} projetos atualizados com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class ArticleHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def fetch_person_productions(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(\"MATCH (p:Person {name: $name}) RETURN p.Produções as produções\", name=name)\n",
    "            record = result.single()\n",
    "            return record['produções'] if record else None\n",
    "\n",
    "    def extract_article_info(self, input_str):\n",
    "        # Encontre todas as abreviaturas de iniciais em maiúsculas e seus índices\n",
    "        abbreviations = [(match.group(), match.start()) for match in re.finditer(r'\\b[A-Z]\\.', input_str)]\n",
    "\n",
    "        # Encontre a posição da maior ocorrência de abreviaturas de iniciais, se houver\n",
    "        if abbreviations:\n",
    "            max_abbr_position = max(abbreviations, key=lambda x: x[1])\n",
    "\n",
    "            # Encontre a primeira ocorrência de '. ' ou ' . ' após a maior ocorrência de abreviaturas de iniciais\n",
    "            first_separator_candidates = [\n",
    "                input_str.find('. ', max_abbr_position[1] + 3),\n",
    "                input_str.find(' . ', max_abbr_position[1] + 3),\n",
    "                input_str.find('.. ')\n",
    "            ]\n",
    "            first_separator_candidates = [pos for pos in first_separator_candidates if pos != -1]\n",
    "\n",
    "            if first_separator_candidates:\n",
    "                first_separator = min(first_separator_candidates)\n",
    "\n",
    "                # Encontre a primeira ocorrência de '. ' após o primeiro separador\n",
    "                second_separator = input_str.find('. ', first_separator + 2)\n",
    "\n",
    "                # Encontre a primeira ocorrência de ', ' após o segundo separador\n",
    "                third_separator = input_str.find(', ', second_separator + 2)\n",
    "            else:\n",
    "                first_separator = second_separator = third_separator = -1\n",
    "        else:\n",
    "            first_separator = second_separator = third_separator = -1\n",
    "\n",
    "        # Defina o padrão para encontrar \"p.\" e o conteúdo até a próxima vírgula\n",
    "        pages_match = re.search(r' p\\.\\s*(.*?),', input_str)\n",
    "        pages = pages_match.group(1) if pages_match else \"\"\n",
    "\n",
    "        # Defina o padrão para encontrar \"v.\" e o conteúdo até a próxima vírgula\n",
    "        volume_match = re.search(r' v\\.\\s*(.*?),', input_str)\n",
    "        volume = volume_match.group(1) if volume_match else \"\"\n",
    "\n",
    "        # Encontre a primeira ocorrência de um ano de quatro dígitos seguido de ponto final após o terceiro separador\n",
    "        year_match = re.search(r' \\d{4}\\.', input_str[third_separator + 2:])\n",
    "        year = year_match.group().strip('.').strip() if year_match else \"\"\n",
    "\n",
    "        # Extraia os dados com base nas posições dos separadores\n",
    "        authors = input_str[:first_separator].strip()\n",
    "        title = input_str[first_separator + 2:second_separator].strip()\n",
    "        journal = input_str[second_separator + 2:third_separator].strip()\n",
    "\n",
    "        # Verifique se a lista de autores e o título não estão vazios\n",
    "        if not authors or not title:\n",
    "            return None  # Retorna None para indicar falha\n",
    "\n",
    "        # Crie um dicionário com os dados extraídos\n",
    "        article_info = {\n",
    "            \"authors\": authors,\n",
    "            \"title\": title,\n",
    "            \"original_title\": journal,\n",
    "            \"pages\": pages,\n",
    "            \"volume\": volume,\n",
    "            \"year\": year\n",
    "        }\n",
    "\n",
    "        return article_info\n",
    "    \n",
    "    def deserialize_and_create_nodes(self, name):\n",
    "        print(f\"Fetching 'Produções' data for {name}...\")\n",
    "        productions_data = self.fetch_person_productions(name)\n",
    "        \n",
    "        if not productions_data:\n",
    "            print(f\"'Produções' data not found or empty for {name}.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Attempting to deserialize 'Produções' data for {name}...\")\n",
    "        try:\n",
    "            productions_data = json.loads(productions_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to deserialize 'Produções' data for {name}: {e}\")\n",
    "            return\n",
    "\n",
    "        successful_articles = 0\n",
    "        unsuccessful_articles = []\n",
    "\n",
    "        with self._driver.session() as session:\n",
    "            print(f\"Processing 'Produção bibliográfica' for {name}...\")\n",
    "            bibliographic_production = productions_data.get(\"Produção bibliográfica\", {})\n",
    "            \n",
    "            if isinstance(bibliographic_production, str):\n",
    "                print(f\"Attempting to deserialize 'Produção bibliográfica' for {name}...\")\n",
    "                try:\n",
    "                    bibliographic_production = json.loads(bibliographic_production)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to deserialize 'Produção bibliográfica' for {name}: {e}\")\n",
    "                    return\n",
    "\n",
    "            articles = json.loads(bibliographic_production.get(\"Artigos completos publicados em periódicos\", \"{}\"))\n",
    "\n",
    "            for _, article_str in articles.items():\n",
    "                article_details = self.extract_article_info(article_str)\n",
    "\n",
    "                # Vamos imprimir os detalhes de cada artigo e verificar se os autores estão presentes.\n",
    "                print(f\"Original Article: {article_str}\")\n",
    "                print(f\"Extracted Details: {article_details}\")\n",
    "\n",
    "                if article_details:\n",
    "                    article_details[\"title\"] = article_details[\"title\"].strip()\n",
    "                    article_details[\"original_title\"] = article_details[\"original_title\"].strip()\n",
    "\n",
    "                    session.run(f\"MERGE (a:Artigo {{title: $title}}) SET a += $details\", title=article_details[\"title\"], details=article_details)\n",
    "                    session.run(f\"MATCH (p:Person {{name: $name}}), (a:Artigo {{title: $title}}) MERGE (p)-[:PUBLICOU]->(a)\", name=name, title=article_details[\"title\"])\n",
    "                    successful_articles += 1\n",
    "                else:\n",
    "                    unsuccessful_articles.append(article_str)\n",
    "\n",
    "        print(f\"Processed {successful_articles} articles successfully for {name}.\")\n",
    "\n",
    "        if unsuccessful_articles:\n",
    "            print(\"Failed to process the following articles:\")\n",
    "            for article in unsuccessful_articles:\n",
    "                print(article)\n",
    "\n",
    "    def process_articles(self, name):\n",
    "        self.deserialize_and_create_nodes(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class DataRemovalHandler:\n",
    "\n",
    "    def __init__(self, uri, user, password):\n",
    "        \"\"\"\n",
    "        Inicializa a classe DataRemovalHandler com informações de conexão ao banco de dados Neo4j.\n",
    "\n",
    "        Parâmetros:\n",
    "        - uri (str): URI de conexão ao Neo4j.\n",
    "        - user (str): Nome de usuário para autenticação.\n",
    "        - password (str): Senha para autenticação.\n",
    "        \"\"\"\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Fecha a conexão com o banco de dados Neo4j.\n",
    "        \"\"\"\n",
    "        self._driver.close()\n",
    "\n",
    "    def delete_nodes_by_label(self, label):\n",
    "        \"\"\"\n",
    "        Deleta todos os nós associados a um label específico no Neo4j.\n",
    "\n",
    "        Parâmetro:\n",
    "        - label (str): O label dos nós a serem deletados.\n",
    "\n",
    "        Retorna:\n",
    "        - int: O número de nós deletados.\n",
    "        \"\"\"\n",
    "        with self._driver.session() as session:\n",
    "            # Esta consulta combina com todos os nós do label especificado e os deleta\n",
    "            result = session.run(f\"MATCH (n:{label}) DETACH DELETE n RETURN count(n) as deleted_count\")\n",
    "            deleted_count = result.single()[\"deleted_count\"]\n",
    "            return deleted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para extrair Lista de Currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  #seletorcss=\"div.paginacao:nth-child(4) > a:nth-child(2)\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas\n",
    "\n",
    "def achar_busca(browser, delay):\n",
    "    '''\n",
    "    Função para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = browser.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               # expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               # logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)\n",
    "\n",
    "def preencher_busca(browser, delay, NOME):\n",
    "    '''\n",
    "    Função para passar o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)\n",
    "\n",
    "def definir_filtros(browser, delay, mestres=True, assunto=False):\n",
    "    '''\n",
    "    Clica nos check-boxes para definir os filtros de buscas\n",
    "    Para buscar por Assuntos usar parâmetro True, caso omitido fará busca por Nome por default\n",
    "    '''\n",
    "    from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import traceback   \n",
    "    \n",
    "    ## Aguardar carregar e clicar em checkbox de Assunto\n",
    "    try:\n",
    "        if mestres == True:\n",
    "            limite=2\n",
    "            ## Aguardar opção dropdown ser carregada e clicar em sua checkbox\n",
    "            css_buscar_demais = '#buscarDemais'\n",
    "            checkbox_buscar_demais = browser.find_element(By.CSS_SELECTOR, css_buscar_demais)\n",
    "            \n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, css_buscar_demais))),\n",
    "                   wait_ms=150,\n",
    "                   limit=limite, \n",
    "                   on_exhaust=(f'Problema clicar em {checkbox_buscar_demais}, {limite} tentativas sem sucesso.'))\n",
    "            \n",
    "            time.sleep(1)\n",
    "            checkbox_buscar_demais.click()\n",
    "            print(f'Clique efetuado em {checkbox_buscar_demais}')\n",
    "\n",
    "        # css_estrangeiros  = '#buscarEstrangeiros'\n",
    "\n",
    "        if assunto == True:\n",
    "            ## Implementar número de retentativas para casos de conexão muito instável\n",
    "            limite=5\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#buscaAssunto\"))).click(),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "        \n",
    "            ## Aguardar opção Atuação Profissional ser carregada e clicar em sua checkbox\n",
    "            xpath_atuacaoprofissional = \".//*[contains(text(), 'Atuação profissional')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_atuacaoprofissional))).click()\n",
    "\n",
    "            ## Aguardar opção Ciências da Saúde ser carregada e clicar em sua checkbox\n",
    "            xpath_cienciassaude = \".//*[contains(text(), 'Ciências da Saúde')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_cienciassaude))).click()\n",
    "            #browser.find_element_by_xpath(xpath_cienciassaude).click()        \n",
    "\n",
    "            ## Aguardar opção Enfermagem ser carregada e clicar em sua checkbox\n",
    "            xpath_enfermagem    = \".//*[contains(text(), 'Enfermagem')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_enfermagem))).click()\n",
    "            #browser.find_element_by_xpath(xpath_enfermagem).click()\n",
    "            aplicar_link  = browser.find_element(By.LINK_TEXT, 'Aplicar')\n",
    "            aplicar_link.click()\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f'Erro na função definir_filtros()')\n",
    "        print(e)\n",
    "        # traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        # print(e, traceback_str) \n",
    "\n",
    "def procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(browser)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(browser)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(browser)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(browser, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', browser\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, browser\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # browser.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = browser.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', browser\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, browser\n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, browser\n",
    "\n",
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    title = soup.title.string if soup.title else \"Unknown\"\n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "\n",
    "\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    return header_node\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_personinfo(soup):\n",
    "    # Step 1: Identify Node Name\n",
    "    properties = {}\n",
    "\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    # div_informacoesautor = soup.find(\"ul\", {\"class\": \"informacoes-autor\"})\n",
    "    if node_name:\n",
    "        properties['name'] = node_name\n",
    "    for li in soup.find_all('li'):\n",
    "        text_content = li.text  # Extract the text content of the 'li' element\n",
    "        span_class = li.span['class'][0] if li.span else 'Unknown'  # Extract the class of the span within the 'li', if present\n",
    "\n",
    "        # Populate dictionary based on span class\n",
    "        if span_class == 'img_link':\n",
    "            properties['CV_URL'] = text_content.replace('Endereço para acessar este CV: ', '').strip()\n",
    "        elif span_class == 'img_identy':\n",
    "            properties['ID_Lattes'] = li.find('span', {'style': 'font-weight: bold; color: #326C99;'}).text.strip() if li.find('span', {'style': 'font-weight: bold; color: #326C99;'}) else 'Unknown'\n",
    "        elif span_class == 'img_cert':\n",
    "            date_str = text_content.replace('Última atualização do currículo em ', '').strip()\n",
    "            \n",
    "            # Assuming that the date string is in the format 'DD/MM/YYYY', parse it into a datetime object\n",
    "            # Modify the format string according to the actual format of date_str\n",
    "            parsed_date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "            # Convert the datetime object to ISO 8601 format (YYYY-MM-DD)\n",
    "            iso_date = parsed_date.strftime('%Y-%m-%d')\n",
    "            properties['Last_Updated'] = iso_date\n",
    "\n",
    "    return properties\n",
    "\n",
    "\n",
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_data[key] = value\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that contains the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_lista(lista, mestres=True, assunto=False):\n",
    "    sucesso=[]\n",
    "    falhas=[]\n",
    "    duvidas=[]\n",
    "    tipo_erro=[]\n",
    "    curriculos=[]\n",
    "    rotulos=[]\n",
    "    conteudos=[]\n",
    "\n",
    "    delay=10\n",
    "    limite=3\n",
    "    instituicao = 'Fundação Oswaldo Cruz'\n",
    "    unidade     = 'Fiocruz Ceará'\n",
    "    termo       = 'Ministerio da Saude'\n",
    "\n",
    "    t0 = time.time()\n",
    "    browser = connect_driver(caminho)\n",
    "    for NOME in lista:\n",
    "        try:\n",
    "            preencher_busca(browser, delay, NOME)\n",
    "            elemento_achado, nome_falha, duvida, erro, browser = procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite)\n",
    "            link_nome     = achar_busca(browser, delay)\n",
    "            window_before = browser.current_window_handle\n",
    "            \n",
    "            if str(elemento_achado) == 'nan':\n",
    "                print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "                falhas.append(nome_falha)\n",
    "                duvidas.append(duvida)\n",
    "                tipo_erro.append(erro)\n",
    "                # print(nome_falha)\n",
    "                # print(erro)\n",
    "                # clear_output(wait=True)\n",
    "                raise Exception\n",
    "            print('Vínculo encontrado no currículo de nome:',elemento_achado.text)\n",
    "\n",
    "            ## Clicar no botão abrir currículo e mudar de aba\n",
    "            try:\n",
    "                ## Aguarda, encontra, clica em buscar nome\n",
    "                link_nome    = achar_busca(browser, delay)\n",
    "                nome_buscado = []\n",
    "                nome_achado  = []\n",
    "                nome_buscado.append(NOME)\n",
    "                \n",
    "                if link_nome.text == None:\n",
    "                    xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "                    # 'Stale file handle'\n",
    "                    print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "                    retry(WebDriverWait(browser, delay).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "                try:\n",
    "                    ActionChains(browser).click(link_nome).perform()\n",
    "                    nome_achado.append(link_nome.text)\n",
    "                except:\n",
    "                    print(f'Currículo não encontrado para: {NOME}.')\n",
    "                    return\n",
    "                \n",
    "                retry(WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "                \n",
    "                # Clicar botão para abrir o currículo\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                time.sleep(0.2)\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "\n",
    "                # Pega o código fonte da página\n",
    "                page_source = browser.page_source\n",
    "\n",
    "                # Usa BeautifulSoup para analisar\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                # Extração e Persistência do cabeçalho\n",
    "                dict_header = parse_header(soup)\n",
    "                header_node = persist_to_neo4j(dict_header)\n",
    "                \n",
    "                # Extração e Persistência dos elementos H1\n",
    "                graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "                parse_h1_elements(soup, header_node, graph)\n",
    "                graph, cv_node, properties = parse_personinfo(soup)                \n",
    "\n",
    "            except Exception as e:\n",
    "                print('Erro',e)\n",
    "                print('Tentando nova requisição ao servidor')\n",
    "                time.sleep(1)\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "                time.sleep(1)\n",
    "\n",
    "            sucesso.append(NOME)\n",
    "\n",
    "        except:\n",
    "            print(f'Currículo não encontrado: {NOME}')\n",
    "            browser.back()\n",
    "            continue\n",
    "\n",
    "    df_dados =pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(curriculos),\n",
    "        'ROTULOS': pd.Series(rotulos),\n",
    "        'CONTEUDOS': pd.Series(conteudos),\n",
    "            })\n",
    "    \n",
    "    t6=time.time()\n",
    "    print('='*95)\n",
    "    # print(f' {len(sucesso)} currículos extraídos com sucesso')\n",
    "    print(f' Tempo total para extrair {len(conteudos)} seções dos currículos: {tempo(t0,t6)}')\n",
    "    # print('='*95)\n",
    "    browser.quit()\n",
    "    \n",
    "    return df_dados, sucesso  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função alternativa para extrair todas seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        data_dict = {}\n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "\n",
    "            higher_order_key = None  # Inicializa variável para armazenar a chave de ordem superior\n",
    "            data_list = []  # Inicialize lista para armazenar entradas de índices de dataframe\n",
    "            \n",
    "            parag_elements = parent_div.find_all('p')\n",
    "            if parag_elements:\n",
    "                for idx, elem in enumerate(parag_elements):\n",
    "                    class_name = elem.get('class', [None])[0]  # Assumes only one class; otherwise, join them into a single string\n",
    "                    higher_order_key = class_name\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "                    data_entry = {'rotulos': class_name, 'conteudos': elem.text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "\n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                kdict_elements = cell.find_all('b')\n",
    "                # print(len(kdict_elements))\n",
    "\n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "\n",
    "                index_elems   = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for key_elem, details_elem in zip(index_elems, details_elems):\n",
    "                    key_text     = key_elem.text.strip() if key_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    data_entry = {'rotulos': key_text, 'conteudos': details_text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "                \n",
    "            if higher_order_key is None:\n",
    "                # Se nenhuma chave de ordem superior for encontrada, associa a lista de dados diretamente ao título\n",
    "                result_dict[title_text] = data_list\n",
    "            else:\n",
    "                # Caso contrário, associa o data_dict contendo chaves de ordem superior ao título\n",
    "                result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def generate_dataframe_and_neo4j_dict(data_dict):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame and a dictionary for Neo4j persistence, incorporating section and subsection names.\n",
    "\n",
    "    Parameters:\n",
    "        data_dict (dict): A nested dictionary containing section and subsection data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame aggregating all sections and subsections, with additional columns specifying their names.\n",
    "        dict: A dictionary intended for Neo4j persistence, formatted according to the Neo4j data model.\n",
    "    \"\"\"\n",
    "\n",
    "    all_frames = []  # List to store DataFrames corresponding to each section and subsection\n",
    "    neo4j_dict = {}  # Dictionary for Neo4j persistence\n",
    "\n",
    "    for section, items in data_dict.items():\n",
    "        if section:  # Exclude empty sections\n",
    "            neo4j_dict[section] = {}\n",
    "            if isinstance(items, list):  # If items is a list, convert to DataFrame\n",
    "                df = pd.DataFrame(items)\n",
    "                df['Section'] = section  # Append a column for the section name\n",
    "                all_frames.append(df)\n",
    "                neo4j_dict[section] = items  # For list items, add them as they are\n",
    "\n",
    "            elif isinstance(items, dict):  # If items is a dictionary, explore subsections\n",
    "                for subsection, subitems in items.items():\n",
    "                    if subitems:  # Exclude empty subsections\n",
    "                        df = pd.DataFrame(subitems)\n",
    "                        df['Subsection'] = subsection  # Append a column for the subsection name\n",
    "                        df['Section'] = section  # Append a column for the section name\n",
    "                        all_frames.append(df)\n",
    "                        neo4j_dict[section][subsection] = subitems  # Store subsection data\n",
    "\n",
    "    # Concatenate all DataFrames vertically to form one unified DataFrame\n",
    "    dataframe = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "    return dataframe, neo4j_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções alternativas diversas para interagir com Neo4j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database credentials and URI\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    try:\n",
    "        graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    except:\n",
    "        print('Erro ao conectar ao Neo4j')\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'].split('(')[1].strip(')'), meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    print(type(header_node))\n",
    "\n",
    "    return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict, List\n",
    "\n",
    "def create_or_update_publications(graph: Graph, publications_dict: Dict[str, Dict[str, List[Dict[str, str]]]]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its associated publications in the Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        publications_dict (Dict[str, Dict[str, List[Dict[str, str]]]]): Dictionary containing publication information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    publications_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Search for existing researcher node by name\n",
    "    existing_node = matcher.match(\"Researcher\", name=publications_dict['Node Name']).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding publications.\")\n",
    "\n",
    "    # Process publications\n",
    "    for publication in publications_dict['Properties']['Produções']:\n",
    "        # Check if a similar publication already exists\n",
    "        existing_pub_node = matcher.match(\"Publication\", doi=publication['doi']).first()\n",
    "\n",
    "        if not existing_pub_node:\n",
    "            pub_node = Node(\"Publication\", **publication)\n",
    "            graph.create(pub_node)\n",
    "            publications_created += 1\n",
    "        else:\n",
    "            for key, value in publication.items():\n",
    "                if key not in existing_pub_node or existing_pub_node[key] != value:\n",
    "                    existing_pub_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            pub_node = existing_pub_node\n",
    "            graph.push(pub_node)\n",
    "\n",
    "        # Create or update relationship between researcher and publication\n",
    "        rel = Relationship(existing_node, \"PUBLICOU\", pub_node)\n",
    "        graph.merge(rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Publications created: {publications_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    if 'nome' in researcher_dict and 'resumo' in researcher_dict:\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['nome'][0] if isinstance(researcher_dict['nome'], list) else researcher_dict['nome']\n",
    "        summary = researcher_dict['resumo'][0] if isinstance(researcher_dict['resumo'], list) else researcher_dict['resumo']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, resumo=summary)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'nome' and 'resumo'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'nome': ['John Doe'], 'resumo': ['This is a summary.']}\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "# create_researcher_node(graph, researcher_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "\n",
    "def generate_lattes_dict(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a dictionary from a BeautifulSoup object to be persisted in Neo4j.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML/XML data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the relevant information for Neo4j persistence.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold the extracted data\n",
    "    lattes_data = {}\n",
    "    \n",
    "    # Extracting researcher's name as an example\n",
    "    name_section = soup.find('div', {'id': 'name-section'})\n",
    "    if name_section:\n",
    "        lattes_data['researcher_name'] = name_section.text.strip()\n",
    "    \n",
    "    # Extracting list of publications as an example\n",
    "    publications = []\n",
    "    for pub in soup.find_all('div', {'class': 'publication'}):\n",
    "        publication_data = {}\n",
    "        title = pub.find('span', {'class': 'title'})\n",
    "        authors = pub.find('span', {'class': 'authors'})\n",
    "        \n",
    "        if title:\n",
    "            publication_data['title'] = title.text.strip()\n",
    "        \n",
    "        if authors:\n",
    "            publication_data['authors'] = authors.text.strip().split(',')\n",
    "        \n",
    "        publications.append(publication_data)\n",
    "    \n",
    "    lattes_data['publications'] = publications\n",
    "    \n",
    "    # Additional extractions can be performed as per the requirements\n",
    "    \n",
    "    return lattes_data\n",
    "\n",
    "# Example usage (Assuming 'some_html_content' contains the HTML content)\n",
    "# soup = BeautifulSoup(some_html_content, 'html.parser')\n",
    "# lattes_dict = generate_lattes_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node_from_dict(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "# create_researcher_node_from_dict(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    nodes_created = 0\n",
    "    nodes_updated = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "\n",
    "        # Create a NodeMatcher object\n",
    "        matcher = NodeMatcher(graph)\n",
    "\n",
    "        # Look for existing nodes with the same name\n",
    "        existing_node = matcher.match(\"Researcher\", name=name).first()\n",
    "\n",
    "        if existing_node:\n",
    "            # Update properties of existing node\n",
    "            for key, value in researcher_dict.items():\n",
    "                if key.lower() not in existing_node or existing_node[key.lower()] != value:\n",
    "                    existing_node[key.lower()] = value\n",
    "                    properties_updated += 1\n",
    "\n",
    "            # Push the changes to the database\n",
    "            graph.push(existing_node)\n",
    "            nodes_updated += 1\n",
    "\n",
    "        else:\n",
    "            # Create a new node of type 'Researcher'\n",
    "            researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "            # Add the node to the Neo4j database\n",
    "            graph.create(researcher_node)\n",
    "            nodes_created += 1\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"Nodes created: {nodes_created}\")\n",
    "        print(f\"Nodes updated: {nodes_updated}\")\n",
    "        print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'NOME': 'John Doe', 'IDLATTES': '0000000000000000', 'ATUALIZAÇÃO': '31/12/2023'}\n",
    "\n",
    "# Create or update a Researcher node in Neo4j based on the dictionary\n",
    "# create_or_update_researcher_node(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_professional_links(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its professional links in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's professional information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    relationships_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Look for existing nodes with the same name\n",
    "    existing_node = matcher.match(\"Researcher\", name=researcher_dict.get('Nome')).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding professional links.\")\n",
    "\n",
    "    # Process professional affiliations and activities\n",
    "    for key, value in researcher_dict.items():\n",
    "        if key not in ['Nome', 'Endereço Profissional']:\n",
    "            # Create or find the organization/affiliation node\n",
    "            org_node = matcher.match(\"Organization\", name=key).first()\n",
    "            if not org_node:\n",
    "                org_node = Node(\"Organization\", name=key)\n",
    "                graph.create(org_node)\n",
    "\n",
    "            # Create or update the relationship\n",
    "            rel_type = \"AFFILIATED_WITH\" if 'Atual' in value else \"HAS_BEEN_AFFILIATED_WITH\"\n",
    "            rel = Relationship(existing_node, rel_type, org_node, details=value)\n",
    "\n",
    "            # Check if a similar relationship already exists\n",
    "            existing_rel = None\n",
    "            for r in graph.match((existing_node, org_node), r_type=rel_type):\n",
    "                if r['details'] == value:\n",
    "                    existing_rel = r\n",
    "                    break\n",
    "\n",
    "            # Create new or update existing relationship\n",
    "            if not existing_rel:\n",
    "                graph.create(rel)\n",
    "                relationships_created += 1\n",
    "            else:\n",
    "                for property_name, property_value in rel.items():\n",
    "                    if property_name not in existing_rel or existing_rel[property_name] != property_value:\n",
    "                        existing_rel[property_name] = property_value\n",
    "                        properties_updated += 1\n",
    "                graph.push(existing_rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Relationships created: {relationships_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update professional links in Neo4j based on the dictionary\n",
    "# create_or_update_professional_links(graph, dict_vinculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "def persist_journals_from_xlsx(graph: Graph, xlsx_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Persist journal information into a Neo4j database from an Excel (.xlsx) file.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        xlsx_path (str): The path to the Excel (.xlsx) file containing the journal information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize counters for tracking\n",
    "    nodes_created = 0\n",
    "    properties_updated = 0\n",
    "    \n",
    "    # Create NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "    \n",
    "    # Read the Excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract journal properties\n",
    "        properties = {'ISSN': row['ISSN'],\n",
    "                      'Título': row['Título'],\n",
    "                      'Área de Avaliação': row['Área de Avaliação'],\n",
    "                      'Estrato': row['Estrato']}\n",
    "        \n",
    "        # Check if a node with the same ISSN already exists\n",
    "        existing_node = matcher.match(\"Journal\", ISSN=row['ISSN']).first()\n",
    "        \n",
    "        if existing_node:\n",
    "            for key, value in properties.items():\n",
    "                if key not in existing_node or existing_node[key] != value:\n",
    "                    existing_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            graph.push(existing_node)\n",
    "        else:\n",
    "            new_node = Node(\"Journal\", **properties)\n",
    "            graph.create(new_node)\n",
    "            nodes_created += 1\n",
    "    \n",
    "    # Print statistical data\n",
    "    print(f\"Nodes created: {nodes_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_path = './../data/classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx'\n",
    "xlsx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Persistir informações sobre ISSN e Qualis do Periódicos Capes da Plataforma Sucurira de Arquivo Excel\n",
    "# Obs.: 154.410 linhas, 30.583 em 30min, 1020 nós/min em IntelCore i5-8500 CPU @ 3.00GHz 16Gb RAM no Windows\n",
    "# Obs.: 154.410 linhas, 31.339 em 20min, 1567 nós/min em AMDRyzen 7 5800X 8-Core 3.80GHz 64Gb RAM no Windows\n",
    "# Aproximadamente 2.5 horas para persistir os dados no Neo4j, terminará 15h\n",
    "\n",
    "# persist_journals_from_xlsx(graph, xlsx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Reconhecimento de Entidades Nomeadas (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_named_entities(text):\n",
    "#     import spacy\n",
    "    \n",
    "#     nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "#     nlp_pt = spacy.load(\"pt_core_news_lg\")\n",
    "#     doc = nlp_pt(text)\n",
    "#     entities = {'ORG': [], 'GPE': [], 'NORP': [], 'PERSON': [], 'PRODUCT': [], 'WORK_OF_ART': []}\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ in entities:\n",
    "#             entities[ent.label_].append(ent.text)\n",
    "#     return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GraphModel:\n",
    "#     def __init__(self, uri, user, password):\n",
    "#         self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "#     def close(self):\n",
    "#         self._driver.close()\n",
    "\n",
    "#     def add_entity(self, entity_type, entity_value):\n",
    "#         with self._driver.session() as session:\n",
    "#             session.write_transaction(self._add_entity, entity_type, entity_value)\n",
    "            \n",
    "#     @staticmethod\n",
    "#     def _add_entity(tx, entity_type, entity_value):\n",
    "#         query = f\"MERGE (a:{entity_type} {{name: $name}})\"\n",
    "#         tx.run(query, name=entity_value)\n",
    "    \n",
    "#     def add_relation(self, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "#         with self._driver.session() as session:\n",
    "#             session.write_transaction(self._add_relation, src_type, src_name, rel_type, tgt_type, tgt_name)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _add_relation(tx, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "#         query = (\n",
    "#             f\"MATCH (a:{src_type} {{name: $src_name}}), (b:{tgt_type} {{name: $tgt_name}}) \"\n",
    "#             f\"MERGE (a)-[:{rel_type}]->(b)\"\n",
    "#         )\n",
    "#         tx.run(query, src_name=src_name, tgt_name=tgt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_dict(d):\n",
    "#     def expand(key, value):\n",
    "#         if isinstance(value, dict):\n",
    "#             return [(str(key) + '.' + str(k), v) for k, v in flatten_dict(value).items()]\n",
    "#         else:\n",
    "#             return [(str(key), value)]\n",
    "        \n",
    "#     items = [item for k, v in d.items() for item in expand(k, v)]\n",
    "#     return dict(items)\n",
    "\n",
    "# def main(data_dict):\n",
    "#     graph_model = GraphModel(\"bolt://localhost:7687\", \"neo4j\", \"password\")    \n",
    "#     flattened_data = flatten_dict(data_dict['Properties']['Identificação'])\n",
    "#     person_name = flattened_data.get('Nome', None)[0]\n",
    "#     print(f'Nome: {person_name}')\n",
    "#     if person_name:\n",
    "#         graph_model.add_entity(\"Person\", person_name)\n",
    "    \n",
    "#     institutional = flatten_dict(data_dict['Properties'])\n",
    "#     for key, value in institutional.items():\n",
    "#         if key in ['Endereço.Endereço Profissional']:\n",
    "#             text = ' '.join(value)\n",
    "#             print(text)\n",
    "#             entities = extract_named_entities(text)\n",
    "#             print(f'Entidades reconhecidas: {entities}')\n",
    "#             entities\n",
    "            \n",
    "#             for org in entities.get('ORG', []):\n",
    "#                 graph_model.add_entity(\"Organization\", org)\n",
    "#                 graph_model.add_relation(\"Person\", person_name, \"AFFILIATED_WITH\", \"Organization\", org)\n",
    "                \n",
    "#             for gpe in entities.get('GPE', []):\n",
    "#                 graph_model.add_entity(\"Location\", gpe)\n",
    "#                 graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "#     projetos = flatten_dict(data_dict['Properties'])\n",
    "#     for key, value in projetos.items():\n",
    "#         if key in ['Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento']:\n",
    "#             entities = extract_named_entities(value)\n",
    "            \n",
    "#             for org in entities.get('ORG', []):\n",
    "#                 graph_model.add_entity(\"Organization\", org)\n",
    "#                 graph_model.add_relation(\"Person\", person_name, \"HAS_PROJECT_IN\", \"Organization\", org)\n",
    "                \n",
    "#             for gpe in entities.get('GPE', []):\n",
    "#                 graph_model.add_entity(\"Location\", gpe)\n",
    "#                 graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "#     graph_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests --trusted-host pypi.org --trusted-host files.pythonhosted.org --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python.exe -m pip install --upgrade pip --trusted-host pypi.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy --trusted-host pypi.org --trusted-host files.pythonhosted.org --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# # Processamento para eliminar strings duplicadas e vazias\n",
    "# def remove_duplicates(entities):\n",
    "#     for key in entities.keys():\n",
    "#         seen = set()\n",
    "#         unique_values = []\n",
    "        \n",
    "#         for value in entities[key]:\n",
    "#             lower_value = value.lower()\n",
    "#             if lower_value not in seen and value.strip():\n",
    "#                 seen.add(lower_value)\n",
    "#                 unique_values.append(value)\n",
    "        \n",
    "#         # Atualização do dicionário\n",
    "#         entities[key] = unique_values\n",
    "#     return entities\n",
    "\n",
    "# def extract_named_entities(text):\n",
    "#     nlp = spacy.load('en_core_web_lg')  # Assuming the use of the small Portuguese model\n",
    "#     doc = nlp(text)\n",
    "#     entities = {'ORG': [], 'GPE': [], 'PHONE': [], 'URL': []}\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ in entities.keys():\n",
    "#             entities[ent.label_].append(ent.text)\n",
    "    \n",
    "#     unique_entities = remove_duplicates(entities)\n",
    "    \n",
    "#     return unique_entities\n",
    "\n",
    "# text = 'Pós-Doutor em Computação pela Sorbonne Université-Pierre et Marie Curie (França - 2020). Doutor em Ciência da Computação pela Universitat Politecnica de Catalunya (Espanha - 2005). Atualmente é professor titular da Universidade de Fortaleza - UNIFOR na qual é membro permanente dos programas de pós-graduação em Informática Aplicada (mestrado e doutorado) e do mestrado profissional em administração. Possui mais de 100 trabalhos publicados em conferências e periódicos nacionais e internacionais. Tem experiência na área de Ciência da Computação, com ênfase em Ciência de Dados e Teleinformática, atuando principalmente nos seguintes temas: Redes Complexas, Grafos de Conhecimento, Aprendizagem de Máquina, Internet das Coisas, Segurança e Blockchain. (Texto informado pelo autor)\\n'\n",
    "\n",
    "# entities = extract_named_entities(text)\n",
    "# entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_dict['Properties'].keys())\n",
    "\n",
    "# main(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 2: Extrair dados dos Currículos para dicionários</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalization of the DOM extraction from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digital representation of the HTML DOM (Document Object Model) in question follow a consistent class-based structure, where the division of information into various classes within 'div' elements serves as an important taxonomy for organizing and categorizing the information.\n",
    "\n",
    "The nested structure predominantly consists of HTML div elements differentiated by their CSS classes. The div elements appear in a tree-like organization, hierarchically grouped under recursive presence of the div elements within the class 'title-wrapper', followed by the div elements marked with 'layout-cell' and hierarquically organized until reaching the more detailed levels where the data of interest is, contained in the classes such 'data-cell', 'text-align-right' or 'layout-cell-pad-5' and tags like 'a', 'b'.\n",
    "\n",
    "The extraction of data from this intricate nested architecture necessitates a recursive methodology that maintains the hierarchical fidelity of the original data. Thus, one approach to transforming this data into a structured JSON object would be to employ depth-first search (DFS) algorithms to traverse through each node in this tree-like structure. Each traversal would examine the class attributes and potentially the text content within each div. \n",
    "\n",
    "**Formalization:**\n",
    "\n",
    "In the formal language of computational theory, let \\( T \\) be the DOM tree with each node \\( n \\) containing a list of attributes \\( A(n) \\) and a text content \\( C(n) \\). Let \\( JSON(n) \\) be the JSON representation of the node \\( n \\). The recursive function to extract data can be described as:\n",
    "\n",
    "\n",
    "JSON(n) = \n",
    "\\begin{cases} \n",
    "\\{ \"type\": A(n), \"content\": C(n), \"children\": \\{ JSON(c) \\,|\\, c \\in \\text{children of } n \\} \\} & \\text{if } n \\text{ has children} \\\\\n",
    "\\{ \"type\": A(n), \"content\": C(n) \\} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "**Python implementation:**\n",
    " \n",
    "In terms of practical implementation, Python's Beautiful Soup library can be particularly effective for this task, allowing for a relatively straightforward traversal of each div element to construct the JSON object.\n",
    "\n",
    "The end result would be a JSON object where each entry corresponds to a 'div' element in the original HTML structure, represented by a dictionary containing the attributes and content of the div, and potentially another dictionary (or list of dictionaries) representing any nested child div elements. This would effectively capture the data within each div while maintaining the hierarchical structure of the original HTML document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair DOM para Dicionários de um Currículo apenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = connect_driver(caminho)\n",
    "\n",
    "NOME = 'João Hermínio Martins da Silva'\n",
    "fill_name(driver, delay, NOME)\n",
    "\n",
    "limite=3\n",
    "instituicao = 'Fundação Oswaldo Cruz'\n",
    "unidade     = 'Fiocruz Ceará'\n",
    "termo       = 'Ministerio da Saude'\n",
    "\n",
    "elm_vinculo, np.NaN, np.NaN, np.NaN, driver = find_terms(NOME, instituicao, unidade, termo, driver, delay, limite)\n",
    "soup = open_curriculum(driver,elm_vinculo)\n",
    "\n",
    "caracteres = len(soup.text)\n",
    "linhas = len(soup.text.split('\\n'))\n",
    "print(f'Total de caracteres extraídos: {caracteres:6d}')\n",
    "print(f'Quantidade extraída de linhas: {linhas:6d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = aggregate_data_dicts(soup)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização dos Dados extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar uma lista de dicionários\n",
    "data_list = dict_doi_list(data_dict)\n",
    "pprint(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Caso não esteja como dicionário e sim listas ou strings\n",
    "# # Convert the entire string representation to a dictionary\n",
    "# outer_dict = json.loads(data_list[0])\n",
    "# # Iterate over the outer dictionary and convert the inner JSON-like strings to valid dictionaries\n",
    "# for key, value in outer_dict.items():\n",
    "#     outer_dict[key] = json.loads(value)\n",
    "\n",
    "# # Now, outer_dict holds the data in nested dictionary format\n",
    "# pprint(outer_dict)\n",
    "\n",
    "# jcr_properties_list = outer_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data_list)\n",
    "\n",
    "# Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "data.replace(['NULL', None], np.nan, inplace=True)\n",
    "data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "data = data.sort_values(by=['impact-factor','issn','doi'], ascending=False)\n",
    "cout_data_issn = data['data-issn'].notnull().sum()\n",
    "print(f'{cout_data_issn} ISSN recuperados dos  tooltips, {len(data.index)-cout_data_issn} ISSN ausentes')\n",
    "\n",
    "# Preencher \n",
    "data['data-issn'].fillna(data['issn'].apply(standardize_issn_format), inplace=True)\n",
    "\n",
    "count_jcrissn = data['issn'].notna().sum()\n",
    "count_doi = data['doi'].notna().sum()\n",
    "count_jci = data['impact-factor'].notna().sum()\n",
    "\n",
    "print(f'{count_jcrissn} ISSN recuperados de id-artigos, {len(data.index)-count_jcrissn} ISSN ausentes')\n",
    "print(f'{count_doi}  DOI recuperados, {len(data.index)-count_jci}  DOI ausentes')\n",
    "print(f'{count_jci}  JCI recuperados, {len(data.index)-count_jci}  JCI ausentes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementar com dados do CrossRef "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_crossref = crossref_complement(data)\n",
    "# data_crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtro1 = data_crossref['impact-factor'].isna()\n",
    "# temp = data_crossref[filtro1]\n",
    "# filtro2 = temp['issn'].notna()\n",
    "# consultar = temp[filtro2]\n",
    "# consultar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementar ISSN de arquivos de texto do JCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(montar_lista_issn(data))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Exemplo dados do Raimir\n",
    "# 30 buscados\n",
    "'1566-2535, 2327-4662, 0890-8044, 1084-8045, 2169-3536, 2169-3536, 2169-3536, 1868-5137, 2076-3417, 1074-5351, 1055-7148, 2494-2715, 2236-0700, 2236-0700, 2076-3417, 1980-086X, 1676-2789, 1676-2789, 1553-877X, 1550-1329, 1548-7709, 1448-5869, 1424-8220, 1424-8220, 1424-8220, 1424-8220, 1424-8220, 1414-5685, 0101-8191, 0101-8191'\n",
    "\n",
    "# 09 duplicados\n",
    "'1980-086X, 1676-2789, 1548-7709, 1448-5869, 2494-2715, 1424-5685, 0101-8191, 2327-4662, 2236-0700'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Em caso de muitos dados de ISSN faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # File paths\n",
    "\n",
    "# file_paths = [\n",
    "#     f'../data/{NOME}_JCR_JournalResults2018.csv',\n",
    "#     f'../data/{NOME}_JCR_JournalResults2019.csv',\n",
    "#     f'../data/{NOME}_JCR_JournalResults2020.csv',\n",
    "#     f'../data/{NOME}_JCR_JournalResults2021.csv',\n",
    "#     f'../data/{NOME}_JCR_JournalResults2022.csv'\n",
    "# ]\n",
    "\n",
    "# # Generate df_aggregated by concatenating all the annual dataframes\n",
    "# df_aggregated = load_and_concatenate(file_paths)\n",
    "# df_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch the most recent data for each ISSN or eISSN and get the most recent year\n",
    "# df_most_recent, most_recent_year = get_most_recent_data(df_aggregated)\n",
    "# df_most_recent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo entradas duplicadas\n",
    "# df_unic = df_most_recent.drop_duplicates(subset='ISSN', keep='last')\n",
    "\n",
    "# # Uso da função\n",
    "# updated_data = complement_jcr(data_crossref, df_unic)\n",
    "# updated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Populate the starting dataframe with missing data\n",
    "# updated_df = populate_missing_data(updated_data, df_most_recent, most_recent_year)\n",
    "# updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para prosseguir com extração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final  = df_to_use.sort_values(by=['impact-factor','issn','jcr-ano','doi'], ascending=False)\n",
    "df_final.columns = ['jcr-ano', 'doi', 'data-issn', 'original_title', 'impact-factor',\n",
    "       'issn', 'volume', 'paginaInicial', 'titulo', 'sequencial',\n",
    "       'journal']\n",
    "\n",
    "count_doi_non_nan = df_final['doi'].notna().sum()\n",
    "count_jci_non_nan = df_final['impact-factor'].notna().sum()\n",
    "count_issn_non_nan = df_final['issn'].notna().sum()\n",
    "print(f'{count_doi_non_nan} valores de DOI identificados')\n",
    "print(f'{count_issn_non_nan} valores de ISSN não nulos extraídos')\n",
    "print(f'{count_jci_non_nan} valores de fator de impacto não nulos')\n",
    "\n",
    "df_final.sort_values(by=['impact-factor','issn','doi'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_updated = fill_na_from_df_aggregated(df_final, df_aggregated)\n",
    "sorted_df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "# data.replace(['NULL', None], np.nan, inplace=True)\n",
    "# data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "# data = data.sort_values(by=['impact-factor','issn'], ascending=False)\n",
    "\n",
    "# # Agrupar pelo 'data-issn', contar ocorrências, maior valor de 'impact-factor' e pegar o 'journal'\n",
    "# grouped = data.groupby('issn').agg(\n",
    "#     count=('issn', 'size'),\n",
    "#     impact_factor_max=('impact-factor', 'max'),\n",
    "#     journal=('journal', max_len_string)\n",
    "# ).reset_index()\n",
    "\n",
    "# # Renomear as colunas para corresponder às suas descrições\n",
    "# grouped.rename(columns={\"count\": \"issn_count\", \"impact_factor_max\": \"impact-factor\"}, inplace=True)\n",
    "\n",
    "# count_nan_issn  = data['issn'].isna().sum()\n",
    "# null_row        = pd.DataFrame({'issn': ['NULL'], 'issn_count': [count_nan_issn]})\n",
    "# data_all_counts = pd.concat([grouped, null_row], ignore_index=True)\n",
    "# data_all_counts = data_all_counts.sort_values(by=['impact-factor'], ascending=False)\n",
    "# print(data_all_counts['issn_count'].sum())\n",
    "# # data_all_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_records = dataframe_to_list_of_dicts(df_final)\n",
    "jcr_properties_list = formatted_records\n",
    "\n",
    "plot_vertbar(jcr_properties_list)\n",
    "plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultar dados do JCR a cada ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avaliar a evolução de cada revista ao longo dos anos quanto ao JCI\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2018.csv'\n",
    "# df2018 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2019.csv'\n",
    "# df2019 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2020.csv'\n",
    "# df2020 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2021.csv'\n",
    "# df2021 = org_lines(file_path)\n",
    "\n",
    "# file_path = '../data/{NAME[0]}_JCR_JournalResults2022.csv'\n",
    "# df2022 = org_lines(file_path)\n",
    "\n",
    "# df_issn = pd.concat([df2018, df2019, df2020, df2021, df2022], ignore_index=True)\n",
    "\n",
    "# df_issn = pd.merge(df1, df2, on='issn', how='inner')\n",
    "# df_issn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesclar eISSN com ISSN para achar mais dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_na_from_df_aggregated(df_final, df_aggregated):\n",
    "    # Filter df_aggregated for rows where eISSN is NaN\n",
    "    df_nan_eissn = df_aggregated[df_aggregated['eISSN'].isna()]\n",
    "\n",
    "    # Iterate through the rows of df_nan_eissn\n",
    "    for idx, row in df_nan_eissn.iterrows():\n",
    "        issn_value = row['ISSN']\n",
    "        \n",
    "        # Find the rows in sorted_df with matching 'issn' and NaN 'impact-factor'\n",
    "        mask = (df_final['issn'] == issn_value) & df_final['impact-factor'].isna()\n",
    "        \n",
    "        if mask.sum() > 0:  # if there are matching rows in sorted_df\n",
    "            df_final.loc[mask, 'impact-factor'] = row['2022 JIF']  # Assuming you want the most recent JIF\n",
    "            df_final.loc[mask, 'journal'] = row['Journal name']\n",
    "    \n",
    "    df_final = df_final.sort_values(by=['impact-factor','issn'], ascending=False, na_position='last')\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_json = fill_na_from_df_aggregated(df_final, df_aggregated)\n",
    "print(len(result_json))\n",
    "result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chame a função com sua lista de dados jcr_properties_list\n",
    "plot_vertbar(result_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscal avaliação Qualis Periódicos em arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/makaires77/fioce/main/source/adapters/input/data/sucupira_todas_as_areas_avaliacao1672761192111.csv\"\n",
    "# data_sucupira = pd.read_csv(url, delimiter=';')\n",
    "# data_sucupira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 3: Persistir em em Neo4j</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistir todos dados como propriedades do nó de Label: Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j_persister import Neo4jPersister  # Para quando já houver pacote neo4j_persister.py\n",
    "# Create a Neo4jPersister instance and persist the data\n",
    "neo4j_persister = Neo4jPersister(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "neo4j_persister.persist_data(data_dict, \"Person\")\n",
    "\n",
    "# Close the Neo4j connection\n",
    "neo4j_persister.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação dos nós secundários e relacionamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the JcrHandler object with the required connection details\n",
    "journals_analysis_obj = JcrHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to create Journals nodes and establish relationships for {name}\n",
    "journals_analysis_obj.createJournalsNodes(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "journals_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AdvisorHandler object with the required connection details\n",
    "advisor_analysis_obj = AdvisorHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for {name}\n",
    "advisor_analysis_obj.create_advisor_relations(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "advisor_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AreasHandler object with the required connection details\n",
    "areas_analysis_obj = AreasHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for {name}\n",
    "areas_analysis_obj.create_areas_relations(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "areas_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ProjectsHandler object with the required connection details\n",
    "projects_analysis_obj = ProjectsHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for {name}\n",
    "projects_analysis_obj.create_projects_relations(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "projects_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ProjectsHandler object with the required connection details\n",
    "articles_analysis_obj = ArticleHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "# Call the function to process advisories data and establish relationships for {name}\n",
    "articles_analysis_obj.process_articles(name=NOME)\n",
    "\n",
    "# Close the connection to the database (optional, but a good practice)\n",
    "articles_analysis_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletar nós por Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handler = DataRemovalHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "# # labels=\"GrandeÁrea\"\n",
    "# # labels=\"Área\"\n",
    "# # labels=\"Subárea\"\n",
    "# labels = 'Artigo'\n",
    "# deleted_count = handler.delete_nodes_by_label(labels)\n",
    "# print(f\"{deleted_count} nós deletados com rótulo {labels}.\")\n",
    "# handler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> FASE 4: Extrações em lote de lista de pesquisadores</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all(NOME):\n",
    "    pastaraiz = 'fioce'\n",
    "    caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "\n",
    "    drives=['C:/Users/','E:/','./home/']\n",
    "    pastas=['marcos.aires/', 'marco/']\n",
    "    pastasraiz=['kgfioce','fioce']\n",
    "    caminho=try_folders(drives,pastas,pastasraiz)\n",
    "    preparar_pastas(caminho)\n",
    "    driver = connect_driver(caminho)\n",
    "\n",
    "    fill_name(driver, delay, NOME)\n",
    "    limite=3\n",
    "    instituicao = 'Fundação Oswaldo Cruz'\n",
    "    unidade     = 'Fiocruz Ceará'\n",
    "    termo       = 'Ministerio da Saude'\n",
    "    elm_vinculo, np.NaN, np.NaN, np.NaN, driver = find_terms(NOME, instituicao, unidade, termo, driver, delay, limite)\n",
    "    soup = open_curriculum(driver,elm_vinculo)\n",
    "\n",
    "    caracteres = len(soup.text)\n",
    "    linhas = len(soup.text.split('\\n'))\n",
    "    print(f'Total de caracteres extraídos: {caracteres:6d}')\n",
    "    print(f'Quantidade extraída de linhas: {linhas:6d}')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def persist_all(soup):\n",
    "    data_dict = aggregate_data_dicts(soup)\n",
    "\n",
    "    # from neo4j_persister import Neo4jPersister  # Para quando já houver pacote neo4j_persister.py\n",
    "    # Create a Neo4jPersister instance and persist the data\n",
    "    neo4j_persister = Neo4jPersister(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "    neo4j_persister.persist_data(data_dict, \"Person\")\n",
    "    neo4j_persister.close()\n",
    "\n",
    "    # Initialize the JcrHandler object with the required connection details\n",
    "    journals_analysis_obj = JcrHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "    # Call the function to create Journals nodes and establish relationships for {name}\n",
    "    journals_analysis_obj.createJournalsNodes(name=NOME)\n",
    "    journals_analysis_obj.close()\n",
    "\n",
    "    # Initialize the AdvisorHandler object with the required connection details\n",
    "    advisor_analysis_obj = AdvisorHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "    # Call the function to process advisories data and establish relationships for {name}\n",
    "    advisor_analysis_obj.create_advisor_relations(name=NOME)\n",
    "    advisor_analysis_obj.close()\n",
    "\n",
    "    # Initialize the AreasHandler object with the required connection details\n",
    "    areas_analysis_obj = AreasHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "    # Call the function to process advisories data and establish relationships for {name}\n",
    "    areas_analysis_obj.create_areas_relations(name=NOME)\n",
    "    areas_analysis_obj.close()\n",
    "\n",
    "    # Initialize the ProjectsHandler object with the required connection details\n",
    "    projects_analysis_obj = ProjectsHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "    # Call the function to process advisories data and establish relationships for {name}\n",
    "    projects_analysis_obj.create_projects_relations(name=NOME)\n",
    "    projects_analysis_obj.close()\n",
    "\n",
    "    # Initialize the ProjectsHandler object with the required connection details\n",
    "    articles_analysis_obj = ArticleHandler(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "\n",
    "    # Call the function to process advisories data and establish relationships for {name}\n",
    "    articles_analysis_obj.process_articles(name=NOME)\n",
    "    articles_analysis_obj.close()\n",
    "\n",
    "def moutn_dataframe(soup):\n",
    "    data_dict = aggregate_data_dicts(soup)\n",
    "\n",
    "    # Gerar uma lista de dicionários\n",
    "    data_list = dict_doi_list(data_dict)\n",
    "\n",
    "    # Montagem de dataframe\n",
    "    data = pd.DataFrame(data_list)\n",
    "    # Substituir 'NULL' e None por np.nan em todo o dataframe\n",
    "    data.replace(['NULL', None], np.nan, inplace=True)\n",
    "    data.loc[:, 'impact-factor'] = data['impact-factor'].astype(float)\n",
    "    data = data.sort_values(by=['impact-factor','issn','doi'], ascending=False)\n",
    "    count_jci = data['impact-factor'].notna().sum()\n",
    "    count_doi = data['doi'].notna().sum()\n",
    "    count_jcrissn = data['issn'].notna().sum()\n",
    "    print(f'{count_jcrissn} ISSN recuperados, {len(data.index)-count_jcrissn} ISSN ausentes')\n",
    "    print(f'{count_doi}  DOI recuperados, {len(data.index)-count_jci}  DOI ausentes')\n",
    "    print(f'{count_jci}  JCI recuperados, {len(data.index)-count_jci}  JCI ausentes')\n",
    "\n",
    "    # Compelentação CrossRef\n",
    "\n",
    "\n",
    "    df_final  = data.sort_values(by=['impact-factor','issn','jcr-ano','doi','journal'], ascending=False)\n",
    "    count_doi_non_nan = df_final['doi'].notna().sum()\n",
    "    count_jci_non_nan = df_final['impact-factor'].notna().sum()\n",
    "    count_issn_non_nan = df_final['issn'].notna().sum()\n",
    "    print(f'{count_doi_non_nan} valores de DOI identificados')\n",
    "    print(f'{count_issn_non_nan} valores de ISSN não nulos extraídos')\n",
    "    print(f'{count_jci_non_nan} valores de fator de impacto não nulos')\n",
    "\n",
    "    # Plotagem\n",
    "    df_final.sort_values(by=['issn','doi'], ascending=False)\n",
    "    formatted_records = dataframe_to_list_of_dicts(df_final)\n",
    "    jcr_properties_list = formatted_records\n",
    "    plot_vertbar(jcr_properties_list)\n",
    "    plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "\n",
    "for NOME in lista:\n",
    "    extract_all(NOME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
