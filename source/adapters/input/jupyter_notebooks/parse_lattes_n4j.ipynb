{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Explorar dados dos currículos Lattes para<br /> propor modelo de Grafo para análises futuras </center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa – Fiocruz Ceará\n",
    "\n",
    "**Introdução**\n",
    "\n",
    "A análise de Grafos é uma potente ferramenta para obtenção de insights e realização de análises envolvendo dados heterogêneos e relações complexas, neste trabalho propomos uma análise dos dados de pesquisa acadêmica tendo como fonte de dados os currículo Lattes de servidores da unidade Fiocruz Ceará.\n",
    "\n",
    "**Objetivo geral:**\n",
    "\n",
    "    Explorar dados dos currículos de servidores da Fiocruz Ceará.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "\n",
    "    1. Extrair dados dos currículos;\n",
    "    2. Propor modelo de grafo para análises futuras;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 0: Preparar e Testar Ambiente</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "        \n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_amb():\n",
    "    ## Visualizar versões dos principais componentes\n",
    "    import os\n",
    "    import pip\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # !pip3 install shutup\n",
    "    # import shutup; shutup.please()\n",
    "    \n",
    "    pyVer      = sys.version\n",
    "    pipVer     = pip.__version__\n",
    "    \n",
    "    print('\\nVERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT')\n",
    "    print('Interpretador em uso:', sys.executable)\n",
    "    print('    Ambiente ativado:',os.environ['CONDA_DEFAULT_ENV'])\n",
    "    print('     Python: '+pyVer, '\\n        Pip:', pipVer,'\\n'\n",
    "         )\n",
    "\n",
    "    !nvcc -V\n",
    "\n",
    "def try_gpu():\n",
    "    print('\\nVERSÕES DO PYTORCH E GPU DISPONÍVEIS')\n",
    "    try:\n",
    "        import torch\n",
    "        print('    PyTorch:',torch.__version__)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Dispositivo:',device)\n",
    "        print('Disponível :',device,torch.cuda.is_available(),' | Inicializado:',torch.cuda.is_initialized(),'| Capacidade:',torch.cuda.get_device_capability(device=None))\n",
    "        print('Nome GPU   :',torch.cuda.get_device_name(0),'         | Quantidade:',torch.cuda.device_count(),'\\n')\n",
    "    except Exception as e:\n",
    "        print('Erro ao configurar a GPU:',e,'\\n')\n",
    "\n",
    "def try_folders(drives,pastas,pastasraiz):\n",
    "    import os\n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    caminho_testado = drive+i+j\n",
    "                    if os.path.isfile(caminho_testado+'/chromedriver/chromedriver.exe'):\n",
    "                        print(f\"Listando arqivos em: {caminho_testado}\")\n",
    "                        print(os.listdir(caminho_testado))\n",
    "                        caminho = caminho_testado+'/'\n",
    "                except:\n",
    "                    caminho=''\n",
    "                    print('Não foi possível encontrar uma pasta de trabalho')\n",
    "    return caminho\n",
    "\n",
    "def try_browser(raiz):\n",
    "    print('\\nVERSÕES DO BROWSER E DO CHROMEDRIVER INSTALADAS')\n",
    "    from selenium import webdriver\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    try:\n",
    "        # Caminho para o seu chromedriver\n",
    "        driver_path=raiz+'chromedriver/chromedriver.exe'\n",
    "        print(driver_path)\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        str1 = driver.capabilities['browserVersion']\n",
    "        str2 = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]\n",
    "        print(f'     Versão do browser: {str1}')\n",
    "        print(f'Versão do chromedriver: {str2}')\n",
    "        driver.quit()\n",
    "\n",
    "        if str1[0:3] != str2[0:3]: \n",
    "            print(\"Versões incompatíveis, atualizar chromedriver!\")\n",
    "            print('  Baixar versão atualizada do Chromedriver em:')\n",
    "            print('  https://googlechromelabs.github.io/chrome-for-testing/#stable')\n",
    "            print('     Ex. Versão 116 PARA WINDOWS:')\n",
    "            print('\t    https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/win64/chromedriver-win64.zip')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def try_chromedriver(caminho):\n",
    "    try:\n",
    "        import os\n",
    "        os.listdir(caminho)\n",
    "    except Exception as e:\n",
    "        raiz=caminho\n",
    "\n",
    "    finally:\n",
    "        print(raiz)\n",
    "    return raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Erro ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "\n",
    "    caminho = drive+':/'+usuario+pastaraiz+'/'\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    import os\n",
    "\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema operacional Windows\n",
      "Drive em uso C\n",
      "Pasta armazenagem local C:/Users/marco/fioce/\n",
      "\n",
      "\n",
      "VERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT\n",
      "Interpretador em uso: c:\\Users\\marco\\.conda\\envs\\beakerx\\python.exe\n",
      "    Ambiente ativado: beakerx\n",
      "     Python: 3.11.2 | packaged by Anaconda, Inc. | (main, Mar 27 2023, 23:35:04) [MSC v.1916 64 bit (AMD64)] \n",
      "        Pip: 23.2.1 \n",
      "\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Jul_11_03:10:21_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.128\n",
      "Build cuda_12.2.r12.2/compiler.33053471_0\n",
      "\n",
      "VERSÕES DO PYTORCH E GPU DISPONÍVEIS\n",
      "    PyTorch: 2.0.1+cu118\n",
      "Dispositivo: cuda\n",
      "Disponível : cuda True  | Inicializado: False | Capacidade: (7, 5)\n",
      "Nome GPU   : NVIDIA GeForce RTX 2060          | Quantidade: 1 \n",
      "\n",
      "\n",
      "VERSÕES DO BROWSER E DO CHROMEDRIVER INSTALADAS\n",
      "C:/Users/marco/fioce/chromedriver/chromedriver.exe\n",
      "     Versão do browser: 117.0.5938.92\n",
      "Versão do chromedriver: 117.0.5938.88\n"
     ]
    }
   ],
   "source": [
    "pastaraiz = 'fioce'\n",
    "caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "try_amb()\n",
    "try_gpu()\n",
    "try_browser(caminho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 1: Extrair DOM para objeto Soup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Importar, conectar e gerar driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from flask import render_template_string\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "delay = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install py2neo --trusted-host pypi.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Função 1: Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    \n",
    "\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "    driver_path=caminho+'chromedriver/chromedriver.exe'\n",
    "    print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)    \n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "\n",
    "    # return browser, url, session_id\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Manipular HTML para chegar ao currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_name(driver, delay, NOME):\n",
    "    '''\n",
    "    Função 2: passa o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def achar_busca(driver, delay):\n",
    "    '''\n",
    "    Função 3: clica no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = driver.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               #expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               #logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_curriculum(driver,elm_vinculo):\n",
    "    link_nome     = achar_busca(driver, delay)\n",
    "    window_before = driver.current_window_handle\n",
    "\n",
    "    limite = 5\n",
    "    if str(elm_vinculo) == 'nan':\n",
    "        print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "        raise Exception\n",
    "    print('Vínculo encontrado no currículo de nome:',elm_vinculo.text)\n",
    "\n",
    "    ## Clicar no botão abrir currículo e mudar de aba\n",
    "    try:\n",
    "        ## Aguarda, encontra, clica em buscar nome\n",
    "        link_nome    = achar_busca(driver, delay)\n",
    "    except Exception as e:\n",
    "        print('Erro')\n",
    "        print(e)\n",
    "        \n",
    "    if link_nome.text == None:\n",
    "        xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "        # 'Stale file handle'\n",
    "        print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "        #    expected_ex_type=ZeroDivisionError, \n",
    "        wait_ms=200,\n",
    "        limit=limite, \n",
    "        #    logger=logger, \n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "    try:\n",
    "        ActionChains(driver).click(link_nome).perform()\n",
    "    except:\n",
    "        print(f'Currículo não encontrado.')\n",
    "\n",
    "    retry(WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "        #    expected_ex_type=ZeroDivisionError, \n",
    "        wait_ms=200,\n",
    "        limit=limite, \n",
    "        #    logger=logger, \n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "    # Clicar botão para abrir o currículo\n",
    "    btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "    ## Gerenciamento das janelas abertas no browser\n",
    "    WebDriverWait(driver, delay).until(EC.number_of_windows_to_be(2))\n",
    "    window_after = driver.window_handles\n",
    "    new_window   = [x for x in window_after if x != window_before][0]\n",
    "    driver.switch_to.window(new_window)\n",
    "\n",
    "    # Pega o código fonte da página\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    # Usa BeautifulSoup para analisar\n",
    "    soup = BeautifulSoup(page_source, 'html.parser') \n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_terms(NOME, instituicao, unidade, termo, driver, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    \n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(driver)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            ## TO-DO inserir a crítica para o 'Stale file handle'\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(driver)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(driver)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, driver\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # driver.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, driver\n",
    "    \n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "def is_abreviation(substring):\n",
    "    pattern = r'(?: [a-zA-Z]\\.)|(?: [a-zA-Z] \\.)'\n",
    "    if re.search(pattern, substring):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_surname(substring):\n",
    "    return not is_abreviation(substring) and substring.endswith(', ')\n",
    "\n",
    "def compose_full_name(surname, parts, marker):\n",
    "    full_name = \"\"\n",
    "    for part in parts:\n",
    "        full_name += part + marker\n",
    "    full_name += ' '+surname\n",
    "    return full_name.strip()\n",
    "\n",
    "def split_authors(string, verbose=False):\n",
    "    authors_names = []\n",
    "    \n",
    "    m1a = \"; \"\n",
    "    m1b = \", \"\n",
    "    \n",
    "    if m1a in string and (string.count(m1a) <= string.count(m1b) or m1b not in string):\n",
    "        marker = m1a\n",
    "        authors_names = string.split(marker)\n",
    "        return [x.strip() for x in authors_names]\n",
    "    else:\n",
    "        marker = m1b\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Selected marker: \"{marker}\"')\n",
    "\n",
    "    parts_list = string.split(marker)\n",
    "    name    = \"\"\n",
    "    surname = \"\"\n",
    "    \n",
    "    for part in parts_list:\n",
    "        if is_abreviation(part):\n",
    "            classification = 'Abrev'\n",
    "            name += part + marker\n",
    "        else:\n",
    "            classification = 'Name'\n",
    "            if is_surname(part) or surname == \"\":\n",
    "                classification = 'SOBRENOME'\n",
    "                if surname:\n",
    "                    full_name = compose_full_name(surname.strip(), name.split(marker), ', ')\n",
    "                    if full_name not in authors_names:\n",
    "                        authors_names.append(full_name)\n",
    "                surname = part\n",
    "                name = part + marker\n",
    "            else:\n",
    "                name = part + marker\n",
    "        if verbose:\n",
    "            # print(f'Quantidade de m1a: {string.count(m1a)} \\nQuantidade de m1b: {string.count(m1b)}')\n",
    "            print(f'Autor: {part:40} | Forma: {classification}')\n",
    "\n",
    "        authors_names.append(part.strip(marker).strip())\n",
    "     \n",
    "    return [x.strip() for x in authors_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extrair_indices(layout_celula):\n",
    "    indice = layout_celula.select_one('div.layout-cell-pad-5.text-align-right')\n",
    "    valor = layout_celula.select_one('div.layout-cell.layout-cell-9 div.layout-cell-pad-5')\n",
    "    \n",
    "    if indice and valor:\n",
    "        valores_extraidos = valor.text.split('<br class=\"clear\">') if '<br class=\"clear\">' in valor.text else valor.text.split('\\n\\t\\t\\n\\t')\n",
    "        return indice.text, valores_extraidos\n",
    "    return None, None\n",
    "\n",
    "def extrair_secao(title_wrapper):\n",
    "    tags_relevantes = ['h1', 'ul', 'b']\n",
    "    tags_encontradas = []\n",
    "    for tag in tags_relevantes:\n",
    "        elemento_encontrado = title_wrapper.select_one(tag)\n",
    "        if elemento_encontrado:\n",
    "            tags_encontradas.append((tag, elemento_encontrado))\n",
    "    tags_ordenadas = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def encontrar_subchave(title_wrapper):\n",
    "    tags_relevantes  = ['ul', 'a', 'b']\n",
    "    tags_encontradas = [(tag, title_wrapper.find(tag)) for tag in tags_relevantes]\n",
    "    tags_ordenadas   = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def extrair_titulo(title_wrapper):\n",
    "    inst_back_texts = {}\n",
    "    inst_back_elements = title_wrapper.find_all('div', class_='inst_back')\n",
    "    for index, inst_back in enumerate(inst_back_elements):\n",
    "        b_tag = inst_back.find('b')\n",
    "        if b_tag:\n",
    "            inst_back_texts[index] = b_tag.text.strip()\n",
    "    return inst_back_texts if inst_back_texts else None\n",
    "\n",
    "def extrair_wraper(soup, json_data):\n",
    "    title_wrappers = soup.select('div.layout-cell-pad-main div.title-wrapper')\n",
    "    for title_wrapper in title_wrappers:\n",
    "        section_name = extrair_secao(title_wrapper)\n",
    "        if section_name:\n",
    "            section_name = section_name.text.strip()\n",
    "            \n",
    "            titles = extrair_titulo(title_wrapper)\n",
    "            json_data[\"Properties\"][section_name] = {}\n",
    "            \n",
    "            if titles:\n",
    "                for index, title in titles.items():\n",
    "                    json_data[\"Properties\"][section_name][title] = {}\n",
    "            \n",
    "            layout_cells = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "            for layout_celula in layout_cells:\n",
    "                indice, valores_extraidos = extrair_indices(layout_celula)\n",
    "                if indice and valores_extraidos:\n",
    "                    if titles and indice in titles.values():\n",
    "                        if len(titles) > 1:\n",
    "                            for title in titles.values():\n",
    "                                if title.strip() in indice:\n",
    "                                    json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                        else:\n",
    "                            title = list(titles.values())[0]\n",
    "                            json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                    else:\n",
    "                        json_data[\"Properties\"][section_name][indice] = valores_extraidos\n",
    "    return json_data\n",
    "\n",
    "def imprimir_informacoes(dados_json, nome_no, indent=0):\n",
    "    indentation = '    ' * indent  # Calculating the current indentation level\n",
    "\n",
    "    if dados_json and nome_no and dados_json.get(nome_no):\n",
    "        if indent == 0:  # Logging node-level information only at the root\n",
    "            logging.info(f\"{indentation}Node: {nome_no}\")\n",
    "            logging.info(f\"{indentation}Total keys extracted: {len(dados_json[nome_no].keys())}\")\n",
    "        \n",
    "        for key in dados_json[nome_no].keys():\n",
    "            logging.info(f\"{indentation}{key.strip() if key else ''}\")\n",
    "\n",
    "            if isinstance(dados_json[nome_no][key], dict):  # Check for nested dictionaries\n",
    "                # Recursive call to handle nested dictionaries\n",
    "                imprimir_informacoes(dados_json[nome_no], key, indent + 1)\n",
    "            else:\n",
    "                logging.info(f\"{indentation}    Values: {dados_json[nome_no][key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERSÃO 01 de extrair dados\n",
    "# def extrair_dados(soup, verbose=False):\n",
    "#     nome_no = soup.select_one('div.infpessoa h2.nome').text if soup.select_one('div.infpessoa h2.nome') else None\n",
    "    \n",
    "#     if not nome_no:\n",
    "#         logging.error(\"Nome do nó não encontrado. Abortando.\")\n",
    "#         return\n",
    "    \n",
    "#     dados_json = {nome_no: {}}\n",
    "#     celula_principal = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "#     title_wrappers = celula_principal.select('div.title-wrapper')\n",
    "#     if verbose:\n",
    "#         logging.info(f'{len(title_wrappers)} seções de dados lidas com sucesso.')\n",
    "    \n",
    "#     for title_wrapper in title_wrappers:\n",
    "#         nome_secao = extrair_secao(title_wrapper)\n",
    "        \n",
    "#         if nome_secao is None:\n",
    "#             continue\n",
    "        \n",
    "#         if verbose:\n",
    "#             logging.info(f'Marcador de Seção: \"{nome_secao.text}\"')\n",
    "\n",
    "#         titulo = extrair_titulo(title_wrapper)\n",
    "#         if verbose:\n",
    "#             logging.info(f'Marcador de Título: \"{titulo}\"')\n",
    "                    \n",
    "#         chave = nome_secao.text\n",
    "#         dados_json[nome_no][chave] = {}\n",
    "\n",
    "#         # A seleção agora ocorre dentro do contexto de title_wrapper, e não de celula_principal.\n",
    "#         celulas_layout = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "        \n",
    "#         for celula_layout in celulas_layout:\n",
    "#             if celula_layout.find_all('div'):\n",
    "#                 indice, valores = extrair_indices(celula_layout)\n",
    "                \n",
    "#                 if indice and valores:\n",
    "#                     dados_json[nome_no][chave][indice] = valores\n",
    "\n",
    "#     if verbose:\n",
    "#         logging.info(f\"Total de índices extraídos: {len(dados_json[nome_no].keys())}\")\n",
    "#         # Outros blocos de código para depuração e verbosidade\n",
    "#     return dados_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrair Objeto Soup com DOM completo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formalization of the DOM extraction from the HTML page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digital representation of the HTML DOM (Document Object Model) in question follow a consistent class-based structure, where the division of information into various classes within 'div' elements serves as an important taxonomy for organizing and categorizing the information.\n",
    "\n",
    "The nested structure predominantly consists of HTML div elements differentiated by their CSS classes. The div elements appear in a tree-like organization, hierarchically grouped under recursive presence of the div elements within the class 'title-wrapper', followed by the div elements marked with 'layout-cell' and hierarquically organized until reaching the more detailed levels where the data of interest is, contained in the classes such 'data-cell', 'text-align-right' or 'layout-cell-pad-5' and tags like 'a', 'b'.\n",
    "\n",
    "The extraction of data from this intricate nested architecture necessitates a recursive methodology that maintains the hierarchical fidelity of the original data. Thus, one approach to transforming this data into a structured JSON object would be to employ depth-first search (DFS) algorithms to traverse through each node in this tree-like structure. Each traversal would examine the class attributes and potentially the text content within each div. \n",
    "\n",
    "**Formalization:**\n",
    "\n",
    "In the formal language of computational theory, let \\( T \\) be the DOM tree with each node \\( n \\) containing a list of attributes \\( A(n) \\) and a text content \\( C(n) \\). Let \\( JSON(n) \\) be the JSON representation of the node \\( n \\). The recursive function to extract data can be described as:\n",
    "\n",
    "\n",
    "JSON(n) = \n",
    "\\begin{cases} \n",
    "\\{ \"type\": A(n), \"content\": C(n), \"children\": \\{ JSON(c) \\,|\\, c \\in \\text{children of } n \\} \\} & \\text{if } n \\text{ has children} \\\\\n",
    "\\{ \"type\": A(n), \"content\": C(n) \\} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "**Python implementation:**\n",
    " \n",
    "In terms of practical implementation, Python's Beautiful Soup library can be particularly effective for this task, allowing for a relatively straightforward traversal of each div element to construct the JSON object.\n",
    "\n",
    "The end result would be a JSON object where each entry corresponds to a 'div' element in the original HTML structure, represented by a dictionary containing the attributes and content of the div, and potentially another dictionary (or list of dictionaries) representing any nested child div elements. This would effectively capture the data within each div while maintaining the hierarchical structure of the original HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listando arqivos em: C:/Users/marco/fioce\n",
      "['.git', '.gitignore', 'assets', 'chromedriver', 'csv', 'doc', 'fig', 'json', 'output', 'scripts', 'source', 'utils', 'xml_zip']\n",
      "Pasta para os xml já existe!\n",
      "Pasta para os CSV já existe!\n",
      "Pasta para os JSON já existe!\n",
      "Pasta para figuras já existe!\n",
      "Pasta para saídas já existe!\n",
      "\n",
      "Caminho da pasta raiz C:/Users/marco/fioce/\n",
      "Caminho arquivos  XML C:/Users/marco/fioce/xml_zip/\n",
      "Caminho arquivos JSON C:/Users/marco/fioce/json/\n",
      "Caminho arquivos  CSV C:/Users/marco/fioce/csv/\n",
      "Caminho para  figuras C:/Users/marco/fioce/fig/\n",
      "Pasta arquivos saídas C:/Users/marco/fioce/output/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:/Users/marco/fioce/xml_zip/',\n",
       " 'C:/Users/marco/fioce/csv/',\n",
       " 'C:/Users/marco/fioce/json/',\n",
       " 'C:/Users/marco/fioce/fig/',\n",
       " 'C:/Users/marco/fioce/',\n",
       " 'C:/Users/marco/fioce/output/')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drives=['C:/Users/','E:/','./home/']\n",
    "pastas=['marcos.aires/', 'marco/']\n",
    "pastasraiz=['kgfioce','fioce']\n",
    "caminho=try_folders(drives,pastas,pastasraiz)\n",
    "\n",
    "preparar_pastas(caminho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando com o servidor do CNPq...\n",
      "C:/Users/marco/fioce/chromedriver/chromedriver.exe\n",
      "1 resultados para ['Antonio Marcos Aires Barbosa']\n",
      "Vínculo encontrado no currículo de nome: Antonio Marcos Aires Barbosa\n",
      "Total de caracteres extraídos:  19613\n",
      "Quantidade extraída de linhas:   1011\n"
     ]
    }
   ],
   "source": [
    "driver = connect_driver(caminho)\n",
    "NOME = ['Antonio Marcos Aires Barbosa']\n",
    "fill_name(driver, delay, NOME)\n",
    "\n",
    "limite=3\n",
    "instituicao = 'Fundação Oswaldo Cruz'\n",
    "unidade     = 'Fiocruz Ceará'\n",
    "termo       = 'Ministerio da Saude'\n",
    "\n",
    "elm_vinculo, np.NaN, np.NaN, np.NaN, driver = find_terms(NOME, instituicao, unidade, termo, driver, delay, limite)\n",
    "soup = open_curriculum(driver,elm_vinculo)\n",
    "\n",
    "caracteres = len(soup.text)\n",
    "linhas = len(soup.text.split('\\n'))\n",
    "print(f'Total de caracteres extraídos: {caracteres:6d}')\n",
    "print(f'Quantidade extraída de linhas: {linhas:6d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 2: Explorar estrutura e dados do DOM</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(soup): \n",
    "    # Extração de todas as classes no documento, armazenadas em uma lista\n",
    "    all_classes = [value for element in soup.find_all(class_=True) for value in element[\"class\"]]\n",
    "    \n",
    "    # Contagem de ocorrências de cada classe\n",
    "    class_count = Counter(all_classes)\n",
    "    \n",
    "    # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "    class_dict = dict(class_count)\n",
    "    \n",
    "    return class_dict\n",
    "\n",
    "def list_divs(soup):\n",
    "    div_elements = soup.find_all('div')\n",
    "    unique_classes = set()\n",
    "\n",
    "    for div in div_elements[1:]:\n",
    "        div_id = div.get('id', 'N/A')\n",
    "        class_list = div.get('class')\n",
    "        print(f'{div_id} {class_list}')\n",
    "        \n",
    "        if class_list:\n",
    "            for i in class_list:\n",
    "                cont = div.find('div',{'class': i})\n",
    "                try:\n",
    "                    text = cont.text.strip().replace('/n/n/n','/n').replace('/n/n','/n')\n",
    "                    print(f'  {text}')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            print('-'*50)\n",
    "            unique_classes.update(class_list)\n",
    "    \n",
    "    return unique_classes\n",
    "\n",
    "def list_divs_with_hierarchy(tag, indent_level=0):\n",
    "    indent = \" \" * indent_level * 4  # Four spaces for each level of indentation\n",
    "    div_elements = tag.find_all('div', recursive=False)  # Find direct children only\n",
    "    unique_classes = set()\n",
    "\n",
    "#     print(f\"{indent}Inspecting level {indent_level}, found {len(div_elements)} divs.\")\n",
    "\n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        if class_list:\n",
    "            unique_classes.update(class_list)\n",
    "            print(f\"{indent} {', '.join(class_list)}\")\n",
    "        else:\n",
    "            print(f\"{indent} None\")\n",
    "\n",
    "        # Recursive call to explore the children of this div\n",
    "        list_divs_with_hierarchy(div, indent_level + 1)\n",
    "\n",
    "    if indent_level == 0:  # Print unique classes only once, at the end of the initial call\n",
    "        print(\"\\nUnique classes:\")\n",
    "        for i in unique_classes:\n",
    "            print(f'    {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de classes únicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize BeautifulSoup with a sample HTML content\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 1,\n",
       " 'min-width': 2,\n",
       " 'header': 1,\n",
       " 'header-content': 1,\n",
       " 'max-width': 2,\n",
       " 'identity': 1,\n",
       " 'center': 4,\n",
       " 'combo': 1,\n",
       " 'escondido': 1,\n",
       " 'menu-header': 1,\n",
       " 'tituloFlow': 1,\n",
       " 'linksFlow': 1,\n",
       " 'btn-br': 1,\n",
       " 'bt-menu-header': 6,\n",
       " 'titBottom': 7,\n",
       " 'icons-top': 6,\n",
       " 'icons-top-print': 1,\n",
       " 'fontMais': 1,\n",
       " 'icons-top-fontMais': 1,\n",
       " 'fontMenos': 2,\n",
       " 'icons-top-fontMenos': 1,\n",
       " 'contraste': 1,\n",
       " 'icons-top-contraste-en': 1,\n",
       " 'icons-top-xml': 1,\n",
       " 'icons-top-help': 1,\n",
       " 'logo': 1,\n",
       " 'navigation-wrapper': 1,\n",
       " 'navigation-wrapper-line': 1,\n",
       " 'menuPrincipal': 1,\n",
       " 'menuCtl': 1,\n",
       " 'menuAcesso': 1,\n",
       " 'menucent': 1,\n",
       " 'acessibilidade': 10,\n",
       " 'separador': 9,\n",
       " 'content-wrapper': 1,\n",
       " 'main-content': 1,\n",
       " 'layout-cell': 170,\n",
       " 'layout-cell-12': 18,\n",
       " 'layout-cell-pad-main': 1,\n",
       " 'foto': 1,\n",
       " 'infpessoa': 1,\n",
       " 'nome': 1,\n",
       " 'informacoes-autor': 1,\n",
       " 'img_link': 1,\n",
       " 'icone-informacao-autor': 3,\n",
       " 'img_identy': 1,\n",
       " 'img_cert': 1,\n",
       " 'clear': 157,\n",
       " 'title-wrapper': 16,\n",
       " 'ui-hidden': 1,\n",
       " 'separator': 15,\n",
       " 'resumo': 1,\n",
       " 'texto': 1,\n",
       " 'data-cell': 17,\n",
       " 'layout-cell-3': 57,\n",
       " 'text-align-right': 152,\n",
       " 'layout-cell-pad-5': 152,\n",
       " 'layout-cell-9': 57,\n",
       " 'alinhamento-texto-img': 1,\n",
       " 'icons-aviso': 1,\n",
       " 'icons-aviso-help': 1,\n",
       " 'icon-cor': 1,\n",
       " 'icone-lattes': 1,\n",
       " 'ajaxCAPES': 3,\n",
       " 'inst_back': 10,\n",
       " 'subtit-1': 7,\n",
       " 'cita-artigos': 5,\n",
       " 'layout-cell-6': 1,\n",
       " 'sub_tit_form': 1,\n",
       " 'input-text': 1,\n",
       " 'input-login-select': 1,\n",
       " 'artigo-completo': 2,\n",
       " 'layout-cell-1': 19,\n",
       " 'layout-cell-11': 19,\n",
       " 'informacao-artigo': 7,\n",
       " 'icone-producao': 2,\n",
       " 'icone-doi': 2,\n",
       " 'ajaxJCR': 1,\n",
       " 'jcrTip': 1,\n",
       " 'citado': 2,\n",
       " 'tooltip': 3,\n",
       " 'rodape-cv': 1,\n",
       " 'control-bar-wrapper': 1,\n",
       " 'button': 2,\n",
       " 'mini-ico': 2,\n",
       " 'mini-ico-down': 1,\n",
       " 'mini-ico-list': 1,\n",
       " 'to-top-bar': 1,\n",
       " 'topo-width': 1,\n",
       " 'id-orgao': 1,\n",
       " 'megamenu': 10,\n",
       " 'column': 12,\n",
       " 'subtitMenu': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_classes(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes_unicas = list_divs(soup)\n",
    "# classes_unicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': ['page', 'min-width']}\n",
      "{'class': ['header']}\n",
      "{'class': ['header-content', 'max-width']}\n",
      "{'class': ['identity', 'center']}\n",
      "{'class': ['combo']}\n",
      "{'class': ['menu-header'], 'style': 'position: absolute; top: 15px;'}\n",
      "{'class': ['logo']}\n",
      "{'class': ['navigation-wrapper']}\n",
      "{'class': ['center', 'navigation-wrapper-line']}\n",
      "{'class': ['menuPrincipal']}\n",
      "{'class': ['menuCtl', 'menuAcesso']}\n",
      "{'class': ['menucent']}\n",
      "{'class': ['content-wrapper']}\n",
      "{'class': ['main-content', 'max-width', 'min-width', 'center']}\n",
      "{'class': ['layout-cell', 'layout-cell-12']}\n",
      "{'class': ['layout-cell-pad-main']}\n",
      "{'class': ['infpessoa']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['inst_back']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['inst_back']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['inst_back']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['inst_back']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['inst_back']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['inst_back']}\n",
      "{'id': 'artigos-completos'}\n",
      "{'class': ['cita-artigos']}\n",
      "{'class': ['layout-cell-6']}\n",
      "{'class': ['sub_tit_form']}\n",
      "{'class': ['artigo-completo']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['citado'],\n",
      " 'cvuri': '/buscatextual/servletcitacoes?doi=10.3390/v15091903&issn=19994915&volume=15&issue=&paginaInicial=1903&titulo=Progress '\n",
      "          'on Phage Display Technology: Tailoring Antibodies for Cancer '\n",
      "          'Immunotherapy&sequencial=1&nomePeriodico=Viruses-Basel',\n",
      " 'tooltip': 'Citações a partir de 1996'}\n",
      "{'class': ['artigo-completo']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['citado'],\n",
      " 'cvuri': '/buscatextual/servletcitacoes?doi=10.9771/cp.v12i3.27256&issn=23170026&volume=12&issue=&paginaInicial=505&titulo=Um '\n",
      "          'Panorama do Desempenho em Inovação no Brasil e a Busca por Boas '\n",
      "          'Práticas de Gestão na Transferência de Tecnologia (TT) nas '\n",
      "          'Instituições de Ciência e Tecnologia (ICT) do '\n",
      "          'Brasil&sequencial=2&nomePeriodico=CADERNOS DE PROSPECÇÃO',\n",
      " 'tooltip': 'Citações a partir de 1996'}\n",
      "{'class': ['cita-artigos']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['cita-artigos']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['inst_back']}\n",
      "{'class': ['cita-artigos']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['inst_back']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['inst_back']}\n",
      "{'class': ['cita-artigos']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-11']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['title-wrapper']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['inst_back']}\n",
      "{'class': ['layout-cell', 'layout-cell-12', 'data-cell']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-9']}\n",
      "{'class': ['layout-cell-pad-5']}\n",
      "{'class': ['rodape-cv']}\n",
      "{'class': ['control-bar-wrapper']}\n",
      "{'class': ['to-top-bar'], 'id': 'toTop', 'style': 'display: none;'}\n",
      "{'class': ['id-orgao']}\n",
      "{'_megamenupos': '0pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-dados-gerais',\n",
      " 'style': 'z-index: 1001; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['clear']}\n",
      "{'_megamenupos': '1pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-formacao',\n",
      " 'style': 'z-index: 1002; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['clear']}\n",
      "{'_megamenupos': '2pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-atuacao',\n",
      " 'style': 'z-index: 1003; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['clear']}\n",
      "{'_megamenupos': '3pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-projetos',\n",
      " 'style': 'z-index: 1004; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['clear']}\n",
      "{'_megamenupos': '4pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-producoes',\n",
      " 'style': 'z-index: 1005; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['subtitMenu']}\n",
      "{'class': ['column']}\n",
      "{'class': ['subtitMenu']}\n",
      "{'class': ['column']}\n",
      "{'class': ['subtitMenu']}\n",
      "{'class': ['clear']}\n",
      "{'_megamenupos': '5pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-potencialInovacao',\n",
      " 'style': 'z-index: 1006; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['clear']}\n",
      "{'_megamenupos': '6pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-eventos',\n",
      " 'style': 'z-index: 1007; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['clear']}\n",
      "{'_megamenupos': '7pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-orientacoes',\n",
      " 'style': 'z-index: 1008; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['clear']}\n",
      "{'_megamenupos': '8pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-bancas',\n",
      " 'style': 'z-index: 1009; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['clear']}\n",
      "{'_megamenupos': '9pos',\n",
      " 'class': ['megamenu'],\n",
      " 'id': 'menu-informacoes-complementares',\n",
      " 'style': 'z-index: 1010; display: none;'}\n",
      "{'class': ['column']}\n",
      "{'class': ['clear']}\n"
     ]
    }
   ],
   "source": [
    "# Achar classe específica (dicionário class) em elemento (div)\n",
    "from pprint import pprint\n",
    "\n",
    "# query0 = soup.select_one('div', {'class':'layout-cell-1'}).attrs\n",
    "keys_dicts = soup.select('div', {'class':'text-align-right'})\n",
    "for i in keys_dicts:\n",
    "    pprint(i.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right', 'subtit-1']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-1', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n",
      "{'class': ['layout-cell', 'layout-cell-3', 'text-align-right']}\n",
      "{'class': ['layout-cell-pad-5', 'text-align-right']}\n"
     ]
    }
   ],
   "source": [
    "# Achar classe específica (dicionário class) em elemento (div)\n",
    "from pprint import pprint\n",
    "\n",
    "# query0 = soup.select_one('div', {'class':'layout-cell-1'}).attrs\n",
    "keys_dicts = soup.find_all('div', {'class':'text-align-right'})\n",
    "for i in keys_dicts:\n",
    "    pprint(i.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar a hierarquia com identação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " page, min-width\n",
      "     header\n",
      "         header-content, max-width\n",
      "             identity, center\n",
      "                 combo\n",
      "                 menu-header\n",
      "                 logo\n",
      "     navigation-wrapper\n",
      "         center, navigation-wrapper-line\n",
      "             menuPrincipal\n",
      "                 menuCtl, menuAcesso\n",
      "                     menucent\n",
      "     content-wrapper\n",
      "         main-content, max-width, min-width, center\n",
      "             layout-cell, layout-cell-12\n",
      "                 layout-cell-pad-main\n",
      "                     infpessoa\n",
      "                     title-wrapper\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             inst_back\n",
      "                             layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             inst_back\n",
      "                             layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             inst_back\n",
      "                             layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             inst_back\n",
      "                             layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             inst_back\n",
      "                             layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-3, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-9\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             inst_back\n",
      "                             None\n",
      "                                 cita-artigos\n",
      "                                 layout-cell-6\n",
      "                                     sub_tit_form\n",
      "                                 artigo-completo\n",
      "                                     layout-cell, layout-cell-1, text-align-right\n",
      "                                         layout-cell-pad-5, text-align-right\n",
      "                                     layout-cell, layout-cell-11\n",
      "                                         layout-cell-pad-5\n",
      "                                             citado\n",
      "                                 artigo-completo\n",
      "                                     layout-cell, layout-cell-1, text-align-right\n",
      "                                         layout-cell-pad-5, text-align-right\n",
      "                                     layout-cell, layout-cell-11\n",
      "                                         layout-cell-pad-5\n",
      "                                             citado\n",
      "                             cita-artigos\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             cita-artigos\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             inst_back\n",
      "                             cita-artigos\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             layout-cell, layout-cell-12, data-cell\n",
      "                                 inst_back\n",
      "                                 layout-cell, layout-cell-1, text-align-right\n",
      "                                     layout-cell-pad-5, text-align-right\n",
      "                                 layout-cell, layout-cell-11\n",
      "                                     layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             inst_back\n",
      "                             cita-artigos\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                             layout-cell, layout-cell-1, text-align-right\n",
      "                                 layout-cell-pad-5, text-align-right\n",
      "                             layout-cell, layout-cell-11\n",
      "                                 layout-cell-pad-5\n",
      "                     title-wrapper\n",
      "                         layout-cell, layout-cell-12, data-cell\n",
      "                             inst_back\n",
      "                             layout-cell, layout-cell-12, data-cell\n",
      "                                 layout-cell, layout-cell-3, text-align-right\n",
      "                                     layout-cell-pad-5, text-align-right\n",
      "                                 layout-cell, layout-cell-9\n",
      "                                     layout-cell-pad-5\n",
      "                                 layout-cell, layout-cell-3, text-align-right\n",
      "                                     layout-cell-pad-5, text-align-right\n",
      "                                 layout-cell, layout-cell-9\n",
      "                                     layout-cell-pad-5\n",
      "                     rodape-cv\n",
      "                     control-bar-wrapper\n",
      " to-top-bar\n",
      " megamenu\n",
      "     column\n",
      "     clear\n",
      " megamenu\n",
      "     column\n",
      "     clear\n",
      " megamenu\n",
      "     column\n",
      "     clear\n",
      " megamenu\n",
      "     column\n",
      "     clear\n",
      " megamenu\n",
      "     column\n",
      "         subtitMenu\n",
      "     column\n",
      "         subtitMenu\n",
      "     column\n",
      "         subtitMenu\n",
      "     clear\n",
      " megamenu\n",
      "     column\n",
      "     clear\n",
      " megamenu\n",
      "     column\n",
      "     clear\n",
      " megamenu\n",
      "     column\n",
      "     clear\n",
      " megamenu\n",
      "     column\n",
      "     clear\n",
      " megamenu\n",
      "     column\n",
      "     clear\n",
      "\n",
      "Unique classes:\n",
      "    megamenu\n",
      "    to-top-bar\n",
      "    min-width\n",
      "    page\n"
     ]
    }
   ],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "\n",
    "# Then try calling list_divs_with_hierarchy on the <body> tag\n",
    "list_divs_with_hierarchy(body_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicial_element = soup.select_one('div', {'class':'title-wrapper'})\n",
    "inicial_element = soup.find('div', {'class':'layout-cell-pad-main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " infpessoa\n",
      " title-wrapper\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         inst_back\n",
      "         layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         inst_back\n",
      "         layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         inst_back\n",
      "         layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         inst_back\n",
      "         layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         inst_back\n",
      "         layout-cell, layout-cell-3, text-align-right, subtit-1\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-3, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-9\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         inst_back\n",
      "         None\n",
      "             cita-artigos\n",
      "             layout-cell-6\n",
      "                 sub_tit_form\n",
      "             artigo-completo\n",
      "                 layout-cell, layout-cell-1, text-align-right\n",
      "                     layout-cell-pad-5, text-align-right\n",
      "                 layout-cell, layout-cell-11\n",
      "                     layout-cell-pad-5\n",
      "                         citado\n",
      "             artigo-completo\n",
      "                 layout-cell, layout-cell-1, text-align-right\n",
      "                     layout-cell-pad-5, text-align-right\n",
      "                 layout-cell, layout-cell-11\n",
      "                     layout-cell-pad-5\n",
      "                         citado\n",
      "         cita-artigos\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         cita-artigos\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         inst_back\n",
      "         cita-artigos\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         layout-cell, layout-cell-12, data-cell\n",
      "             inst_back\n",
      "             layout-cell, layout-cell-1, text-align-right\n",
      "                 layout-cell-pad-5, text-align-right\n",
      "             layout-cell, layout-cell-11\n",
      "                 layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         inst_back\n",
      "         cita-artigos\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      "         layout-cell, layout-cell-1, text-align-right\n",
      "             layout-cell-pad-5, text-align-right\n",
      "         layout-cell, layout-cell-11\n",
      "             layout-cell-pad-5\n",
      " title-wrapper\n",
      "     layout-cell, layout-cell-12, data-cell\n",
      "         inst_back\n",
      "         layout-cell, layout-cell-12, data-cell\n",
      "             layout-cell, layout-cell-3, text-align-right\n",
      "                 layout-cell-pad-5, text-align-right\n",
      "             layout-cell, layout-cell-9\n",
      "                 layout-cell-pad-5\n",
      "             layout-cell, layout-cell-3, text-align-right\n",
      "                 layout-cell-pad-5, text-align-right\n",
      "             layout-cell, layout-cell-9\n",
      "                 layout-cell-pad-5\n",
      " rodape-cv\n",
      " control-bar-wrapper\n",
      "\n",
      "Unique classes:\n",
      "    control-bar-wrapper\n",
      "    title-wrapper\n",
      "    rodape-cv\n",
      "    infpessoa\n"
     ]
    }
   ],
   "source": [
    "list_divs_with_hierarchy(inicial_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar atributos de elemento e classe especificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alt': 'Currículo Lattes',\n",
      " 'src': 'images/titulo-sistema.png',\n",
      " 'title': 'Currículo Lattes'}\n"
     ]
    }
   ],
   "source": [
    "query1 = soup.select_one('img', {'class':'foto'}).attrs\n",
    "pprint(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Extrair dados e seções específicas\n",
    "\n",
    "- extrair_dados(soup, verbose=False)\n",
    "- extrair_artigos(soup, verbose=False)\n",
    "- extrair_orientacoes(soup, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_dados(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node name: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node Name\": node_name, \"Properties\": {}}\n",
    "    \n",
    "    # Step 3: Traverse to Find Sections\n",
    "    json_data = extrair_wraper(soup, json_data)\n",
    "\n",
    "    # Step 4: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    producoes = []\n",
    "    for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        artigo_dict['ano']     = ano\n",
    "        artigo_dict['autores'] = autores\n",
    "        artigo_dict['revista'] = revista\n",
    "        artigo_dict['titulo']  = titulo\n",
    "        artigo_dict['jcr']     = jcr\n",
    "        artigo_dict['doi']     = doi\n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = producoes\n",
    "\n",
    "    projdevtec=[]\n",
    "    for projdevtec_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        projdevtec_dict = {}\n",
    "        projdevtec.append(projdevtec_dict)\n",
    "        json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projdevtec\n",
    "\n",
    "    # projpesq=[]\n",
    "    # for projpesq_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "    #     projpesq_dict = {}\n",
    "    \n",
    "    \n",
    "    #     projpesq.append(projdevtec_dict)\n",
    "    #     json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projpesq\n",
    "\n",
    "    orientacoes=[]\n",
    "    for orientacoes_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        orientacoes_dict = {}   \n",
    "        orientacoes.append(orientacoes_dict)\n",
    "        json_data[\"Properties\"][\"Orientações\"] = orientacoes\n",
    "\n",
    "    bancas=[]\n",
    "    for bancas_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        bancas_dict = {}\n",
    "        bancas.append(bancas_dict)\n",
    "        json_data[\"Properties\"][\"Bancas\"] = bancas\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_artigos(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node name: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node Name\": node_name, \"Properties\": {}}\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    producoes = []\n",
    "    for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        \n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        str_autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        autores = split_authors(str_autores)\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        artigo_dict['ano']     = ano\n",
    "        artigo_dict['autores'] = autores\n",
    "        artigo_dict['revista'] = revista\n",
    "        artigo_dict['titulo']  = titulo\n",
    "        artigo_dict['jcr']     = jcr\n",
    "        artigo_dict['doi']     = doi\n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = producoes\n",
    "\n",
    "    # projdevtec=[]\n",
    "    # for projdevtec_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "    #     projdevtec_dict = {}\n",
    "    \n",
    "    \n",
    "    #     projdevtec.append(projdevtec_dict)\n",
    "    #     json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projdevtec\n",
    "\n",
    "    # projpesq=[]\n",
    "    # for projpesq_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "    #     projpesq_dict = {}\n",
    "    \n",
    "    \n",
    "    #     projpesq.append(projdevtec_dict)\n",
    "    #     json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projpesq\n",
    "\n",
    "    # orientacoes=[]\n",
    "    # for orientacoes_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "    #     orientacoes_dict = {}\n",
    "    \n",
    "    \n",
    "    #     bancas.append(orientacoes_dict)\n",
    "    #     json_data[\"Properties\"][\"Orientações\"] = orientacoes\n",
    "\n",
    "    # bancas=[]\n",
    "    # for bancas_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "    #     bancas_dict = {}\n",
    "    \n",
    "    \n",
    "    #     bancas.append(bancas_dict)\n",
    "    #     json_data[\"Properties\"][\"Bancas\"] = bancas\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_resume(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node name: {node_name}\")   \n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node Name\": node_name, \"Properties\": {}}\n",
    "\n",
    "    paragraph = []\n",
    "    result_dict = {}\n",
    "    parag_elements = soup.find_all('p')\n",
    "\n",
    "    for elem in parag_elements:\n",
    "        # Retrieve the class attribute, returning None if not found\n",
    "        class_name = elem.get('class', [None])[0]\n",
    "        \n",
    "        # Retrieve the text content of the element\n",
    "        text_content = elem.text\n",
    "        \n",
    "        # Update the dictionary\n",
    "        if class_name:  # Only update if class_name is not None\n",
    "            result_dict[class_name] = text_content\n",
    "\n",
    "        paragraph.append(result_dict)\n",
    "        json_data[\"Properties\"]['Resumo'] = paragraph\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Node Name': 'Antonio Marcos Aires Barbosa',\n",
       " 'Properties': {'Resumo': [{'resumo': 'Doutorando em Ciência de Dados e Inteligência Artificial do Programa de Informática Aplicada da Universidade de Fortaleza - UNIFOR, é mestre em Propriedade Intelecutal e Transferência de Tecnologia para Inovação (2019), graduado em Engenharia de Produção Mecânica pela Universidade Federal do Ceará (2006). Com experiência profissional nas áreas de Análise de Negócio, Business Intelligence para Gestão, Análise de Dados, com maior ênfase na Gestão da Inovação, Gestão por Processos, Gestão de Projetos, e Estratégia. Obteve certificação Certified Business Process Professional (CBPP) em 2010 junto à ABPMP Brasil. Na iniciativa pública, é servidor de carreira da Fundação Oswaldo Cruz desde 2011, atuando atualmente na Fiocruz no Ceará, organização âncora do Polo Industrial e Tecnológico da Saúde do Ceará. Realiza atividades relativas à gestão da inovação, propriedade intelectual, transferência de tecnologia e inovação, na Coordenação de Pesquisa da Fiocruz Ceará, tendo também atuado por 08 anos no Núcleo de Inovação Tecnológica da Fiocruz Ceará. Mantém interesse acadêmico e profissional na aplicação da ciência de dados à Prospecção Tecnológica, visando o desenvolvimento, proteção e transferência de tecnologias. Tem como missão pessoal estudar, propor e implementar modelos e processos de negócio, com desempenho ótimo em inteligência competitiva, baeada na prospecção tecnológica estratégica, para gerar melhores e maiores resultados no ecossistema local e nacional de PDI. (Texto informado pelo autor)\\n'}]}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_resume(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "Found in parent element: div\n",
      "Texts of all child elements: ['Orientações e supervisões concluídas']\n"
     ]
    }
   ],
   "source": [
    "text_contents = []\n",
    "elements_list = soup.find_all('b')\n",
    "print(len(elements_list))\n",
    "for i in elements_list:\n",
    "    # Check if text content matches the targeted string\n",
    "    if i.text == 'Orientações e supervisões concluídas':\n",
    "        # Retrieve the parent element\n",
    "        parent_element = i.find_parent()\n",
    "        \n",
    "        # Retrieve the name of the parent element\n",
    "        parent_name = parent_element.name if parent_element else 'Unknown'\n",
    "        \n",
    "        # Retrieve and store the text contents of all child elements of the parent\n",
    "        for child in parent_element.find_all(True):\n",
    "            text_contents.append(child.text)\n",
    "        \n",
    "        print(f\"Found in parent element: {parent_name}\")\n",
    "        print(\"Texts of all child elements:\", text_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome\n",
      "Nome em citações bibliográficas\n",
      "Lattes iD\n",
      "Orcid iD \n",
      "Endereço Profissional\n",
      "2020\n",
      "2017 - 2019\n",
      "2013 - 2013\n",
      "2009 - 2010\n",
      "2011 - 2011\n",
      "2008 - 2008\n",
      "2007 interrompida\n",
      "2000 - 2006\n",
      "1991 - 1995\n",
      "2019 - 2019\n",
      "2019 - 2019\n",
      "2016 - 2016\n",
      "2012 - 2012\n",
      "2011 - 2011\n",
      "2010 - 2010\n",
      "2010 - 2010\n",
      "2008 - 2009\n",
      "Fundação Oswaldo Cruz, FIOCRUZ, Brasil.\n",
      "\t\t\n",
      "\t\n",
      "Vínculo institucional\n",
      "2011 - Atual\n",
      "Outras informações\n",
      "Atividades\n",
      "01/2014 - 12/2022\n",
      "\n",
      "01/2011 - 12/2014\n",
      "\n",
      "Centro Universitário Christus, UNICHRISTUS, Brasil.\n",
      "\t\t\n",
      "\t\n",
      "Vínculo institucional\n",
      "2019 - 2023\n",
      "Instituto Federal do Ceará, IFCE, Brasil.\n",
      "\t\t\n",
      "\t\n",
      "Vínculo institucional\n",
      "2017 - 2019\n",
      "Atividades\n",
      "01/2017 - 12/2019\n",
      "\n",
      "Ministério da Saúde, MS, Brasil.\n",
      "\t\t\n",
      "\t\n",
      "Vínculo institucional\n",
      "2009 - 2011\n",
      "Companhia Energética do Ceará, COELCE, Brasil.\n",
      "\t\t\n",
      "\t\n",
      "Vínculo institucional\n",
      "1996 - 2009\n",
      "1. \n",
      "2. \n",
      "3. \n",
      "\n",
      "2019 - Atual\n",
      "\n",
      "1. \n",
      "2. \n",
      "3. \n",
      "4. \n",
      "5. \n",
      "6. \n",
      "Inglês\n",
      "Espanhol\n",
      "Produção bibliográfica\n",
      "Artigos completos publicados em periódicos\n",
      "1. \n",
      "BARBOSA, A. M. A\n",
      "2. \n",
      "BARBOSA, A. M. A.\n",
      "Livros publicados/organizados ou edições\n",
      "1. \n",
      "BARBOSA, A. M. A.\n",
      "Trabalhos completos publicados em anais de congressos\n",
      "1. \n",
      "BARBOSA, A. M. A.\n",
      "2. \n",
      "BARBOSA, A. M. A.\n",
      "3. \n",
      "BARBOSA, A. M. A.\n",
      "4. \n",
      "BARBOSA, A. M. A.\n",
      "5. \n",
      "BARBOSA, A. M. A.\n",
      "6. \n",
      "BARBOSA, A. M. A.\n",
      "7. \n",
      "BARBOSA, A. M. A.\n",
      "8. \n",
      "BARBOSA, A. M. A.\n",
      "9. \n",
      "BARBOSA, A. M. A.\n",
      "Participação em bancas de trabalhos de conclusão\n",
      "Trabalhos de conclusão de curso de graduação\n",
      "1. \n",
      "BARBOSA, A. M. A.\n",
      "2. \n",
      "BARBOSA, A. M. A.\n",
      "3. \n",
      "BARBOSA, A. M. A.\n",
      "4. \n",
      "BARBOSA, A. M. A.\n",
      "Participação em eventos, congressos, exposições e feiras\n",
      "1. \n",
      "Orientações e supervisões concluídas\n",
      "Trabalho de conclusão de curso de graduação\n",
      "1. \n",
      "2. \n",
      "Projeto de desenvolvimento tecnológico\n",
      "2019 - Atual\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in elements_list:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_orientacoes(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node name: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node Name\": node_name, \"Properties\": {}}\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    general_div = soup.find('div', {'class': 'Orientações e supervisões concluídas'})\n",
    "    if verbose:\n",
    "        if general_div:\n",
    "            print(f\"Qte.Divs: {len(general_div)}\")\n",
    "        else:\n",
    "            print('Nenhuma div encontrada com essa hierarquia')\n",
    "            return\n",
    "    orientacoes=[]\n",
    "    for selected_div in general_div.find_all('div', {'class': 'identity'}):\n",
    "        orientacoes_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = selected_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = selected_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = selected_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = selected_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = selected_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        \n",
    "        dados   = selected_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        str_autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        autores = split_authors(str_autores)\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        orientacoes_dict['ano']     = ano\n",
    "        orientacoes_dict['autores'] = autores\n",
    "        orientacoes_dict['revista'] = revista\n",
    "        orientacoes_dict['titulo']  = titulo\n",
    "        orientacoes_dict['jcr']     = jcr\n",
    "        orientacoes_dict['doi']     = doi\n",
    "        orientacoes.append(orientacoes_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = orientacoes\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node name: Antonio Marcos Aires Barbosa\n",
      "Nenhuma div encontrada com essa hierarquia\n"
     ]
    }
   ],
   "source": [
    "extrair_orientacoes(soup, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Extrair do DOM Dados Identificação\n",
    "\n",
    "- parse_personinfo(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from py2neo import Graph, Node\n",
    "\n",
    "def parse_personinfo(soup):\n",
    "    # Localizar o elemento de link que contém o título do currículo\n",
    "    link_element = soup.find(\"a\", {\"href\": lambda x: x and \"abreDetalhe\" in x})\n",
    "\n",
    "    # Extrair o texto do link para usar como título do nó\n",
    "    node_title = link_element.text if link_element else \"Unknown\"\n",
    "\n",
    "    # Localizar o elemento div contendo as propriedades\n",
    "    properties_div = soup.find(\"div\", {\"class\": \"resultado\"})\n",
    "\n",
    "    # Localizar o elemento li que contém as informações\n",
    "    li_element = soup.find(\"li\")\n",
    "\n",
    "    # Inicializar um dicionário para armazenar as propriedades\n",
    "    properties = {}\n",
    "\n",
    "    # Extrair e armazenar as propriedades relevantes\n",
    "    if properties_div:\n",
    "        properties['Nacionalidade'] = 'Brasil'\n",
    "        properties['Cargo'] = properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}).text if properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}) else 'Desconhecido'\n",
    "        properties['Titulação'] = properties_div.contents[-4] if len(properties_div.contents) > 4 else 'Desconhecido'\n",
    "\n",
    "        # Extração de nome e identificador único\n",
    "        a_element = li_element.find(\"a\")\n",
    "        properties[\"Nome\"] = a_element.text\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Títulos Acadêmicos e outras informações\n",
    "\n",
    "\n",
    "\n",
    "    return node_title, properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 3: Extrair Currículos Lattes</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de extração do Lattes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  #seletorcss=\"div.paginacao:nth-child(4) > a:nth-child(2)\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas\n",
    "\n",
    "def achar_busca(browser, delay):\n",
    "    '''\n",
    "    Função para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = browser.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)\n",
    "\n",
    "def preencher_busca(browser, delay, NOME):\n",
    "    '''\n",
    "    Função para passar o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)\n",
    "\n",
    "def definir_filtros(browser, delay, mestres=True, assunto=False):\n",
    "    '''\n",
    "    Clica nos check-boxes para definir os filtros de buscas\n",
    "    Para buscar por Assuntos usar parâmetro True, caso omitido fará busca por Nome por default\n",
    "    '''\n",
    "    from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import traceback   \n",
    "    \n",
    "    ## Aguardar carregar e clicar em checkbox de Assunto\n",
    "    try:\n",
    "        if mestres == True:\n",
    "            limite=2\n",
    "            ## Aguardar opção dropdown ser carregada e clicar em sua checkbox\n",
    "            css_buscar_demais = '#buscarDemais'\n",
    "            checkbox_buscar_demais = browser.find_element(By.CSS_SELECTOR, css_buscar_demais)\n",
    "            \n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, css_buscar_demais))),\n",
    "                   wait_ms=150,\n",
    "                   limit=limite, \n",
    "                   on_exhaust=(f'Problema clicar em {checkbox_buscar_demais}, {limite} tentativas sem sucesso.'))\n",
    "            \n",
    "            time.sleep(1)\n",
    "            checkbox_buscar_demais.click()\n",
    "            print(f'Clique efetuado em {checkbox_buscar_demais}')\n",
    "\n",
    "        # css_estrangeiros  = '#buscarEstrangeiros'\n",
    "\n",
    "        if assunto == True:\n",
    "            ## Implementar número de retentativas para casos de conexão muito instável\n",
    "            limite=5\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#buscaAssunto\"))).click(),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "        \n",
    "            ## Aguardar opção Atuação Profissional ser carregada e clicar em sua checkbox\n",
    "            xpath_atuacaoprofissional = \".//*[contains(text(), 'Atuação profissional')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_atuacaoprofissional))).click()\n",
    "\n",
    "            ## Aguardar opção Ciências da Saúde ser carregada e clicar em sua checkbox\n",
    "            xpath_cienciassaude = \".//*[contains(text(), 'Ciências da Saúde')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_cienciassaude))).click()\n",
    "            #browser.find_element_by_xpath(xpath_cienciassaude).click()        \n",
    "\n",
    "            ## Aguardar opção Enfermagem ser carregada e clicar em sua checkbox\n",
    "            xpath_enfermagem    = \".//*[contains(text(), 'Enfermagem')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_enfermagem))).click()\n",
    "            #browser.find_element_by_xpath(xpath_enfermagem).click()\n",
    "            aplicar_link  = browser.find_element(By.LINK_TEXT, 'Aplicar')\n",
    "            aplicar_link.click()\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f'Erro na função definir_filtros()')\n",
    "        print(e)\n",
    "        # traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        # print(e, traceback_str) \n",
    "\n",
    "def procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(browser)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            ## TO-DO inserir a crítica para o 'Stale file handle'\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(browser)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(browser)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(browser, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', browser\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, browser\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # browser.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = browser.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', browser\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, browser\n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, browser\n",
    "\n",
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    title = soup.title.string if soup.title else \"Unknown\"\n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "\n",
    "\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    return header_node\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_data[key] = value\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that contains the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "\n",
    "\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de extração de lista de currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_lista(lista, mestres=True, assunto=False):\n",
    "    sucesso=[]\n",
    "    falhas=[]\n",
    "    duvidas=[]\n",
    "    tipo_erro=[]\n",
    "    curriculos=[]\n",
    "    rotulos=[]\n",
    "    conteudos=[]\n",
    "\n",
    "    delay=10\n",
    "    limite=3\n",
    "    instituicao = 'Fundação Oswaldo Cruz'\n",
    "    unidade     = 'Fiocruz Ceará'\n",
    "    termo       = 'Ministerio da Saude'\n",
    "\n",
    "    t0 = time.time()\n",
    "    browser = connect_driver(caminho)\n",
    "    for NOME in lista:\n",
    "        try:\n",
    "            preencher_busca(browser, delay, NOME)\n",
    "            elemento_achado, nome_falha, duvida, erro, browser = procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite)\n",
    "            link_nome     = achar_busca(browser, delay)\n",
    "            window_before = browser.current_window_handle\n",
    "            \n",
    "            if str(elemento_achado) == 'nan':\n",
    "                print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "                falhas.append(nome_falha)\n",
    "                duvidas.append(duvida)\n",
    "                tipo_erro.append(erro)\n",
    "                # print(nome_falha)\n",
    "                # print(erro)\n",
    "                # clear_output(wait=True)\n",
    "                raise Exception\n",
    "            print('Vínculo encontrado no currículo de nome:',elemento_achado.text)\n",
    "\n",
    "            ## Clicar no botão abrir currículo e mudar de aba\n",
    "            try:\n",
    "                ## Aguarda, encontra, clica em buscar nome\n",
    "                link_nome    = achar_busca(browser, delay)\n",
    "                nome_buscado = []\n",
    "                nome_achado  = []\n",
    "                nome_buscado.append(NOME)\n",
    "                \n",
    "                if link_nome.text == None:\n",
    "                    xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "                    # 'Stale file handle'\n",
    "                    print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "                    retry(WebDriverWait(browser, delay).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "                try:\n",
    "                    ActionChains(browser).click(link_nome).perform()\n",
    "                    nome_achado.append(link_nome.text)\n",
    "                except:\n",
    "                    print(f'Currículo não encontrado para: {NOME}.')\n",
    "                    return\n",
    "                \n",
    "                retry(WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "                \n",
    "                # Clicar botão para abrir o currículo\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                time.sleep(0.2)\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "\n",
    "                # Pega o código fonte da página\n",
    "                page_source = browser.page_source\n",
    "\n",
    "                # Usa BeautifulSoup para analisar\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                # Extração e Persistência do cabeçalho\n",
    "                dict_header = parse_header(soup)\n",
    "                header_node = persist_to_neo4j(dict_header)\n",
    "                \n",
    "                # Extração e Persistência dos elementos H1\n",
    "                graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "                parse_h1_elements(soup, header_node, graph)\n",
    "                graph, cv_node, properties = parse_parsoninfo(soup)                \n",
    "\n",
    "            except Exception as e:\n",
    "                print('Erro',e)\n",
    "                print('Tentando nova requisição ao servidor')\n",
    "                time.sleep(1)\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "                time.sleep(1)\n",
    "\n",
    "            sucesso.append(NOME)\n",
    "\n",
    "        except:\n",
    "            print(f'Currículo não encontrado: {NOME}')\n",
    "            browser.back()\n",
    "            continue\n",
    "\n",
    "    df_dados =pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(curriculos),\n",
    "        'ROTULOS': pd.Series(rotulos),\n",
    "        'CONTEUDOS': pd.Series(conteudos),\n",
    "            })\n",
    "    \n",
    "    t6=time.time()\n",
    "    print('='*95)\n",
    "    # print(f' {len(sucesso)} currículos extraídos com sucesso')\n",
    "    print(f' Tempo total para extrair {len(conteudos)} seções dos currículos: {tempo(t0,t6)}')\n",
    "    # print('='*95)\n",
    "    browser.quit()\n",
    "    \n",
    "    return df_dados, sucesso  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 4: Funções montar dicionários</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_data[key] = value\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that contains the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        data_dict = {}\n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "\n",
    "            higher_order_key = None  # Inicializa variável para armazenar a chave de ordem superior\n",
    "            data_list = []  # Inicialize lista para armazenar entradas de índices de dataframe\n",
    "            \n",
    "            parag_elements = parent_div.find_all('p')\n",
    "            if parag_elements:\n",
    "                for idx, elem in enumerate(parag_elements):\n",
    "                    class_name = elem.get('class', [None])[0]  # Assumes only one class; otherwise, join them into a single string\n",
    "                    higher_order_key = class_name\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "                    data_entry = {'rotulos': class_name, 'conteudos': elem.text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "\n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                kdict_elements = cell.find_all('b')\n",
    "                # print(len(kdict_elements))\n",
    "\n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "\n",
    "                index_elems   = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for key_elem, details_elem in zip(index_elems, details_elems):\n",
    "                    key_text     = key_elem.text.strip() if key_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    data_entry = {'rotulos': key_text, 'conteudos': details_text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "                \n",
    "            if higher_order_key is None:\n",
    "                # Se nenhuma chave de ordem superior for encontrada, associa a lista de dados diretamente ao título\n",
    "                result_dict[title_text] = data_list\n",
    "            else:\n",
    "                # Caso contrário, associa o data_dict contendo chaves de ordem superior ao título\n",
    "                result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def generate_dataframe_and_neo4j_dict(data_dict):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame and a dictionary for Neo4j persistence, incorporating section and subsection names.\n",
    "\n",
    "    Parameters:\n",
    "        data_dict (dict): A nested dictionary containing section and subsection data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame aggregating all sections and subsections, with additional columns specifying their names.\n",
    "        dict: A dictionary intended for Neo4j persistence, formatted according to the Neo4j data model.\n",
    "    \"\"\"\n",
    "\n",
    "    all_frames = []  # List to store DataFrames corresponding to each section and subsection\n",
    "    neo4j_dict = {}  # Dictionary for Neo4j persistence\n",
    "\n",
    "    for section, items in data_dict.items():\n",
    "        if section:  # Exclude empty sections\n",
    "            neo4j_dict[section] = {}\n",
    "            if isinstance(items, list):  # If items is a list, convert to DataFrame\n",
    "                df = pd.DataFrame(items)\n",
    "                df['Section'] = section  # Append a column for the section name\n",
    "                all_frames.append(df)\n",
    "                neo4j_dict[section] = items  # For list items, add them as they are\n",
    "\n",
    "            elif isinstance(items, dict):  # If items is a dictionary, explore subsections\n",
    "                for subsection, subitems in items.items():\n",
    "                    if subitems:  # Exclude empty subsections\n",
    "                        df = pd.DataFrame(subitems)\n",
    "                        df['Subsection'] = subsection  # Append a column for the subsection name\n",
    "                        df['Section'] = section  # Append a column for the section name\n",
    "                        all_frames.append(df)\n",
    "                        neo4j_dict[section][subsection] = subitems  # Store subsection data\n",
    "\n",
    "    # Concatenate all DataFrames vertically to form one unified DataFrame\n",
    "    dataframe = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "    return dataframe, neo4j_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de persistência em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database credentials and URI\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    try:\n",
    "        graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    except:\n",
    "        print('Erro ao conectar ao Neo4j')\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'].split('(')[1].strip(')'), meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    print(type(header_node))\n",
    "\n",
    "    return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict, List\n",
    "\n",
    "def create_or_update_publications(graph: Graph, publications_dict: Dict[str, Dict[str, List[Dict[str, str]]]]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its associated publications in the Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        publications_dict (Dict[str, Dict[str, List[Dict[str, str]]]]): Dictionary containing publication information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    publications_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Search for existing researcher node by name\n",
    "    existing_node = matcher.match(\"Researcher\", name=publications_dict['Node Name']).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding publications.\")\n",
    "\n",
    "    # Process publications\n",
    "    for publication in publications_dict['Properties']['Produções']:\n",
    "        # Check if a similar publication already exists\n",
    "        existing_pub_node = matcher.match(\"Publication\", doi=publication['doi']).first()\n",
    "\n",
    "        if not existing_pub_node:\n",
    "            pub_node = Node(\"Publication\", **publication)\n",
    "            graph.create(pub_node)\n",
    "            publications_created += 1\n",
    "        else:\n",
    "            for key, value in publication.items():\n",
    "                if key not in existing_pub_node or existing_pub_node[key] != value:\n",
    "                    existing_pub_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            pub_node = existing_pub_node\n",
    "            graph.push(pub_node)\n",
    "\n",
    "        # Create or update relationship between researcher and publication\n",
    "        rel = Relationship(existing_node, \"PUBLISHED\", pub_node)\n",
    "        graph.merge(rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Publications created: {publications_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução das extrações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair todas seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = extract_data(soup)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair seções específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_articles = extrair_artigos(soup)\n",
    "dict_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_dict.items():\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe, neo4j_dict = generate_dataframe_and_neo4j_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exibição dos dataframes por seção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(neo4j_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Identificação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Endereço']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Formação acadêmica/titulação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Formação Complementar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Linhas de pesquisa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Projetos de desenvolvimento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Áreas de atuação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Idiomas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Atuação Profissional_Fundação Oswaldo Cruz, FIOCRUZ, Brasil.'\n",
    " 'Produções_Produção bibliográfica',\n",
    " 'Bancas_Participação em bancas de trabalhos de conclusão',\n",
    " 'Eventos_Participação em eventos, congressos, exposições e feiras',\n",
    " 'Orientações_Orientações e supervisões concluídas',\n",
    " 'Inovação_Projeto de desenvolvimento tecnológico']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa estruturação funcionou bem para as seções que tem anos como chaves, mas falha para em seções com subníveis de detalhamento, como nas produções e no resumo, onde os dados de valores não foram populados no dicionário embora as chaves tenham sido extraídas com sucesso.\n",
    "\n",
    "Pode ser por falta de algum elemento, classe ou marcador não apontada ou não hierarquizada corretamente na função de extração.\n",
    "\n",
    "Deve-se escolher outros marcadores para complementar a extração, ou usar o atual somente para seção 'Formação acadêmica/titulação'.\n",
    "\n",
    "Falhas não capturou corretamente:\n",
    "- subchaves de Atuação Profissional\n",
    "- subchaves de Orientações\n",
    "- subchaves de Produções pra Livros e Capítulos de livros\n",
    "\n",
    "Falhas não capturoud e forma alguma:\n",
    "- Projetos de Pesquisa\n",
    "- Inovação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não funcionou tão bem para a Atuação profissional pois não capturou chaves corretas\n",
    "atuaprof_df = neo4j_dict['Atuação Profissional_Fundação Oswaldo Cruz, FIOCRUZ, Brasil.']\n",
    "atuaprof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_df = neo4j_dict['Linhas de pesquisa']\n",
    "linhas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projdesv_df = neo4j_dict['Projetos de desenvolvimento']\n",
    "projdesv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projpesq_df = neo4j_dict['Áreas de atuação']\n",
    "projpesq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Produção bibliográfica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair marcadores CSS específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_classes_content(soup_element, target_classes):\n",
    "    result_dict = defaultdict(list)\n",
    "    \n",
    "    for t_class in target_classes:\n",
    "        for element in soup_element.find_all(class_=t_class):\n",
    "            text_content = element.text.strip()\n",
    "            result_dict[t_class].append(text_content)\n",
    "            \n",
    "    return dict(result_dict)\n",
    "\n",
    "target_classes = [\n",
    "        # 'infpessoa', \n",
    "        'nome', \n",
    "        'resumo', \n",
    "        # 'artigo-completo', \n",
    "        # 'cita-artigos', \n",
    "        # 'citacoes', \n",
    "        # 'detalhes', \n",
    "        # 'fator', \n",
    "        # 'foto', \n",
    "        # 'informacao-artigo', \n",
    "        # 'informacoes-autor', \n",
    "        # 'rodape-cv', \n",
    "        # 'science_cont', \n",
    "        # 'texto',\n",
    "        #  \n",
    "        # 'cita', \n",
    "        # 'trab'\n",
    "        ]\n",
    "\n",
    "result_dict = extract_classes_content(soup.body, target_classes)\n",
    "from pprint import pprint\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções para interagir com Neo4j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    if 'nome' in researcher_dict and 'resumo' in researcher_dict:\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['nome'][0] if isinstance(researcher_dict['nome'], list) else researcher_dict['nome']\n",
    "        summary = researcher_dict['resumo'][0] if isinstance(researcher_dict['resumo'], list) else researcher_dict['resumo']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, resumo=summary)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'nome' and 'resumo'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'nome': ['John Doe'], 'resumo': ['This is a summary.']}\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "# create_researcher_node(graph, researcher_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infopessoa(result_dict):\n",
    "    nome,_,_,endereco_lattes,_,idlattes,_,_,atualizacao = result_dict['infpessoa'][0].split('\\n')\n",
    "\n",
    "    dict_info = {\n",
    "        'NOME' : nome,\n",
    "        # 'link_lattes' : endereco_lattes.split(': ')[1],\n",
    "        'IDLATTES' : idlattes.split(': ')[1],\n",
    "        'ATUALIZAÇÃO' : atualizacao.split('Última atualização do currículo em ')[1],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(dict_info, index=[0]).T\n",
    "    df.columns = ['DADOS_CURRICULO']\n",
    "    return df, dict_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "\n",
    "def generate_lattes_dict(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a dictionary from a BeautifulSoup object to be persisted in Neo4j.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML/XML data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the relevant information for Neo4j persistence.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold the extracted data\n",
    "    lattes_data = {}\n",
    "    \n",
    "    # Extracting researcher's name as an example\n",
    "    name_section = soup.find('div', {'id': 'name-section'})\n",
    "    if name_section:\n",
    "        lattes_data['researcher_name'] = name_section.text.strip()\n",
    "    \n",
    "    # Extracting list of publications as an example\n",
    "    publications = []\n",
    "    for pub in soup.find_all('div', {'class': 'publication'}):\n",
    "        publication_data = {}\n",
    "        title = pub.find('span', {'class': 'title'})\n",
    "        authors = pub.find('span', {'class': 'authors'})\n",
    "        \n",
    "        if title:\n",
    "            publication_data['title'] = title.text.strip()\n",
    "        \n",
    "        if authors:\n",
    "            publication_data['authors'] = authors.text.strip().split(',')\n",
    "        \n",
    "        publications.append(publication_data)\n",
    "    \n",
    "    lattes_data['publications'] = publications\n",
    "    \n",
    "    # Additional extractions can be performed as per the requirements\n",
    "    \n",
    "    return lattes_data\n",
    "\n",
    "# Example usage (Assuming 'some_html_content' contains the HTML content)\n",
    "# soup = BeautifulSoup(some_html_content, 'html.parser')\n",
    "# lattes_dict = generate_lattes_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infopessoa, dict_pessoa = infopessoa(result_dict)\n",
    "df_infopessoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pessoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node_from_dict(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "create_researcher_node_from_dict(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    nodes_created = 0\n",
    "    nodes_updated = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "\n",
    "        # Create a NodeMatcher object\n",
    "        matcher = NodeMatcher(graph)\n",
    "\n",
    "        # Look for existing nodes with the same name\n",
    "        existing_node = matcher.match(\"Researcher\", name=name).first()\n",
    "\n",
    "        if existing_node:\n",
    "            # Update properties of existing node\n",
    "            for key, value in researcher_dict.items():\n",
    "                if key.lower() not in existing_node or existing_node[key.lower()] != value:\n",
    "                    existing_node[key.lower()] = value\n",
    "                    properties_updated += 1\n",
    "\n",
    "            # Push the changes to the database\n",
    "            graph.push(existing_node)\n",
    "            nodes_updated += 1\n",
    "\n",
    "        else:\n",
    "            # Create a new node of type 'Researcher'\n",
    "            researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "            # Add the node to the Neo4j database\n",
    "            graph.create(researcher_node)\n",
    "            nodes_created += 1\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"Nodes created: {nodes_created}\")\n",
    "        print(f\"Nodes updated: {nodes_updated}\")\n",
    "        print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'NOME': 'John Doe', 'IDLATTES': '0000000000000000', 'ATUALIZAÇÃO': '31/12/2023'}\n",
    "\n",
    "# Create or update a Researcher node in Neo4j based on the dictionary\n",
    "create_or_update_researcher_node(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vinculo = extract_academic(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_professional_links(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its professional links in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's professional information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    relationships_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Look for existing nodes with the same name\n",
    "    existing_node = matcher.match(\"Researcher\", name=researcher_dict.get('Nome')).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding professional links.\")\n",
    "\n",
    "    # Process professional affiliations and activities\n",
    "    for key, value in researcher_dict.items():\n",
    "        if key not in ['Nome', 'Endereço Profissional']:\n",
    "            # Create or find the organization/affiliation node\n",
    "            org_node = matcher.match(\"Organization\", name=key).first()\n",
    "            if not org_node:\n",
    "                org_node = Node(\"Organization\", name=key)\n",
    "                graph.create(org_node)\n",
    "\n",
    "            # Create or update the relationship\n",
    "            rel_type = \"AFFILIATED_WITH\" if 'Atual' in value else \"HAS_BEEN_AFFILIATED_WITH\"\n",
    "            rel = Relationship(existing_node, rel_type, org_node, details=value)\n",
    "\n",
    "            # Check if a similar relationship already exists\n",
    "            existing_rel = None\n",
    "            for r in graph.match((existing_node, org_node), r_type=rel_type):\n",
    "                if r['details'] == value:\n",
    "                    existing_rel = r\n",
    "                    break\n",
    "\n",
    "            # Create new or update existing relationship\n",
    "            if not existing_rel:\n",
    "                graph.create(rel)\n",
    "                relationships_created += 1\n",
    "            else:\n",
    "                for property_name, property_value in rel.items():\n",
    "                    if property_name not in existing_rel or existing_rel[property_name] != property_value:\n",
    "                        existing_rel[property_name] = property_value\n",
    "                        properties_updated += 1\n",
    "                graph.push(existing_rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Relationships created: {relationships_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update professional links in Neo4j based on the dictionary\n",
    "create_or_update_professional_links(graph, dict_vinculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('./../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_path = './../data/classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx'\n",
    "xlsx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "def persist_journals_from_xlsx(graph: Graph, xlsx_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Persist journal information into a Neo4j database from an Excel (.xlsx) file.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        xlsx_path (str): The path to the Excel (.xlsx) file containing the journal information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize counters for tracking\n",
    "    nodes_created = 0\n",
    "    properties_updated = 0\n",
    "    \n",
    "    # Create NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "    \n",
    "    # Read the Excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract journal properties\n",
    "        properties = {'ISSN': row['ISSN'],\n",
    "                      'Título': row['Título'],\n",
    "                      'Área de Avaliação': row['Área de Avaliação'],\n",
    "                      'Estrato': row['Estrato']}\n",
    "        \n",
    "        # Check if a node with the same ISSN already exists\n",
    "        existing_node = matcher.match(\"Journal\", ISSN=row['ISSN']).first()\n",
    "        \n",
    "        if existing_node:\n",
    "            for key, value in properties.items():\n",
    "                if key not in existing_node or existing_node[key] != value:\n",
    "                    existing_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            graph.push(existing_node)\n",
    "        else:\n",
    "            new_node = Node(\"Journal\", **properties)\n",
    "            graph.create(new_node)\n",
    "            nodes_created += 1\n",
    "    \n",
    "    # Print statistical data\n",
    "    print(f\"Nodes created: {nodes_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given the path to the Excel file\n",
    "# xlsx_path = \"path/to/excel/file.xlsx\"\n",
    "\n",
    "# Persist the journal information from the Excel file\n",
    "persist_journals_from_xlsx(graph, xlsx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = pd.DataFrame([extract_academic(soup)]).T\n",
    "df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_projects = extract_research_project(soup)\n",
    "extracted_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência do cabeçalho\n",
    "header_data = parse_header(soup)\n",
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_node = \n",
    "# header_node = persist_to_neo4j(header_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_article = result_dict['artigo-completo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados=[]\n",
    "for i in result_dict['artigo-completo']:\n",
    "    dados.append(i.split('\\n\\n'))\n",
    "\n",
    "def quebra(linha):\n",
    "    return [x.split('\\n') for x in linha]\n",
    "\n",
    "for i in dados:\n",
    "    print(quebra(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\n",
    "        'artigo-completo', 'cita', 'cita-artigos', 'citacoes', 'detalhes', 'fator', \n",
    "        'foto', 'informacao-artigo', 'informacoes-autor', 'infpessoa', 'nome', \n",
    "        'resumo', 'rodape-cv', 'science_cont', 'texto', 'trab'\n",
    "        ]\n",
    "\n",
    "def extract_selected_classes(soup, target_classes):\n",
    "    \"\"\"\n",
    "    Extrai conteúdos de classes específicas de um objeto soup extraído de documento HTML.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Objeto soup e lista de classes a serem extraídas.\n",
    "\n",
    "    Retorno:\n",
    "    - Um dicionário que mapeia o nome da classe à lista de conteúdos extraídos.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Dicionário para armazenar os conteúdos\n",
    "    content_dict = defaultdict(list)\n",
    "    \n",
    "    # Iteração através das classes alvo para extração de conteúdo\n",
    "    for target_class in target_classes:\n",
    "        elements = soup.find_all(class_=target_class)\n",
    "        for element in elements:\n",
    "            print(element.text)\n",
    "            dado = element.text.split('\\n')\n",
    "\n",
    "            for i in dado:\n",
    "                if i != '':\n",
    "                    content_dict[target_class].append(dado)\n",
    "            \n",
    "    return dict(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\n",
    "        'informacao-artigo'\n",
    "        ]\n",
    "extract_selected_classes(soup, target_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução da persistência em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or update publications in Neo4j based on the dictionary\n",
    "create_or_update_publications(graph, dict_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update publications in Neo4j based on the dictionary\n",
    "create_or_update_publications(graph, dict_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar Grafo em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência dos elementos H1\n",
    "try:\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "except Exception as e:\n",
    "    print('Erro ao conectar ao Neo4j')\n",
    "    print(e)\n",
    "try:    \n",
    "    header_node = persist_to_neo4j(header_data)\n",
    "    print({type(header_node)})\n",
    "    parse_h1_elements(soup, header_node, graph)\n",
    "    cv_node, properties = parse_parsoninfo(soup)\n",
    "except Exception as e:\n",
    "    print('Erro ao persistir nó')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node(\"Curriculum\", \n",
    "     title=header_data['title'].split('(')[1].strip(')'), \n",
    "     meta_keywords=header_data['meta_keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar extração de Listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ['Raimir Holanda Filho', 'Carlos Jose Araujo Pinheiro','Antonio Marcos Aires Barbosa']\n",
    "df_dados, sucesso = extrair_lista(lista, mestres=True, assunto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes em desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', {'class': 'layout-cell-pad-main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def list_divs(soup_element, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):  # check if the child is an instance of Tag\n",
    "            # Check for 'div' tags and list classes\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')  # Default to 'None' if class is not present\n",
    "                print('  ' * indent + f\"div {class_name}\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            # # Check for 'h1' tags\n",
    "            # elif child.name == 'h1':\n",
    "            #     print('  ' * indent + f\"h1 {child.text.strip()}\")\n",
    "            #     list_divs(child, indent)\n",
    "\n",
    "            # Check for 'h2' tags\n",
    "            elif child.name == 'h2':\n",
    "                print('  ' * indent + f\"h2 {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            # Check for 'a' tags\n",
    "            elif child.name == 'a':\n",
    "                print('  ' * indent + f\"a {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            # Check for 'ul' tags\n",
    "            elif child.name == 'ul':\n",
    "                print('  ' * indent + \"ul\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            # Check for 'li' tags\n",
    "            elif child.name == 'li':\n",
    "                print('  ' * indent + \"li: \" + child.text.strip())\n",
    "                list_divs(child, indent + 1)\n",
    "                            \n",
    "            # Check for 'inst_back' as class in any tag\n",
    "            elif child.has_attr('class') and 'inst_back' in child['class']:\n",
    "                print('  ' * indent + f\"{child.name} inst_back\")\n",
    "                list_divs(child, indent + 1)\n",
    "            \n",
    "            # Check for 'b' tags\n",
    "            elif child.name == 'b':\n",
    "                print('  ' * indent + f\"b {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "        else:\n",
    "            list_divs(child, indent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "list_divs(starting_div, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrando dados extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    \n",
    "    ignorar = [None, ' ','Baixar Currículo','Imprimir Currículo']\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "\n",
    "                # Checar classe 'layout-cell-pad-5' com os valores dos dicionários\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    print('  ' * (indent + 1) + f\" val: {dividir(child.get_text())}\")                    \n",
    "                \n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            elif child.name == 'h2':\n",
    "                print('  ' * indent + f\"Node Name: {child.string}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    print('  ' * indent + f\"Properties: {rotulo}\")\n",
    "                    list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                print('  ' * indent + f\"Subtitulo: {child.get_text()}\")\n",
    "                list_divs(child, indent + 1)\n",
    "                \n",
    "            elif child.name == 'inst_back':\n",
    "                print('  ' * indent + f\"Subtitulo: {child.get_text()}\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # print('  ' * indent + f\"Group: {rotulo}\")\n",
    "                    list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                print('  ' * indent + f\" key: {rotulo}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'p':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                print('  ' * indent + f\" val: {rotulo}\")\n",
    "                list_divs(child, indent)                \n",
    "            \n",
    "            # elif child.name == 'span':\n",
    "            #     rotulo = dividir(child.get_text())[0]\n",
    "            #     print('  ' * indent + f\" val: {rotulo}\")\n",
    "            #     list_divs(child, indent)   \n",
    "\n",
    "        else:\n",
    "            list_divs(child, indent)\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "list_divs(starting_div, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ainda com problemas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo chaves corretamente mas não valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_data(element, parent_dict, current_section=None):\n",
    "    key = None\n",
    "    \n",
    "    if isinstance(element, Tag):\n",
    "        if element.name == 'a' or element.name == 'h2':\n",
    "            current_section = element.text.strip()\n",
    "            parent_dict[current_section] = {}\n",
    "        \n",
    "        elif 'text-align-right' in element.get('class', []):\n",
    "            key = element.find('b').text.strip() if element.find('b') else None\n",
    "            if key and current_section:\n",
    "                parent_dict[current_section][key] = ''\n",
    "        \n",
    "        elif 'layout-cell-9 layout-cell-pad-5' in element.get('class', []):\n",
    "            value = element.find('a').text.strip() if element.find('a') else element.text.strip()\n",
    "            if value and current_section and key:\n",
    "                parent_dict[current_section][key] = value\n",
    "\n",
    "    if isinstance(element, Tag):\n",
    "        for child in element.children:\n",
    "            extract_data(child, parent_dict, current_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "initial_dict = {}\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Invoke the function\n",
    "extract_data(starting_div, initial_dict)\n",
    "initial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, json_output):\n",
    "    stack = deque([json_output])\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            current_dict = stack[-1]\n",
    "            \n",
    "            if child.name == 'h1':\n",
    "                key = dividir(child.get_text())[0]\n",
    "                current_dict[key] = {}\n",
    "                stack.append(current_dict[key])\n",
    "                \n",
    "            elif child.name == 'b':\n",
    "                key = dividir(child.get_text())[0]\n",
    "                current_dict[key] = {}\n",
    "                stack.append(current_dict[key])\n",
    "                \n",
    "            elif child.name == 'div':\n",
    "                class_name = child.get('class', [])\n",
    "                \n",
    "                # Ensure the class matches exactly\n",
    "                if 'layout-cell-pad-5' in class_name:\n",
    "                    value = dividir(child.get_text())\n",
    "                    \n",
    "                    # Additional safety check\n",
    "                    if len(stack) > 1:\n",
    "                        parent_dict = stack[-2]\n",
    "                        parent_key = list(parent_dict.keys())[-1]\n",
    "                        parent_dict[parent_key] = value\n",
    "                        stack.pop()\n",
    "                    else:\n",
    "                        print(\"Warning: Stack size insufficient.\")\n",
    "\n",
    "            # Debugging information\n",
    "            print(f\"Stack size: {len(stack)}, Current dict keys: {current_dict.keys()}\")\n",
    "\n",
    "            # Continue recursion\n",
    "            list_divs(child, current_dict)\n",
    "            \n",
    "    if len(stack) > 1:\n",
    "        stack.pop()\n",
    "\n",
    "# Initialize empty dictionary\n",
    "json_output = {}\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Assuming 'soup' contains your BeautifulSoup object\n",
    "list_divs(starting_div, json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Não monta os dicionários no Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, parent_dict):\n",
    "    stack = deque([(soup_element, parent_dict)])  # Initialize the stack with parent element and parent dictionary\n",
    "\n",
    "    while stack:\n",
    "        element, current_dict = stack.pop()  # Get the last tuple (element, dictionary) from the stack\n",
    "\n",
    "        for child in element.children:\n",
    "            if isinstance(child, Tag):\n",
    "                \n",
    "                if child.name == 'h1':\n",
    "                    key = dividir(child.get_text())[0]\n",
    "                    current_dict[key] = {}\n",
    "                    stack.append((child, current_dict[key]))  # Append child element and its dictionary to the stack\n",
    "                    \n",
    "                elif child.name == 'b':\n",
    "                    key = dividir(child.get_text())[0]\n",
    "                    current_dict[key] = {}\n",
    "                    stack.append((child, current_dict[key]))  # Append child element and its dictionary to the stack\n",
    "                \n",
    "                elif child.name == 'div':\n",
    "                    class_name = child.get('class', [])\n",
    "                    \n",
    "                    if 'layout-cell-pad-5' in class_name:\n",
    "                        value = dividir(child.get_text())\n",
    "                        if value:  # Only populate if value is non-empty\n",
    "                            current_dict.update(value)  # Add the value to the current dictionary\n",
    "                            \n",
    "                else:\n",
    "                    stack.append((child, current_dict))  # Append child element and current dictionary for other cases\n",
    "\n",
    "\n",
    "# Initialize empty dictionary\n",
    "json_output = {}\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Assuming 'soup' contains your BeautifulSoup object\n",
    "list_divs(starting_div, json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, dados, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    \n",
    "    ignorar = [None, ' ', 'Baixar Currículo', 'Imprimir Currículo']\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    valor = dividir(child.get_text())\n",
    "                    # Adicionar valor ao dicionário\n",
    "                    dados[f'Value_Level_{indent}'] = valor\n",
    "                \n",
    "                list_divs(child, dados, indent + 1)\n",
    "            \n",
    "            elif child.name == 'h2':\n",
    "                # Adicionar nome do nó ao dicionário\n",
    "                dados[f'Node_Name'] = child.string\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == \"foto\":\n",
    "                # Adicionar nome do nó ao dicionário\n",
    "                dados[f'Informações'] = child.string\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # Adicionar chave ao dicionário de primeiro nível\n",
    "                    dados[f'Properties'] = rotulo\n",
    "                    list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                # Adicionar subtítulo ao dicionário\n",
    "                dados[f'Subtitle_Level_{indent}'] = child.get_text()\n",
    "                list_divs(child, dados, indent + 1)\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # Adicionar grupo ao dicionário\n",
    "                    dados[f'Group_Level_{indent}'] = rotulo\n",
    "                    list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                # Adicionar chave ao dicionário\n",
    "                chave = dividir(child.get_text())[0]\n",
    "                dados[f'Group_Level_{indent}'] = chave\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "        else:\n",
    "            list_divs(child, dados, indent)\n",
    "\n",
    "# Iniciar a função principal\n",
    "if __name__ == '__main__':\n",
    "    # soup é uma variável que contém o objeto BeautifulSoup do seu HTML\n",
    "    dados = {}\n",
    "\n",
    "    # Encontra a div específica a partir da qual iniciar a travessia\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    # Chame a função list_divs passando o objeto BeautifulSoup e o dicionário vazio\n",
    "    list_divs(starting_div, dados)\n",
    "    \n",
    "    # Convertendo o dicionário para JSON\n",
    "    dados_json = json.dumps(dados, indent=4, ensure_ascii=False)\n",
    "    print(dados_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dados_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\", \"\")\n",
    "    string = string.replace(\"\\t\", '')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, graph_dict=None, indent=0, verbose=False):\n",
    "    if graph_dict is None:\n",
    "        graph_dict = {}\n",
    "    \n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return graph_dict\n",
    "    \n",
    "    ignorar = [None, ' ', 'Baixar Currículo', 'Imprimir Currículo']\n",
    "\n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if verbose:\n",
    "                conteudo = dividir(child.get_text())[0]\n",
    "                print(f\"Tag encontrada: {child.name}, Classe: {child.get('class', 'None')}, {conteudo}\")\n",
    "\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    conteudo = dividir(child.get_text())[0]\n",
    "                    graph_dict[\"values\"] = conteudo\n",
    "                \n",
    "                list_divs(child, graph_dict, indent + 1, verbose)\n",
    "\n",
    "            elif child.name == 'h2':\n",
    "                node_name = child.string\n",
    "                if \"nodes\" not in graph_dict:\n",
    "                    graph_dict[\"nodes\"] = []\n",
    "                graph_dict[\"nodes\"].append({\"name\": node_name})\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    graph_dict[\"properties\"] = {\"label\": rotulo}\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                graph_dict[\"subtitles\"] = child.get_text()\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    if \"groups\" not in graph_dict:\n",
    "                        graph_dict[\"groups\"] = []\n",
    "                    graph_dict[\"groups\"].append({\"label\": rotulo})\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                graph_dict[\"keys\"] = dividir(child.get_text())[0]\n",
    "\n",
    "        else:\n",
    "            list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "result = list_divs(starting_div, verbose=True)\n",
    "json.dumps(result, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    return string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "def extract_content(element, keys, key_classes, val_classes, verbose=False):\n",
    "    result = {}\n",
    "    try:\n",
    "        if verbose is True:\n",
    "            print(f\"Debug: Processing element of type {type(element)} with attributes {element.attrs}\")\n",
    "\n",
    "        for key_class in key_classes:\n",
    "            key_elements = element.find_all(class_=key_class, recursive=True)\n",
    "            for key_elem in key_elements:\n",
    "                key_text = key_elem.text.strip()\n",
    "                if verbose is True:\n",
    "                    print(f\"Debug: Found key element with text: {key_text}\")\n",
    "\n",
    "                value_elem = key_elem.find_next_sibling(class_=val_classes)\n",
    "                if value_elem:\n",
    "                    value_text = rep(value_elem.text.strip())\n",
    "                    if verbose is True:\n",
    "                        print(f\"Debug: Found value element with text: {value_text}\")\n",
    "                    result[key_text] = value_text\n",
    "                else:\n",
    "                    if verbose is True:\n",
    "                        print(f\"Debug: No value element found for the key '{key_text}' with attributes {key_elem.attrs}\")\n",
    "\n",
    "        for section_class in keys:\n",
    "            section_elements = element.find_all('div', {'class': section_class}, recursive=True)\n",
    "            for idx, section_elem in enumerate(section_elements):\n",
    "               if verbose is True:\n",
    "                print(f\"Debug: Found section element with attributes: {section_elem.attrs}\")\n",
    "\n",
    "                section_dict = extract_content(section_elem, keys, key_classes, val_classes)\n",
    "                section_key = section_elem.get('id', f'UnnamedSection{idx}')\n",
    "                result[section_key] = section_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in extract_content: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "    # keys = ['infpessoa', 'id','title_wraper','infpessoa','artigo-completo']\n",
    "    # key_classes = ['title_wraper','infpessoa','inst_back', 'id','layout-cell layout-cell-3 text-align-right','artigo-completo', 'layout-cell-1','cita-artigos']\n",
    "    # val_classes = ['layout-cell-9','data-cell','layout-cell-11']\n",
    "\n",
    "try:\n",
    "    # Initialize BeautifulSoup object (Assuming that 'soup' has been initialized)\n",
    "    keys = [\n",
    "            'b',\n",
    "            'id',\n",
    "            'infpessoa',\n",
    "            'title_wraper',\n",
    "            'artigo-completo',\n",
    "            'title_wraper',\n",
    "            'infpessoa',\n",
    "            'id',\n",
    "            'artigo-completo',\n",
    "            ]\n",
    "    key_classes = [\n",
    "                   'text-align-right',\n",
    "                   'title_wraper',        \n",
    "                   'inst_back',\n",
    "                #    'layout-cell-1', # Trabalhos em eventos\n",
    "                #    'cita-artigos', # Trabalhos completos e conclusão de curso de graduação\n",
    "                   ]\n",
    "    val_classes = [\n",
    "        'layout-cell-9',\n",
    "        'data-cell',\n",
    "        'layout-cell-11',\n",
    "        'title_wraper',\n",
    "        # 'infpessoa',\n",
    "        'id',\n",
    "        'artigo-completo',        \n",
    "        ]\n",
    "\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "    master_dict = {}\n",
    "    \n",
    "    if starting_div:\n",
    "        master_dict = extract_content(starting_div, keys, key_classes, val_classes)\n",
    "        print(json.dumps(master_dict, indent=4, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"Starting div not found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred in main: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    return string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "def extract_content(element, keys, key_classes, val_classes):\n",
    "    result = {}\n",
    "    \n",
    "    if isinstance(element, Tag):\n",
    "        for key in keys:\n",
    "            occurrences = element.find_all(key)\n",
    "            for c in occurrences:\n",
    "                if c is not None:\n",
    "                    result[key] = rep(c.text).strip()\n",
    "\n",
    "        for key_class in key_classes:\n",
    "            occurrences = element.find_all('div', {'class': key_class})\n",
    "            for idx, c in enumerate(occurrences):\n",
    "                if c is not None:\n",
    "                    key = c.text.strip() if c.text.strip() else f\"Unnamed-{idx}\"\n",
    "                    result[key] = {}\n",
    "                    \n",
    "                    sibling = c.find_next_sibling('div', {'class': val_classes})\n",
    "                    if sibling:\n",
    "                        result[key] = rep(sibling.text).strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "try:\n",
    "    # Initialize BeautifulSoup object (Assuming that 'soup' has been initialized)\n",
    "    keys = ['infpessoa', 'id','title_wraper','infpessoa','artigo-completo']\n",
    "    key_classes = ['title_wraper','infpessoa','inst_back', 'id','layout-cell layout-cell-3 text-align-right','artigo-completo', 'layout-cell-1','cita-artigos']\n",
    "    val_classes = ['layout-cell-9','data-cell','layout-cell-11']\n",
    "\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    master_dict = {}\n",
    "    for i in starting_div.children:\n",
    "        if isinstance(i, Tag):\n",
    "            master_dict.update(extract_content(i, keys, key_classes, val_classes))\n",
    "\n",
    "    print(json.dumps(master_dict, indent=4, ensure_ascii=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "def rep(string):\n",
    "    return string.replace(\"\\n\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\")\n",
    "\n",
    "keys = ['infpessoa','h2','a', 'b', 'ul', 'id', 'inst_back','layout-cell-3','layout-cell-9','layout-cell-12','layout-cell-pad-5']\n",
    "\n",
    "try:\n",
    "    # Assuming 'soup' has been defined and initialized with the HTML document\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})        \n",
    "\n",
    "    for i in starting_div.children:\n",
    "        if isinstance(i, Tag):  # Confirming that the child is an HTML Tag\n",
    "            for key in keys:\n",
    "                # Use find_all to get a list of all occurrences\n",
    "                occurrences = i.find_all(key)\n",
    "                \n",
    "                for c in occurrences:  # Iterate over each occurrence\n",
    "                    if c is not None:\n",
    "                        print(f'{key:10} {rep(c.text).strip()}')\n",
    "                        print('-' * 75)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(element):\n",
    "    \"\"\"Recursively extracts content and organizes it into nested dictionaries.\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Check for 'title-wrapper' class as the first-level dictionary key\n",
    "    if 'title-wrapper' in element.attrs.get('class', []):\n",
    "        key = element.text.strip()\n",
    "        result[key] = {}\n",
    "        \n",
    "        for child in element.findChildren('div', recursive=False):\n",
    "            if 'layout-cell-12' in child.attrs.get('class', []):\n",
    "                second_level_key = child.text.strip()\n",
    "                result[key][second_level_key] = {}\n",
    "                \n",
    "                for grandchild in child.findChildren('div', recursive=False):\n",
    "                    if 'layout-cell-3' in grandchild.attrs.get('class', []):\n",
    "                        third_level_key = grandchild.text.strip()\n",
    "                        \n",
    "                        for great_grandchild in grandchild.findChildren('div', recursive=False):\n",
    "                            if 'layout-cell-9' in great_grandchild.attrs.get('class', []):\n",
    "                                value = great_grandchild.text.strip()\n",
    "                                result[key][second_level_key][third_level_key] = value\n",
    "    \n",
    "    # Recursively process child elements\n",
    "    for child in element.findChildren('div', recursive=False):\n",
    "        result.update(extract_content(child))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "# print(layout_cell_main.text.strip())\n",
    "print(len(layout_cell_main.find_all('div', 'title-wrapper')))\n",
    "for title_wrapper in layout_cell_main.find_all('div', 'title-wrapper'):\n",
    "    for data in title_wrapper.find_all('div', 'layout-cell-3'):\n",
    "        key = data.find('div', 'layout-cell-3')\n",
    "        val = data.find('div', 'layout-cell-9')\n",
    "        if key is not None:\n",
    "            print(f'{key.text.strip()}: {val.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_structure = extract_content(soup)\n",
    "json_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_scrape(tag, result_dict):\n",
    "    classes = tag.get(\"class\", [])\n",
    "    \n",
    "    if \"title-wrapper\" in classes:\n",
    "        section_name = tag.text.strip()\n",
    "        result_dict[section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[section_name])\n",
    "\n",
    "    elif \"text-align-right\" in classes or \"subtit-1\" in classes:\n",
    "        sub_section_name = tag.text.strip()\n",
    "        result_dict[sub_section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[sub_section_name])\n",
    "            \n",
    "    elif \"layout-cell-9\" in classes or \"layout-cell-pad-5\" in classes:\n",
    "        value = tag.text.strip()\n",
    "        result_dict[\"value\"] = value\n",
    "    \n",
    "    elif \"layout-cell\" in classes:\n",
    "        for child in tag.children:\n",
    "            if child.name:\n",
    "                recursive_scrape(child, result_dict)\n",
    "\n",
    "def main(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Initial recursive call\n",
    "    for child in soup.find_all('div'):\n",
    "        recursive_scrape(child, result_dict)\n",
    "        \n",
    "    # Convert the result to JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(result_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_scrape(tag, result_dict):\n",
    "    classes = tag.get(\"class\", [])\n",
    "    \n",
    "    if \"title-wrapper\" in classes:\n",
    "        section_name = tag.text.strip()\n",
    "        result_dict[section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[section_name])\n",
    "\n",
    "    elif \"text-align-right\" in classes or \"subtit-1\" in classes:\n",
    "        sub_section_name = tag.text.strip()\n",
    "        result_dict[sub_section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[sub_section_name])\n",
    "            \n",
    "    elif \"layout-cell-9\" in classes or \"layout-cell-pad-5\" in classes:\n",
    "        value = tag.text.strip()\n",
    "        result_dict[\"value\"] = value\n",
    "    \n",
    "    elif \"layout-cell\" in classes:\n",
    "        for child in tag.children:\n",
    "            if child.name:\n",
    "                recursive_scrape(child, result_dict)\n",
    "\n",
    "def main(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Initial recursive call\n",
    "    for child in soup.find_all('div'):\n",
    "        recursive_scrape(child, result_dict)\n",
    "        \n",
    "    # Convert the result to JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(result_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados hierárquicos de um Soup de documento HTML e retorna um dicionário aninhado.\n",
    "\n",
    "    Parâmetros:\n",
    "        Objeto Soup do html_content (object): O conteúdo HTML como uma string.\n",
    "\n",
    "    Retorna:\n",
    "        dict: Um dicionário aninhado contendo os dados extraídos.\n",
    "    \"\"\"\n",
    "\n",
    "    def recursive_extraction(element):\n",
    "        children_data = {}\n",
    "        children = element.find_all(recursive=False)\n",
    "        \n",
    "        for child in children:\n",
    "            if child.has_attr('class') and 'text-align-right' in child['class']:\n",
    "                key = child.get_text().strip()\n",
    "                \n",
    "                value_container = child.find_next_sibling(class_='layout-cell-9')\n",
    "                if value_container:\n",
    "                    value = recursive_extraction(value_container)\n",
    "                    children_data[key] = value if value else value_container.get_text().strip()\n",
    "        \n",
    "        return children_data\n",
    "\n",
    "    # Instanciar um objeto BeautifulSoup\n",
    "    # soup = BeautifulSoup(html_content, 'html.parser', from_encoding='utf-8')\n",
    "  \n",
    "    # Iniciar a extração recursiva a partir do elemento raiz\n",
    "    # root = soup.body\n",
    "    # result_dict = recursive_extraction(root)\n",
    "\n",
    "    # Encontra a div específica a partir da qual iniciar a travessia\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    # Inicia a travessia a partir da div encontrada\n",
    "    result_dict = recursive_extraction(starting_div)\n",
    "\n",
    "    # Converter o dicionário em um objeto JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "  \n",
    "    # Salvar o JSON em um arquivo, especificando o encoding como UTF-8\n",
    "    with open('output_nested.json', 'w', encoding='utf-8') as file:\n",
    "        file.write(result_json)\n",
    "  \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = extrair_dados(soup, True)\n",
    "nome_no = 'Antonio Marcos Aires Barbosa'\n",
    "imprimir_informacoes({nome_no: json_data['Properties']}, nome_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "for title_wrapper in layout_cell_main.find_all('div.title-wrapper'):\n",
    "    index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_from_html(soup):\n",
    "    \"\"\"\n",
    "    Generate a JSON object from an Soup object for a Neo4j integration.\n",
    "\n",
    "    Parameters:\n",
    "        soup (object): Soup object from a HTML content.\n",
    "\n",
    "    Returns:\n",
    "        json_data (dict): A dictionary representing the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract node name\n",
    "    node_name = soup.select_one('div.infpessoa h2.nome').text.strip()\n",
    "\n",
    "    # Initialize the master dictionary\n",
    "    json_data = {node_name: {}}\n",
    "\n",
    "    # Locate the main layout cell\n",
    "    layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "    # Iterate over title-wrapper elements\n",
    "    for title_wrapper in layout_cell_main.select('div.title-wrapper'):\n",
    "        index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "        \n",
    "        # Initialize the child dictionary\n",
    "        json_data[node_name][index] = {}\n",
    "\n",
    "        # Iterate over layout cells\n",
    "        for layout_cell_3 in title_wrapper.select('div.layout-cell.layout-cell-3.text-align-right'):\n",
    "            grandchild_index = layout_cell_3.select_one('div.layout-cell-pad-5.text-align-right').text.strip()\n",
    "            \n",
    "            # Find the corresponding layout cell for values\n",
    "            layout_cell_9 = layout_cell_3.find_next_sibling('div', class_='layout-cell.layout-cell-9')\n",
    "            \n",
    "            values_text = layout_cell_9.select_one('div.layout-cell-pad-5').text\n",
    "            \n",
    "            # Create a list of values\n",
    "            values = values_text.split('<br class=\"clear\">' or '\\n\\n\\n')\n",
    "\n",
    "            # Add the grandchild dictionary\n",
    "            json_data[node_name][index][grandchild_index] = values\n",
    "\n",
    "    return json_data\n",
    "\n",
    "# Generate JSON data\n",
    "json_data = generate_json_from_html(soup)\n",
    "\n",
    "# Print or persist the generated JSON data\n",
    "print(json.dumps(json_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lattes = extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint(json_lattes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(json_data, parent_key='', separator='.'):\n",
    "    \"\"\"\n",
    "    Convert a nested dictionary into a flat dictionary, suitable for DataFrame conversion.\n",
    "    \n",
    "    Parameters:\n",
    "    - json_data (dict): The nested dictionary to flatten.\n",
    "    - parent_key (str, optional): The concatenated key used to represent nesting.\n",
    "    - separator (str, optional): The character to use for separating nested keys.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame representing the flattened dictionary.\n",
    "    \"\"\"\n",
    "    flat_dict = {}\n",
    "    \n",
    "    for k, v in json_data.items():\n",
    "        new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "        \n",
    "        if isinstance(v, dict):\n",
    "            flat_dict.update(dict_to_dataframe(v, new_key, separator=separator))\n",
    "        else:\n",
    "            flat_dict[new_key] = v\n",
    "            \n",
    "    return pd.DataFrame([flat_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming json_lattes is a JSON-formatted string\n",
    "json_lattes_dict = json.loads(json_lattes)\n",
    "\n",
    "# Then call the dict_to_dataframe function\n",
    "df = dict_to_dataframe(json_lattes_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    splpt = 'Currículo do Sistema de Currículos Lattes ('\n",
    "    string_title = soup.title.string if soup.title else \"Unknown\"\n",
    "    title = string_title.split(splpt)[1].strip(')')\n",
    "    \n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "    from py2neo import Graph, Node, Relationship\n",
    "    \n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    secoes = []\n",
    "    \n",
    "    print(f'{len(h1_elements[2:])} elementos encontrados')\n",
    "    for n,i in enumerate(h1_elements):\n",
    "        if n>1:\n",
    "            secao = i.text\n",
    "            print(f'    {secao}')\n",
    "            secoes.append(secao)\n",
    "    \n",
    "    for elem in h1_elements[2:]:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        \n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_parsoninfo(soup):\n",
    "    # Localizar o elemento de link que contém o título do currículo\n",
    "    link_element = soup.find(\"a\", {\"href\": lambda x: x and \"abreDetalhe\" in x})\n",
    "\n",
    "    # Extrair o texto do link para usar como título do nó\n",
    "    node_title = link_element.text if link_element else \"Unknown\"\n",
    "    print(f'Título do Nó: {node_title}')\n",
    "\n",
    "    # Localizar o elemento div contendo as propriedades\n",
    "    properties_div = soup.find(\"div\", {\"class\": \"resultado\"})\n",
    "    if properties_div:\n",
    "        print(f'Resultado: {properties_div.text}')\n",
    "    else:\n",
    "        print('Resultados não encontrados')\n",
    "\n",
    "    # Inicializar um dicionário para armazenar as propriedades\n",
    "    properties = {}\n",
    "    \n",
    "    # Localizar o elemento li que contém as informações do idlattes\n",
    "    li_element = soup.find(\"li\")\n",
    "    for i in li_element:\n",
    "        if 'http://lattes.cnpq.br/' in i:\n",
    "            idlattes = i.split('http://lattes.cnpq.br/')[1]\n",
    "            properties['Idlattes'] = idlattes\n",
    "            print(idlattes)\n",
    "\n",
    "    # Extrair e armazenar as propriedades relevantes\n",
    "    if properties_div:\n",
    "        properties['Nacionalidade'] = 'Brasil'\n",
    "        properties['Cargo'] = properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}).text if properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}) else 'Desconhecido'\n",
    "        properties['Titulação'] = properties_div.contents[-4] if len(properties_div.contents) > 4 else 'Desconhecido'\n",
    "\n",
    "        # Extração de nome e identificador único\n",
    "        a_element = li_element.find(\"a\")\n",
    "        properties[\"Nome\"] = a_element.text\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Títulos Acadêmicos e outras informações\n",
    "\n",
    "    return node_title, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_annotation_in_db(uri, user, password, annot_html, cv_id):\n",
    "    \"\"\"\n",
    "    Store the annotation HTML in a Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        uri (str): URI of the Neo4j database\n",
    "        user (str): Username for the Neo4j database\n",
    "        password (str): Password for the Neo4j database\n",
    "        annot_html (str): The HTML string containing annotations\n",
    "        cv_id (str): The unique identifier for the annotated CV\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize Neo4j driver\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    # Define Cypher query for adding an annotation\n",
    "    add_annotation_query = '''\n",
    "    MERGE (cv:CV {id: $cv_id})\n",
    "    CREATE (a:Annotation {html: $annot_html})\n",
    "    MERGE (cv)-[:HAS]->(a)\n",
    "    '''\n",
    "\n",
    "    # Execute query\n",
    "    with driver.session() as session:\n",
    "        session.run(add_annotation_query, cv_id=cv_id, annot_html=annot_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# from collections import Counter\n",
    "\n",
    "# def enumerate_tags(soup):\n",
    "#     # Extração de todos os marcadores (tags) no documento\n",
    "#     all_tags = [tag.name for tag in soup.find_all(True)]\n",
    "    \n",
    "#     # Contagem de ocorrências de cada marcador\n",
    "#     tag_count = Counter(all_tags)\n",
    "    \n",
    "#     # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "#     tag_dict = dict(tag_count)\n",
    "    \n",
    "#     return tag_dict\n",
    "\n",
    "# # Exemplo de uso\n",
    "# tag_dictionary = enumerate_tags(soup)\n",
    "# print(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div_elements = soup.find_all('div')\n",
    "# div_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_div_data(soup):\n",
    "#     \"\"\"\n",
    "#     Extrai dados das divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "#     Parâmetros:\n",
    "#     - html_document (str): String contendo o documento HTML.\n",
    "    \n",
    "#     Retorno:\n",
    "#     - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "#     \"\"\"\n",
    "#     # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "#     extracted_data = {}\n",
    "    \n",
    "#     # Localiza todas as divs com a classe 'layout-cell-pad-5 text-align-right'\n",
    "# #     divs_key = soup.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "#     divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "#     for div_key in divs_key:\n",
    "#         # Extrai o conteúdo da tag <b> dentro da div\n",
    "#         key = div_key.find('b').text if div_key.find('b') else None\n",
    "        \n",
    "#         # Encontra a div que segue imediatamente\n",
    "#         div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5'})\n",
    "        \n",
    "#         # Extrai o conteúdo da div\n",
    "#         value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "#         # Armazena no dicionário se ambas chave e valor existirem\n",
    "#         if key and value:\n",
    "#             extracted_data[key] = value\n",
    "    \n",
    "#     return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_div_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(extracted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        \n",
    "        data_dict = {}\n",
    "        \n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "            \n",
    "            current_higher_order_dict = None  # Initialize a variable to store the current higher-order dictionary\n",
    "            \n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                \n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    current_higher_order_dict = {}  # Create a new dictionary for this higher-order key\n",
    "                    data_dict[higher_order_key] = current_higher_order_dict  # Associate the new dictionary with the higher-order key\n",
    "                    \n",
    "                year_elems = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for year_elem, details_elem in zip(year_elems, details_elems):\n",
    "                    year_text = year_elem.text.strip() if year_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    if current_higher_order_dict is not None:\n",
    "                        # Insert the year-details pair into the current higher-order dictionary\n",
    "                        current_higher_order_dict[year_text] = details_text\n",
    "                    else:\n",
    "                        # If no higher-order key is present, associate the year-details pair directly with the title\n",
    "                        data_dict[year_text] = details_text\n",
    "                \n",
    "            result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(input_dict):\n",
    "    def recursive_descent(current_dict, parent_key='', separator='.'):\n",
    "        nonlocal flattened_dict\n",
    "        for k, v in current_dict.items():\n",
    "            new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                recursive_descent(v, new_key, separator=separator)\n",
    "            else:\n",
    "                flattened_dict[new_key] = v\n",
    "                \n",
    "    flattened_dict = {}\n",
    "    recursive_descent(input_dict)\n",
    "    \n",
    "    # Create DataFrame from the flattened dictionary\n",
    "    df = pd.DataFrame([flattened_dict])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dict = extract_data(soup)\n",
    "df = dict_to_dataframe(nested_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag, NavigableString\n",
    "\n",
    "html_content = '''\n",
    "<div class=\"layout-cell-pad-5\">\n",
    "    Doutorado em andamento em Informática Aplicada.\n",
    "    <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "    <br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde\n",
    "    <br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho.\n",
    "</div>\n",
    "'''\n",
    "\n",
    "soup_sample = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup_sample)\n",
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def extract_text_from_selectors(soup,select_path):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        elements = title_wrapper.select(select_path)\n",
    "#         print(len(elements))\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag, NavigableString\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=\"layout-cell.layout-cell-3.text-align-right\"\n",
    "vals=\"layout-cell layout-cell-9\"\n",
    "# <div class=\"layout-cell layout-cell-9\">\n",
    "# <div class=\"layout-cell-pad-5\">Doutorado em andamento em Informática Aplicada. <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "\t\t\n",
    "# \t<br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde<br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho. </div>\n",
    "# </div>\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "father     = 'div.title-wrapper'\n",
    "sons       = 'h1'\n",
    "grandchild = '' \n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        son_elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for grandchild in son_elements:\n",
    "            text_content = grandchild.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_titles(soup,father,sons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "father = 'div.title-wrapper'\n",
    "sons   = 'h1'\n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-12.data-cell'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "select_path='layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_to_dict(soup):\n",
    "    result_list = []\n",
    "    \n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        temp_dict = defaultdict(dict)\n",
    "        first_level_key = title_wrapper.select_one('div.layout-cell.layout-cell-12.data-cell').text.strip()\n",
    "        \n",
    "        for cell in title_wrapper.select('div.layout-cell.layout-cell-12.data-cell'):\n",
    "            second_level_keys = cell.select('div.layout-cell-pad-5.text-align-right')\n",
    "            values = cell.select('div.layout-cell.layout-cell-3.text-align-right')\n",
    "            \n",
    "            if len(second_level_keys) == len(values):\n",
    "                for key, value in zip(second_level_keys, values):\n",
    "                    second_level_key = key.text.strip()\n",
    "                    value_text = value.text.strip()\n",
    "                    temp_dict[first_level_key][second_level_key] = value_text\n",
    "        \n",
    "        result_list.append(temp_dict)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "# Execute the function\n",
    "result_list = extract_to_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['layout-cell', 'layout-cell-3', 'text-align-right']\n",
    "\n",
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "# 'text-align-right': 152,\n",
    "# 'layout-cell-pad-5': 152,\n",
    "\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-pad-main'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'text-align-right'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-9'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-3'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'data-cell'},\n",
    "#                                     {'class': 'layout-cell'},\n",
    "#                                     {'class': 'layout-cell-9},                                    \n",
    "#                                     {'class': 'layout-cell-12'},\n",
    "#                                     {'class': 'layout-cell-pad-5'},\n",
    "#                                     {'class': 'data-cell'}\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_text_from_hierarchy(tag, result_dict, parent_key=\"root\"):\n",
    "    div_elements = tag.find_all('div', recursive=False)\n",
    "    \n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        class_key = ', '.join(class_list) if class_list else \"None\"\n",
    "        \n",
    "        # Criando uma chave única que incorpora o caminho da raiz até esta div\n",
    "        full_key = f\"{parent_key} -> {class_key}\"\n",
    "\n",
    "        # Coleta o texto contido no elemento div atual\n",
    "        text_content = div.get_text(strip=True)\n",
    "\n",
    "        # Armazenar o conteúdo textual sob esta chave única\n",
    "        if full_key not in result_dict:\n",
    "            result_dict[full_key] = []\n",
    "        result_dict[full_key].append(text_content)\n",
    "\n",
    "        # Chamada recursiva para extrair textos dos filhos deste div\n",
    "        extract_text_from_hierarchy(div, result_dict, full_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)\n",
    "\n",
    "extract_text_from_hierarchy(soup.body, result_dict, 'text-align-right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.values():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes, key_stack=None):\n",
    "    if key_stack is None:\n",
    "        key_stack = []\n",
    "    \n",
    "    popped_key = None\n",
    "    if soup_element.name == \"div\":\n",
    "        class_list = soup_element.get('class', [])\n",
    "        \n",
    "        if any(k in class_list for k in key_classes):\n",
    "            new_key = soup_element.text.strip()\n",
    "#             print(f'Key found: {new_key}')  # Debugging line\n",
    "            key_stack.append(new_key)\n",
    "        \n",
    "        elif any(v in class_list for v in value_classes) and key_stack:\n",
    "            value = soup_element.text.strip()\n",
    "#             print(f'Value found: {value}')  # Debugging line\n",
    "            current_key = key_stack[-1]\n",
    "            if current_key in result_dict:\n",
    "                result_dict[current_key].append(value)\n",
    "            else:\n",
    "                result_dict[current_key] = [value]\n",
    "                \n",
    "#     print(f\"Current key_stack: {key_stack}\")  # Debugging line\n",
    "\n",
    "    for child in soup_element.find_all(\"div\", recursive=False):\n",
    "        extract_key_value_pairs(child, result_dict, key_classes, value_classes, key_stack)\n",
    "\n",
    "    if popped_key:\n",
    "        key_stack.append(popped_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'text-align-right'})\n",
    "# key_classes   = ['data-cell']\n",
    "# value_classes = ['text-align-right']\n",
    "\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    pprint(i)\n",
    "\n",
    "print()\n",
    "\n",
    "for i in result_dict.values():\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'layout-cell-pad-5'})\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "\n",
    "def extract_target_classes(soup,classe):\n",
    "    soup_element  = soup.find('div', {'class': classe})\n",
    "#     key_classes   = ['data-cell']\n",
    "#     value_classes = ['text-align-right']\n",
    "    key_classes   = ['layout-cell-9']\n",
    "    value_classes = ['layout-cell-pad-5']\n",
    "    \n",
    "    target_classes = [\n",
    "        'Produção '\n",
    "        'Bibliográfica',\n",
    "        'Produção '\n",
    "        'Técnica',\n",
    "        'Produção '\n",
    "        'Artística/Cultural'\n",
    "        'nome', \n",
    "        'resumo', \n",
    "        'artigo-completo', \n",
    "        'cita', \n",
    "        'cita-artigos', \n",
    "        'citacoes', \n",
    "        'detalhes', \n",
    "        'fator', \n",
    "        'foto', \n",
    "        'informacao-artigo', \n",
    "        'informacoes-autor', \n",
    "        'infpessoa', \n",
    "        'rodape-cv', \n",
    "        'science_cont', \n",
    "        'texto', \n",
    "        'trab'\n",
    "        ]\n",
    "    try:\n",
    "        extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "        for x,y in zip(result_dict.keys(),result_dict.values()):\n",
    "            if x in target_classes:\n",
    "                try:\n",
    "                    print(f\"{x:>12} | {y[0]}\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        filtered_dict = {k: v for k, v in result_dict.items() if k in target_classes}\n",
    "\n",
    "    except:\n",
    "        print(f'Classe \"{classe}\" não encontrada')\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variações das funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para anotação de dados em HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_published_articles(soup, qualis_data):\n",
    "    # Localiza elementos contendo os artigos publicados\n",
    "    articles_elements = soup.find_all('div', class_='published-article')\n",
    "    \n",
    "    # Inicializa lista para conter dados dos artigos publicados\n",
    "    annotated_articles = []\n",
    "    \n",
    "    # Itera sobre cada elemento e anotando as informações necessárias\n",
    "    for article in articles_elements:\n",
    "        title = article.find('div', class_='article-title').text\n",
    "        issn = article.find('div', class_='article-issn').text\n",
    "        qualis = qualis_data.get(issn, 'N/A')  # Buscando o Qualis correspondente\n",
    "        \n",
    "        # Adiciona ao conjunto de artigos anotados\n",
    "        annotated_articles.append({\n",
    "            'title': title,\n",
    "            'issn': issn,\n",
    "            'qualis': qualis\n",
    "        })\n",
    "    \n",
    "    # Persiste em SQLite\n",
    "    conn = sqlite3.connect(\"lattes_data.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Cria tabela de artigos publicados, se não existir\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS published_articles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT,\n",
    "        issn TEXT,\n",
    "        qualis TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insere artigos anotados na tabela\n",
    "    for article in annotated_articles:\n",
    "        cursor.execute(\"INSERT INTO published_articles (title, issn, qualis) VALUES (?, ?, ?)\",\n",
    "                       (article['title'], article['issn'], article['qualis']))\n",
    "    \n",
    "    # Commit e fecha a conexão\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "async def annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info):\n",
    "    print(\"Procurando por publicações de periódicos...\")\n",
    "    \n",
    "    # Inicializa BeautifulSoup para analisar o conteúdo HTML da página\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extrai e anota informações dos artigos publicados\n",
    "    qualis_info = await annotate_published_articles(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup)\n",
    "    \n",
    "    return qualis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_annotation_message(images_urls, visualization_url, recent_updates_url, pub_count):\n",
    "    annot_header_html = ''\n",
    "    annot_buttons_html = ''\n",
    "    pub_count_string = ''\n",
    "    \n",
    "    if pub_count > 0:\n",
    "        s_char = 's' if pub_count > 1 else ''\n",
    "        pub_count_string = f'anotou o Qualis de {pub_count} artigo{s_char} em periódico{s_char} neste CV.'\n",
    "        annot_buttons_html = render_template_string(\"\"\"\n",
    "            <a href=\"#artigos-completos\">\n",
    "                <button> Ver anotações </button>\n",
    "            </a>\n",
    "        \"\"\")\n",
    "    else:\n",
    "        pub_count_string = 'não anotou nenhum artigo em periódico neste CV.'\n",
    "\n",
    "    annot_header_html = render_template_string(\"\"\"\n",
    "        <a href=\"{{visualization_url}}\" target=\"_blank\" id=\"qlattes-logo\">\n",
    "            <img src=\"{{images_urls['qlattesLogoURL']}}\" width=\"70\">\n",
    "        </a>{{pub_count_string}}\n",
    "        </br>\n",
    "    \"\"\", visualization_url=visualization_url, images_urls=images_urls, pub_count_string=pub_count_string)\n",
    "\n",
    "    # Aqui a informação seria armazenada em um banco de dados em vez de ser injetada em um elemento HTML\n",
    "    store_annotation_in_db(annot_header_html + annot_buttons_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_qualis_data(qualis_info):\n",
    "    pub_data = []\n",
    "    pub_data_year = []\n",
    "    curr_year = 0\n",
    "    \n",
    "    for i in range(len(qualis_info)):\n",
    "        if curr_year != qualis_info[i]['year']:\n",
    "            if curr_year > 0:\n",
    "                pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "                pub_data_year = []\n",
    "            \n",
    "            curr_year = qualis_info[i]['year']\n",
    "        \n",
    "        pub_data_item = {\n",
    "            'issn': qualis_info[i]['issn'],\n",
    "            'title': qualis_info[i]['title'],\n",
    "            'pubName': qualis_info[i]['pubName'],\n",
    "            'qualis': qualis_info[i]['qualisLabels']['qualis'],\n",
    "            'baseYear': qualis_info[i]['qualisLabels']['baseYear'],\n",
    "            'jcr': qualis_info[i]['jcrData']['jcr'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else 0,\n",
    "            'jcrYear': qualis_info[i]['jcrData']['baseYear'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else ''\n",
    "        }\n",
    "        \n",
    "        pub_data_year.append(pub_data_item)\n",
    "        \n",
    "    if len(qualis_info) > len(pub_data):\n",
    "        pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "    \n",
    "    return pub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_lattes_page(name_link, images_urls, recent_updates_url, qualis_data, data_source_info):\n",
    "    print(qualis_data)\n",
    "    \n",
    "    # Inicializando o WebDriver\n",
    "    driver_service = Service('path/to/chromedriver')\n",
    "    driver = webdriver.Chrome(service=driver_service)\n",
    "    driver.get(name_link['link'])\n",
    "    \n",
    "    # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "    visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "    # Limpar cache de dados de Qualis\n",
    "    qualis_data_cache = {}\n",
    "    \n",
    "    # Annotate Lattes page com informações de Qualis\n",
    "    lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "    if lattes_info:\n",
    "        await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "    # Consolidar dados de publicação a partir das informações de Lattes\n",
    "    pub_info = consolidate_qualis_data(lattes_info)\n",
    "    print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "    # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "    conn = sqlite3.connect('lattes_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "    existing_data = cursor.fetchall()\n",
    "    \n",
    "    lattes_data_array = []\n",
    "    \n",
    "    if existing_data:\n",
    "        # Filtrar dados existentes para evitar duplicatas\n",
    "        lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "    # Adicionar dados de Lattes atuais ao array\n",
    "    lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "    # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "    cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_link_html(text, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">{{text}}</a>\n",
    "    \"\"\", text=text, tooltip=tooltip, target_url=target_url)\n",
    "\n",
    "def create_icon_link_html(icon_url, icon_style, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">\n",
    "            <img src=\"{{icon_url}}\" style=\"{{icon_style}}\">\n",
    "        </a>\n",
    "    \"\"\", icon_url=icon_url, icon_style=icon_style, tooltip=tooltip, target_url=target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qualis_from_percentil(percentil):\n",
    "    if not percentil:\n",
    "        return 'N'\n",
    "\n",
    "    qualis_class_list     = ['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4']\n",
    "    qualis_threshold_list = [87.5, 75, 62.5, 50, 37.5, 25, 12.5, 0]\n",
    "\n",
    "    for i in range(len(qualis_threshold_list)):\n",
    "        if percentil >= qualis_threshold_list[i]:\n",
    "            return qualis_class_list[i]\n",
    "\n",
    "    return 'N'\n",
    "\n",
    "\n",
    "def get_alternative_issn(issn, capes_alt_data, scopus_data):\n",
    "    # Pesquisar ISSN nos dados complementares da CAPES\n",
    "    match = next((elem for elem in capes_alt_data if elem['issn'] == issn or elem['alt_issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        if 'alt_issn' in match and match['alt_issn'] != issn:\n",
    "            return match['alt_issn']\n",
    "        elif 'issn' in match and match['issn'] != issn:\n",
    "            return match['issn']\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        # Pesquisar ISSN nos dados do Scopus\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "        \n",
    "        if match:\n",
    "            if 'e-issn' in match and len(match['e-issn']) > 0 and match['e-issn'] != issn:\n",
    "                return match['e-issn']\n",
    "            elif 'issn' in match and len(match['issn']) > 0 and match['issn'] != issn:\n",
    "                return match['issn']\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualis_from_capes_data(issn, alt_issn, capes_data, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procurar pelo ISSN na base de dados da CAPES\n",
    "    match = next((elem for elem in capes_data if elem['issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        qualis_labels['source'] = 'capes'\n",
    "    elif alt_issn != '':\n",
    "        match = next((elem for elem in capes_data if elem['issn'] == alt_issn), None)\n",
    "        \n",
    "        if match:\n",
    "            qualis_labels['source'] = 'capes_alt'\n",
    "            \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = match['qualis']\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['baseYear'] = data_source_info[qualis_labels['source']]['baseYear']\n",
    "\n",
    "        qualis_labels_scopus = get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info)\n",
    "        \n",
    "        if qualis_labels_scopus['qualis'] != 'N':\n",
    "            qualis_labels['linkScopus'] = qualis_labels_scopus['linkScopus']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_pucrs_data(issn, alt_issn, pub_name, pucrs_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    labels_map = {\n",
    "        'pubName': 'periodico',\n",
    "        'qualis': 'Qualis_Final',\n",
    "        'percentil': 'percentil',\n",
    "        'linkScopus': 'link_scopus',\n",
    "        'adjusted': 'Ajuste_SBC'\n",
    "    }\n",
    "    \n",
    "    match = next((elem for elem in pucrs_data if elem['issn'] == issn or (alt_issn and elem['issn'] == alt_issn)), None)\n",
    "    \n",
    "    if match:\n",
    "        for key in labels_map.keys():\n",
    "            if labels_map[key] in match and match[labels_map[key]] != 'nulo':\n",
    "                qualis_labels[key] = match[labels_map[key]]\n",
    "                \n",
    "        qualis_labels['source'] = 'pucrs'\n",
    "        qualis_labels['baseYear'] = data_source_info['pucrs']['baseYear']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procura pelo ISSN nos dados da Scopus\n",
    "    match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "    \n",
    "    if not match and alt_issn != '':\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == alt_issn or elem['e-issn'] == alt_issn), None)\n",
    "        \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = calculate_qualis_from_percentil(match['percentil'])\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['percentil'] = match['percentil']\n",
    "        qualis_labels['linkScopus'] = match['source-id-url']\n",
    "        qualis_labels['source'] = 'scopus'\n",
    "        qualis_labels['baseYear'] = data_source_info['scopus']['baseYear']\n",
    "    \n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "async def get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    if not issn:\n",
    "        return qualis_labels\n",
    "\n",
    "    alt_issn = ''\n",
    "\n",
    "    # Verificar se o ISSN já está em cache\n",
    "    if issn in qualis_data_cache:\n",
    "        return qualis_data_cache[issn]\n",
    "    else:\n",
    "        # Verificar se um ISSN alternativo existe e está em cache\n",
    "        alt_issn = await get_alternative_issn(issn, qualis_data['capes_alt'], qualis_data['scopus'])\n",
    "        if alt_issn and alt_issn in qualis_data_cache:\n",
    "            return qualis_data_cache[alt_issn]\n",
    "\n",
    "    # Procurar pelo ISSN nos dados CAPES\n",
    "    qualis_labels = await get_qualis_from_capes_data(\n",
    "        issn, alt_issn, qualis_data['capes'], qualis_data['scopus'], data_source_info\n",
    "    )\n",
    "\n",
    "    # Se não encontrado\n",
    "    if qualis_labels['qualis'] == 'N':\n",
    "        # Procurar pelo ISSN nos dados PUC-RS\n",
    "        qualis_labels = await get_qualis_from_pucrs_data(\n",
    "            issn, alt_issn, pub_name, qualis_data['pucrs'], data_source_info\n",
    "        )\n",
    "\n",
    "        # Se ainda não encontrado\n",
    "        if qualis_labels['qualis'] == 'N':\n",
    "            # Procurar pelo ISSN nos dados Scopus\n",
    "            qualis_labels = await get_qualis_from_scopus_data(\n",
    "                issn, alt_issn, qualis_data['scopus'], data_source_info\n",
    "            )\n",
    "\n",
    "    # Adicionar rótulos ao cache Qualis\n",
    "    qualis_data_cache[issn] = qualis_labels\n",
    "\n",
    "    if alt_issn:\n",
    "        qualis_data_cache[alt_issn] = qualis_labels\n",
    "\n",
    "    return qualis_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_qualis_annotation(pub_info, images_URLs, data_source_info):\n",
    "    annotation_dict = {}\n",
    "    \n",
    "    # Create QLattes icon element\n",
    "    qlattes_img_elem = BeautifulSoup('<img>', 'html.parser')\n",
    "    qlattes_img_elem['src'] = images_URLs['qlattesIconURL']\n",
    "    qlattes_img_elem['style'] = 'margin-bottom:-4px'\n",
    "    \n",
    "    annotation_dict['qlattes_img_elem'] = str(qlattes_img_elem)\n",
    "    \n",
    "    # Create Qualis labels annotations\n",
    "    issn_label = f', ISSN {pub_info[\"issn\"]}' if pub_info.get('issn') else ''\n",
    "    \n",
    "    if pub_info['qualisLabels']['qualis'] == 'N':\n",
    "        qualis_annot = f' Não classificado{issn_label}'\n",
    "    else:\n",
    "        qualis_annot = f' {pub_info[\"qualisLabels\"][\"qualis\"]}{issn_label}'\n",
    "        \n",
    "        # add Data source and base year\n",
    "        source = pub_info['qualisLabels']['source']\n",
    "        source_info = data_source_info[source]\n",
    "        \n",
    "        data_source_label = f'{source_info[\"label\"]} ({source_info[\"baseYear\"]})'\n",
    "        qualis_annot += f', fonte {data_source_label}'\n",
    "    \n",
    "    annotation_dict['qualis_annot'] = qualis_annot\n",
    "    \n",
    "    # add icon with link to Google Scholar\n",
    "    base_url = 'https://scholar.google.com/scholar?q='\n",
    "    title_param = f'intitle%3A%22{pub_info[\"title\"].replace(\" \", \"+\")}%22'\n",
    "    link_scholar = f'{base_url}{title_param}'\n",
    "    \n",
    "    annotation_dict['link_scholar'] = link_scholar\n",
    "    \n",
    "    # add icon with link to Scopus (if available)\n",
    "    if pub_info['qualisLabels'].get('linkScopus'):\n",
    "        annotation_dict['link_scopus'] = pub_info['qualisLabels']['linkScopus']\n",
    "    \n",
    "    return annotation_dict\n",
    "\n",
    "def persist_annotation_div():\n",
    "    alert_div = BeautifulSoup('<div>', 'html.parser')\n",
    "    \n",
    "    alert_div['class'] = 'main-content max-width min-width'\n",
    "    alert_div['id'] = 'annot-div'\n",
    "    \n",
    "    print('Alert div persisted!')\n",
    "    \n",
    "    return str(alert_div)\n",
    "\n",
    "\n",
    "def persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo):\n",
    "    annot_dict = {}\n",
    "    \n",
    "    issnLabel = f\", ISSN {pubInfo['issn']}\" if pubInfo['issn'] else pubInfo['issn']\n",
    "    if pubInfo['qualisLabels']['qualis'] == 'N':\n",
    "        qualisAnnot = f\"Não classificado{issnLabel}\"\n",
    "    else:\n",
    "        qualisAnnot = f\"{pubInfo['qualisLabels']['qualis']}{issnLabel}\"\n",
    "        \n",
    "        dataSourceLabel = dataSourceInfo[pubInfo['qualisLabels']['source']]['label']\n",
    "        baseYear = dataSourceInfo[pubInfo['qualisLabels']['source']]['baseYear']\n",
    "        dataSourceLabel += f\" ({baseYear})\"\n",
    "        qualisAnnot += f\", fonte {dataSourceLabel}\"\n",
    "        \n",
    "    annot_dict['qualisAnnot'] = qualisAnnot\n",
    "    \n",
    "    titleParam = f\"intitle:\\\"{pubInfo['title']}\\\"\"\n",
    "    linkScholar = f\"https://scholar.google.com/scholar?q={titleParam}\"\n",
    "    annot_dict['linkScholar'] = linkScholar\n",
    "    \n",
    "    if pubInfo['qualisLabels'].get('linkScopus'):\n",
    "        annot_dict['linkScopus'] = pubInfo['qualisLabels']['linkScopus']\n",
    "    \n",
    "    elem['annotation'] = annot_dict\n",
    "\n",
    "def persist_annotation_div(imagesURLs, visualizationURL):\n",
    "    alert_div_dict = {}\n",
    "    alert_div_dict['id'] = \"annot-div\"\n",
    "    return alert_div_dict\n",
    "\n",
    "def persist_annotation_message(imagesURLs, visualizationURL, recentUpdatesURL, pubCount):\n",
    "    annot_dict = {}\n",
    "    if pubCount > 0:\n",
    "        sChar = 's' if pubCount > 1 else ''\n",
    "        pubCountString = f\"anotou o Qualis de {pubCount} artigo{sChar} em periódico{sChar}  neste CV.\"\n",
    "    else:\n",
    "        pubCountString = \"não anotou nenhum artigo em periódico neste CV.\"\n",
    "        \n",
    "    annot_dict['pubCountString'] = pubCountString\n",
    "    annot_dict['visualizationURL'] = visualizationURL\n",
    "    annot_dict['recentUpdatesURL'] = recentUpdatesURL\n",
    "    return annot_dict\n",
    "\n",
    "def set_attributes(elem, attrs):\n",
    "    for key, value in attrs.items():\n",
    "        elem[key] = value\n",
    "\n",
    "def consolidate_qualis_data(qualisInfo):\n",
    "    pubData = []\n",
    "    pubDataYear = []\n",
    "    currYear = 0\n",
    "    \n",
    "    for qInfo in qualisInfo:\n",
    "        if currYear != qInfo['year']:\n",
    "            if currYear > 0:\n",
    "                pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "                pubDataYear = []\n",
    "            currYear = qInfo['year']\n",
    "        \n",
    "        pubDataItem = {\n",
    "            'issn': qInfo['issn'],\n",
    "            'title': qInfo['title'],\n",
    "            'pubName': qInfo['pubName'],\n",
    "            'qualis': qInfo['qualisLabels']['qualis'],\n",
    "            'baseYear': qInfo['qualisLabels']['baseYear'],\n",
    "            'jcr': qInfo['jcrData']['jcr'] if 'jcr' in qInfo['jcrData'] else 0,\n",
    "            'jcrYear': qInfo['jcrData']['baseYear'] if 'baseYear' in qInfo['jcrData'] else ''\n",
    "        }\n",
    "        pubDataYear.append(pubDataItem)\n",
    "    \n",
    "    if len(qualisInfo) > len(pubData):\n",
    "        pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "        \n",
    "    return pubData\n",
    "\n",
    "# Suponhamos que 'html_content' seja o conteúdo HTML em que as anotações serão inseridas.\n",
    "# html_content = ...\n",
    "\n",
    "# Criamos um objeto BeautifulSoup\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Aqui você pode utilizar os métodos acima para persistir as informações.\n",
    "# Exemplo:\n",
    "# elem = {}\n",
    "# persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML annotation and CV ID\n",
    "annot_html = \"<a href='some_url'>Annotated Data</a>\"\n",
    "cv_id = \"cv_123\"\n",
    "\n",
    "# Store annotation\n",
    "store_annotation_in_db(uri, user, password, annot_html, cv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def process_lattes_page_v1(name_link, images_urls, recent_updates_url, qualis_data, data_source_info): \n",
    "#     print(qualis_data)\n",
    "    \n",
    "#     # Inicializando o WebDriver\n",
    "#     driver_service = Service('path/to/chromedriver')\n",
    "#     driver = webdriver.Chrome(service=driver_service)\n",
    "#     driver.get(name_link['link'])\n",
    "    \n",
    "#     # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "#     visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "#     # Limpar cache de dados de Qualis\n",
    "#     qualis_data_cache = {}\n",
    "    \n",
    "#     # Annotate Lattes page com informações de Qualis\n",
    "#     lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "#     if lattes_info:\n",
    "#         await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "#     # Consolidar dados de publicação a partir das informações de Lattes\n",
    "#     pub_info = consolidate_qualis_data(lattes_info)\n",
    "#     print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "#     # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "#     conn = sqlite3.connect('lattes_data.db')\n",
    "#     cursor = conn.cursor()\n",
    "#     cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "#     existing_data = cursor.fetchall()\n",
    "    \n",
    "#     lattes_data_array = []\n",
    "    \n",
    "#     if existing_data:\n",
    "#         # Filtrar dados existentes para evitar duplicatas\n",
    "#         lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "#     # Adicionar dados de Lattes atuais ao array\n",
    "#     lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "#     # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "#     cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "    \n",
    "#     print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def annotate_published_articles_v2(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup):\n",
    "#     # Localizar o primeiro elemento de artigo publicado\n",
    "#     start_elem = soup.find(\"div\", id=\"artigos-completos\")\n",
    "\n",
    "#     # Retornar uma lista vazia se não houver nenhum artigo publicado no CV\n",
    "#     if start_elem is None:\n",
    "#         return []\n",
    "\n",
    "#     qualis_info = []\n",
    "\n",
    "#     # Encontrar todos os artigos publicados\n",
    "#     pub_elems = start_elem.find_all(\"div\", class_=\"artigo-completo\")\n",
    "\n",
    "#     for pub_elem in pub_elems:\n",
    "#         qualis_pub_info = {\n",
    "#             'year': None,\n",
    "#             'issn': '',\n",
    "#             'title': '',\n",
    "#             'pubName': '',\n",
    "#             'qualisLabels': '',\n",
    "#             'jcrData': {}\n",
    "#         }\n",
    "#         # Obter o ano de publicação\n",
    "#         year_span = pub_elem.find(\"span\", class_=\"informacao-artigo\", attrs={\"data-tipo-ordenacao\": \"ano\"})\n",
    "#         if year_span:\n",
    "#             qualis_pub_info['year'] = int(year_span.text)\n",
    "        \n",
    "#         # Obter dados de publicação\n",
    "#         pub_elem_data = pub_elem.find(\"div\", attrs={\"cvuri\": True})\n",
    "\n",
    "#         if pub_elem_data:\n",
    "#             # Obter informações do periódico\n",
    "#             pub_info_string = pub_elem_data['cvuri']\n",
    "#             # Para fins de simplicidade, omitimos a função escapeHtml já que não é relevante para o BeautifulSoup\n",
    "\n",
    "#             # Obter ISSN, título e nome do periódico\n",
    "#             # Detalhes de implementação podem variar, pois o exemplo original usa JavaScript para manipular atributos DOM\n",
    "#             issn = ''  # Implemente a lógica para extrair o ISSN\n",
    "#             title = ''  # Implemente a lógica para extrair o título\n",
    "#             pub_name = ''  # Implemente a lógica para extrair o nome do periódico\n",
    "            \n",
    "#             qualis_pub_info['issn'] = issn\n",
    "#             qualis_pub_info['title'] = title\n",
    "#             qualis_pub_info['pubName'] = pub_name.upper()\n",
    "\n",
    "#             # Obter classificação Qualis do periódico\n",
    "#             qualis_labels = await get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info)\n",
    "#             qualis_pub_info['qualisLabels'] = qualis_labels\n",
    "\n",
    "#             # Obter dados JCR (omitido neste exemplo; pode ser implementado conforme a necessidade)\n",
    "            \n",
    "#             # Adicionar informações ao vetor qualis_info\n",
    "#             qualis_info.append(qualis_pub_info)\n",
    "            \n",
    "#     return qualis_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
