{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Explorar dados dos currículos Lattes para<br /> propor modelo de Grafo para análises futuras </center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa – Fiocruz Ceará\n",
    "\n",
    "**Introdução**\n",
    "\n",
    "A análise de Grafos permite obter insights como produtos de análises em contextos da realidade com base em modelos capazes de lidar dados heterogêneos e relações complexas.\n",
    "\n",
    "Neste trabalho propomos uma análise dos dados de pesquisa acadêmica tendo como fonte de dados os currículo Lattes de servidores da unidade Fiocruz Ceará.\n",
    "\n",
    "**Objetivo geral:**\n",
    "\n",
    "    Explorar dados dos currículos de servidores da Fiocruz Ceará.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "\n",
    "    1. Extrair dados dos currículos;\n",
    "    2. Propor modelo de grafo para análises futuras;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 0: Preparar e Testar Ambiente</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "        \n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_amb():\n",
    "    ## Visualizar versões dos principais componentes\n",
    "    import os\n",
    "    import pip\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # !pip3 install shutup\n",
    "    # import shutup; shutup.please()\n",
    "    \n",
    "    pyVer      = sys.version\n",
    "    pipVer     = pip.__version__\n",
    "    \n",
    "    print('\\nVERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT')\n",
    "    print('Interpretador em uso:', sys.executable)\n",
    "    print('    Ambiente ativado:',os.environ['CONDA_DEFAULT_ENV'])\n",
    "    print('     Python: '+pyVer, '\\n        Pip:', pipVer,'\\n'\n",
    "         )\n",
    "\n",
    "    !nvcc -V\n",
    "\n",
    "def try_gpu():\n",
    "    print('\\nVERSÕES DO PYTORCH E GPU DISPONÍVEIS')\n",
    "    try:\n",
    "        import torch\n",
    "        print('    PyTorch:',torch.__version__)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Dispositivo:',device)\n",
    "        print('Disponível :',device,torch.cuda.is_available(),' | Inicializado:',torch.cuda.is_initialized(),'| Capacidade:',torch.cuda.get_device_capability(device=None))\n",
    "        print('Nome GPU   :',torch.cuda.get_device_name(0),'         | Quantidade:',torch.cuda.device_count(),'\\n')\n",
    "    except Exception as e:\n",
    "        print('Erro ao configurar a GPU:',e,'\\n')\n",
    "\n",
    "def try_folders(drives,pastas,pastasraiz):\n",
    "    import os\n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    caminho_testado = drive+i+j\n",
    "                    if os.path.isfile(caminho_testado+'/chromedriver/chromedriver.exe'):\n",
    "                        print(f\"Listando arqivos em: {caminho_testado}\")\n",
    "                        print(os.listdir(caminho_testado))\n",
    "                        caminho = caminho_testado+'/'\n",
    "                except:\n",
    "                    caminho=''\n",
    "                    print('Não foi possível encontrar uma pasta de trabalho')\n",
    "    return caminho\n",
    "\n",
    "def try_browser(raiz):\n",
    "    print('\\nVERSÕES DO BROWSER E DO CHROMEDRIVER INSTALADAS')\n",
    "    from selenium import webdriver\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    try:\n",
    "        # Caminho para o seu chromedriver\n",
    "        driver_path=raiz+'chromedriver/chromedriver.exe'\n",
    "        print(driver_path)\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        str1 = driver.capabilities['browserVersion']\n",
    "        str2 = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]\n",
    "        print(f'     Versão do browser: {str1}')\n",
    "        print(f'Versão do chromedriver: {str2}')\n",
    "        driver.quit()\n",
    "\n",
    "        if str1[0:3] != str2[0:3]: \n",
    "            print(\"Versões incompatíveis, atualizar chromedriver!\")\n",
    "            print('  Baixar versão atualizada do Chromedriver em:')\n",
    "            print('  https://googlechromelabs.github.io/chrome-for-testing/#stable')\n",
    "            print('     Ex. Versão 116 PARA WINDOWS:')\n",
    "            print('\t    https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/win64/chromedriver-win64.zip')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def try_chromedriver(caminho):\n",
    "    try:\n",
    "        import os\n",
    "        os.listdir(caminho)\n",
    "    except Exception as e:\n",
    "        raiz=caminho\n",
    "\n",
    "    finally:\n",
    "        print(raiz)\n",
    "    return raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Erro ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "\n",
    "    caminho = drive+':/'+usuario+pastaraiz+'/'\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    import os\n",
    "\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema operacional Windows\n",
      "Drive em uso C\n",
      "Pasta armazenagem local C:/Users/marco/fioce/\n",
      "\n",
      "\n",
      "VERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT\n",
      "Interpretador em uso: c:\\Users\\marco\\.conda\\envs\\beakerx\\python.exe\n",
      "    Ambiente ativado: beakerx\n",
      "     Python: 3.11.2 | packaged by Anaconda, Inc. | (main, Mar 27 2023, 23:35:04) [MSC v.1916 64 bit (AMD64)] \n",
      "        Pip: 23.2.1 \n",
      "\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Jul_11_03:10:21_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.128\n",
      "Build cuda_12.2.r12.2/compiler.33053471_0\n",
      "\n",
      "VERSÕES DO PYTORCH E GPU DISPONÍVEIS\n",
      "    PyTorch: 2.0.1+cu118\n",
      "Dispositivo: cuda\n",
      "Disponível : cuda True  | Inicializado: False | Capacidade: (7, 5)\n",
      "Nome GPU   : NVIDIA GeForce RTX 2060          | Quantidade: 1 \n",
      "\n",
      "\n",
      "VERSÕES DO BROWSER E DO CHROMEDRIVER INSTALADAS\n",
      "C:/Users/marco/fioce/chromedriver/chromedriver.exe\n",
      "     Versão do browser: 117.0.5938.92\n",
      "Versão do chromedriver: 117.0.5938.88\n"
     ]
    }
   ],
   "source": [
    "pastaraiz = 'fioce'\n",
    "caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "try_amb()\n",
    "try_gpu()\n",
    "try_browser(caminho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 1: Extrair DOM para objeto Soup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Importar, conectar e gerar driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from flask import render_template_string\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "delay = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install py2neo --trusted-host pypi.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Função 1: Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    \n",
    "\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "    driver_path=caminho+'chromedriver/chromedriver.exe'\n",
    "    print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)    \n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "\n",
    "    # return browser, url, session_id\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Manipular HTML para chegar ao currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_name(driver, delay, NOME):\n",
    "    '''\n",
    "    Função 2: passa o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def achar_busca(driver, delay):\n",
    "    '''\n",
    "    Função 3: clica no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = driver.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               #expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               #logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_curriculum(driver,elm_vinculo):\n",
    "    link_nome     = achar_busca(driver, delay)\n",
    "    window_before = driver.current_window_handle\n",
    "\n",
    "    limite = 5\n",
    "    if str(elm_vinculo) == 'nan':\n",
    "        print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "        raise Exception\n",
    "    print('Vínculo encontrado no currículo de nome:',elm_vinculo.text)\n",
    "\n",
    "    ## Clicar no botão abrir currículo e mudar de aba\n",
    "    try:\n",
    "        ## Aguarda, encontra, clica em buscar nome\n",
    "        link_nome    = achar_busca(driver, delay)\n",
    "    except Exception as e:\n",
    "        print('Erro')\n",
    "        print(e)\n",
    "        \n",
    "    if link_nome.text == None:\n",
    "        xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "        # 'Stale file handle'\n",
    "        print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "        #    expected_ex_type=ZeroDivisionError, \n",
    "        wait_ms=200,\n",
    "        limit=limite, \n",
    "        #    logger=logger, \n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "    try:\n",
    "        ActionChains(driver).click(link_nome).perform()\n",
    "    except:\n",
    "        print(f'Currículo não encontrado.')\n",
    "\n",
    "    retry(WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "        #    expected_ex_type=ZeroDivisionError, \n",
    "        wait_ms=200,\n",
    "        limit=limite, \n",
    "        #    logger=logger, \n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "    # Clicar botão para abrir o currículo\n",
    "    btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "    ## Gerenciamento das janelas abertas no browser\n",
    "    WebDriverWait(driver, delay).until(EC.number_of_windows_to_be(2))\n",
    "    window_after = driver.window_handles\n",
    "    new_window   = [x for x in window_after if x != window_before][0]\n",
    "    driver.switch_to.window(new_window)\n",
    "\n",
    "    # Pega o código fonte da página\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    # Usa BeautifulSoup para analisar\n",
    "    soup = BeautifulSoup(page_source, 'html.parser') \n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_terms(NOME, instituicao, unidade, termo, driver, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    \n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(driver)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            ## TO-DO inserir a crítica para o 'Stale file handle'\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(driver)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(driver)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, driver\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # driver.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, driver\n",
    "    \n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "def is_abreviation(substring):\n",
    "    pattern = r'(?: [a-zA-Z]\\.)|(?: [a-zA-Z] \\.)'\n",
    "    if re.search(pattern, substring):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_surname(substring):\n",
    "    return not is_abreviation(substring) and substring.endswith(', ')\n",
    "\n",
    "def compose_full_name(surname, parts, marker):\n",
    "    full_name = \"\"\n",
    "    for part in parts:\n",
    "        full_name += part + marker\n",
    "    full_name += ' '+surname\n",
    "    return full_name.strip()\n",
    "\n",
    "def split_authors(string, verbose=False):\n",
    "    authors_names = []\n",
    "    \n",
    "    m1a = \"; \"\n",
    "    m1b = \", \"\n",
    "    \n",
    "    if m1a in string and (string.count(m1a) <= string.count(m1b) or m1b not in string):\n",
    "        marker = m1a\n",
    "        authors_names = string.split(marker)\n",
    "        return [x.strip() for x in authors_names]\n",
    "    else:\n",
    "        marker = m1b\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Selected marker: \"{marker}\"')\n",
    "\n",
    "    parts_list = string.split(marker)\n",
    "    name    = \"\"\n",
    "    surname = \"\"\n",
    "    \n",
    "    for part in parts_list:\n",
    "        if is_abreviation(part):\n",
    "            classification = 'Abrev'\n",
    "            name += part + marker\n",
    "        else:\n",
    "            classification = 'Name'\n",
    "            if is_surname(part) or surname == \"\":\n",
    "                classification = 'SOBRENOME'\n",
    "                if surname:\n",
    "                    full_name = compose_full_name(surname.strip(), name.split(marker), ', ')\n",
    "                    if full_name not in authors_names:\n",
    "                        authors_names.append(full_name)\n",
    "                surname = part\n",
    "                name = part + marker\n",
    "            else:\n",
    "                name = part + marker\n",
    "        if verbose:\n",
    "            # print(f'Quantidade de m1a: {string.count(m1a)} \\nQuantidade de m1b: {string.count(m1b)}')\n",
    "            print(f'Autor: {part:40} | Forma: {classification}')\n",
    "\n",
    "        authors_names.append(part.strip(marker).strip())\n",
    "     \n",
    "    return [x.strip() for x in authors_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extrair_indices(layout_celula):\n",
    "    indice = layout_celula.select_one('div.layout-cell-pad-5.text-align-right')\n",
    "    valor = layout_celula.select_one('div.layout-cell.layout-cell-9 div.layout-cell-pad-5')\n",
    "    \n",
    "    if indice and valor:\n",
    "        valores_extraidos = valor.text.split('<br class=\"clear\">') if '<br class=\"clear\">' in valor.text else valor.text.split('\\n\\t\\t\\n\\t')\n",
    "        return indice.text, valores_extraidos\n",
    "    return None, None\n",
    "\n",
    "def extrair_secao(title_wrapper):\n",
    "    tags_relevantes = ['h1', 'ul', 'b']\n",
    "    tags_encontradas = []\n",
    "    for tag in tags_relevantes:\n",
    "        elemento_encontrado = title_wrapper.select_one(tag)\n",
    "        if elemento_encontrado:\n",
    "            tags_encontradas.append((tag, elemento_encontrado))\n",
    "    tags_ordenadas = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def encontrar_subchave(title_wrapper):\n",
    "    tags_relevantes  = ['ul', 'a', 'b']\n",
    "    tags_encontradas = [(tag, title_wrapper.find(tag)) for tag in tags_relevantes]\n",
    "    tags_ordenadas   = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def extrair_titulo(title_wrapper):\n",
    "    inst_back_texts = {}\n",
    "    inst_back_elements = title_wrapper.find_all('div', class_='inst_back')\n",
    "    for index, inst_back in enumerate(inst_back_elements):\n",
    "        b_tag = inst_back.find('b')\n",
    "        if b_tag:\n",
    "            inst_back_texts[index] = b_tag.text.strip()\n",
    "    return inst_back_texts if inst_back_texts else None\n",
    "\n",
    "def extrair_wraper(soup, json_data):\n",
    "    title_wrappers = soup.select('div.layout-cell-pad-main div.title-wrapper')\n",
    "    for title_wrapper in title_wrappers:\n",
    "        section_name = extrair_secao(title_wrapper)\n",
    "        if section_name:\n",
    "            section_name = section_name.text.strip()\n",
    "            \n",
    "            titles = extrair_titulo(title_wrapper)\n",
    "            json_data[\"Properties\"][section_name] = {}\n",
    "            \n",
    "            if titles:\n",
    "                for index, title in titles.items():\n",
    "                    json_data[\"Properties\"][section_name][title] = {}\n",
    "            \n",
    "            layout_cells = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "            for layout_celula in layout_cells:\n",
    "                indice, valores_extraidos = extrair_indices(layout_celula)\n",
    "                if indice and valores_extraidos:\n",
    "                    if titles and indice in titles.values():\n",
    "                        if len(titles) > 1:\n",
    "                            for title in titles.values():\n",
    "                                if title.strip() in indice:\n",
    "                                    json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                        else:\n",
    "                            title = list(titles.values())[0]\n",
    "                            json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                    else:\n",
    "                        json_data[\"Properties\"][section_name][indice] = valores_extraidos\n",
    "    return json_data\n",
    "\n",
    "def imprimir_informacoes(dados_json, nome_no, indent=0):\n",
    "    indentation = '    ' * indent  # Calculating the current indentation level\n",
    "\n",
    "    if dados_json and nome_no and dados_json.get(nome_no):\n",
    "        if indent == 0:  # Logging node-level information only at the root\n",
    "            logging.info(f\"{indentation}Node: {nome_no}\")\n",
    "            logging.info(f\"{indentation}Total keys extracted: {len(dados_json[nome_no].keys())}\")\n",
    "        \n",
    "        for key in dados_json[nome_no].keys():\n",
    "            logging.info(f\"{indentation}{key.strip() if key else ''}\")\n",
    "\n",
    "            if isinstance(dados_json[nome_no][key], dict):  # Check for nested dictionaries\n",
    "                # Recursive call to handle nested dictionaries\n",
    "                imprimir_informacoes(dados_json[nome_no], key, indent + 1)\n",
    "            else:\n",
    "                logging.info(f\"{indentation}    Values: {dados_json[nome_no][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 2: Exportar DOM para Dicionários</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalization of the DOM extraction from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digital representation of the HTML DOM (Document Object Model) in question follow a consistent class-based structure, where the division of information into various classes within 'div' elements serves as an important taxonomy for organizing and categorizing the information.\n",
    "\n",
    "The nested structure predominantly consists of HTML div elements differentiated by their CSS classes. The div elements appear in a tree-like organization, hierarchically grouped under recursive presence of the div elements within the class 'title-wrapper', followed by the div elements marked with 'layout-cell' and hierarquically organized until reaching the more detailed levels where the data of interest is, contained in the classes such 'data-cell', 'text-align-right' or 'layout-cell-pad-5' and tags like 'a', 'b'.\n",
    "\n",
    "The extraction of data from this intricate nested architecture necessitates a recursive methodology that maintains the hierarchical fidelity of the original data. Thus, one approach to transforming this data into a structured JSON object would be to employ depth-first search (DFS) algorithms to traverse through each node in this tree-like structure. Each traversal would examine the class attributes and potentially the text content within each div. \n",
    "\n",
    "**Formalization:**\n",
    "\n",
    "In the formal language of computational theory, let \\( T \\) be the DOM tree with each node \\( n \\) containing a list of attributes \\( A(n) \\) and a text content \\( C(n) \\). Let \\( JSON(n) \\) be the JSON representation of the node \\( n \\). The recursive function to extract data can be described as:\n",
    "\n",
    "\n",
    "JSON(n) = \n",
    "\\begin{cases} \n",
    "\\{ \"type\": A(n), \"content\": C(n), \"children\": \\{ JSON(c) \\,|\\, c \\in \\text{children of } n \\} \\} & \\text{if } n \\text{ has children} \\\\\n",
    "\\{ \"type\": A(n), \"content\": C(n) \\} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "**Python implementation:**\n",
    " \n",
    "In terms of practical implementation, Python's Beautiful Soup library can be particularly effective for this task, allowing for a relatively straightforward traversal of each div element to construct the JSON object.\n",
    "\n",
    "The end result would be a JSON object where each entry corresponds to a 'div' element in the original HTML structure, represented by a dictionary containing the attributes and content of the div, and potentially another dictionary (or list of dictionaries) representing any nested child div elements. This would effectively capture the data within each div while maintaining the hierarchical structure of the original HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listando arqivos em: C:/Users/marco/fioce\n",
      "['.git', '.gitignore', 'assets', 'chromedriver', 'csv', 'doc', 'fig', 'json', 'output', 'scripts', 'source', 'utils', 'xml_zip']\n",
      "Pasta para os xml já existe!\n",
      "Pasta para os CSV já existe!\n",
      "Pasta para os JSON já existe!\n",
      "Pasta para figuras já existe!\n",
      "Pasta para saídas já existe!\n",
      "\n",
      "Caminho da pasta raiz C:/Users/marco/fioce/\n",
      "Caminho arquivos  XML C:/Users/marco/fioce/xml_zip/\n",
      "Caminho arquivos JSON C:/Users/marco/fioce/json/\n",
      "Caminho arquivos  CSV C:/Users/marco/fioce/csv/\n",
      "Caminho para  figuras C:/Users/marco/fioce/fig/\n",
      "Pasta arquivos saídas C:/Users/marco/fioce/output/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:/Users/marco/fioce/xml_zip/',\n",
       " 'C:/Users/marco/fioce/csv/',\n",
       " 'C:/Users/marco/fioce/json/',\n",
       " 'C:/Users/marco/fioce/fig/',\n",
       " 'C:/Users/marco/fioce/',\n",
       " 'C:/Users/marco/fioce/output/')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drives=['C:/Users/','E:/','./home/']\n",
    "pastas=['marcos.aires/', 'marco/']\n",
    "pastasraiz=['kgfioce','fioce']\n",
    "caminho=try_folders(drives,pastas,pastasraiz)\n",
    "\n",
    "preparar_pastas(caminho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando com o servidor do CNPq...\n",
      "C:/Users/marco/fioce/chromedriver/chromedriver.exe\n",
      "1 resultados para ['Raimir Holanda Filho']\n",
      "Vínculo encontrado no currículo de nome: Raimir Holanda Filho\n",
      "Total de caracteres extraídos:  91492\n",
      "Quantidade extraída de linhas:   3610\n"
     ]
    }
   ],
   "source": [
    "driver = connect_driver(caminho)\n",
    "NOME = ['Raimir Holanda Filho']\n",
    "fill_name(driver, delay, NOME)\n",
    "\n",
    "limite=3\n",
    "instituicao = 'Fundação Oswaldo Cruz'\n",
    "unidade     = 'Fiocruz Ceará'\n",
    "termo       = 'Ministerio da Saude'\n",
    "\n",
    "elm_vinculo, np.NaN, np.NaN, np.NaN, driver = find_terms(NOME, instituicao, unidade, termo, driver, delay, limite)\n",
    "soup = open_curriculum(driver,elm_vinculo)\n",
    "\n",
    "caracteres = len(soup.text)\n",
    "linhas = len(soup.text.split('\\n'))\n",
    "print(f'Total de caracteres extraídos: {caracteres:6d}')\n",
    "print(f'Quantidade extraída de linhas: {linhas:6d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair DOM para Dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def extract_data_from_cell(cell):\n",
    "    \"\"\"\n",
    "    Extracts relevant text data from a layout cell.\n",
    "    \"\"\"\n",
    "    texts = cell.stripped_strings\n",
    "    list_texts = [x.replace('\\n\\t\\t\\t\\t\\t\\t\\t',' ') for x in texts]\n",
    "    list_texts = [x.replace('\\t\\t\\t\\t\\t\\t',' ') for x in list_texts]\n",
    "    list_texts = [x.replace('\\t\\t\\t\\t\\t',' ') for x in list_texts]\n",
    "    list_texts = [x.replace('\\n',' ') for x in list_texts]\n",
    "    \n",
    "    return list_texts\n",
    "\n",
    "def parse_resume(soup):\n",
    "    result_dict = {}\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None \n",
    "    json_data['name'] = node_name\n",
    "\n",
    "    parag_elements = soup.find_all('p')\n",
    "    for elem in parag_elements:\n",
    "        # Retrieve the class attribute, returning None if not found\n",
    "        class_name = elem.get('class', [None])[0]\n",
    "        text_content = elem.text\n",
    "        \n",
    "        # Update the dictionary\n",
    "        if class_name:  # Only update if class_name is not None\n",
    "            result_dict[class_name] = text_content\n",
    "\n",
    "        json_data[\"Properties\"] = result_dict\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def parse_cita_artigos(soup):\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    json_data['name'] = node_name\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    div_cita = producao_bibliografica_div.find_patent()\n",
    "    producoes = []\n",
    "    for artigo_div in div_cita.find_all('div', {'class': 'cita-artigos'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        \n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['OutrasProduções'] = producoes\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def parse_jcr_articles(soup):\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    json_data['name'] = node_name\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    producoes = []\n",
    "    for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        \n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        # str_autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        # autores = split_authors(str_autores)\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        artigo_dict['ano']     = ano\n",
    "        # artigo_dict['autores'] = autores\n",
    "        artigo_dict['revista'] = revista\n",
    "        artigo_dict['titulo']  = titulo\n",
    "        artigo_dict['jcr']     = jcr\n",
    "        artigo_dict['doi']     = doi\n",
    "        \n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = producoes\n",
    "   \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(soup.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag\n",
    "\n",
    "def traverse(soup, parent_dict, current_section=None, root_properties=None):\n",
    "    \"\"\"\n",
    "    Traverses through the soup object recursively and populates the dictionary.\n",
    "    root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "    current_section: the current section being processed, allows to set subsections.\n",
    "    \"\"\"\n",
    "    section = None\n",
    "    subsection = None\n",
    "    section_dict = {}\n",
    "    subsection_dict = {}\n",
    "    \n",
    "    # Existing code for name extraction\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    if node_name is not None:\n",
    "        parent_dict['name'] = node_name\n",
    "\n",
    "    # Existing code for paragraph extraction\n",
    "    parag_elements = soup.find_all('p')\n",
    "    for elem in parag_elements:\n",
    "        class_name = elem.get('class', [None])[0]\n",
    "        text_content = elem.get_text()\n",
    "        if class_name:\n",
    "            parent_dict[class_name] = text_content\n",
    "\n",
    "    # Initialization of root_properties\n",
    "    if root_properties is None:\n",
    "        root_properties = parent_dict.setdefault('Properties', {})\n",
    "    \n",
    "    for child in soup.children:\n",
    "        if isinstance(child, Tag):\n",
    "            ## Segmento para extrair os dados de JCR dos artigos completos em periódicos\n",
    "            # if child.get('id') == 'artigos-completos':\n",
    "            #     section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")          \n",
    "            #     if section_elem:\n",
    "            #         section = section_elem.get_text().strip()\n",
    "            #         section_dict = root_properties.setdefault(section, {})\n",
    "\n",
    "            #     subsection_elem = child.find('b')\n",
    "            #     if subsection_elem:\n",
    "            #         subsection = subsection_elem.get_text().strip()\n",
    "            #         if section:  \n",
    "            #             subsection_dict = section_dict.setdefault(subsection, {})\n",
    "\n",
    "            #     jcr_articles_dict = parse_jcr_articles(soup)\n",
    "            #     if jcr_articles_dict:\n",
    "            #         if subsection:  \n",
    "            #             subsection_dict.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "            ## Segmento para extrair até Produções (não tem subseções)\n",
    "            if \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)\n",
    "\n",
    "                    sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "                    if sibling_cell:\n",
    "                        cell_values = extract_data_from_cell(sibling_cell)\n",
    "\n",
    "                        if current_section:\n",
    "                            root_properties[current_section][cell_key] = cell_values\n",
    "                        else:\n",
    "                            root_properties[cell_key] = cell_values\n",
    "\n",
    "            ## Segmento para extrair de Produções até o final\n",
    "            elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                section_elem = child.findParent().findParent().find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                if section_elem:\n",
    "                    section = section_elem.get_text().strip()\n",
    "                    section_dict = root_properties.setdefault(section, {})\n",
    "                elif section_elem == None:\n",
    "                    list_sub = ['Dissertação de mestrado','Tese de doutorado']\n",
    "                    subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        if subsection_elem.get_text().strip() in list_sub:\n",
    "                            section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                    if section_elem:\n",
    "                        section = section_elem.get_text().strip()\n",
    "                        section_dict = root_properties.setdefault(section, {})                \n",
    "                else:\n",
    "                    print('Erro ao seção')     \n",
    "\n",
    "                subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                if subsection_elem:\n",
    "                    subsection = subsection_elem.get_text().strip()\n",
    "                elif subsection_elem == None:\n",
    "                    subsection_elem = child.findParent().find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        subsection = subsection_elem.get_text().strip()                \n",
    "                else:\n",
    "                    print('Erro ao definir subseção')\n",
    "                \n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)    \n",
    "\n",
    "                if section:\n",
    "                    current_section_dict = section_dict.get(current_section, {})\n",
    "                else:\n",
    "                    current_section_dict = root_properties.get(current_section, {})\n",
    "                    \n",
    "                # Converte par dictionário se for lista\n",
    "                if isinstance(current_section_dict, list):\n",
    "                    current_section_dict = {i: item for i, item in enumerate(current_section_dict)}\n",
    "                    if section:\n",
    "                        section_dict[current_section] = current_section_dict\n",
    "                    else:\n",
    "                        root_properties[current_section] = current_section_dict\n",
    "                        \n",
    "                subsection_dict = current_section_dict.setdefault(subsection, {})\n",
    "                \n",
    "                # Retira valores da próxima div irmã que contém classe layout-cell-11\n",
    "                sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "                if sibling_cell:\n",
    "                    cell_values = extract_data_from_cell(sibling_cell)\n",
    "                    subsection_dict[cell_key] = cell_values\n",
    "\n",
    "            # Code for headers\n",
    "            title = child.find(\"h1\")\n",
    "            if title:\n",
    "                current_section = title.get_text().strip()\n",
    "                new_dict = {}\n",
    "                if section:\n",
    "                    section_dict[current_section] = new_dict\n",
    "                else:\n",
    "                    root_properties[current_section] = new_dict\n",
    "                \n",
    "                traverse(child, new_dict, current_section, root_properties)\n",
    "            else:\n",
    "                traverse(child, parent_dict, current_section, root_properties)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_empty_list(value):\n",
    "#     return isinstance(value, list) and not value\n",
    "\n",
    "# def convert_to_hashable(value):\n",
    "#     if is_empty_list(value):\n",
    "#         return None\n",
    "#     elif isinstance(value, list):\n",
    "#         return tuple(value)\n",
    "#     return value\n",
    "\n",
    "# def print_dictionary(dictionary, indent=0):\n",
    "#     \"\"\"\n",
    "#     Recursively prints nested dictionaries.\n",
    "\n",
    "#     Parameters:\n",
    "#         dictionary (dict): The dictionary to print.\n",
    "#         indent (int): The current indentation level for nested dictionaries.\n",
    "#     \"\"\"\n",
    "#     for key, value in dictionary.items():\n",
    "#         if isinstance(value, dict):\n",
    "#             print('  ' * indent + str(key) + ': ')\n",
    "#             print_dictionary(value, indent + 1)\n",
    "#         else:\n",
    "#             print('  ' * indent + str(key) + ': ' + str(value))\n",
    "\n",
    "# def append_to_key(dictionary, key, value):\n",
    "#     \"\"\"Appends a value to a list associated with a given key in a dictionary.\n",
    "#     If the key does not exist, a new list is created and the value is appended to it.\n",
    "#     \"\"\"\n",
    "#     if key in dictionary:\n",
    "#         if isinstance(dictionary[key], list):\n",
    "#             dictionary[key].append(value)\n",
    "#         else:\n",
    "#             dictionary[key] = [dictionary[key], value]\n",
    "#     else:\n",
    "#         dictionary[key] = [value]\n",
    "\n",
    "# def find_sibling_cell(cell):\n",
    "#     # Your logic here to find sibling cells\n",
    "#     return None\n",
    "\n",
    "# def find_main_cell(soup):\n",
    "#     main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     return main_cell\n",
    "\n",
    "# def traverse(child, parent_dict, current_section=None, root_properties=None):\n",
    "#     if root_properties is None:\n",
    "#         root_properties = {}\n",
    "    \n",
    "#     new_dict = {\"Label\": None, \"Properties\": {}}\n",
    "#     cell_key = convert_to_hashable(extract_data_from_cell(child))\n",
    "    \n",
    "#     if cell_key is not None:\n",
    "#         cell_values = extract_data_from_cell(find_sibling_cell(child))\n",
    "#         parent_dict[cell_key] = convert_to_hashable(cell_values)\n",
    "        \n",
    "#     for sub_child in child.find_all_next():\n",
    "#         traverse(sub_child, new_dict, current_section, root_properties)\n",
    "        \n",
    "#     root_properties[current_section] = merge_properties_ordered(root_properties.get(current_section, {}), new_dict)\n",
    "#     return root_properties\n",
    "\n",
    "# def parse_soup(soup):\n",
    "#     main_cell = find_main_cell(soup)\n",
    "#     merged_dict = {\"Label\": \"Person\", \"Properties\": {}}\n",
    "#     if main_cell:\n",
    "#         parsed_data = traverse(main_cell, merged_dict)\n",
    "#         return merge_properties_ordered(merged_dict, parsed_data)\n",
    "#     return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_properties_ordered(dic1, dic2):\n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    \"\"\"Merges the 'Properties' subdictionaries from dic1 and dic2 into a new ordered dictionary.\"\"\"\n",
    "    merged_dict = OrderedDict()\n",
    "    merged_dict['Label'] = 'Person'\n",
    "    merged_dict['name'] = dic1.get('name', '')\n",
    "    merged_dict['Properties'] = OrderedDict()\n",
    "\n",
    "    # Explicitly extract the 'Properties' subdictionaries from dic1 and dic2\n",
    "    prop1 = dic1.get('Properties', {})\n",
    "    prop2 = dic2.get('Properties', {})\n",
    "    \n",
    "    # Iterate over the keys of dic1 first to preserve their order\n",
    "    for key in prop1.keys():\n",
    "        merged_dict['Properties'][key] = prop1[key]\n",
    "\n",
    "    # Next, iterate over the keys of dic2, updating values and potentially inserting new keys\n",
    "    for key in prop2.keys():\n",
    "        val1 = merged_dict['Properties'].get(key)\n",
    "        val2 = prop2[key]\n",
    "        \n",
    "        # Merge logic: concatenate if both are lists, otherwise use the value from dic2\n",
    "        if isinstance(val1, list) and isinstance(val2, list):\n",
    "            merged_dict['Properties'][key] = val1 + val2\n",
    "        else:\n",
    "            merged_dict['Properties'][key] = val2\n",
    "\n",
    "    return merged_dict\n",
    "\n",
    "def parse_soup(soup):\n",
    "    resume_dict   = parse_resume(soup)\n",
    "    articles_dict = {\"Label\": \"Person\", \"name\":{}, \"Properties\": {}}\n",
    "    articles_dict = parse_jcr_articles(soup)\n",
    "    merged_dict   = merge_properties_ordered(resume_dict, articles_dict)\n",
    "\n",
    "    main_cell     = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    generic_dict = {\"Label\": \"Person\", \"Properties\": {}}\n",
    "    if main_cell:\n",
    "        traverse(main_cell, generic_dict)\n",
    "\n",
    "    merged_dict = merge_properties_ordered(merged_dict, generic_dict)\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['Label', 'name', 'Properties'])\n",
      "odict_keys(['resumo', 'Produções', '', 'Identificação', 'Endereço', 'Formação acadêmica/titulação', 'Pós-doutorado', 'Formação Complementar', 'Atuação Profissional', 'Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento', 'Revisor de periódico', 'Revisor de projeto de fomento', 'Áreas de atuação', 'Idiomas', 'Produção bibliográfica', 'Bancas', 'Eventos', 'Orientações', 'Orientações e supervisões em andamento', 'Orientações e supervisões concluídas', 'Inovação'])\n",
      "dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "data_dict = parse_soup(soup)\n",
    "print(data_dict.keys())\n",
    "print(data_dict['Properties'].keys())\n",
    "print(data_dict['Properties']['Orientações'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('resumo',\n",
       "              'Pós-Doutor em Computação pela Sorbonne Université-Pierre et Marie Curie (França - 2020). Doutor em Ciência da Computação pela Universitat Politecnica de Catalunya (Espanha - 2005). Atualmente é professor titular da Universidade de Fortaleza - UNIFOR na qual é membro permanente dos programas de pós-graduação em Informática Aplicada (mestrado e doutorado) e do mestrado profissional em administração. Possui mais de 100 trabalhos publicados em conferências e periódicos nacionais e internacionais. Tem experiência na área de Ciência da Computação, com ênfase em Ciência de Dados e Teleinformática, atuando principalmente nos seguintes temas: Redes Complexas, Grafos de Conhecimento, Aprendizagem de Máquina, Internet das Coisas, Segurança e Blockchain. (Texto informado pelo autor)\\n'),\n",
       "             ('Produções',\n",
       "              {'Livros publicados/organizados ou edições': {'1.': ['HOLANDA FILHO, RAIMIR',\n",
       "                 '; CUNHA, G. H. M. (Org.) . Riscos e Fraudes no Setor Público: Ensaios e Estudos de Casos para o Estado do Ceará - Volume 2. 1. ed. Fortaleza: Edições IPC, 2023. v. 1. 165p .'],\n",
       "                '2.': ['CUNHA, G. H. M. (Org.) ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '(Org.) . Riscos e Fraudes no Setor Público: ensaios e estudos de casos para o estado do Ceará.. 1. ed. Fortaleza: Edições IPC, 2022. v. 1. 250p .']},\n",
       "               'Capítulos de livros publicados': {'1.': ['HOLANDA FILHO, RAIMIR',\n",
       "                 '. Governo Digital e Inovação Pública. In: Ernesto Saboia de Figueiredo Júnior, Luís Eduardo Menezes Lima, Eloísa Maia Vidal. (Org.). Cidadania e Controle Social das Contas Públicas: democracia, direitos e participação.. 1ed.Fortaleza: Edições IPC, 2023, v. 1, p. 90-94.'],\n",
       "                '2.': ['PINHEIRO, P. G. C. D. ; PINHEIRO, L. I. ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; PEREIRA, M. L. D. ;',\n",
       "                 'Pinheiro, Placido R.',\n",
       "                 '; SANTIAGO, P. J. L. ; NUNES, R. C. . An Application of Machine Learning in the Early Diagnosis of Meningitis. In: Anna Visvizi, Orlando Troisi, Mara Grimaldi. (Org.). Springer Proceedings in Complexity. 1ed.: Springer, Cham, 2023, v. 1, p. 97-106.'],\n",
       "                '3.': ['HOLANDA FILHO, RAIMIR',\n",
       "                 '; Brito, Wellington Alves ; SOUSA, D. C. B. ; Ribeiro, Victor ; CHAVES, J. L. M. S. ; SA, E. L. . A Fault-Tolerant IoT Solution for Solid Waste Collection. In: Leonard Barolli. (Org.). Lecture Notes in Networks and Systems. 1ed.Suiça: Springer, Cham, 2023, v. 661, p. 473-484.'],\n",
       "                '4.': ['LIMA, S. M. ; MATOS, P. R. F. ; JESUS FILHO, J. ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; CUNHA, G. H. M. . Modelagem da Eficiência do Gasto com Medicamentos nos Municípios do Ceará. In: Cunha, George Henrique de Moura; Holanda Filho, Raimir. (Org.). Riscos e Fraudes no Setor Público: Ensaios e Estudos de Casos para o Estado do Ceará - Volume 2. 1ed.Fortaleza: Edições IPC, 2023, v. 2, p. 29-79.'],\n",
       "                '5.': ['CUNHA, G. H. M. ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; BRITO, R. W. ; AGUIAR JUNIOR, J. C. . Acumulação de Cargos Públicos. In: Cunha, George Henrique de Moura; Holanda Filho, Raimir. (Org.). Riscos e Fraudes no Setor Público: Ensaios e Estudos de Casos para o Estado do Ceará - Volume 2. 1ed.Fortaleza: Edições IPC, 2023, v. 2, p. 81-100.'],\n",
       "                '6.': ['LIMA, S. M. ; BRITO, R. W. C. ; MATOS, P. R. F. ; JESUS FILHO, J. ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; CUNHA, G. H. M. . Prevenção de Fraudes na Aquisição de Combustível: um estudo de caso do estado do Ceará. In: Cunha, George Henrique de Moura; Holanda Filho, Raimir. (Org.). Riscos e Fraudes no Setor Público: Ensaios e Estudos de Casos para o Estado do Ceará - Volume 2. 1ed.Fortaleza: Edições IPC, 2023, v. 2, p. 101-165.'],\n",
       "                '7.': ['MARINHO, R. ; SANTOS, M. ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; HORTA, A. . Introdução à Análise de Códigos Maliciosos para ambiente Windows. In: Carlos Raniery Paula dos Santos, Walter Priesnitz Filho, Paulo André da Silva Gonçalves, Marcia Henke.. (Org.). Mini Cursos SBSEG 2022. 1ed.Porto Alegre, RS: Sociedade Brasileira de Computação, 2022, v. 1, p. 100-154.'],\n",
       "                '8.': ['RAMOS, A. L. ; AQUINO, B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; RODRIGUES, JOEL J.P.C. . QUANTIFYING NODE SECURITY IN WIRELESS SENSOR NETWORKS UNDER WORM ATTACKS. In: Barbosa, Frederico Celestino. (Org.). Ciência da Computação Princípios Fundamentais. 1ed.Piracanjuba GO: Editora Conhecimento Livre, 2020, v. 1, p. 72-92.'],\n",
       "                '9.': ['PINHEIRO, P. G. C. D. ; RODRIGUES, M. M. ; BARROZO, J. P. A. ; OLIVEIRA, J. P. M. ;',\n",
       "                 'Pinheiro, Placido R.',\n",
       "                 ';',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '. A Mobile Terrestrial Surveillance Robot Using the Wall-Following Technique and a Derivative Integrative Proportional Controller. In: Radek Silhavy;Petr Silhavy;Zdenka Prokopova. (Org.). Intelligent Systems in Cybernetics and Automation Control Theory. 1ed.: , 2019, v. 860, p. 276-286.'],\n",
       "                '10.': ['ARAUJO, H. S. ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; RODRIGUES, J. J. P. C. ; RABELO, RICARDO A.L. ; SOUSA, N. ; SOBRAL, J. ; C.C.L.S. FILHO, J. . An Approach for IOT Dynamic Routes Selection using Fuzzy Logic. In: Eliana Freire do Nascimento; Christianne Matos de Paiva; Francisco Robert Bandeira Gomes da Silva; Renato Souza do Nascimento. (Org.). Estudos Avançados: Multidisciplinaridade do conhecimento científico. 1aed.Sao Paulo: Garcia, 2018, v. 001, p. 257-278.'],\n",
       "                '11.': ['ARAÚJO, HARILTON ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; RABELO, RICARDO ; RODRIGUES, JOEL J. P. C. ; SOUSA, NATANAEL ; LIMA, J. C. ; SOBRAL, JOSÉ . Simulação em IoT: Uma abordagem para Seleção de Rotas Ciente de Contexto. In: Baluz, Rodrigo. (Org.). III Escola Regional de Informática do Piauí (ERIPI 2017). 1ed.Picos, Piaui: , 2017, v. 1, p. 429-453.'],\n",
       "                '12.': ['OLIVEIRA, D. C. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. A Highly Parallel scheduling model for IT change management. In: Sobh et. al.. (Org.). Novel Algorithms and Techniques in Telecommunications and Networking. Berlin Heidelberg: Springer, 2010, v. , p. -.'],\n",
       "                '13.': ['Ribeiro, Victor ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. An Attack Classification Tool Based On Traffic Properties and Machine Learning. In: Sobh et. al.. (Org.). Novel Algorithms and Techniques in Telecommunications and Networking. Berlin Heidelberg: Springer, 2010, v. , p. -.'],\n",
       "                '14.': ['SOUSA, M. V. ; LEAL, L. B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. A New Collaborative Approach for Intrusion Detection System on Wireless Sensor Networks. In: Sobh et. al.. (Org.). Novel Algorithms and Techniques in Telecommunications and Networking. Berlin Heidelberg: Springer, 2010, v. , p. -.'],\n",
       "                '15.': ['Silva, Geneflides L. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Voice Gateway to IP-PSTN Integration and NGN Architecture. In: Khaled Elleithy, Tarek Sobh, Magued Iskander, Vikram Kapila, Mohammad A. Karim and Ausif Mahmood. (Org.). Technological Developments in Networking, Education and Automation. : Springer, 2010, v. , p. 585-589.'],\n",
       "                '16.': ['LEAL, L. B. ; SOUSA, M. V. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. An Algorithm for Route Selection on Multi-sink Wireless Sensor Network Using Fuzzy Logic. In: Khaled Elleithy, Tarek Sobh, Magued Iskander, Vikram Kapila, Mohammad A. Karim and Ausif Mahmood. (Org.). Technological Developments in Networking, Education and Automation. : Springer, 2010, v. , p. 591-596.'],\n",
       "                '17.': ['ARAUJO, H. S. ; Castro, Wagner L. T. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Reduction of Energy Consumption in WSN using an Approach Geocast. In: Khaled Elleithy, Tarek Sobh, Magued Iskander, Vikram Kapila, Mohammad A. Karim and Ausif Mahmood. (Org.). Technological Developments in Networking, Education and Automation. : Springer, 2010, v. , p. 567-572.'],\n",
       "                '18.': ['OLIVEIRA, D. C. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. A IT Change Management Tool to Optimize Scheduling and Planning. In: Khaled Elleithy, Tarek Sobh, Magued Iskander, Vikram Kapila, Mohammad A. Karim and Ausif Mahmood. (Org.). Technological Developments in Networking, Education and Automation. : Springer, 2010, v. , p. 399-403.'],\n",
       "                '19.': ['ARAUJO, H. S. ; Castro, Wagner L. T. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Simulação em RSSF para Protocolos de Roteamento usando uma Abordagem Geocast. ERCEMAPI 2009. : SBC, 2009, v. , p. -.'],\n",
       "                '20.': ['RODRIGUES, M. A. F. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Collaborative Virtual Environments and Multimedia Communication Technologies in Healthcare (to appear). In: Athina A. Lazakidou; Konstantinos M. Siassiakos. (Org.). Handbook of Research on Distributed Medical Informatics and E-Health. 30ed.: Idea Group Inc, 2008, v. , p. -16.'],\n",
       "                '21.': ['LEAL, L. B. ; ARAUJO, H. S. ; ALMEIDA, L. H. P. ; SOUSA, M. V. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Uma Abordagem Cross-Layer para Controle e Gerenciamento em RSSF. II Escola Regional de Computacao: Ceará, Maranhao e Piaui. : , 2008, v. 1, p. -.'],\n",
       "                '22.': ['HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B. . Administração Avançada de Redes. In: Bernadette Farias Lóscio; Flavio Horacio Souza Vieira; Itamar de Souza Lima; Milton Escossia Barbosa Neto; Pedro Porfírio Muniz Farias; Raimir Holanda Filho. (Org.). I Escola Regional de Computação - Ceará, Maranhão e Piauí. : , 2007, v. 1, p. 1-26.'],\n",
       "                '23.': ['Geneflides L da Silva ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. A Flow Based Traffic Characterization of IP Telephony Protocols. Advances and Innovations in Systems. Berlin Heidelberg: Springer-Verlag, 2007, v. , p. -.'],\n",
       "                '24.': ['OLIVEIRA, K. B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Ranking Projects Using Multivariate Statistics. Advances and Innovations in Systems. Berlin Heidelberg: Springer-Verlag, 2007, v. , p. -.']},\n",
       "               'Trabalhos completos publicados em anais de congressos': {'1.': ['LIMA, S. M. ; MATOS, P. R. F. ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; BRITO, R. W. . On the efficiency in spending on fuel in cities in the state of Ceará. In: VIII Congresso Internacional de Controle e Políticas Públicas, 2023, Salvador, BA. Anais do VIII Congresso Internacional de Controle e Políticas Públicas. Salvador, BA, 2023.'],\n",
       "                '2.': ['HOLANDA FILHO, RAIMIR',\n",
       "                 '; RAVIOLO, B. P. Y. ; CUNHA, G. H. M. ; SOUSA, L. S. ; BRITO, R. W. . Análise de Decisão Multicritério Aplicada ao Controle Externo de Convênios Celebrados com o Setor Público Brasileiro. In: Congresso Internacional de Controle e Políticas Públicas, 2023, Salvador, BA. Anais do VIII Congresso Internacional de Controle e Políticas Públicas. Salvador, BA, 2023.'],\n",
       "                '3.': ['HORTA, ANTONIO ;',\n",
       "                 'HOLANDA, RAIMIR',\n",
       "                 '; MARINHO, RENATO . A Multi-criteria Approach to Improve the Cyber Security Visibility Through Breach Attack Simulations. In: Simpósio Brasileiro de Segurança da Informação e de Sistemas Computacionais, 2022, Brasil. Anais do XXII Simpósio Brasileiro de Segurança da Informação e de Sistemas Computacionais (SBSeg 2022), 2022. p. 330.'],\n",
       "                '4.': ['HOLANDA FILHO, RAIMIR',\n",
       "                 '; CARLA BARBOZA DE SOUSA, DEBORA ; ALVES DE BRITO, WELLINGTON ; PASKNEL DE ALENCAR RIBEIRO, VICTOR ; LEÃO SÁ, EMANUEL ; LUCAS MARQUES DE SOUSA CHAVES, JOAN . Smart Contract Implementation to Improve Security on Solid Waste Management Application *. In: 2022 Ninth International Conference on Software Defined Systems (SDS), 2022, Paris. 2022 Ninth International Conference on Software Defined Systems (SDS). Paris: IEEE, 2022. p. 1-6.'],\n",
       "                '5.': ['Ribeiro, Victor ;',\n",
       "                 'FILHO, RAIMIR HOLANDA',\n",
       "                 '; RAMOS, ALEX . A Secure and Fault-Tolerant Architecture for LoRaWAN Based on Blockchain. In: 2019 3rd Cyber Security in Networking Conference (CSNet), 2019, Quito. 2019 3rd Cyber Security in Networking Conference (CSNet), 2019. p. 35-41.'],\n",
       "                '6.': ['RAMOS, A. L. ; LAZAR, M. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; RODRIGUES, J. J. P. C. . A Security Metric for the Evaluation of Collaborative Intrusion Detection Systems in Wireless Sensor Networks. In: IEEE International Conference on Communications (IEEE ICC 2017), 2017, Paris. Anais do ICC 2017, 2017.'],\n",
       "                '7.': ['RAMOS, A. L. ; AQUINO, B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; RODRIGUES, JOEL J. P. C. . Quantifying Node Security in Wireless Sensor Networks under Worm Attacks. In: Brazilian Symposium on Computer Networks and Distributed Systems (SBRC 2017), 2017, Belem - Pará. Anais do SBRC 2017, 2017.'],\n",
       "                '8.': ['SOUSA, M. V. ; CARVALHO, C. G. ; LOPES, D. ; Rabelo, Ricardo A. L. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Reducing Energy Consumption in Provisioning of Virtual Sensors by Similarity of Heterogenous Sensors. In: The 31st IEEE International Conference on Advanced Information Networking and Applications (AINA-2017), 2017, Taipei. Proceeding of AINA 2017, 2017.'],\n",
       "                '9.': ['RAMOS, ALEX ; AQUINO, B. ; LAZAR, MARCELLA ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; RODRIGUES, JOEL J. P. C. . A Quantitative Model for Dynamic Security Analysis of Wireless Sensor Networks. In: IEEE Global Communications Conference (IEEE GLOBECOM 2017), 2017, Singapure. IEEE GLOBECOM 2017, 2017.'],\n",
       "                '10.': ['FUJIWARA, C. S. ; ADERALDO, C. M. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; CHAVES, D. A. A. . The Internet of Things as a Helping Tool in the Daily Lyfe of Adult Patients with ADHD. In: IEEE Global Communications Conference (GLOBECOM 2017), 2017, Singapure. GLOBECOM 2017, 2017.'],\n",
       "                '11.': ['LEMOS, MARCUS ; CARVALHO, CARLOS ; RABELO, RICARDO ; MENDES, DOUGLAS ;',\n",
       "                 'FILHO, RAIMIR HOLANDA',\n",
       "                 '. An algorithm based on ant colony optimization for provisioning virtual sensor in sensor cloud. In: 2017 IEEE International Conference on Systems, Man and Cybernetics (SMC), 2017, Banff. 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2017. p. 2897.'],\n",
       "                '12.': ['ARAUJO, H. S. ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; RABELO, RICARDO A.L. ; RODRIGUES, J. J. P. C. ; SOUSA, N. ; C.C.L.S. FILHO, J. ; SOBRAL, J. . Uma Otimização de Protocolo RPL para Aplicações de Internet das Coisas. In: III Escola Regional de Informática do Piauí - ERIPI, 2017, Picos- PI. Anais da III Escola Regional de Informática do Piauí, 2017. v. 1. p. 153-158.'],\n",
       "                '13.': ['ARAUJO, H. S. ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; RODRIGUES, J. J. P. C. ; RABELO, RICARDO A.L. ; SOUSA, N. ; C.C.L.S. FILHO, J. ; SOBRAL, J. . A Proposal for Route Selection in IoT Bases on the Context of Specific Applications. In: Encontro Unificado de Computação (ENUCOMP), 2017, Parnaiba-PI. Anais do Encontro Unificado de Computação, 2017. v. 1. p. 401-408.'],\n",
       "                '14.': ['SOUSA, M. V. ; RABELO, RICARDO ;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '. An approach for clustering sensor nodes by data similarity. In: ENUCOMP, 2017, Parnaíba. Anais do ENUCOMP 2017, 2017.'],\n",
       "                '15.': ['ARAUJO, Paulo Regis C. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; RODRIGUES, Antonio. Wendell. O. ; ARAUJO, Andre. Luiz. C. ; MORAES FILHO, Jose. A. ; BRAYNER, A. ; OLIVEIRA, Joao. Paolo. C. M. . Context-Aware Query for High-Voltage Transmission Line Fault Detection using Wireless Sensor Network. In: Sensors Applications Symposium, 2015, Zadar, Croatia. Proceedings of the SAS 2015, 2015.'],\n",
       "                '16.': ['SOBRAL, J. ; Rabelo, Ricardo A. L. ; OLIVEIRA, D. ; LIMA, J. ; ARAUJO, H. S. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 \". A Framework for Improving the Performance of IoT Applications. In: ICWN'15 - The 14th International Conference on Wireless Networks, 2015, Las Vegas. Proceedings of the 14th International Conference on Wireless Networks, 2015.\"],\n",
       "                '17.': ['SOBRAL, J. ; Rabelo, Ricardo A. L. ; ARAUJO, H. S. ; LIMA, J. C. ; SOUZA, R. ; SOUSA, N. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. UMA ESTRUTURA BASEADA EM INTELIGÊNCIA COMPUTACIONAL PARA MELHORAR O DESEMPENHO DE APLICAÇÕES IOT. In: XII Simpósio Brasileiro de Automação Inteligente, 2015, Natal. Anais do SBAI 2015, 2015.'],\n",
       "                '18.': ['BEZERRA, V. ; C. JUNIOR, M. ; VALERIA, O. ; D. NETO, C. ; LEAL, L. B. ; SOUSA, M. V. ; CARVALHO, C. G. ; BRINGEL FILHO, J. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; AGOULMINE, N. . A Quality-Aware and Energy-Efficient Context Management Framework for Ubiquitous Systems. In: The 28th IEEE International Conference on Advanced Information Networking and Applications (AINA-2014), 2014, Victoria - Canada. Proceedings of the AINA 2014, 2014.'],\n",
       "                '19.': ['SILVA, T. A. R. ; LEAL, L. B. ; SOUSA, M. V. ; CARVALHO, C. G. N. ; BRINGEL FILHO, J. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Sistemas Ubíquos Eficientes em Consumo de Energia por Meio da Redução de Dado. In: CSBC/SBCUP - VI Simpósio Brasileiro de Computação Ubíqua e Pervasiva, 2014, Brasilia. Anais do CSBC 2014, 2014.'],\n",
       "                '20.': ['SOBRAL, J. ; Rabelo, Ricardo A. L. ; ARAUJO, H. S. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; BALUZ, R. ; SANTOS, Flavio. A. . An Enhancement in Directed Diffusion and AOMDV Routing Protocols using Hybrid Intelligent Systems. In: IEEE International Conference on Systems, Man and Cybernetics (SMC), 2014, San Diego, CA. Proceedings ot the SMC 2014, 2014.'],\n",
       "                '21.': ['VALERIA, O. ; RIBEIRO, A. ; LEAL, L. B. ; SOUSA, M. V. ; CARVALHO, C. G. ; BRINGEL FILHO, J. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; AGOULMINE, N. . Using QoC for improving energy-efficient context management in U-Health Systems. In: Healthcom 2014, 2014. Proceedings of the Healthcom 2014, 2014. p. 317-322.'],\n",
       "                '22.': ['SOBRAL, J. ; SOUSA, A. ; ARAUJO, H. S. ; BALUZ, R. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; SOUSA, M. V. ; Rabelo, Ricardo A. L. . A Fuzzy Inference System for Increasing of Survivability and Efficiency in Wireless Sensor Networks. In: ICN 2013, The Twelfth International Conference on Networks, 2013, Seville, Spain. Proceedings of the ICN 2013, 2013.'],\n",
       "                '23.': ['RAMOS, A. L. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Estimando o Nível de Segurança de Dados de Redes de Sensores sem Fio. In: 31o Simpósio Brasileiro de Redes de Computadores e Sistemas Distribuídos - SBRC 2013, 2013, Brasilia. Anais do SBRC 2013, 2013. p. 193-206.'],\n",
       "                '24.': ['Rabelo, Ricardo A. L. ; SOBRAL, J. ; ARAUJO, H. S. ; BALUZ, R. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Automated Design of Fuzzy Rule Base using Ant Colony Optimization for Improving the Performance in Wireless Sensor Networks. In: IEEE International Conference on Fuzzy Systems, 2013, Hyderabad, India. Proceedings of the IEEE Fuzzy Systems 2013, 2013.'],\n",
       "                '25.': ['Rabelo, Ricardo A. L. ; SOBRAL, J. ; BALUZ, R. ; ARAUJO, H. S. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. An Approach Based on Fuzzy Inference System and Ant Colony Optimization for Improving the Performance of Routing Protocols in Wireless Sensor Networks. In: IEEE Congress on Evolutionary Computation, 2013, Cancun, Mexico. Proceedings of the IEEE CEC 2013, 2013.'],\n",
       "                '26.': ['SILVA, O. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 ';',\n",
       "                 'Pinheiro, Placido R.',\n",
       "                 '. C3M ? Change Management Based on Multicriteria Methodology. In: 6th World Summit on the Knowledge Socity, 2013, Aveiro, Portugal. Proceedings of the WSKS 2013, 2013.'],\n",
       "                '27.': ['MARINHO, R. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B. . P2P Botnet Detection Based on Sub-flow Statistical Discriminators. In: 6th World Summit on the Knowledge Socity, 2013, Aveiro, Portugal. Proceedings of the WSKS 2013, 2013.'],\n",
       "                '28.': ['SOBRAL, J. ; Rabelo, Ricardo A. L. ; BALUZ, R. ; ARAUJO, H. S. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Uma abordagem baseada em sistemas fuzzy e algoritmos de colonia de formigas para classificação de rotas em RSSF. In: BRICS-CCI & CBIC 2013, 2013, Recife. Technical Program of CBIC, 2013.'],\n",
       "                '29.': ['BEZERRA, V. ; C. JUNIOR, M. ; VALERIA, O. ; D. NETO, C. ; LEAL, L. B. ; SOUSA, M. V. ; CARVALHO, C. G. ; BRINGEL FILHO, J. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; AGOULMINE, N. . An Energy-Efficient Context Management Framework for Ubiquitous Systems. In: UIC/ATC 2013, 2013. Proceedings of the UIC/ATC 2013, 2013. p. 697-702.'],\n",
       "                '30.': ['Leitão, Leonardo ; Sampaio, Américo ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. NaturalCloud: Um framework para integração de Rede de Sensores sem Fio na Nuvem. In: X Workshop em Clouds, Grids e Aplicações ? WCGA, 2012, Ouro Preto -MG. Anais do X Workshop em Clouds, Grids e Aplicações, 2012.'],\n",
       "                '31.': ['Ribeiro, Victor ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B. . Online Traffic Classification Based on Sub-Flows. In: IFIP/IEEE International Symposium on Integrated Network Management, 2011, Dublin. Proceedings of the IFIP/IEEE IM 2011, 2011.'],\n",
       "                '32.': ['LEAL, L. B. ; SOUSA, M. V. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; Rabelo, Ricardo A. L. . An Algorithm Based on Genetic Fuzzy Systems for the Selection of Routes in Multi-Sink Wireless Sensor Networks. In: 6th International Conference on Hybrid Artificial Intelligence Systems, 2011, Wroclaw - Polonia. Anais do 6th International Conference on Hybrid Artificial Intelligence Systems, 2011.'],\n",
       "                '33.': ['Rios, Gesiel ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; Coelho, André Luis Carvalho . An Autonomic Security Mechanism based on Novelty detection and Concept Drift. In: The Seventh International Conference on Autonomic and Autonomous Systems, 2011, Venice/Mestre, Italy. Proceedings of the Seventh International Conference on Autonomic and Autonomous Systems, 2011.'],\n",
       "                '34.': ['Ribeiro, Victor ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B. . Classificação online dos tráfegos TCP e UDP baseada em subfluxos. In: SBRC - Simpósio Brasileiro de Redes de Computadores, 2011, Campo Grande. Anais do SBRC 2011, 2011.'],\n",
       "                '35.': ['LEAL, L. B. ; SOUSA, M. V. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; Rabelo, Ricardo A. L. ; Borges, Fabio A. S. . A hybrid approach based on genetic fuzzy systems for Wireless Sensor Networks. In: IEEE Congress on Evolutionary Computation 2011, 2011, New Orleans, LA. Proceedings of the IEEE CEC 2011, 2011. p. 965-972.'],\n",
       "                '36.': ['LEAL, L. B. ; SOUSA, M. V. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; Rabelo, Ricardo A. L. ; Borges, Fabio A. S. . An application of Genetic Fuzzy Systems for Wireless Sensor Networks. In: IEEE International Conference on Fuzzy Systems, 2011, Taipei, Taiwan. Proceedings of the FUZZ-IEEE 2011, 2011. p. 2473-2480.'],\n",
       "                '37.': ['Silva, Helber W. ; NOGUEIRA, M. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; SANTOS, Aldri L dos . Um Arcabouço para Adaptação Segura e Eficiente do Serviço de Roteamento em Redes Mesh. In: Workshop de Redes de Acesso em Banda Larga, 2011, Campo Grande, MS. Anais do Workshop de Redes de Acesso em Banda Larga, 2011.'],\n",
       "                '38.': ['LEAL, L. B. ; SOUSA, M. V. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; Rabelo, Ricardo A. L. ; Borges, Fabio A. S. . Uma Abordagem para Seleção de Rotas em Redes de Sensores Sem Fio Utilizando Sistemas Fuzzy Genéticos. In: XXXI Congresso da Sociedade Brasileira de Computação - Encontro Nacional de Inteligência Artificial, 2011, Natal - RN. Anais do CSBC - ENIA, 2011. p. 773-784.'],\n",
       "                '39.': ['ARAUJO, H. S. ; Castro, Wagner L. T. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. A Proposal for Energy Otimization in WSN using an Geocast Approach. In: 17th International Conference on Telecommunications - ICT 2010, 2010, Doha - Qatar. Anais do 17th International Conference on Telecommunications, 2010.'],\n",
       "                '40.': ['ARAUJO, H. S. ; Castro, Wagner L. T. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. WSN Routing: An Geocast Approach for Reducing Consumption Energy. In: IEEE Wireless Communications & Networking Conference 2010 - WCNC 2010, 2010, Sydney. Anais do IEEE Wireless Communications & Networking Conference 2010, 2010.'],\n",
       "                '41.': ['ARAUJO, H. S. ; Castro, Wagner L. T. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. A Proposal of Self-Configuration in Wireless Sensor Network for Recovery of Broken Paths. In: Sensors Applications Symposium - SAS 2010, 2010, Limerick - Irlanda. Anais do Sensors Applications Symposium, 2010.'],\n",
       "                '42.': ['HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B. . Networks Traffic Prediction using  PCA  and K-means. In: 12th IEEE/IFIP Network Operations and Management Symposium - NOMS 2010, 2010, Osaka - Japao. Anais do 12th IEEE/IFIP Network Operations and Management Symposium, 2010.'],\n",
       "                '43.': ['OLIVEIRA, D. C. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. An Automated Roll-Back Plan to IT Change Management Tool. In: IEEE International Conference on Network and Services Management, 2010, Niagara Falls. Proceedings of the CNSM 2010, 2010.'],\n",
       "                '44.': ['Silva, H. ; NOGUEIRA, M. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; SANTOS, Aldri L dos . A Framework for Self-Configuration on WMNs Aware of Performance and Security Issues. In: IEEE International Conference on Network and Services Management, 2010, Niagara Falls. Proceedings of the CNSM 2010, 2010.'],\n",
       "                '45.': ['Silva, H. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; NOGUEIRA, M. ; SANTOS, Aldri L dos . A Cross-layer and Adaptative Scheme for Balancing Performance and Security on WMN Data Routing. In: IEEE Global Communications Conference - Globecom 2010, 2010, Miami, Florida. Proceedings of the Globecom 2010, 2010.'],\n",
       "                '46.': ['MAIA, J. E. B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Internet Traffic Classification using a Hidden Markov Model. In: IEEE International Conference on Hybrid Intelligent Systems, 2010, Atlanta. Proceedings of the HIS 2010, 2010.'],\n",
       "                '47.': ['SOUSA, M. V. ; LEAL, L. B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Detecção de Intrusão em RSSF Utilizando uma Abordagem Colaborativa e Cross-layer. In: XXVII Simpósio Brasileiro de Redes de Computadores e Sistemas Distribuídos, 2009, Recife. Anais do SBRC 2009, 2009.'],\n",
       "                '48.': ['OLIVEIRA, D. C. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. A Time and Financial Loss Estimation using a Highly Parallel Scheduling Model for IT Change Management. In: 4th IFIP/IEEE International Workshop on Business-driven IT Management (BDIM 2009), 2009, Nova York. Proceedings of the BDIM 2009, 2009.'],\n",
       "                '49.': ['MAIA, J. E. B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. One-against-all Methodology for Features Selection and Classification of Internet Applications. In: 9th IEEE International Workshop on IP Operations and Management, 2009, Veneza. Proceedings of the 9th IEEE International Workshop on IP Operations and Management, 2009.'],\n",
       "                '50.': ['HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B. ; Marcus Fabio Fontenelle do Carmo ; PAULINO, G. . An Internet Traffic Classification Methodology based on Statistical Discriminators. In: IEEE/IFIP Network Operations & Management Symposium, 2008, Salvador, Bahia. Anais do NOMS 2008, 2008. p. 907-910.'],\n",
       "                '51.': ['Ribeiro, Victor ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Fatadist: Uma Ferramenta para Classificação de Ataques baseada em Discriminantes Estatísticos. In: CLEI 2008 - XXXIV Conferencia Latinoamericana de Informática, 2008, Santa Fé. Anais da XXXIV Conferencia Latinoamericana de Informatica, 2008.'],\n",
       "                '52.': ['PAULINO, G. ; MAIA, J. E. B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; SOUZA, José Neuman de . P2P Traffic Identification using Cluster Analysis. In: IEEE International Global Information Infrastructure Symposium, 2007, Marrakech. Anais do IEEE International Global Information Infrastructure Symposium, 2007.'],\n",
       "                '53.': ['CARMO, M. F. F. ; MAIA, J. E. B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; SOUZA, José Neuman de . Attack Detection based on Statistical Discriminators. In: IEEE International Global Information Infrastructure Symposium, 2007, Marrakech. Anais do IEEE International Global Information Infrastructure Symposium, 2007.'],\n",
       "                '54.': ['HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B. ; CARMO, M. F. F. . Identificação da Componente de Tráfego de Ataque baseada em Discriminantes Estatísticos. In: WPERFORMANCE, 2007, Rio de Janeiro. Anais da SBC 2007, 2007.'],\n",
       "                '55.': ['OLIVEIRA, K. B. ; OLIVEIRA, J. M. M. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; BELCHIOR, A. D. . A New Methodology to Rank Projects using Multivariate statistics. In: IEEE International Conference on Service Systems and Service Management, 2007, Chengdu, China. Anais do IEEE International Conference on Service Systems and Service Management, 2007.'],\n",
       "                '56.': ['Marcus Fabio Fontenelle do Carmo ; PAULINO, G. ; MAIA, J. E. B. ;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '. Using Statistical Discriminators and Cluster Analysis to P2P and Attack Traffic Monitoring. In: 5th Latin American Network Operations and Management Symposium, 2007, Petrópolis. Anais do 5th Latin American Network Operations and Management Symposium, 2007.'],\n",
       "                '57.': ['HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B. ; CARMO, M. F. F. . Seleção de Discriminantes Estatísticos para Identificação de Tráfego de Ataques. In: SBRC - WGRS, 2007, Pará. Anais do SBRC 2007, 2007.'],\n",
       "                '58.': ['HOLANDA FILHO, R.',\n",
       "                 '; SANTOS, Aldri L dos ; SOUZA, José Neuman de . Identifying Anomalous Patterns of Network-wide Traffic using Flow Clustering. In: SBRC - WGRS, 2006, Curitiba. Anais do WGRS 2006, 2006. v. 1. p. 97-108.'],\n",
       "                '59.': ['HOLANDA FILHO, R.',\n",
       "                 '. Characterization and Compression of Internet Packet Header Traces using Cluster Analysis. In: WPERFORMANCE - V Workshop em Desempenho de Sistemas Computacionais e de Comunicação, 2006, Campo Grande. Anais do XXVI Congresso da Sociedade Brasileira de Computação, 2006.'],\n",
       "                '60.': ['HOLANDA FILHO, R.',\n",
       "                 '. A Packet Header Compression Algorithm based on TCP Flow Clustering and Huffman Encoding. In: SEMISH - XXXIII Seminário Integrado de Software e Hardware, 2006, Campo Grande. Anais do XXVI Congresso da Sociedade Brasileira de Computação, 2006.'],\n",
       "                '61.': ['HOLANDA FILHO, R.',\n",
       "                 '; VIDAL, J. G. . A New Methodology for Packet Trace Classification and Compression based on Semantic Traffic Characterization. In: International Teletraffic Congress - ITC, 2005, Pequin. Performance Challenges for Efficient Next Generation Networks. Pequin: Publishing House, BUPT, 2005. v. 6a. p. 719-728.'],\n",
       "                '62.': ['HOLANDA FILHO, R.',\n",
       "                 '; VERDU, J. ; VIDAL, J. G. ; VALERO, M. . Performance Analysis of a New Packet Trace Compressor based on TCP Flow Clustering. In: IEEE International Symposium on Performance Analysis of Systems and Software - ISPASS, 2005, Austin - Texas. IEEE ISPASS 2005, 2005. v. 1.'],\n",
       "                '63.': ['HOLANDA FILHO, R.',\n",
       "                 '; VIDAL, J. G. . A Lossless Compression Method for Internet Packet Headers. In: EuroNGI Conference on Next Generation Internet Networks, 2005, Roma. Traffic Engineering, 2005. v. 1.'],\n",
       "                '64.': ['HOLANDA FILHO, R.',\n",
       "                 '; VIDAL, J. G. ; ALMEIDA, V. . Flow Clustering: A New Approach to Semantic Traffic Characterization. In: 12th GI/ITG Conference on Measuring, Modeling and Evaluation of Computer and Communication Systems, 2004, Dresden. GI/ITG, 2004.'],\n",
       "                '65.': ['HOLANDA FILHO, R.',\n",
       "                 '; VIDAL, J. G. . An Integrated Synthetic Traffic Generation Methodology for Network Processors Evaluation. In: UPC-DAC Technical Reports, 2003, Barcelona. UPC-DAC Technical Reports, 2003.'],\n",
       "                '66.': ['HOLANDA FILHO, R.',\n",
       "                 '; VIDAL, J. G. ; ALMEIDA, V. . IP Address Structure Properties in a Packet Trace. In: UPC-DAC Technical Reports, 2003. UPC-DAC Technical Reports.'],\n",
       "                '67.': ['HOLANDA FILHO, R.',\n",
       "                 '; GIL, M. . An Analysis of Embedded Handheld s Linux and its Differencial Features from General Purpose Systems. In: Techinical Report DAC-UPC 2002, 2002, Barcelona. UPC-DAC Technical Reports, 2002.'],\n",
       "                '68.': ['HOLANDA FILHO, R.',\n",
       "                 '; VIDAL, J. G. . Proceedings for Performance Monitoring at ADI80200EVB Evaluation Board. In: UPC-DAC Technical Reports, 2002. UPC-DAC Technical Reports.'],\n",
       "                '69.': ['HOLANDA FILHO, R.',\n",
       "                 '; BORGES NETO, H. ; FURTADO, E. . Tele-Ambiente: Desenvolvimento e Aplicação de Ferramentas Cooperativas, Adaptativas e Interativas Aplicadas ao Ensino a Distância.. In: XV Encontro de Pesquisa Educacional dasRegiões Norte e Nordeste, 2001, São Luis. Anais do XV EPENN, 2001.'],\n",
       "                '70.': ['HOLANDA FILHO, R.',\n",
       "                 '. Utilização da Tecnologia de Agentes Inteligentes para a Construção de Ambientes Colaborativos de Ensino-Aprendizagem. In: 1o. Encontro de Pesquisa e Pós-Graduação da UNIFOR, 2001, Fortaleza. Anais do I EPPG UNIFOR, 2001.'],\n",
       "                '71.': ['HOLANDA FILHO, R.',\n",
       "                 '; BORGES NETO, H. . Especificando o Tele-Ambiente no Contexto da Educação a Distância. In: SBIE 2000, 2000, Maceio-Al. Anais do XI Simpósio Brasileiro de Informática na Educação, 2000. v. 01. p. 154-159.'],\n",
       "                '72.': ['HOLANDA FILHO, R.',\n",
       "                 '. Internet 2 - Perspectivas para o Ensino a Distância (EAD).. In: 4o. INFOEDUCAR, 1999, Fortaleza. Anais do 4o. INFOEDUCAR, 1999.'],\n",
       "                '73.': ['HOLANDA FILHO, R.',\n",
       "                 '; OLIVEIRA, M. . Sistemas Baseados em Conhecimento na Gerência de Redes. In: 2a. Escola de Verão de Redes de Computadores, 1998, Itapipoca, 1998.'],\n",
       "                '74.': ['HOLANDA FILHO, R.',\n",
       "                 '; OLIVEIRA, M. . Implementando conhecimento em um sistema para apoio à gerência de redes.. In: Seminário Franco Brasileiro de Sistemas Informáticos Distribuídos, 1997, Fortaleza. Anais do SFBSID, 1997.']},\n",
       "               'Programas de computador sem registro': {'1.': ['HOLANDA FILHO, R.',\n",
       "                 '. SAGRES. 1998.']}}),\n",
       "             ('',\n",
       "              {'resumo': 'Pós-Doutor em Computação pela Sorbonne Université-Pierre et Marie Curie (França - 2020). Doutor em Ciência da Computação pela Universitat Politecnica de Catalunya (Espanha - 2005). Atualmente é professor titular da Universidade de Fortaleza - UNIFOR na qual é membro permanente dos programas de pós-graduação em Informática Aplicada (mestrado e doutorado) e do mestrado profissional em administração. Possui mais de 100 trabalhos publicados em conferências e periódicos nacionais e internacionais. Tem experiência na área de Ciência da Computação, com ênfase em Ciência de Dados e Teleinformática, atuando principalmente nos seguintes temas: Redes Complexas, Grafos de Conhecimento, Aprendizagem de Máquina, Internet das Coisas, Segurança e Blockchain. (Texto informado pelo autor)\\n'}),\n",
       "             ('Identificação',\n",
       "              {'Nome': ['Raimir Holanda Filho'],\n",
       "               'Nome em citações bibliográficas': ['HOLANDA FILHO, R.;FILHO, RAIMIR HOLANDA;FILHO, RAIMIR;HOLANDA, RAIMIR;HOLANDA FILHO, RAIMIR;FILHO, RAIMIR H.'],\n",
       "               'Lattes iD': ['http://lattes.cnpq.br/2607811863279622']}),\n",
       "             ('Endereço',\n",
       "              {'Endereço Profissional': ['Universidade de Fortaleza, Mestrado em Informatica Aplicada, Mestrado Em Informatica Aplicada.',\n",
       "                'Av. Washington Soares,1321',\n",
       "                'Edson Queiroz',\n",
       "                '60811341 - Fortaleza, CE - Brasil',\n",
       "                'Telefone: (85) 34773268',\n",
       "                'URL da Homepage:',\n",
       "                'http://www.unifor.br']}),\n",
       "             ('Formação acadêmica/titulação',\n",
       "              {'2001 - 2005': ['Doutorado em Ciência da Computação',\n",
       "                '.',\n",
       "                'Universitat Politècnica de Catalunya, UPC, Espanha.',\n",
       "                'Título: A New Methodology for Packet Trace Classification and Compression based on Semantic Traffic Characterization, Ano de obtenção: 2005.',\n",
       "                'Orientador: Jorge Garcia Vidal.',\n",
       "                'Bolsista do(a): Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, Brasil.',\n",
       "                'Palavras-chave: Traffic Characterization; TCP flow clustering; Performance; Packet trace classification; Compression.',\n",
       "                'Grande área: Ciências Exatas e da Terra',\n",
       "                'Setores de atividade: Educação Superior.'],\n",
       "               '1996 - 1998': ['Mestrado em Ciências da Computação',\n",
       "                '.',\n",
       "                'Universidade Federal do Ceará, UFC, Brasil.',\n",
       "                'Título: SAGRES - Um Sistema Baseado em Conhecimento para Apoia a Gerencia de Falhas em Redes de Computadores, Ano de Obtenção: 1998.',\n",
       "                'Orientador: Mauro Oliveira.',\n",
       "                'Palavras-chave: SAGRES; GERENCIA DE REDES; REDES DE COMPUTADORES; SISTEMAS BASEADOS EM CONHECIMENTO; GERENCIA DE FALHAS.',\n",
       "                'Grande área: Ciências Exatas e da Terra',\n",
       "                'Setores de atividade: Educação; Informática.'],\n",
       "               '1994 - 1994': ['Especialização em Informatica',\n",
       "                '.  (Carga Horária: 360h).',\n",
       "                'Universidade Federal do Ceará, UFC, Brasil.'],\n",
       "               '1985 - 1989': ['Graduação em Engenharia Civil',\n",
       "                '.',\n",
       "                'Universidade Federal do Ceará, UFC, Brasil.']}),\n",
       "             ('Pós-doutorado',\n",
       "              {'2020 - 2020': ['Pós-Doutorado.',\n",
       "                'Université Pierre et Marie Curie, UPMC, França.',\n",
       "                'Grande área: Ciências Exatas e da Terra',\n",
       "                'Grande Área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Machine Learning.']}),\n",
       "             ('Formação Complementar', {}),\n",
       "             ('Atuação Profissional',\n",
       "              {'Vínculo institucional': [],\n",
       "               '1995 - Atual': ['Vínculo: , Enquadramento Funcional: Professor titular, Carga horária: 40'],\n",
       "               'Atividades': [],\n",
       "               '04/2008 - Atual': ['Conselhos, Comissões e Consultoria, Reitoria, Conselho de Centro do CCT.'],\n",
       "               '03/2006 - Atual': ['Ensino, Informática Aplicada, Nível: Pós-Graduação'],\n",
       "               '02/2006 - Atual': ['Ensino, Engenharia de Telecomunicações, Nível: Graduação'],\n",
       "               '10/2005 - Atual': ['Pesquisa e desenvolvimento, Mestrado em Informatica Aplicada.'],\n",
       "               '3/1995 - Atual': ['Ensino, Informatica, Nível: Graduação'],\n",
       "               '1/2006 - 1/2006': ['Treinamentos ministrados , Mestrado em Informatica Aplicada, Mestrado Em Informatica Aplicada.']}),\n",
       "             ('Linhas de pesquisa', {'1.': ['Redes de Comunicação']}),\n",
       "             ('Projetos de pesquisa',\n",
       "              {'2021 - Atual': ['Monitor Fiscal TCE/CE'],\n",
       "               '2011 - 2014': ['CIA2-Construindo Cidades Inteligentes da Instrumentação dos Ambientes ao Desenvolvimento de Aplicações'],\n",
       "               '2010 - 2010': ['DeLAtoR - Detecção e localização de disparos de armas baseado em redes de sensores sem fio'],\n",
       "               '2009 - 2011': ['MOVERES: Sistema de Monitoramento Veicular Através de Redes de Sensores sem Fio. Projeto financiado pela FINEP (Edital Subvenção Economica a Inovacao - 01/2007)'],\n",
       "               '2008 - 2011': ['Especificaçao de uma arquitetura de gerenciamento distribuído para RSSF. Projeto de Pesquisa cadastrado na Unifor.'],\n",
       "               '2008 - 2010': ['FATADIST - Uma Ferramenta para Classificação de Tráfego de Ataques. Projeto financiado com recursos do Edital AT - CNPq'],\n",
       "               '2007 - 2009': ['CLASTRIN - Um Classificador de Tráfego Internet baseado em Discriminantes Estatísticos. Projeto financiado com recursos do Edital Universal - CNPq'],\n",
       "               '2007 - 2008': ['Sistema de Detecção de ataques DoS (Denial of Service) em redes móveis através de técnicas de modelagem.'],\n",
       "               '2006 - 2008': ['Caracterização, classificação e modelagem de tráfego de redes convergentes'],\n",
       "               '2005 - 2007': ['Telemedicina - Ampliaçao de Laboratório para Pesquisa e Desenvolvimento de Aplicações em Telemedicina. Projeto de pesquisa e desenvolvimento em tecnologia da informacao incentivado através da Lei de Informática. (Celéstica do Brasil Ltda)'],\n",
       "               '2005 - 2006': ['Implantacao do laboratorio de Redes Convergentes. Projeto financiado com recursos da Lei de Informática (UNIFOR - SANMINA-SCI DO BRASIL INTEGRATION LTDA)']}),\n",
       "             ('Projetos de extensão',\n",
       "              {'2009 - 2009': ['S2IPro - Sistema de Identificação Inequívoca do Proprietário']}),\n",
       "             ('Projetos de desenvolvimento',\n",
       "              {'2010 - 2012': ['Desenvolvimento de Software para Gerenciamento de Arquitetura de Telecomunicações de Baixo Custo para Comunidades Rurais, Pequenas Empresas, Órgãos e Instituições Municipais/Estaduais']}),\n",
       "             ('Revisor de periódico',\n",
       "              {'2020 - Atual': ['Periódico: IEEE Latin America Transactions'],\n",
       "               '2021 - Atual': ['Periódico: Computer Networks']}),\n",
       "             ('Revisor de projeto de fomento',\n",
       "              {'2015 - Atual': ['Agência de fomento: Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico']}),\n",
       "             ('Áreas de atuação',\n",
       "              {'1.': ['Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Sistemas de Computação/Especialidade: Teleinformática.'],\n",
       "               '2.': ['Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Security.'],\n",
       "               '3.': ['Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Machine Learning.'],\n",
       "               '4.': ['Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Natural Language Processing.'],\n",
       "               '5.': ['Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Internet of Things.'],\n",
       "               '6.': ['Grande área: Ciências Exatas e da Terra / Área: Ciência da Computação / Subárea: Blockchain.']}),\n",
       "             ('Idiomas',\n",
       "              {'Inglês': ['Compreende Bem, Fala Bem, Lê Bem, Escreve Bem.'],\n",
       "               'Espanhol': ['Compreende Bem, Fala Bem, Lê Bem, Escreve Bem.'],\n",
       "               'Português': ['Compreende Bem, Fala Bem, Lê Bem, Escreve Bem.'],\n",
       "               'Francês': ['Compreende Bem, Fala Razoavelmente, Lê Bem, Escreve Razoavelmente.']}),\n",
       "             ('Produção bibliográfica', {}),\n",
       "             ('Bancas',\n",
       "              {'Mestrado': {'1.': ['TORRES, A. L. M. M.; BORGES NETO, H.;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '.  Participação em banca de ANDRÉ SANTOS SILVA. TeleMeios: uma proposta de virtualização do ensino ancorada na Sequência Fedathi. 2022. Dissertação (Mestrado em Educação) - Universidade Federal do Ceará.'],\n",
       "                '2.': ['MACHADO, V. P.; SANTOS, P. A.; MOURA, R. S.; LEMOS, MARCUS;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '.  Participação em banca de Joselito Mendes de Sousa Junior. Modelo para Classificação de Fornecedores da Adm Publ baseado em Aprend Maquina. 2019. Dissertação (Mestrado em Ciência da Computação) - Universidade Federal do Piauí.'],\n",
       "                '3.': ['HOLANDA FILHO, R.',\n",
       "                 ';',\n",
       "                 'Pinheiro, Placido R.',\n",
       "                 '; BESSA, A.; THOMAZ, A. C. F..  Participação em banca de Odecilia Barreira da Silva. C3M - Gerenciamento de Mudanças Estruturado em uma Metodologia de Multicritério. 2013. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '4.': ['HOLANDA FILHO, R.',\n",
       "                 '; ANDRADE, R. M. C.; SAMPAIO, A. T. F..  Participação em banca de Alex Lacerda Ramos. Sensor Data Security Estimator: um Framework para Estimativa do Nível de Segurança dos dados de Redes de Sensores sem Fio. 2013. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '5.': ['HOLANDA FILHO, R.',\n",
       "                 '; Rabelo, Ricardo A. L.; BRAYNER, A.; FERNANDES, R. A. S..  Participação em banca de Rodrigo Augusto Rocha Souza Baluz. Uma Aplicação de Sistemas Inteligentes Híbridos ACO-Fuzzy para a Otimização do Desempenho em Redes de Sensores sem Fio. 2013. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '6.': ['MENDONCA, N. C.; SAMPAIO, A. T. F.; FONSECA, N. L. S.;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Matheus Ciríaco Cerqueira Cunha. Um Ambiente Programável para Avaliar o Desempenho de Aplicações em Nuvens de Infraestrutura. 2012. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '7.': ['SAMPAIO, A. T. F.;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; UEYAMA, J.; VASCONCELOS FILHO, J. E..  Participação em banca de Leonardo Moura Leitão. NaturalCloud: Um framework para integração de Rede de Sensores na Nuvem. 2012. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '8.': ['HOLANDA FILHO, R.',\n",
       "                 '; ZIVIANI, A.; MENDONCA, N. C..  Participação em banca de Victor Pasknel de Alencar Ribeiro. Classificação de tráfego online baseada em sub-fluxos. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '9.': ['HOLANDA FILHO, R.',\n",
       "                 '; Coelho, André Luis Carvalho; GOMES, D. G.; SANTOS, C. N..  Participação em banca de Gesiel Rios Lopes. GERAU: Um mecanismo de gerenciamento de segurança autonômico baseado em detecção de novidades e mudança de conceito. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '10.': ['HOLANDA FILHO, R.',\n",
       "                 '; Rabelo, Ricardo A. L.; SAMPAIO, A. T. F..  Participação em banca de Harilton da Silva Araújo. LARP: UM PROTOCOLO DE ROTEAMENTO TOLERANTE A FALHAS PARA REDES DE SENSORES SEM FIO. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '11.': ['HOLANDA FILHO, R.',\n",
       "                 '; NOGUEIRA, M.; MADEIRA, E. R. M.; MENDONCA, N. C.; RODRIGUES, M. A. F..  Participação em banca de Helber Wagner da Silva. Um Esquema de Seleção de Rotas para o Balanceamento de Segurança e Desempenho em Redes em Malha sem Fio. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '12.': ['HOLANDA FILHO, R.',\n",
       "                 '; Rabelo, Ricardo A. L.; SANTOS, Aldri L dos; SANTOS, C. N..  Participação em banca de Liliam Barroso Leal. Uma Abordagem para Estimação da Qualidade de Rotas em Redes de Sensores sem Fio Multi-Sink Baseada em Sistemas Fuzzy Genéticos. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '13.': ['HOLANDA FILHO, R.',\n",
       "                 '; SILVA, J. S. V.; BESSA, A..  Participação em banca de Denilson Cursino de Oliveira. Uma Sistematização para o Planejamento da Gerência de Mudanças em TI e Modelagem de Uma Ferramenta. 2011. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '14.': ['HOLANDA FILHO, R.',\n",
       "                 '; ANDRADE, R. M. C.; BRAYNER, A. R. A..  Participação em banca de Marcus Vinicius de Sousa Lemos. Detecção de Intrusão em Redes de Sensores Sem Fio Utilizando uma Abordagem Colaborativa e Cross-layer. 2010. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '15.': ['HOLANDA FILHO, R.',\n",
       "                 '; Souza, Jefferson T.; Silva, Jorge Luiz de C..  Participação em banca de Fabiano Carneiro Ribeiro. Estimação de Matrizes de Tráfego Origem-Destino utilizando Algoritmo Genético. 2009. Dissertação (Mestrado em Mestrado Acadêmico em Ciência da Computação) - Universidade Estadual do Ceará.'],\n",
       "                '16.': ['HOLANDA FILHO, R.',\n",
       "                 '; SANTOS, Aldri L dos; Coelho, André Luis Carvalho.  Participação em banca de MARCUS FÁBIO FONTENELLE DO CARMO. CLASTRIN - Um Classificador de Tráfego de Aplicações Internet utilizando a Abordagem Um-Contra-Todos. 2009. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '17.': ['HOLANDA FILHO, R.',\n",
       "                 ';',\n",
       "                 'Pinheiro, Placido R.',\n",
       "                 '; Fernandez, Marcial P..  Participação em banca de Geneflides Laureno da Silva. Gateway de Voz para Integração IP-PSTN Aderente à Arquitetura das Redes de Nova Geração. 2009. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '18.': ['BRAYNER, A.; Coelho, André Luis Carvalho; Nakamura, Eduardo freire;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Karina Marinho de Souza. Processamento de Consultas em Redes de Sensores sem Fio: Uma Abordagem de Detecção de Novidades para o Controle da Qualidade dos Serviços das Consultas. 2009. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '19.': ['HOLANDA FILHO, R.',\n",
       "                 '; Silva, Jorge Luiz de C.; Bezerra, Francisco N..  Participação em banca de Gabriel Paulino Siqueira Júnior. Uma Metodologia para Identificação de Classes de Tráfego Baseada em Discriminantes Estatísticos e Análise de Agrupamentos. 2008. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '20.': ['HOLANDA FILHO, R.',\n",
       "                 '; BELCHIOR, A. D.; Falbo, Ricardo de A.; Farias, Pedro P. M..  Participação em banca de Karlson Bernardo de Oliveira. Aplicação da Estatística Multivariada para Apoiar a Avaliação Organizacional. 2008. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '21.': ['HOLANDA FILHO, R.',\n",
       "                 '; MENDONÇA, Nabor das Chagas; MACHADO, Javam de Castro.  Participação em banca de Fabricio Albuquerque Diógenes. Logmiddle: um middleware para o compartilhamento de dados em redes móveis ad hoc. 2006. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.'],\n",
       "                '22.': ['HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Ricardo Régis Cavalcante Chaves. Uma arquitetura para ambientes virtuais colaborativos com dispositivos de force feedback. 2006. Dissertação (Mestrado em Informática Aplicada) - Universidade de Fortaleza.']},\n",
       "               'Teses de doutorado': {'1.': ['Pinheiro, Placido R.',\n",
       "                 '; RABELO, RICARDO A.L.;',\n",
       "                 'HOLANDA FILHO, RAIMIR',\n",
       "                 '; RODRIGUES, J. J. P. C.; ALMEIDA, O. M.; CRUZ NETO, J. X..  Participação em banca de Jaclason Machado Veras. Home Energy Management System: A Multi-objective optimization Model for Scheduling Loads. 2019. Tese (Doutorado em Informática Aplicada)  - Universidade de Fortaleza.']},\n",
       "               'Trabalhos de conclusão de curso de graduação': {'1.': ['HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B..  Participação em banca de Alvaro Garcia de Miguel.Estudo das Redes Mesh e sua Aplicação na Telemedicina. 2011. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '2.': ['HOLANDA FILHO, R.',\n",
       "                 '; Brito, Wellington Alves; Rios, Clauson Sales do N..  Participação em banca de Dráulio Brasil Soares Neto.Estudo das Redes de Sensores sem Fio com um Estudo de Caso. 2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '3.': ['HOLANDA FILHO, R.',\n",
       "                 '; Silva, Geneflides L.; MAIA, J. E. B..  Participação em banca de Gustavo Gurgel Nóbrega.QoS em Redes IP. 2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '4.': ['HOLANDA FILHO, R.',\n",
       "                 '; GARCIA, Fernando Parente.  Participação em banca de Leandro Orofino Enck.Estudo Comparativo entre Algoritmos de Roteamento para Redes de Sensores sem Fio. 2009. Trabalho de Conclusão de Curso (Graduação em Ciência da Computação)  - Universidade de Fortaleza.'],\n",
       "                '5.': ['HOLANDA FILHO, R.',\n",
       "                 '; MAIA, J. E. B.; GARCIA, Fernando Parente.  Participação em banca de Thais Lucas da Rocha Sousa.Avaliação da Qualidade Percebida em Serviços de Telecomunicações. 2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '6.': ['HOLANDA FILHO, R.',\n",
       "                 '; Brito, Wellington Alves; Alcocer, Juan C. A..  Participação em banca de Bráulio Crisóstomo de Quental.Transmissão de Dados sobre Redes Elétricas. Estudo de Caso do Projeto Piloto PLC - Coelce. 2009. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '7.': ['HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Emanoela de Jesus Lopes Soares.Redes de Acesso Opticas, HFC e HFW. 2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '8.': ['HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Noele Ingrid da Rosa Silva.Um estudo para convergência TV com outras plataformas para identificar padrões de interação do usuário com serviços de televisão. 2007. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.'],\n",
       "                '9.': ['HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Claudio Franklin Mesquita Araújo.Gerência e operação de redes GSM. 2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '10.': ['HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Francisco Arnaldo de Araújo Filho.Segurança em redes sem fio 802.11x. 2007. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.'],\n",
       "                '11.': ['MAIA, J. E. B.; Antonio Macilio Pereira de Lucena;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Liana de Andrade Gomes.Evolucao das Redes GSM para UMTS (3G). 2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '12.': ['MAIA, J. E. B.; GARCIA, Fernando Parente;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Naiana Edilma Coelho de Freitas.Metodologia em Gestão de Serviços em Telecomunicações e Tecnologia da Informação. 2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '13.': ['Colares, Ricardo Fialho;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '; Brito, Wellington Alves.  Participação em banca de Diego Buarque Mancera.H.323: Conceitos, Aplicações e Integração com Asterisk. 2007. Trabalho de Conclusão de Curso (Graduação em Engenharia de Telecomunicações)  - Universidade de Fortaleza.'],\n",
       "                '14.': ['GARCIA, Fernando Parente;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Samuel Limaverde Verissimo.Redes WMAN sem Fio: Cenários de utilização das tecnologias WiMAX e WiMesh. 2007. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.'],\n",
       "                '15.': ['HOLANDA FILHO, R.',\n",
       "                 '; GARCIA, Fernando Parente.  Participação em banca de Igor Louback de Castro Moura.Implementação de um aplicativo WEB para gerência de redes sem fio. 2006. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.'],\n",
       "                '16.': ['HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Marcus Antonio Borges Sales.Tecnologias de acesso fixo sem fio para telecomunicações. 2006. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.'],\n",
       "                '17.': ['MAIA, J. E. B.;',\n",
       "                 'HOLANDA FILHO, R.',\n",
       "                 '.  Participação em banca de Danilo Roosevelt Perdigão Coimbra.Uma revisao das tecnologias de sistemas de armazenamento de dados. 2006. Trabalho de Conclusão de Curso (Graduação em Informatica)  - Universidade de Fortaleza.']},\n",
       "               'Outras participações': {'1.': ['HOLANDA FILHO, R.',\n",
       "                 ';',\n",
       "                 'Pinheiro, Placido R.',\n",
       "                 '; Vasco, J.J. Peixoto Furtado; MENDONÇA, Nabor das Chagas. Comissão de Seleção da Turma XI do MIA. 2008. Universidade de Fortaleza.'],\n",
       "                '2.': ['HOLANDA FILHO, R.',\n",
       "                 '. Membro do Comitê Científico do XII Seminário Apec. 2007. Universidade de Fortaleza.'],\n",
       "                '3.': ['HOLANDA FILHO, R.',\n",
       "                 '. Parecerista no VII Encontro de Pós-Graduação e Pesquisa. 2007. Universidade de Fortaleza.']}}),\n",
       "             ('Eventos',\n",
       "              {None: {'1.': ['HOLANDA FILHO, R.',\n",
       "                 '. Euro-Par 2008 Conference. 2008. (Congresso).'],\n",
       "                '2.': ['HOLANDA FILHO, R.',\n",
       "                 '. ERCEMAPI - Escola Regional de Computacao Ceará-Maranhao-Piaui. 2007. (Congresso).'],\n",
       "                '3.': ['HOLANDA FILHO, R.',\n",
       "                 '. XII Seminario Associacao dos Estudantes e Pesquisadores Brasileiros na Catalunha. 2007. (Outro).'],\n",
       "                '4.': ['Escola Regional de Computação: Ceará, Maranhão e Piauí.Simulação em RSSF oara Protocolos de Roteamento usando uma Abordagem Geocast. 2009. (Encontro).'],\n",
       "                '5.': ['Mundo Unifor 2009.Tecnologia de Redes de Sensores sem Fio. 2009. (Encontro).'],\n",
       "                '6.': ['Congresso Ceará de Gestão Pública.Gestão Pública. 2008. (Encontro).'],\n",
       "                '7.': ['Escola Regional de Computação: Ceará, Maranhão e Piauí.ERCEMAPI 2008. 2008. (Encontro).'],\n",
       "                '8.': ['6a Semana de Tecnologia da UNIFOR.Novas Tecnologias de Redes sem Fio. 2007. (Encontro).'],\n",
       "                '9.': ['Escola Regional de Computação: Ceará, Maranhão e Piauí.Administração Avançada de Redes. 2007. (Encontro).'],\n",
       "                '10.': ['XIII Encontro de Iniciação à Pesquisa da Unifor.Sistema de Detecção de Ataques baseado em Anomalias de Tráfego. 2007. (Encontro).'],\n",
       "                '11.': ['Curso AWFSS - Aironet Wireless LAN Fundamentals and Cisco Aironet Wireless Site Survey.Curso AWFSS - Aironet Wireless LAN Fundamentals and Cisco Aironet Wireless Site Survey. 2006. (Outra).'],\n",
       "                '12.': ['First Euro-NGI Summer School.Traffic Engineering for the Next Generation Internet. 2004. (Oficina).'],\n",
       "                '13.': ['INTEL Internship Program.INTEL Internship Program. 2004. (Outra).'],\n",
       "                '14.': ['Curso CCNA 4 - Wan Technologies.Curso CCNA 4 - Wan Technologies. 2003. (Outra).'],\n",
       "                '15.': ['Curso CISCO CCNA 1.Curso CISCO CCNA 1. 2003. (Outra).'],\n",
       "                '16.': ['Curso CISCO CCNA 2 - Routers and Routing Basics.Curso CISCO CCNA 2 - Routers and Routing Basics. 2003. (Outra).'],\n",
       "                '17.': ['Curso CISCO CCNA 3 - Switching Basics and Intermediate Routing.Curso CISCO CCNA 3 - Switching Basics and Intermediate Routing. 2003. (Outra).'],\n",
       "                '18.': ['COST 279 First European Summer School.Fundamentals of Wireless Networks. 2002. (Oficina).'],\n",
       "                '19.': ['COST 279 First European Summer School.Queueing Systems and Loss Networks. 2002. (Oficina).'],\n",
       "                '20.': ['COST 279 First European Summer School.Spatial Queueing Models. 2002. (Oficina).'],\n",
       "                '21.': ['Curso Administering Unicenter TNG Asset Management Option / AimIT.Curso Administering Unicenter TNG Asset Management Option / AimIT. 2001. (Outra).'],\n",
       "                '22.': ['Curso Administering Unicenter TNG Software Delivery Option / ShipIT.Curso Administering Unicenter TNG Software Delivery Option / ShipIT. 2001. (Outra).'],\n",
       "                '23.': ['Curso ArcserverIT Administration Fundamentals.Curso ArcserverIT Administration Fundamentals. 2001. (Outra).'],\n",
       "                '24.': ['Curso E Trust Admin Seminar.Curso E Trust Admin Seminar. 2001. (Outra).'],\n",
       "                '25.': ['Curso Unicenter TNG Basics.Curso Unicenter TNG Basics. 2001. (Outra).'],\n",
       "                '26.': ['Curso Unicenter TNG Remote Control Option.Curso Unicenter TNG Remote Control Option. 2001. (Outra).'],\n",
       "                '27.': ['I Encontro de Pós-Graduação e Pesquisa.Trabalho Científico Intitulado Utilização da Tecnologia de Agentes Inteligentes para a Construção de Ambientes Colaborativos de Ensino-Aprendizagem. 2001. (Encontro).'],\n",
       "                '28.': ['XV Encontro de Pesquisa Educacional das Regiões Norte e Nordeste.Encontro de Pesquisa Educacional das Regiões Norte e Nordeste. 2001. (Encontro).'],\n",
       "                '29.': ['5a. Semana Insoft de Tecnologia da Informação.5a. Semana Insoft de tecnologia da Informação. 2000. (Encontro).'],\n",
       "                '30.': ['Curso Lotus.Curso Implementando uma infra-estrutura DOMINO. 2000. (Outra).'],\n",
       "                '31.': ['Projeto de Pesquisa Tele-Ambiente (Protem CC-IE).Palestra com o tema Ambientes Virtuais Cooperativos. 2000. (Oficina).'],\n",
       "                '32.': ['Simpósio Brasilieiro de Informática na Educação.XI Simpósio Brasileiro de Informática na Educação. 2000. (Simpósio).'],\n",
       "                '33.': ['Curso.Curso Lotus Notes - System Administration 2. 1999. (Outra).']}}),\n",
       "             ('Orientações', {}),\n",
       "             ('Orientações e supervisões em andamento', {}),\n",
       "             ('Orientações e supervisões concluídas', {}),\n",
       "             ('Inovação', {'2021 - Atual': ['Monitor Fiscal TCE/CE']})])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['Properties']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 3: Persistir em Neo4j</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Persistir todos dados como propriedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def recursive_persist(tx, parent_node_id, properties):\n",
    "    for key, value in properties.items():\n",
    "        if key is None or key == '':\n",
    "            continue\n",
    "\n",
    "        if isinstance(value, dict):\n",
    "            # Create a new node for the nested dictionary and link it to the parent\n",
    "            result = tx.run(\n",
    "                \"MATCH (p) WHERE id(p) = $parent_id \"\n",
    "                \"CREATE (p)-[:HAS]->(m:SubNode {name: $key}) \"\n",
    "                \"RETURN id(m)\",\n",
    "                parent_id=parent_node_id,\n",
    "                key=key\n",
    "            )\n",
    "            new_node_id = result.single()[0]\n",
    "\n",
    "            # Recursive call to set properties for the new node\n",
    "            recursive_persist(tx, new_node_id, value)\n",
    "        else:\n",
    "            # Directly set the primitive property on the parent node\n",
    "            tx.run(\n",
    "                \"MATCH (n) WHERE id(n) = $id \"\n",
    "                f\"SET n.`{key}` = $value\",\n",
    "                id=parent_node_id,\n",
    "                value=value\n",
    "            )\n",
    "\n",
    "def persist_to_neo4j(extracted_data):\n",
    "    driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            def transactional_work(tx):\n",
    "                name = extracted_data.get('name', 'UNKNOWN')\n",
    "                properties = {k: v for k, v in extracted_data.items() if k != 'name'}\n",
    "                result = tx.run(\n",
    "                    \"MERGE (n:MyLabel {name: $name}) \"\n",
    "                    \"RETURN id(n)\",\n",
    "                    name=name\n",
    "                )\n",
    "                root_node_id = result.single()[0]\n",
    "                recursive_persist(tx, root_node_id, properties)\n",
    "\n",
    "            session.write_transaction(transactional_work)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with your `extracted_data`\n",
    "# persist_to_neo4j(extracted_data)\n",
    "# # Sample invocation\n",
    "# extracted_data = {\n",
    "#     'name': 'João Doé',\n",
    "#     'age': 30,\n",
    "#     None: 'This will be ignored',\n",
    "#     '': 'This will be ignored',\n",
    "#     'nested_dict': {\n",
    "#         'key1': 'value1',\n",
    "#         'key2': None\n",
    "#     },\n",
    "#     'nested_list': [1, 2, 3]  # This will now be converted to a JSON string\n",
    "# }\n",
    "# persist_to_neo4j(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_primitive(data):\n",
    "    if isinstance(data, dict):\n",
    "        new_data = {}\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, dict):\n",
    "                new_data[key] = json.dumps(value, ensure_ascii=False)\n",
    "            else:\n",
    "                new_data[key] = value\n",
    "        return new_data\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def recursive_persist(tx, parent_node, properties):\n",
    "    for key, value in properties.items():\n",
    "        if key is None or key == '':\n",
    "            continue\n",
    "\n",
    "        if isinstance(value, dict):\n",
    "            # Create a new node for the nested dictionary\n",
    "            result = tx.run(\n",
    "                \"CREATE (m:SubNode) \"\n",
    "                \"SET m.name = $key \"\n",
    "                \"RETURN m\",\n",
    "                key=key\n",
    "            )\n",
    "            new_node = result.single()[0]\n",
    "\n",
    "            # Link the new node to the parent node\n",
    "            tx.run(\n",
    "                \"MATCH (a), (b) \"\n",
    "                \"WHERE id(a) = $id_a AND id(b) = $id_b \"\n",
    "                \"CREATE (a)-[:HAS]->(b)\",\n",
    "                id_a=parent_node.id,\n",
    "                id_b=new_node.id\n",
    "            )\n",
    "\n",
    "            # Recursive call to set properties for the new node\n",
    "            recursive_persist(tx, new_node, value)\n",
    "        else:\n",
    "            # Directly set the primitive property on the parent node\n",
    "            query = f\"MATCH (n) WHERE id(n) = $id SET n.{key} = $value\"\n",
    "            tx.run(query, id=parent_node.id, value=value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Armazenta tudo dentro do Propriedades no Neo4j\n",
    "# def persist_to_neo4j(extracted_data):\n",
    "#     converted_data = convert_to_primitive(extracted_data)\n",
    "#     driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     try:\n",
    "#         with driver.session() as session:\n",
    "#             def transactional_work(tx):\n",
    "#                 name = converted_data.get('name', 'UNKNOWN')\n",
    "#                 properties = {k: v for k, v in converted_data.items() if k != 'name'}\n",
    "#                 result = tx.run(\n",
    "#                     \"MERGE (n:MyLabel {name: $name}) \"\n",
    "#                     \"RETURN n\",\n",
    "#                     name=name\n",
    "#                 )\n",
    "#                 root_node = result.single()[0]\n",
    "#                 recursive_persist(tx, root_node, properties)\n",
    "                \n",
    "#             session.write_transaction(transactional_work)\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#     finally:\n",
    "#         driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para criar parâmetros dinâmicos e persistir níveis de detales usa-se APOC\n",
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    items = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_dict(v, new_key, sep=sep))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "def persist_to_neo4j(extracted_data):\n",
    "    driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            name = extracted_data.get('name', 'UNKNOWN')\n",
    "            session.write_transaction(lambda tx: tx.run(\n",
    "                \"MERGE (n:Person {name: $name}) RETURN n\",\n",
    "                name=name,\n",
    "            ))\n",
    "            \n",
    "            # Flatten the dictionary\n",
    "            flat_data = flatten_dict(extracted_data)\n",
    "\n",
    "            # Persist each flattened key-value pair\n",
    "            for key, value in flat_data.items():\n",
    "                if key is None or key == '':\n",
    "                    continue\n",
    "                \n",
    "                query = f\"MATCH (n:Person {{name: $name}}) SET n.`{key}` = $value RETURN n\"\n",
    "                session.write_transaction(lambda tx, query=query, value=value: tx.run(query, name=name, value=value))\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Persistir produções como nós secundários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "## Persiste as produções como nós secundários ligados ao nó principal\n",
    "def add_producao(tx, production_dict):\n",
    "    # Extract person's name and properties sub-dictionary\n",
    "    person_name = production_dict.get('name', 'UNKNOWN')\n",
    "    properties_dict = production_dict.get('Properties', {})\n",
    "\n",
    "    # First, merge the Person node based on the name\n",
    "    tx.run(\"MERGE (a:Person {name: $person_name})\", person_name=person_name)\n",
    "\n",
    "    # Initialize an empty dictionary to hold the cleaned production data\n",
    "    producoes = properties_dict.get('Produções', [])\n",
    "    \n",
    "    for producao in producoes:\n",
    "        clean_producao = {}\n",
    "        \n",
    "        # Replace None with 'NULL' and ignore empty strings\n",
    "        for key, value in producao.items():\n",
    "            if value == '':\n",
    "                continue\n",
    "            if value is None:\n",
    "                value = 'NULL'\n",
    "            clean_producao[key] = value\n",
    "\n",
    "        # Create or merge the Producao node based on unique keys ('ano' and 'revista' here)\n",
    "        tx.run(\n",
    "            \"MERGE (p:Producao {ano: $ano, revista: $revista}) \"\n",
    "            \"SET p += $props\",\n",
    "            ano=clean_producao.get('ano'),\n",
    "            revista=clean_producao.get('revista'),\n",
    "            props=clean_producao\n",
    "        )\n",
    "\n",
    "        # Connect the Person node to the Producao node\n",
    "        tx.run(\n",
    "            \"MATCH (a:Person {name: $person_name}), (p:Producao {ano: $ano, revista: $revista}) \"\n",
    "            \"MERGE (a)-[r:HAS_PRODUCOES]->(p)\",\n",
    "            person_name=person_name,\n",
    "            ano=clean_producao.get('ano'),\n",
    "            revista=clean_producao.get('revista')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teste funcional\n",
    "# def main():\n",
    "#     uri = \"bolt://localhost:7687\"\n",
    "#     driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "#     production_dict = {\n",
    "#         'name': 'John Doe',\n",
    "#         'Properties': {\n",
    "#             'Produções': [\n",
    "#                 {'ano': '2023', 'revista': 'Journal A', 'jcr': '3.662', 'doi': 'some_doi'},\n",
    "#                 {'ano': '2022', 'revista': 'Journal B', 'jcr': None, 'doi': 'another_doi'}\n",
    "#             ]\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     with driver.session() as session:\n",
    "#         session.write_transaction(add_producao, production_dict)\n",
    "\n",
    "#     driver.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dict = parse_resume(soup)\n",
    "resume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_to_neo4j(resume_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = {\"Label\": \"Person\", \"name\": {}, \"Properties\": {}}\n",
    "articles_dict = parse_jcr_articles(soup)\n",
    "# articles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create a session\n",
    "session = driver.session()\n",
    "\n",
    "# Call the function within a transaction\n",
    "session.write_transaction(add_producao, articles_dict)\n",
    "\n",
    "# Optionally, close the session and driver\n",
    "session.close()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Persiste todod dados como propriedades de um nó\n",
    "main_cell     = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "generic_dict = {\"Label\": \"Person\", \"Properties\": {}}\n",
    "if main_cell:\n",
    "    traverse(main_cell, generic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_to_neo4j(generic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento de Entidades Nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    import spacy\n",
    "    \n",
    "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "    nlp_pt = spacy.load(\"pt_core_news_lg\")\n",
    "    doc = nlp_pt(text)\n",
    "    entities = {'ORG': [], 'GPE': [], 'NORP': [], 'PERSON': [], 'PRODUCT': [], 'WORK_OF_ART': []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModel:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def add_entity(self, entity_type, entity_value):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._add_entity, entity_type, entity_value)\n",
    "            \n",
    "    @staticmethod\n",
    "    def _add_entity(tx, entity_type, entity_value):\n",
    "        query = f\"MERGE (a:{entity_type} {{name: $name}})\"\n",
    "        tx.run(query, name=entity_value)\n",
    "    \n",
    "    def add_relation(self, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._add_relation, src_type, src_name, rel_type, tgt_type, tgt_name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_relation(tx, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "        query = (\n",
    "            f\"MATCH (a:{src_type} {{name: $src_name}}), (b:{tgt_type} {{name: $tgt_name}}) \"\n",
    "            f\"MERGE (a)-[:{rel_type}]->(b)\"\n",
    "        )\n",
    "        tx.run(query, src_name=src_name, tgt_name=tgt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    def expand(key, value):\n",
    "        if isinstance(value, dict):\n",
    "            return [(str(key) + '.' + str(k), v) for k, v in flatten_dict(value).items()]\n",
    "        else:\n",
    "            return [(str(key), value)]\n",
    "        \n",
    "    items = [item for k, v in d.items() for item in expand(k, v)]\n",
    "    return dict(items)\n",
    "\n",
    "def main(data_dict):\n",
    "    graph_model = GraphModel(\"bolt://localhost:7687\", \"neo4j\", \"password\")    \n",
    "    flattened_data = flatten_dict(data_dict['Properties']['Identificação'])\n",
    "    person_name = flattened_data.get('Nome', None)[0]\n",
    "    print(f'Nome: {person_name}')\n",
    "    if person_name:\n",
    "        graph_model.add_entity(\"Person\", person_name)\n",
    "    \n",
    "    institutional = flatten_dict(data_dict['Properties'])\n",
    "    for key, value in institutional.items():\n",
    "        if key in ['Endereço.Endereço Profissional']:\n",
    "            text = ' '.join(value)\n",
    "            print(text)\n",
    "            entities = extract_named_entities(text)\n",
    "            print(f'Entidades reconhecidas: {entities}')\n",
    "            entities\n",
    "            \n",
    "            for org in entities.get('ORG', []):\n",
    "                graph_model.add_entity(\"Organization\", org)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"AFFILIATED_WITH\", \"Organization\", org)\n",
    "                \n",
    "            for gpe in entities.get('GPE', []):\n",
    "                graph_model.add_entity(\"Location\", gpe)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "    projetos = flatten_dict(data_dict['Properties'])\n",
    "    for key, value in projetos.items():\n",
    "        if key in ['Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento']:\n",
    "            entities = extract_named_entities(value)\n",
    "            \n",
    "            for org in entities.get('ORG', []):\n",
    "                graph_model.add_entity(\"Organization\", org)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"HAS_PROJECT_IN\", \"Organization\", org)\n",
    "                \n",
    "            for gpe in entities.get('GPE', []):\n",
    "                graph_model.add_entity(\"Location\", gpe)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "    graph_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Processamento para eliminar strings duplicadas e vazias\n",
    "def remove_duplicates(entities):\n",
    "    for key in entities.keys():\n",
    "        seen = set()\n",
    "        unique_values = []\n",
    "        \n",
    "        for value in entities[key]:\n",
    "            lower_value = value.lower()\n",
    "            if lower_value not in seen and value.strip():\n",
    "                seen.add(lower_value)\n",
    "                unique_values.append(value)\n",
    "        \n",
    "        # Atualização do dicionário\n",
    "        entities[key] = unique_values\n",
    "    return entities\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    nlp = spacy.load('en_core_web_lg')  # Assuming the use of the small Portuguese model\n",
    "    doc = nlp(text)\n",
    "    entities = {'ORG': [], 'GPE': [], 'PHONE': [], 'URL': []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities.keys():\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    \n",
    "    unique_entities = remove_duplicates(entities)\n",
    "    \n",
    "    return unique_entities\n",
    "\n",
    "text = 'Pós-Doutor em Computação pela Sorbonne Université-Pierre et Marie Curie (França - 2020). Doutor em Ciência da Computação pela Universitat Politecnica de Catalunya (Espanha - 2005). Atualmente é professor titular da Universidade de Fortaleza - UNIFOR na qual é membro permanente dos programas de pós-graduação em Informática Aplicada (mestrado e doutorado) e do mestrado profissional em administração. Possui mais de 100 trabalhos publicados em conferências e periódicos nacionais e internacionais. Tem experiência na área de Ciência da Computação, com ênfase em Ciência de Dados e Teleinformática, atuando principalmente nos seguintes temas: Redes Complexas, Grafos de Conhecimento, Aprendizagem de Máquina, Internet das Coisas, Segurança e Blockchain. (Texto informado pelo autor)\\n'\n",
    "entities = extract_named_entities(text)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = node_raimir\n",
    "print(data_dict['Properties'].keys())\n",
    "\n",
    "main(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 3: Extrair Lista de Currículos Lattes</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de extração de lista de currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  #seletorcss=\"div.paginacao:nth-child(4) > a:nth-child(2)\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas\n",
    "\n",
    "def achar_busca(browser, delay):\n",
    "    '''\n",
    "    Função para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = browser.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)\n",
    "\n",
    "def preencher_busca(browser, delay, NOME):\n",
    "    '''\n",
    "    Função para passar o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)\n",
    "\n",
    "def definir_filtros(browser, delay, mestres=True, assunto=False):\n",
    "    '''\n",
    "    Clica nos check-boxes para definir os filtros de buscas\n",
    "    Para buscar por Assuntos usar parâmetro True, caso omitido fará busca por Nome por default\n",
    "    '''\n",
    "    from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import traceback   \n",
    "    \n",
    "    ## Aguardar carregar e clicar em checkbox de Assunto\n",
    "    try:\n",
    "        if mestres == True:\n",
    "            limite=2\n",
    "            ## Aguardar opção dropdown ser carregada e clicar em sua checkbox\n",
    "            css_buscar_demais = '#buscarDemais'\n",
    "            checkbox_buscar_demais = browser.find_element(By.CSS_SELECTOR, css_buscar_demais)\n",
    "            \n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, css_buscar_demais))),\n",
    "                   wait_ms=150,\n",
    "                   limit=limite, \n",
    "                   on_exhaust=(f'Problema clicar em {checkbox_buscar_demais}, {limite} tentativas sem sucesso.'))\n",
    "            \n",
    "            time.sleep(1)\n",
    "            checkbox_buscar_demais.click()\n",
    "            print(f'Clique efetuado em {checkbox_buscar_demais}')\n",
    "\n",
    "        # css_estrangeiros  = '#buscarEstrangeiros'\n",
    "\n",
    "        if assunto == True:\n",
    "            ## Implementar número de retentativas para casos de conexão muito instável\n",
    "            limite=5\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#buscaAssunto\"))).click(),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "        \n",
    "            ## Aguardar opção Atuação Profissional ser carregada e clicar em sua checkbox\n",
    "            xpath_atuacaoprofissional = \".//*[contains(text(), 'Atuação profissional')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_atuacaoprofissional))).click()\n",
    "\n",
    "            ## Aguardar opção Ciências da Saúde ser carregada e clicar em sua checkbox\n",
    "            xpath_cienciassaude = \".//*[contains(text(), 'Ciências da Saúde')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_cienciassaude))).click()\n",
    "            #browser.find_element_by_xpath(xpath_cienciassaude).click()        \n",
    "\n",
    "            ## Aguardar opção Enfermagem ser carregada e clicar em sua checkbox\n",
    "            xpath_enfermagem    = \".//*[contains(text(), 'Enfermagem')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_enfermagem))).click()\n",
    "            #browser.find_element_by_xpath(xpath_enfermagem).click()\n",
    "            aplicar_link  = browser.find_element(By.LINK_TEXT, 'Aplicar')\n",
    "            aplicar_link.click()\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f'Erro na função definir_filtros()')\n",
    "        print(e)\n",
    "        # traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        # print(e, traceback_str) \n",
    "\n",
    "def procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(browser)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            ## TO-DO inserir a crítica para o 'Stale file handle'\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(browser)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(browser)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(browser, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', browser\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, browser\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # browser.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = browser.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', browser\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, browser\n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, browser\n",
    "\n",
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    title = soup.title.string if soup.title else \"Unknown\"\n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "\n",
    "\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    return header_node\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_data[key] = value\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that contains the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_lista(lista, mestres=True, assunto=False):\n",
    "    sucesso=[]\n",
    "    falhas=[]\n",
    "    duvidas=[]\n",
    "    tipo_erro=[]\n",
    "    curriculos=[]\n",
    "    rotulos=[]\n",
    "    conteudos=[]\n",
    "\n",
    "    delay=10\n",
    "    limite=3\n",
    "    instituicao = 'Fundação Oswaldo Cruz'\n",
    "    unidade     = 'Fiocruz Ceará'\n",
    "    termo       = 'Ministerio da Saude'\n",
    "\n",
    "    t0 = time.time()\n",
    "    browser = connect_driver(caminho)\n",
    "    for NOME in lista:\n",
    "        try:\n",
    "            preencher_busca(browser, delay, NOME)\n",
    "            elemento_achado, nome_falha, duvida, erro, browser = procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite)\n",
    "            link_nome     = achar_busca(browser, delay)\n",
    "            window_before = browser.current_window_handle\n",
    "            \n",
    "            if str(elemento_achado) == 'nan':\n",
    "                print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "                falhas.append(nome_falha)\n",
    "                duvidas.append(duvida)\n",
    "                tipo_erro.append(erro)\n",
    "                # print(nome_falha)\n",
    "                # print(erro)\n",
    "                # clear_output(wait=True)\n",
    "                raise Exception\n",
    "            print('Vínculo encontrado no currículo de nome:',elemento_achado.text)\n",
    "\n",
    "            ## Clicar no botão abrir currículo e mudar de aba\n",
    "            try:\n",
    "                ## Aguarda, encontra, clica em buscar nome\n",
    "                link_nome    = achar_busca(browser, delay)\n",
    "                nome_buscado = []\n",
    "                nome_achado  = []\n",
    "                nome_buscado.append(NOME)\n",
    "                \n",
    "                if link_nome.text == None:\n",
    "                    xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "                    # 'Stale file handle'\n",
    "                    print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "                    retry(WebDriverWait(browser, delay).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "                try:\n",
    "                    ActionChains(browser).click(link_nome).perform()\n",
    "                    nome_achado.append(link_nome.text)\n",
    "                except:\n",
    "                    print(f'Currículo não encontrado para: {NOME}.')\n",
    "                    return\n",
    "                \n",
    "                retry(WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "                \n",
    "                # Clicar botão para abrir o currículo\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                time.sleep(0.2)\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "\n",
    "                # Pega o código fonte da página\n",
    "                page_source = browser.page_source\n",
    "\n",
    "                # Usa BeautifulSoup para analisar\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                # Extração e Persistência do cabeçalho\n",
    "                dict_header = parse_header(soup)\n",
    "                header_node = persist_to_neo4j(dict_header)\n",
    "                \n",
    "                # Extração e Persistência dos elementos H1\n",
    "                graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "                parse_h1_elements(soup, header_node, graph)\n",
    "                graph, cv_node, properties = parse_parsoninfo(soup)                \n",
    "\n",
    "            except Exception as e:\n",
    "                print('Erro',e)\n",
    "                print('Tentando nova requisição ao servidor')\n",
    "                time.sleep(1)\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "                time.sleep(1)\n",
    "\n",
    "            sucesso.append(NOME)\n",
    "\n",
    "        except:\n",
    "            print(f'Currículo não encontrado: {NOME}')\n",
    "            browser.back()\n",
    "            continue\n",
    "\n",
    "    df_dados =pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(curriculos),\n",
    "        'ROTULOS': pd.Series(rotulos),\n",
    "        'CONTEUDOS': pd.Series(conteudos),\n",
    "            })\n",
    "    \n",
    "    t6=time.time()\n",
    "    print('='*95)\n",
    "    # print(f' {len(sucesso)} currículos extraídos com sucesso')\n",
    "    print(f' Tempo total para extrair {len(conteudos)} seções dos currículos: {tempo(t0,t6)}')\n",
    "    # print('='*95)\n",
    "    browser.quit()\n",
    "    \n",
    "    return df_dados, sucesso  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 4: Funções montar dicionários</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_data[key] = value\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that contains the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        data_dict = {}\n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "\n",
    "            higher_order_key = None  # Inicializa variável para armazenar a chave de ordem superior\n",
    "            data_list = []  # Inicialize lista para armazenar entradas de índices de dataframe\n",
    "            \n",
    "            parag_elements = parent_div.find_all('p')\n",
    "            if parag_elements:\n",
    "                for idx, elem in enumerate(parag_elements):\n",
    "                    class_name = elem.get('class', [None])[0]  # Assumes only one class; otherwise, join them into a single string\n",
    "                    higher_order_key = class_name\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "                    data_entry = {'rotulos': class_name, 'conteudos': elem.text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "\n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                kdict_elements = cell.find_all('b')\n",
    "                # print(len(kdict_elements))\n",
    "\n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "\n",
    "                index_elems   = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for key_elem, details_elem in zip(index_elems, details_elems):\n",
    "                    key_text     = key_elem.text.strip() if key_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    data_entry = {'rotulos': key_text, 'conteudos': details_text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "                \n",
    "            if higher_order_key is None:\n",
    "                # Se nenhuma chave de ordem superior for encontrada, associa a lista de dados diretamente ao título\n",
    "                result_dict[title_text] = data_list\n",
    "            else:\n",
    "                # Caso contrário, associa o data_dict contendo chaves de ordem superior ao título\n",
    "                result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def generate_dataframe_and_neo4j_dict(data_dict):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame and a dictionary for Neo4j persistence, incorporating section and subsection names.\n",
    "\n",
    "    Parameters:\n",
    "        data_dict (dict): A nested dictionary containing section and subsection data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame aggregating all sections and subsections, with additional columns specifying their names.\n",
    "        dict: A dictionary intended for Neo4j persistence, formatted according to the Neo4j data model.\n",
    "    \"\"\"\n",
    "\n",
    "    all_frames = []  # List to store DataFrames corresponding to each section and subsection\n",
    "    neo4j_dict = {}  # Dictionary for Neo4j persistence\n",
    "\n",
    "    for section, items in data_dict.items():\n",
    "        if section:  # Exclude empty sections\n",
    "            neo4j_dict[section] = {}\n",
    "            if isinstance(items, list):  # If items is a list, convert to DataFrame\n",
    "                df = pd.DataFrame(items)\n",
    "                df['Section'] = section  # Append a column for the section name\n",
    "                all_frames.append(df)\n",
    "                neo4j_dict[section] = items  # For list items, add them as they are\n",
    "\n",
    "            elif isinstance(items, dict):  # If items is a dictionary, explore subsections\n",
    "                for subsection, subitems in items.items():\n",
    "                    if subitems:  # Exclude empty subsections\n",
    "                        df = pd.DataFrame(subitems)\n",
    "                        df['Subsection'] = subsection  # Append a column for the subsection name\n",
    "                        df['Section'] = section  # Append a column for the section name\n",
    "                        all_frames.append(df)\n",
    "                        neo4j_dict[section][subsection] = subitems  # Store subsection data\n",
    "\n",
    "    # Concatenate all DataFrames vertically to form one unified DataFrame\n",
    "    dataframe = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "    return dataframe, neo4j_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução das extrações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair todas seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = extract_data(soup)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair seções específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_articles = extrair_artigos(soup)\n",
    "dict_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_dict.items():\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe, neo4j_dict = generate_dataframe_and_neo4j_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exibição dos dataframes por seção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(neo4j_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Identificação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Endereço']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Formação acadêmica/titulação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Formação Complementar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Linhas de pesquisa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Projetos de desenvolvimento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Áreas de atuação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Idiomas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Atuação Profissional_Fundação Oswaldo Cruz, FIOCRUZ, Brasil.'\n",
    " 'Produções_Produção bibliográfica',\n",
    " 'Bancas_Participação em bancas de trabalhos de conclusão',\n",
    " 'Eventos_Participação em eventos, congressos, exposições e feiras',\n",
    " 'Orientações_Orientações e supervisões concluídas',\n",
    " 'Inovação_Projeto de desenvolvimento tecnológico']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa estruturação funcionou bem para as seções que tem anos como chaves, mas falha para em seções com subníveis de detalhamento, como nas produções e no resumo, onde os dados de valores não foram populados no dicionário embora as chaves tenham sido extraídas com sucesso.\n",
    "\n",
    "Pode ser por falta de algum elemento, classe ou marcador não apontada ou não hierarquizada corretamente na função de extração.\n",
    "\n",
    "Deve-se escolher outros marcadores para complementar a extração, ou usar o atual somente para seção 'Formação acadêmica/titulação'.\n",
    "\n",
    "Falhas não capturou corretamente:\n",
    "- subchaves de Atuação Profissional\n",
    "- subchaves de Orientações\n",
    "- subchaves de Produções pra Livros e Capítulos de livros\n",
    "\n",
    "Falhas não capturoud e forma alguma:\n",
    "- Projetos de Pesquisa\n",
    "- Inovação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não funcionou tão bem para a Atuação profissional pois não capturou chaves corretas\n",
    "atuaprof_df = neo4j_dict['Atuação Profissional_Fundação Oswaldo Cruz, FIOCRUZ, Brasil.']\n",
    "atuaprof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_df = neo4j_dict['Linhas de pesquisa']\n",
    "linhas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projdesv_df = neo4j_dict['Projetos de desenvolvimento']\n",
    "projdesv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projpesq_df = neo4j_dict['Áreas de atuação']\n",
    "projpesq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Produção bibliográfica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair marcadores CSS específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_classes_content(soup_element, target_classes):\n",
    "    result_dict = defaultdict(list)\n",
    "    \n",
    "    for t_class in target_classes:\n",
    "        for element in soup_element.find_all(class_=t_class):\n",
    "            text_content = element.text.strip()\n",
    "            result_dict[t_class].append(text_content)\n",
    "            \n",
    "    return dict(result_dict)\n",
    "\n",
    "target_classes = [\n",
    "        # 'infpessoa', \n",
    "        'nome', \n",
    "        'resumo', \n",
    "        # 'artigo-completo', \n",
    "        # 'cita-artigos', \n",
    "        # 'citacoes', \n",
    "        # 'detalhes', \n",
    "        # 'fator', \n",
    "        # 'foto', \n",
    "        # 'informacao-artigo', \n",
    "        # 'informacoes-autor', \n",
    "        # 'rodape-cv', \n",
    "        # 'science_cont', \n",
    "        # 'texto',\n",
    "        #  \n",
    "        # 'cita', \n",
    "        # 'trab'\n",
    "        ]\n",
    "\n",
    "result_dict = extract_classes_content(soup.body, target_classes)\n",
    "from pprint import pprint\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções para interagir com Neo4j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database credentials and URI\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    try:\n",
    "        graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    except:\n",
    "        print('Erro ao conectar ao Neo4j')\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'].split('(')[1].strip(')'), meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    print(type(header_node))\n",
    "\n",
    "    return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict, List\n",
    "\n",
    "def create_or_update_publications(graph: Graph, publications_dict: Dict[str, Dict[str, List[Dict[str, str]]]]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its associated publications in the Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        publications_dict (Dict[str, Dict[str, List[Dict[str, str]]]]): Dictionary containing publication information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    publications_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Search for existing researcher node by name\n",
    "    existing_node = matcher.match(\"Researcher\", name=publications_dict['Node Name']).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding publications.\")\n",
    "\n",
    "    # Process publications\n",
    "    for publication in publications_dict['Properties']['Produções']:\n",
    "        # Check if a similar publication already exists\n",
    "        existing_pub_node = matcher.match(\"Publication\", doi=publication['doi']).first()\n",
    "\n",
    "        if not existing_pub_node:\n",
    "            pub_node = Node(\"Publication\", **publication)\n",
    "            graph.create(pub_node)\n",
    "            publications_created += 1\n",
    "        else:\n",
    "            for key, value in publication.items():\n",
    "                if key not in existing_pub_node or existing_pub_node[key] != value:\n",
    "                    existing_pub_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            pub_node = existing_pub_node\n",
    "            graph.push(pub_node)\n",
    "\n",
    "        # Create or update relationship between researcher and publication\n",
    "        rel = Relationship(existing_node, \"PUBLISHED\", pub_node)\n",
    "        graph.merge(rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Publications created: {publications_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    if 'nome' in researcher_dict and 'resumo' in researcher_dict:\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['nome'][0] if isinstance(researcher_dict['nome'], list) else researcher_dict['nome']\n",
    "        summary = researcher_dict['resumo'][0] if isinstance(researcher_dict['resumo'], list) else researcher_dict['resumo']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, resumo=summary)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'nome' and 'resumo'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'nome': ['John Doe'], 'resumo': ['This is a summary.']}\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "# create_researcher_node(graph, researcher_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infopessoa(result_dict):\n",
    "    nome,_,_,endereco_lattes,_,idlattes,_,_,atualizacao = result_dict['infpessoa'][0].split('\\n')\n",
    "\n",
    "    dict_info = {\n",
    "        'NOME' : nome,\n",
    "        # 'link_lattes' : endereco_lattes.split(': ')[1],\n",
    "        'IDLATTES' : idlattes.split(': ')[1],\n",
    "        'ATUALIZAÇÃO' : atualizacao.split('Última atualização do currículo em ')[1],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(dict_info, index=[0]).T\n",
    "    df.columns = ['DADOS_CURRICULO']\n",
    "    return df, dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "\n",
    "def generate_lattes_dict(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a dictionary from a BeautifulSoup object to be persisted in Neo4j.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML/XML data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the relevant information for Neo4j persistence.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold the extracted data\n",
    "    lattes_data = {}\n",
    "    \n",
    "    # Extracting researcher's name as an example\n",
    "    name_section = soup.find('div', {'id': 'name-section'})\n",
    "    if name_section:\n",
    "        lattes_data['researcher_name'] = name_section.text.strip()\n",
    "    \n",
    "    # Extracting list of publications as an example\n",
    "    publications = []\n",
    "    for pub in soup.find_all('div', {'class': 'publication'}):\n",
    "        publication_data = {}\n",
    "        title = pub.find('span', {'class': 'title'})\n",
    "        authors = pub.find('span', {'class': 'authors'})\n",
    "        \n",
    "        if title:\n",
    "            publication_data['title'] = title.text.strip()\n",
    "        \n",
    "        if authors:\n",
    "            publication_data['authors'] = authors.text.strip().split(',')\n",
    "        \n",
    "        publications.append(publication_data)\n",
    "    \n",
    "    lattes_data['publications'] = publications\n",
    "    \n",
    "    # Additional extractions can be performed as per the requirements\n",
    "    \n",
    "    return lattes_data\n",
    "\n",
    "# Example usage (Assuming 'some_html_content' contains the HTML content)\n",
    "# soup = BeautifulSoup(some_html_content, 'html.parser')\n",
    "# lattes_dict = generate_lattes_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infopessoa, dict_pessoa = infopessoa(result_dict)\n",
    "df_infopessoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pessoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node_from_dict(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "create_researcher_node_from_dict(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    nodes_created = 0\n",
    "    nodes_updated = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "\n",
    "        # Create a NodeMatcher object\n",
    "        matcher = NodeMatcher(graph)\n",
    "\n",
    "        # Look for existing nodes with the same name\n",
    "        existing_node = matcher.match(\"Researcher\", name=name).first()\n",
    "\n",
    "        if existing_node:\n",
    "            # Update properties of existing node\n",
    "            for key, value in researcher_dict.items():\n",
    "                if key.lower() not in existing_node or existing_node[key.lower()] != value:\n",
    "                    existing_node[key.lower()] = value\n",
    "                    properties_updated += 1\n",
    "\n",
    "            # Push the changes to the database\n",
    "            graph.push(existing_node)\n",
    "            nodes_updated += 1\n",
    "\n",
    "        else:\n",
    "            # Create a new node of type 'Researcher'\n",
    "            researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "            # Add the node to the Neo4j database\n",
    "            graph.create(researcher_node)\n",
    "            nodes_created += 1\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"Nodes created: {nodes_created}\")\n",
    "        print(f\"Nodes updated: {nodes_updated}\")\n",
    "        print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'NOME': 'John Doe', 'IDLATTES': '0000000000000000', 'ATUALIZAÇÃO': '31/12/2023'}\n",
    "\n",
    "# Create or update a Researcher node in Neo4j based on the dictionary\n",
    "create_or_update_researcher_node(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vinculo = extract_academic(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_professional_links(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its professional links in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's professional information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    relationships_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Look for existing nodes with the same name\n",
    "    existing_node = matcher.match(\"Researcher\", name=researcher_dict.get('Nome')).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding professional links.\")\n",
    "\n",
    "    # Process professional affiliations and activities\n",
    "    for key, value in researcher_dict.items():\n",
    "        if key not in ['Nome', 'Endereço Profissional']:\n",
    "            # Create or find the organization/affiliation node\n",
    "            org_node = matcher.match(\"Organization\", name=key).first()\n",
    "            if not org_node:\n",
    "                org_node = Node(\"Organization\", name=key)\n",
    "                graph.create(org_node)\n",
    "\n",
    "            # Create or update the relationship\n",
    "            rel_type = \"AFFILIATED_WITH\" if 'Atual' in value else \"HAS_BEEN_AFFILIATED_WITH\"\n",
    "            rel = Relationship(existing_node, rel_type, org_node, details=value)\n",
    "\n",
    "            # Check if a similar relationship already exists\n",
    "            existing_rel = None\n",
    "            for r in graph.match((existing_node, org_node), r_type=rel_type):\n",
    "                if r['details'] == value:\n",
    "                    existing_rel = r\n",
    "                    break\n",
    "\n",
    "            # Create new or update existing relationship\n",
    "            if not existing_rel:\n",
    "                graph.create(rel)\n",
    "                relationships_created += 1\n",
    "            else:\n",
    "                for property_name, property_value in rel.items():\n",
    "                    if property_name not in existing_rel or existing_rel[property_name] != property_value:\n",
    "                        existing_rel[property_name] = property_value\n",
    "                        properties_updated += 1\n",
    "                graph.push(existing_rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Relationships created: {relationships_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update professional links in Neo4j based on the dictionary\n",
    "create_or_update_professional_links(graph, dict_vinculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('./../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_path = './../data/classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx'\n",
    "xlsx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "def persist_journals_from_xlsx(graph: Graph, xlsx_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Persist journal information into a Neo4j database from an Excel (.xlsx) file.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        xlsx_path (str): The path to the Excel (.xlsx) file containing the journal information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize counters for tracking\n",
    "    nodes_created = 0\n",
    "    properties_updated = 0\n",
    "    \n",
    "    # Create NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "    \n",
    "    # Read the Excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract journal properties\n",
    "        properties = {'ISSN': row['ISSN'],\n",
    "                      'Título': row['Título'],\n",
    "                      'Área de Avaliação': row['Área de Avaliação'],\n",
    "                      'Estrato': row['Estrato']}\n",
    "        \n",
    "        # Check if a node with the same ISSN already exists\n",
    "        existing_node = matcher.match(\"Journal\", ISSN=row['ISSN']).first()\n",
    "        \n",
    "        if existing_node:\n",
    "            for key, value in properties.items():\n",
    "                if key not in existing_node or existing_node[key] != value:\n",
    "                    existing_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            graph.push(existing_node)\n",
    "        else:\n",
    "            new_node = Node(\"Journal\", **properties)\n",
    "            graph.create(new_node)\n",
    "            nodes_created += 1\n",
    "    \n",
    "    # Print statistical data\n",
    "    print(f\"Nodes created: {nodes_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given the path to the Excel file\n",
    "# xlsx_path = \"path/to/excel/file.xlsx\"\n",
    "\n",
    "# Persist the journal information from the Excel file\n",
    "persist_journals_from_xlsx(graph, xlsx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = pd.DataFrame([extract_academic(soup)]).T\n",
    "df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_projects = extract_research_project(soup)\n",
    "extracted_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência do cabeçalho\n",
    "header_data = parse_header(soup)\n",
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_node = \n",
    "# header_node = persist_to_neo4j(header_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_article = result_dict['artigo-completo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados=[]\n",
    "for i in result_dict['artigo-completo']:\n",
    "    dados.append(i.split('\\n\\n'))\n",
    "\n",
    "def quebra(linha):\n",
    "    return [x.split('\\n') for x in linha]\n",
    "\n",
    "for i in dados:\n",
    "    print(quebra(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\n",
    "        'artigo-completo', 'cita', 'cita-artigos', 'citacoes', 'detalhes', 'fator', \n",
    "        'foto', 'informacao-artigo', 'informacoes-autor', 'infpessoa', 'nome', \n",
    "        'resumo', 'rodape-cv', 'science_cont', 'texto', 'trab'\n",
    "        ]\n",
    "\n",
    "def extract_selected_classes(soup, target_classes):\n",
    "    \"\"\"\n",
    "    Extrai conteúdos de classes específicas de um objeto soup extraído de documento HTML.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Objeto soup e lista de classes a serem extraídas.\n",
    "\n",
    "    Retorno:\n",
    "    - Um dicionário que mapeia o nome da classe à lista de conteúdos extraídos.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Dicionário para armazenar os conteúdos\n",
    "    content_dict = defaultdict(list)\n",
    "    \n",
    "    # Iteração através das classes alvo para extração de conteúdo\n",
    "    for target_class in target_classes:\n",
    "        elements = soup.find_all(class_=target_class)\n",
    "        for element in elements:\n",
    "            print(element.text)\n",
    "            dado = element.text.split('\\n')\n",
    "\n",
    "            for i in dado:\n",
    "                if i != '':\n",
    "                    content_dict[target_class].append(dado)\n",
    "            \n",
    "    return dict(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\n",
    "        'informacao-artigo'\n",
    "        ]\n",
    "extract_selected_classes(soup, target_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outras formas de extrair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_subsection(section_title, parent_dict, data):\n",
    "#     sub_key = f\"Produções-{section_title}\"\n",
    "#     parent_dict[sub_key] = data\n",
    "\n",
    "# def traverse(soup, parent_dict, current_section=None, root_properties=None):\n",
    "#     \"\"\"\n",
    "#     Traverses through the soup object recursively and populates the dictionary.\n",
    "#     root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "#     current_section: the current section being processed, allows to set subsections.\n",
    "#     \"\"\"\n",
    "#     subsections = ['Artigos completos publicados em periódicos',\n",
    "#                    'Livros publicados/organizados ou edições',\n",
    "#                    'Trabalhos completos publicados em anais de congressos',\n",
    "#                    'Trabalhos de conclusão de curso de graduação',\n",
    "#                    'Participação em eventos, congressos, exposições e feiras',\n",
    "#                    'Trabalho de conclusão de curso de graduação'\n",
    "#                    ]\n",
    "\n",
    "#     if root_properties is None:\n",
    "#         root_properties = parent_dict.setdefault('Properties', {})\n",
    "\n",
    "#     for child in soup.children:\n",
    "#         if isinstance(child, Tag):\n",
    "#             # Additional case for JCR articles extraction\n",
    "#             if child.get('id') == 'artigos-completos':\n",
    "#                 jcr_articles_dict = parse_jcr_articles(soup)\n",
    "#                 if jcr_articles_dict:\n",
    "#                     root_properties.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "#             # Standard extraction for cells\n",
    "#             elif \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "\n",
    "#                     sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "#                     if sibling_cell:\n",
    "#                         cell_values = extract_data_from_cell(sibling_cell)\n",
    "#                         if current_section:\n",
    "#                             root_properties[current_section][cell_key] = cell_values\n",
    "#                         else:\n",
    "#                             root_properties[cell_key] = cell_values\n",
    "\n",
    "#             # Non-Standard extraction\n",
    "#             elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "\n",
    "#                     sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "#                     if sibling_cell:\n",
    "#                         cell_values = extract_data_from_cell(sibling_cell)\n",
    "#                         clean_values = [x.replace('\\n\\t\\t\\t\\t\\t\\t\\t',' ') for x in cell_values]\n",
    "\n",
    "#                         if current_section:\n",
    "#                             if isinstance(root_properties[current_section], list):\n",
    "#                                 converted_dict = {i: item for i, item in enumerate(root_properties[current_section])}\n",
    "#                                 root_properties[current_section] = converted_dict\n",
    "#                             root_properties[current_section][cell_key] = cell_values\n",
    "#                         else:\n",
    "#                             root_properties[cell_key] = clean_values\n",
    "\n",
    "#             # Handling headers to define sections\n",
    "#             title = child.find(\"h1\")\n",
    "#             if title:\n",
    "#                 current_section = title.get_text().strip()\n",
    "#                 new_dict = {}\n",
    "#                 root_properties[current_section] = new_dict\n",
    "#                 traverse(child, new_dict, current_section, root_properties)\n",
    "\n",
    "#             # Detecting the subsection\n",
    "#             subsection = child.find_previous(\"div\", class_=\"cita-artigos\").get_text().strip() if child.find_previous(\"div\", class_=\"cita-artigos\") else None\n",
    "#             if subsection in subsections:\n",
    "#                 print(subsection)\n",
    "\n",
    "#                 if isinstance(root_properties.get(current_section), list):\n",
    "#                     root_properties[current_section] = {i: item for i, item in enumerate(root_properties[current_section])}\n",
    "                    \n",
    "#                 new_key = f\"Produções-{subsection}\"\n",
    "\n",
    "#                 if current_section and isinstance(root_properties.get(current_section), dict):\n",
    "#                     # Check if subsection exists before attempting to pop it\n",
    "#                     if subsection in root_properties[current_section]:\n",
    "#                         root_properties[new_key] = root_properties[current_section].pop(subsection)\n",
    "#                 elif not current_section:\n",
    "#                     print(\"current_section is not defined.\")\n",
    "#                 else:\n",
    "#                     print(f\"Unexpected type for root_properties[{current_section}]: {type(root_properties[current_section])}\")\n",
    "            \n",
    "#             else:\n",
    "#                 traverse(child, parent_dict, current_section, root_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(soup): \n",
    "    # Extração de todas as classes no documento, armazenadas em uma lista\n",
    "    all_classes = [value for element in soup.find_all(class_=True) for value in element[\"class\"]]\n",
    "    \n",
    "    # Contagem de ocorrências de cada classe\n",
    "    class_count = Counter(all_classes)\n",
    "    \n",
    "    # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "    class_dict = dict(class_count)\n",
    "    \n",
    "    return class_dict\n",
    "\n",
    "def list_divs(soup):\n",
    "    div_elements = soup.find_all('div')\n",
    "    unique_classes = set()\n",
    "\n",
    "    for div in div_elements[1:]:\n",
    "        div_id = div.get('id', 'N/A')\n",
    "        class_list = div.get('class')\n",
    "        print(f'{div_id} {class_list}')\n",
    "        \n",
    "        if class_list:\n",
    "            for i in class_list:\n",
    "                cont = div.find('div',{'class': i})\n",
    "                try:\n",
    "                    text = cont.text.strip().replace('/n/n/n','/n').replace('/n/n','/n')\n",
    "                    print(f'  {text}')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            print('-'*50)\n",
    "            unique_classes.update(class_list)\n",
    "    \n",
    "    return unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_divs_with_hierarchy(tag, indent_level=0, verbose=False):\n",
    "    indent = \" \" * indent_level * 4  # Four spaces for each level of indentation\n",
    "    div_elements = tag.find_all('div', recursive=False)  # Find direct children only\n",
    "    unique_classes = set()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"{indent}Inspecting level {indent_level}, found {len(div_elements)} divs.\")\n",
    "\n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        if class_list:\n",
    "            unique_classes.update(class_list)\n",
    "            print(f\"{indent} {', '.join(class_list)}\")\n",
    "        else:\n",
    "            print(f\"{indent} None\")\n",
    "\n",
    "        # Recursive call to explore the children of this div\n",
    "        list_divs_with_hierarchy(div, indent_level + 1)\n",
    "\n",
    "    if indent_level == 0:  # Print unique classes only once, at the end of the initial call\n",
    "        print(\"\\nUnique classes:\")\n",
    "        for i in unique_classes:\n",
    "            print(f'    {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de classes únicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize BeautifulSoup with a sample HTML content\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_classes(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_divs(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achar atributos de classes e divs definidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# keys_dicts = soup.find_all('div', {'class':'layout-cell-pad-5'})\n",
    "# for i in keys_dicts:\n",
    "#     pprint(i.attrs)\n",
    "#     text = i.get_text().replace('\\n\\n\\n\\n','').replace('\\t\\t\\n\\t','').replace('\\t\\t\\t','').strip()\n",
    "#     pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achar classe específica (dicionário class) em elemento (div)\n",
    "# from pprint import pprint\n",
    "\n",
    "# # query0 = soup.select_one('div', {'class':'layout-cell-1'}).attrs\n",
    "# keys_dicts = soup.select('div', {'class':'text-align-right'})\n",
    "# for i in keys_dicts:\n",
    "#     pprint(i.attrs)\n",
    "#     pprint(i.get_text().replace('\\n\\n\\n\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achar classe específica (dicionário class) em elemento (div)\n",
    "# from pprint import pprint\n",
    "\n",
    "# # query0 = soup.select_one('div', {'class':'layout-cell-1'}).attrs\n",
    "# keys_dicts = soup.find_all('div', {'class':'text-align-right'})\n",
    "# for i in keys_dicts:\n",
    "#     pprint(i.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar a hierarquia com identação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "# body_tag = soup.body\n",
    "\n",
    "# # Then try calling list_divs_with_hierarchy on the <body> tag\n",
    "# list_divs_with_hierarchy(body_tag, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicial_element = soup.select_one('div', {'class':'title-wrapper'})\n",
    "# inicial_element = soup.find('div', {'class':'layout-cell-pad-main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_divs_with_hierarchy(inicial_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar atributos de elemento e classe especificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query1 = soup.select_one('img', {'class':'foto'}).attrs\n",
    "# pprint(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Extrair dados e seções específicas\n",
    "\n",
    "- extrair_dados(soup, verbose=False)\n",
    "- extrair_artigos(soup, verbose=False)\n",
    "- extrair_orientacoes(soup, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume(soup):\n",
    "    # <h2 class=\"nome\" tabindex=\"0\">Antonio Marcos Aires Barbosa</h2>\n",
    "    name_element = soup.find('h2', {'class': 'nome'})\n",
    "    node_title = name_element.text if name_element else \"Unknown\"\n",
    "    found_list = []\n",
    "    nodes = {}\n",
    "    properties = {}\n",
    "    recurrent_div = soup.find_all('div', {'title-wrapper'})\n",
    "\n",
    "    for div in recurrent_div:\n",
    "        parag_element = div.find('p')\n",
    "\n",
    "        if parag_element:\n",
    "            key   = parag_element.get('class')[0]\n",
    "            value = parag_element.text.strip('\\n')\n",
    "            properties[f'{key}'] = value\n",
    "            found_list.append(properties)\n",
    "            \n",
    "    nodes[node_title] = properties\n",
    "                       \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_resume(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "node_name = node_name_element.text if node_name_element else None\n",
    "node_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair estrutura de árvore recursivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "import json\n",
    "\n",
    "def extract_tree_structure(soup, level=0):\n",
    "    \"\"\" \n",
    "    Extrai recursivamente a estrutura de árvore de um objeto soup.\n",
    "    \n",
    "    Parâmetros:\n",
    "        soup (bs4.BeautifulSoup | bs4.Tag): O objeto soup ou tag para explorar.\n",
    "        level (int): Nível atual da árvore para fins de aninhamento.\n",
    "        \n",
    "    Retorna:\n",
    "        dict: Dicionário que representa a estrutura da árvore.\n",
    "    \"\"\"\n",
    "    \n",
    "    if soup is None:\n",
    "        return None\n",
    "    \n",
    "    tree_structure = {}\n",
    "    \n",
    "    if isinstance(soup, Tag):\n",
    "        tree_structure['name'] = soup.name\n",
    "        tree_structure['classes'] = soup.get('class', [])\n",
    "    \n",
    "    children_structure = []\n",
    "    for child in soup.children:\n",
    "        if isinstance(child, Tag):\n",
    "            child_structure = extract_tree_structure(child, level + 1)\n",
    "            children_structure.append(child_structure)\n",
    "    \n",
    "    if children_structure:\n",
    "        tree_structure['children'] = children_structure\n",
    "    \n",
    "    return tree_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicial_element = soup.find('div',{'class': 'content-wrapper'})\n",
    "# tree_structure = extract_tree_structure(inicial_element)\n",
    "# print(json.dumps(tree_structure, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_texts(soup):\n",
    "#     json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    \n",
    "#     # Extract name from 'h2' tag as the first entry of the main dictionary\n",
    "#     name_tag = soup.find('h2', {'class': 'nome'})\n",
    "#     if name_tag:\n",
    "#         json_data['Properties']['name'] = name_tag.get_text().strip()\n",
    "    \n",
    "#     main_layout_cells = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "#     for main_cell in main_layout_cells:\n",
    "#         current_section = None\n",
    "#         current_subsection = None\n",
    "#         current_subtit = None\n",
    "\n",
    "#         # Handle 'h1' tags for sections\n",
    "#         h1_tags = main_cell.find_all('h1')\n",
    "#         for h1_tag in h1_tags:\n",
    "#             current_section = h1_tag.get_text().strip()\n",
    "#             json_data['Properties'].setdefault(current_section, {})\n",
    "        \n",
    "#         # Handle 'inst_back' tags for subsections\n",
    "#         inst_back_tags = main_cell.find_all('div', {'class': 'inst_back'})\n",
    "#         for inst_back_tag in inst_back_tags:\n",
    "#             current_subsection = inst_back_tag.get_text().strip()\n",
    "#             if current_section:\n",
    "#                 json_data['Properties'][current_section].setdefault(current_subsection, {})\n",
    "\n",
    "#         # # Handle 'subtit-1' tags for sub-subsections\n",
    "#         # subtit_tags = main_cell.find_all('div', {'class': 'subtit-1'})\n",
    "#         # for subtit_tag in subtit_tags:\n",
    "#         #     current_subtit = subtit_tag.get_text().strip()\n",
    "#         #     if current_section and current_subsection:\n",
    "#         #         json_data['Properties'][current_section][current_subsection].setdefault(current_subtit, {})\n",
    "        \n",
    "#         # Handle key-value pairs\n",
    "#         keys = main_cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "#         vals = main_cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "#         for key_elem, val_elem in zip(keys, vals):\n",
    "#             key_text = key_elem.get_text().strip()\n",
    "#             val_text = val_elem.get_text().strip()\n",
    "\n",
    "#             if current_subtit and current_subsection and current_section:\n",
    "#                 json_data['Properties'][current_section][current_subsection][current_subtit][key_text] = val_text\n",
    "#             elif current_subsection and current_section:\n",
    "#                 json_data['Properties'][current_section][current_subsection][key_text] = val_text\n",
    "#             elif current_section:\n",
    "#                 json_data['Properties'][current_section][key_text] = val_text\n",
    "\n",
    "#     return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_texts(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from py2neo import Node\n",
    "\n",
    "def list_secoes(soup, tag):\n",
    "    elements = soup.find_all(tag, {'tabindex': '0'})\n",
    "    for elem in elements:\n",
    "        text = elem.get_text().strip()\n",
    "        node = Node(\"H1Element\", text=text)\n",
    "        print(f'{text}')\n",
    "\n",
    "def list_subsection(soup, classe):\n",
    "    class_elements = soup.find_all('div', {classe})\n",
    "    for elem in class_elements:\n",
    "        class_text = elem.get_text().strip()\n",
    "        class_node = Node(\"Class_Element\", text=class_text)\n",
    "        print(f'{class_text}')\n",
    "\n",
    "def list_b(soup):\n",
    "    # Find all 'b' elements without filtering by tabindex\n",
    "    b_elements = soup.find_all('b')\n",
    "    for elem in b_elements:\n",
    "        # Retrieve the text content within the 'b' element and strip any leading/trailing white space\n",
    "        b_text = elem.get_text().strip()\n",
    "\n",
    "        # Locate the parent 'div' of the current 'b' element\n",
    "        parent_div = elem.find_parent('div')\n",
    "        \n",
    "        # Initialize variables to store class and id attributes\n",
    "        parent_div_class = None\n",
    "        parent_div_id = None\n",
    "\n",
    "        # Retrieve the class and id attributes if they exist\n",
    "        if parent_div is not None:\n",
    "            parent_div_class = parent_div.get('class', [None])[0]  # Retrieve the first class name if multiple are present\n",
    "            # parent_div_id = parent_div.get('id', None)  # Retrieve the id attribute\n",
    "\n",
    "        # The Node object creation step has been retained for further use\n",
    "        b_node = Node(\"BElement\", text=b_text)\n",
    "        \n",
    "        # Print the extracted text, along with the parent div's class and id\n",
    "        print(f'Text: {b_text:40} | Div: {parent_div_class}')\n",
    "\n",
    "def list_ascendants(soup, tag):\n",
    "    # Find all 'b' elements\n",
    "    b_elements = soup.find_all(tag)\n",
    "    for elem in b_elements:\n",
    "        # Retrieve the text content within the 'b' element, stripping any leading/trailing white space\n",
    "        b_text = elem.get_text().strip()\n",
    "\n",
    "        # Locate the parent element of the current 'b' element\n",
    "        parent_elem = elem.find_parent()\n",
    "        \n",
    "        # Locate the grandparent element of the current 'b' element\n",
    "        grandparent_elem = parent_elem.find_parent() if parent_elem is not None else None\n",
    "\n",
    "        # Retrieve the class names of parent and grandparent elements, returning None if not found\n",
    "        parent_class_name = parent_elem.get('class', [None])[0] if parent_elem is not None else None\n",
    "        grandparent_class_name = grandparent_elem.get('class', [None])[0] if grandparent_elem is not None else None\n",
    "\n",
    "        # Create a Node object for further use\n",
    "        b_node = Node(\"BElement\", text=b_text)\n",
    "\n",
    "        # Print the extracted text along with the class names of the parent and grandparent elements\n",
    "        print(f'{grandparent_class_name} | {parent_class_name} | Text: {b_text}')\n",
    "\n",
    "def parse_h1(soup, parent_node, graph):\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <h2 class=\"nome\" tabindex=\"0\">Antonio Marcos Aires Barbosa</h2>\n",
    "name_element = soup.find('h2', {'class': 'nome'})\n",
    "node_title = name_element.text if name_element else \"Unknown\"\n",
    "print(node_title)\n",
    "found_list = []\n",
    "nodes = {}\n",
    "properties = {}\n",
    "recurrent_div = soup.find_all('div', {'title-wrapper'})\n",
    "# print(len(recurrent_div))\n",
    "\n",
    "print('Valores de p')\n",
    "for div in recurrent_div:\n",
    "    parag_element = div.find('p')\n",
    "    if parag_element:\n",
    "        key   = parag_element.get('class', [None])[0]\n",
    "        value = parag_element.get_text().split('\\n')        \n",
    "        print(f'{key:22}: {value}')\n",
    "\n",
    "    sections = div.find_all('inst_back')\n",
    "    for section in sections:\n",
    "        title_element = section.get_text().strip()\n",
    "        if title_element:\n",
    "            print('\\nValores de inst_back')\n",
    "            print(len(title_element))\n",
    "\n",
    "for n,div in enumerate(recurrent_div):\n",
    "    print('\\nValores de h1')            \n",
    "    section_element = div.find('h1')\n",
    "    if section_element and n>0:\n",
    "        key = section_element.get_text().strip()\n",
    "        value = section_element.find_next().get_text().split('\\n\\n')\n",
    "        print(f'{key:22}: {value}')\n",
    "\n",
    "print('\\nValores de b')\n",
    "for div in recurrent_div:\n",
    "    subsections = div.find_all('b')\n",
    "    for element in subsections:\n",
    "        if element:\n",
    "            key = element.get_text().strip()\n",
    "            filtered_list = [x.strip() for x in element.find_next().get_text().split('\\n') if x.strip()]\n",
    "            value = filtered_list\n",
    "            print(f'{key:22}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "def extract_sections(soup):\n",
    "    master_dict = {}  # Initialize an empty dictionary to store the hierarchical data\n",
    "    \n",
    "    # Loop through sections designated by 'title-wrapper'\n",
    "    for section_elem in soup.find_all('div', {'class': 'title-wrapper'}):\n",
    "        section_title_elem = section_elem.find('h1')\n",
    "        section_title = section_title_elem.get_text().strip() if section_title_elem else None\n",
    "        \n",
    "        if section_title:\n",
    "            master_dict[section_title] = {}\n",
    "       \n",
    "            layouts = soup.find_all('div', {'class': ['layout-cell']})\n",
    "            print(len(layouts), 'células de layout')\n",
    "            # Loop to extract keys and values from specific classes\n",
    "            for layout in layouts:\n",
    "                subsection_title = layout.get_text().strip()\n",
    "                master_dict[section_title][subsection_title] = {}                    \n",
    "                tags_b = layout.find_all('b')\n",
    "                sections = layout.find_all('div', {'inst_back'})\n",
    "                subsections = layout.find_all('h1')\n",
    "                tags_a = layout.find_all('a')\n",
    "                cell05 = layout.find_all('div', {'layout-cell-pad-5 text-align-right'})\n",
    "                rights = layout.find_all('div', {'text-align-right'})\n",
    "                cell12 = layout.find_all('div', {'layout-cell-12'})\n",
    "                cell03 = layout.find_all('div', {'layout-cell-3'})\n",
    "                cell09 = layout.find_all('div', {'layout-cell-9'})\n",
    "                # print(len(tags_b), len(rights), len(sections), len(subsections), len(tags_a), len(cell12), len(cell05), len(cell03), len(cell09))\n",
    "\n",
    "                content_key = layout.get_text().replace('\\n', '').strip()\n",
    "                print('Classe:',layout.get('class', [None]))\n",
    "                print(' Dados:',content_key)\n",
    "                content_value = {}\n",
    "                master_dict[section_title][subsection_title][content_key] = content_value\n",
    "    return master_dict\n",
    "\n",
    "result_dict = extract_sections(soup)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts(soup, element):\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "\n",
    "    # Find the parent element that contains relevant sections\n",
    "    parent_element = element.find_parent() if element.find_parent() else soup\n",
    "    \n",
    "    current_section = None\n",
    "    current_subsection = None\n",
    "\n",
    "    # Iterate over each child of the parent element\n",
    "    for child in parent_element.children:\n",
    "        # Handling section tags (Level 1)\n",
    "        if child.name == 'h1':\n",
    "            current_section = child.get_text().strip()\n",
    "            json_data[\"Properties\"][current_section] = {}\n",
    "            current_subsection = None\n",
    "        # Handling subsection tags (Level 2)\n",
    "        elif child.get('class') == ['inst_back']:\n",
    "            if current_section:\n",
    "                current_subsection = child.find('b').get_text().strip()\n",
    "                json_data[\"Properties\"][current_section][current_subsection] = {}\n",
    "        # Handling layout cells for keys and values (Level 3)\n",
    "        elif child.get('class') == ['layout-cell']:\n",
    "            keys = child.find_all('div', {'class': 'layout-cell-3'}, limit=1)\n",
    "            vals = child.find_all('div', {'class': 'layout-cell-9'}, limit=1)\n",
    "            for key_elem, value_elem in zip(keys, vals):\n",
    "                key_text = key_elem.get_text().strip()\n",
    "                val_text = value_elem.get_text().strip()\n",
    "                target_dict = json_data[\"Properties\"]\n",
    "                if current_section:\n",
    "                    target_dict = target_dict[current_section]\n",
    "                    if current_subsection:\n",
    "                        target_dict = target_dict[current_subsection]\n",
    "                target_dict[key_text] = val_text\n",
    "                \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaves Dicionários Nível01: Seções, Nível02: Subseções, Nível03: Títulos\n",
    "def find_class_ascendants(soup,classe):\n",
    "    div_sec = soup.find_all('div',{'class':classe})\n",
    "    for i in div_sec:\n",
    "        level0 = i.find_parent().find_parent().find_parent().find_parent()\n",
    "        level1 = i.find_parent().find_parent().find_parent()\n",
    "        level2 = i.find_parent().find_parent()\n",
    "        level3 = i.find_parent()\n",
    "        print(f'{level0.name} {level0.get(\"class\", [None])}')\n",
    "        print(f'  {level1.name} {level1.get(\"class\", [None])}')\n",
    "        print(f'    {level2.name} {level2.get(\"class\", [None])}')\n",
    "        print(f'      {level3.name} {level3.get(\"class\", [None])}')\n",
    "        print(f'        {i.name} {i.get(\"class\", [None])}')\n",
    "        print(f'        {i.get_text().strip()}')\n",
    "\n",
    "def find_tag_ascendants(soup,tag):\n",
    "    div_sec = soup.find_all(tag)\n",
    "    for i in div_sec:\n",
    "        level0 = i.find_parent().find_parent().find_parent().find_parent()\n",
    "        level1 = i.find_parent().find_parent().find_parent()\n",
    "        level2 = i.find_parent().find_parent()\n",
    "        level3 = i.find_parent()\n",
    "        print(f'{level0.name} {level0.get(\"class\", [None])}')\n",
    "        print(f'  {level1.name} {level1.get(\"class\", [None])}')\n",
    "        print(f'    {level2.name} {level2.get(\"class\", [None])}')\n",
    "        print(f'      {level3.name} {level3.get(\"class\", [None])}')\n",
    "        print(f'        {i.name} {i.get(\"class\", [None])}')\n",
    "        print(f'        {i.get_text().strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_class_ascendants(soup, 'subtit-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão as chave de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'layout-cell-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'layout-cell-pad-5 text-align-right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'layout-cell-9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_tag_ascendants(soup,'h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_tag_ascendants(soup,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'subtit-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'inst_back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.select_one('layout-cell layout-cell-12 layout-cell-pad-main'))\n",
    "# print(soup.select_one('div.layout-cell.layout-cell-12'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaves Dicionários Nível 02 - Subseções\n",
    "div_tit = soup.find_all('h1')\n",
    "for i in div_tit:\n",
    "    tit_text = i.get_text().strip()\n",
    "    print(tit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaves Dicionários Nível 02 - Subseções\n",
    "div_tit = soup.find_all('div', 'inst_back')\n",
    "for i in div_tit:\n",
    "    tit_text = i.find('b').get_text().strip()\n",
    "    print(tit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    master_dict = {}\n",
    "    \n",
    "    # Extract sections\n",
    "    section_elements = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    for section_elem in section_elements:\n",
    "        section_title_elem = section_elem.find('h1')\n",
    "        section_title = section_title_elem.get_text().strip() if section_title_elem else None\n",
    "        \n",
    "        if section_title:\n",
    "            master_dict[section_title] = {}\n",
    "            \n",
    "            # Extract subsections\n",
    "            subsection_elements = section_elem.find_all('div', {'class': 'inst_back'})\n",
    "            for subsection_elem in subsection_elements:\n",
    "                subsection_title = subsection_elem.get_text().strip()\n",
    "                master_dict[section_title][subsection_title] = {}\n",
    "                \n",
    "                # Extract key-value pairs using classes you've defined for the last level\n",
    "                layout_cells = subsection_elem.find_parent().find_all('div', {'layout-cell'})\n",
    "                \n",
    "                for layout_cell in layout_cells:\n",
    "                    data_dict = extract_texts(layout_cell)\n",
    "                    master_dict[section_title][subsection_title] = data_dict\n",
    "                        \n",
    "    return master_dict\n",
    "\n",
    "result_dict = extract_data(soup)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_sections(soup):\n",
    "    # Initialize an empty dictionary to hold the hierarchical data\n",
    "    master_dict = {}\n",
    "    \n",
    "    # Find all elements that are considered as sections\n",
    "    section_elements = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    for section_elem in section_elements:\n",
    "        # Extract section title if available\n",
    "        section_title_elem = section_elem.find('h1')\n",
    "        section_title = section_title_elem.get_text().strip() if section_title_elem else None\n",
    "        \n",
    "        if section_title:\n",
    "            master_dict[section_title] = {}\n",
    "            \n",
    "            # Extract subsections within each section\n",
    "            subsection_elements = section_elem.find_all('div', {'class': 'inst_back'})\n",
    "            for subsection_elem in subsection_elements:\n",
    "                subsection_title = subsection_elem.get_text().strip()\n",
    "                master_dict[section_title][subsection_title] = {}\n",
    "                \n",
    "                # Extract content within each subsection and store as key-value pairs\n",
    "                content_elements_par = subsection_elem.find_all('p')\n",
    "                content_elements_b = subsection_elem.find_all('b')\n",
    "                for content_elem in content_elements_par:\n",
    "                    content_key = content_elem.get('class')[0] if content_elem.get('class') else None\n",
    "                    content_value = content_elem.get_text().strip()\n",
    "                    \n",
    "                    if content_key and content_value:\n",
    "                        master_dict[section_title][subsection_title][content_key] = content_value\n",
    "\n",
    "                for content_elem in content_elements_b:\n",
    "                    content_key = content_elem.get('class')[0] if content_elem.get('class') else None\n",
    "                    content_value = content_elem.get_text().strip()\n",
    "                    \n",
    "                    if content_key and content_value:\n",
    "                        master_dict[section_title][subsection_title][content_key] = content_value\n",
    "                        \n",
    "    return master_dict\n",
    "\n",
    "result_dict = extract_sections(soup)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = soup.find_all('div', {'title-wrapper'})\n",
    "\n",
    "for elem in elements:\n",
    "    par = elem.find('p')\n",
    "    if par:\n",
    "        key = par.get('class')[0]\n",
    "        val = par.get_text().strip()\n",
    "        print(f\"{key}: {val}\")  \n",
    "    \n",
    "    tit = elem.find('h1')\n",
    "    if tit:\n",
    "        section = tit.get_text()\n",
    "        if section != '':\n",
    "            key = tit.get_text().strip()\n",
    "            if tit.find_next('b'):\n",
    "                for i in tit.find_next('b'):\n",
    "                    val = i.get_text()\n",
    "                    if val.strip() != '':\n",
    "                        print(val)\n",
    "            lin = tit.find('b')\n",
    "            print(f\"{key:35}: {val}\")\n",
    "        subsect = soup.find('inst_back')\n",
    "        if subsect:\n",
    "            print(len(subsect))\n",
    "            print(f\"{subsect.get_text().strip()}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_secoes(soup,'h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subsection(soup, 'inst_back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ascendants(soup,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alldata(soup):\n",
    "    name_element = soup.find('h2', {'class': 'nome'})\n",
    "    node_title = name_element.text if name_element else \"Unknown\"\n",
    "    found_list = []\n",
    "    nodes = {}\n",
    "    properties = {}\n",
    "    recurrent_div = soup.find_all('div', {'title-wrapper'})\n",
    "\n",
    "    for div in recurrent_div:\n",
    "        parag_element = div.find('p')\n",
    "        title_element = div.find_all('inst_back')\n",
    "\n",
    "        if parag_element:\n",
    "            key   = parag_element.get('class')[0]\n",
    "            value = parag_element.text.strip('\\n')\n",
    "            properties[f'{key}'] = value\n",
    "            found_list.append(properties)\n",
    "\n",
    "        if title_element:\n",
    "            key   = title_element.get('class')\n",
    "            value = title_element.text.strip('\\n')\n",
    "            properties[f'{key}'] = value\n",
    "            found_list.append(properties)\n",
    "            \n",
    "    nodes[node_title] = properties\n",
    "                       \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(get_alldata(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes de extração de seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_dados(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node\": node_name, \"Properties\": {}}\n",
    "    \n",
    "    # Step 3: Traverse to Find Sections\n",
    "    json_data = extrair_wraper(soup, json_data)\n",
    "\n",
    "    # Step 4: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    producoes = []\n",
    "    for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        artigo_dict['ano']     = ano\n",
    "        artigo_dict['autores'] = autores\n",
    "        artigo_dict['revista'] = revista\n",
    "        artigo_dict['titulo']  = titulo\n",
    "        artigo_dict['jcr']     = jcr\n",
    "        artigo_dict['doi']     = doi\n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = producoes\n",
    "\n",
    "    projdevtec=[]\n",
    "    for projdevtec_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        projdevtec_dict = {}\n",
    "        projdevtec.append(projdevtec_dict)\n",
    "        json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projdevtec\n",
    "\n",
    "    # projpesq=[]\n",
    "    # for projpesq_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "    #     projpesq_dict = {}\n",
    "    \n",
    "    \n",
    "    #     projpesq.append(projdevtec_dict)\n",
    "    #     json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projpesq\n",
    "\n",
    "    orientacoes=[]\n",
    "    for orientacoes_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        orientacoes_dict = {}   \n",
    "        orientacoes.append(orientacoes_dict)\n",
    "        json_data[\"Properties\"][\"Orientações\"] = orientacoes\n",
    "\n",
    "    bancas=[]\n",
    "    for bancas_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        bancas_dict = {}\n",
    "        bancas.append(bancas_dict)\n",
    "        json_data[\"Properties\"][\"Bancas\"] = bancas\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_orientacoes(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node\": node_name, \"Properties\": {}}\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    general_div = soup.find('div', {'class': 'Orientações e supervisões concluídas'})\n",
    "    if verbose:\n",
    "        if general_div:\n",
    "            print(f\"Qte.Divs: {len(general_div)}\")\n",
    "        else:\n",
    "            print('Nenhuma div encontrada com essa hierarquia')\n",
    "            return\n",
    "    orientacoes=[]\n",
    "    for selected_div in general_div.find_all('div', {'class': 'identity'}):\n",
    "        orientacoes_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = selected_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = selected_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = selected_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = selected_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = selected_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        \n",
    "        dados   = selected_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        str_autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        autores = split_authors(str_autores)\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        orientacoes_dict['ano']     = ano\n",
    "        orientacoes_dict['autores'] = autores\n",
    "        orientacoes_dict['revista'] = revista\n",
    "        orientacoes_dict['titulo']  = titulo\n",
    "        orientacoes_dict['jcr']     = jcr\n",
    "        orientacoes_dict['doi']     = doi\n",
    "        orientacoes.append(orientacoes_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = orientacoes\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "node_name = node_name_element.text if node_name_element else 'Unknown'\n",
    "\n",
    "text_contents = []\n",
    "elements_list = soup.find_all('b')\n",
    "print(len(elements_list))\n",
    "for i in elements_list:\n",
    "    # Check if text content matches the targeted string\n",
    "    if i.text == 'Orientações e supervisões concluídas':\n",
    "        # Retrieve the parent element\n",
    "        parent_element = i.find_parent()\n",
    "        \n",
    "        # Retrieve the name of the parent element\n",
    "        parent_name = parent_element.name if parent_element else 'Unknown'\n",
    "        \n",
    "        # Retrieve and store the text contents of all child elements of the parent\n",
    "        for child in parent_element.find_all(True):\n",
    "            text_contents.append(child.text)\n",
    "        \n",
    "        print(f\"Found in parent element: {parent_name}\")\n",
    "        print(\"Texts of all child elements:\", text_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in elements_list:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrair_orientacoes(soup, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Extrair do DOM Dados Identificação\n",
    "\n",
    "- parse_personinfo(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_personinfo(soup):\n",
    "    # Step 1: Identify Node Name\n",
    "    properties = {}\n",
    "\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    # div_informacoesautor = soup.find(\"ul\", {\"class\": \"informacoes-autor\"})\n",
    "    if node_name:\n",
    "        properties['name'] = node_name\n",
    "    for li in soup.find_all('li'):\n",
    "        text_content = li.text  # Extract the text content of the 'li' element\n",
    "        span_class = li.span['class'][0] if li.span else 'Unknown'  # Extract the class of the span within the 'li', if present\n",
    "\n",
    "        # Populate dictionary based on span class\n",
    "        if span_class == 'img_link':\n",
    "            properties['CV_URL'] = text_content.replace('Endereço para acessar este CV: ', '').strip()\n",
    "        elif span_class == 'img_identy':\n",
    "            properties['ID_Lattes'] = li.find('span', {'style': 'font-weight: bold; color: #326C99;'}).text.strip() if li.find('span', {'style': 'font-weight: bold; color: #326C99;'}) else 'Unknown'\n",
    "        elif span_class == 'img_cert':\n",
    "            date_str = text_content.replace('Última atualização do currículo em ', '').strip()\n",
    "            \n",
    "            # Assuming that the date string is in the format 'DD/MM/YYYY', parse it into a datetime object\n",
    "            # Modify the format string according to the actual format of date_str\n",
    "            parsed_date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "            # Convert the datetime object to ISO 8601 format (YYYY-MM-DD)\n",
    "            iso_date = parsed_date.strftime('%Y-%m-%d')\n",
    "            properties['Last_Updated'] = iso_date\n",
    "\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Initialize the Neo4j driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "n4j_driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "def create_unique_composite_constraint(tx, label, properties):\n",
    "    # Build the updated query string for creating the unique composite constraint\n",
    "    # Note: The 'properties' parameter is expected to be a list of property names.\n",
    "    prop_string = \", \".join([f\"n.{prop}\" for prop in properties])\n",
    "    constraint_name = \"_\".join([label.lower()] + [prop.lower() for prop in properties])\n",
    "    \n",
    "    query = (\n",
    "        f\"CREATE CONSTRAINT {constraint_name} \"\n",
    "        f\"IF NOT EXISTS FOR (n:{label}) REQUIRE ({prop_string}) IS NODE KEY\"\n",
    "    )\n",
    "    # Execute the transaction to create the constraint\n",
    "    tx.run(query)\n",
    "\n",
    "def create_or_update_node(tx, node_label, properties):\n",
    "    find_query = f\"MATCH (n:{node_label} {{name: $name}}) RETURN n\"\n",
    "    existing_nodes = list(tx.run(find_query, name=properties['name']))\n",
    "\n",
    "    if existing_nodes:\n",
    "        existing_node = existing_nodes[0]['n']\n",
    "        if existing_node['Last_Updated'] < properties['Last_Updated']:\n",
    "            update_query = (\n",
    "                f\"MATCH (n:{node_label} {{name: $name}}) \"\n",
    "                f\"SET n += $properties\"\n",
    "            )\n",
    "            tx.run(update_query, properties=properties, name=properties['name'])\n",
    "    else:\n",
    "        create_query = f\"CREATE (n:{node_label} $properties)\"\n",
    "        tx.run(create_query, properties=properties)\n",
    "\n",
    "def update(n4j_driver, person_properties, verbose=False):\n",
    "    with n4j_driver.session() as session:\n",
    "        try:\n",
    "            session.write_transaction(create_or_update_node, 'Person', person_properties)\n",
    "            if verbose:\n",
    "                print(\"Operation completed:\")\n",
    "                print(f\"{person_properties.values()} has been evaluated for creation or update.\")\n",
    "        except Exception as e:\n",
    "            pprint(f\"An error occurred: {e}\")\n",
    "\n",
    "def augment_node(tx, node_label, node_name, properties_to_add):\n",
    "    query = (\n",
    "        f\"MATCH (n:{node_label} {{name: $node_name}}) \"\n",
    "        f\"SET n += $properties_to_add \"\n",
    "        f\"RETURN n\"\n",
    "    )\n",
    "    result = tx.run(query, node_name=node_name, properties_to_add=properties_to_add)\n",
    "    return [record['n'] for record in result]\n",
    "\n",
    "def update_node_with_new_properties(n4j_driver, node_label, node_name, properties_to_add, verbose=False):\n",
    "    with n4j_driver.session() as session:\n",
    "        try:\n",
    "            updated_nodes = session.write_transaction(augment_node, node_label, node_name, properties_to_add)\n",
    "            if verbose:\n",
    "                print(\"Operation completed:\")\n",
    "                for node in updated_nodes:\n",
    "                    print(f\"Updated Node: {node}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Function for parsing resume and returning JSON\n",
    "def parse_resume(soup, verbose=False):\n",
    "    # Existing parsing logic...\n",
    "    # Assuming json_data contains the parsed data\n",
    "    json_data = {\"Node\": \"node_name\", \"Properties\": {\"property_1\": \"value_1\"}}\n",
    "\n",
    "    # Update the Node with new properties\n",
    "    update_node_with_new_properties(\n",
    "        n4j_driver, \n",
    "        'Person', \n",
    "        json_data['Node'], \n",
    "        json_data['Properties'], \n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar a restrição nome-data_atualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session and Transaction for creating unique composite constraint\n",
    "with n4j_driver.session() as session:\n",
    "    session.write_transaction(create_unique_composite_constraint, 'Person', ['name', 'Last_Updated'])\n",
    "\n",
    "# Close the Neo4j driver\n",
    "n4j_driver.close()\n",
    "\n",
    "# Log the completion of constraint creation\n",
    "print(\"Unique composite constraint has been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_properties = parse_personinfo(soup)\n",
    "person_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_to_add = parse_resume(soup)\n",
    "\n",
    "# Invoke the function to update the node\n",
    "update_node_with_new_properties(n4j_driver, 'Person', 'John Doe', properties_to_add, verbose=True)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "n4j_driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(n4j_driver, person_properties, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '12/09/2023'\n",
    "# Assuming that the date string is in the format 'DD/MM/YYYY', parse it into a datetime object\n",
    "# Modify the format string according to the actual format of date_str\n",
    "parsed_date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "# Convert the datetime object to ISO 8601 format (YYYY-MM-DD)\n",
    "iso_date = parsed_date.strftime('%Y-%m-%d')\n",
    "person_properties['Last_Updated'] = iso_date\n",
    "person_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução da persistência em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or update publications in Neo4j based on the dictionary\n",
    "create_or_update_publications(graph, dict_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update publications in Neo4j based on the dictionary\n",
    "create_or_update_publications(graph, dict_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar Grafo em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência dos elementos H1\n",
    "try:\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "except Exception as e:\n",
    "    print('Erro ao conectar ao Neo4j')\n",
    "    print(e)\n",
    "try:    \n",
    "    header_node = persist_to_neo4j(header_data)\n",
    "    print({type(header_node)})\n",
    "    parse_h1_elements(soup, header_node, graph)\n",
    "    cv_node, properties = parse_parsoninfo(soup)\n",
    "except Exception as e:\n",
    "    print('Erro ao persistir nó')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node(\"Curriculum\", \n",
    "     title=header_data['title'].split('(')[1].strip(')'), \n",
    "     meta_keywords=header_data['meta_keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes em desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', {'class': 'layout-cell-pad-main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def list_divs(soup_element, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):  # check if the child is an instance of Tag\n",
    "            # Check for 'div' tags and list classes\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')  # Default to 'None' if class is not present\n",
    "                print('  ' * indent + f\"div {class_name}\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            # # Check for 'h1' tags\n",
    "            # elif child.name == 'h1':\n",
    "            #     print('  ' * indent + f\"h1 {child.text.strip()}\")\n",
    "            #     list_divs(child, indent)\n",
    "\n",
    "            # Check for 'h2' tags\n",
    "            elif child.name == 'h2':\n",
    "                print('  ' * indent + f\"h2 {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            # Check for 'a' tags\n",
    "            elif child.name == 'a':\n",
    "                print('  ' * indent + f\"a {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            # Check for 'ul' tags\n",
    "            elif child.name == 'ul':\n",
    "                print('  ' * indent + \"ul\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            # Check for 'li' tags\n",
    "            elif child.name == 'li':\n",
    "                print('  ' * indent + \"li: \" + child.text.strip())\n",
    "                list_divs(child, indent + 1)\n",
    "                            \n",
    "            # Check for 'inst_back' as class in any tag\n",
    "            elif child.has_attr('class') and 'inst_back' in child['class']:\n",
    "                print('  ' * indent + f\"{child.name} inst_back\")\n",
    "                list_divs(child, indent + 1)\n",
    "            \n",
    "            # Check for 'b' tags\n",
    "            elif child.name == 'b':\n",
    "                print('  ' * indent + f\"b {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "        else:\n",
    "            list_divs(child, indent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "list_divs(starting_div, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrando dados extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    \n",
    "    ignorar = [None, ' ','Baixar Currículo','Imprimir Currículo']\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "\n",
    "                # Checar classe 'layout-cell-pad-5' com os valores dos dicionários\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    print('  ' * (indent + 1) + f\" val: {dividir(child.get_text())}\")                    \n",
    "                \n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            elif child.name == 'h2':\n",
    "                print('  ' * indent + f\"Node Name: {child.string}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    print('  ' * indent + f\"Properties: {rotulo}\")\n",
    "                    list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                print('  ' * indent + f\"Subtitulo: {child.get_text()}\")\n",
    "                list_divs(child, indent + 1)\n",
    "                \n",
    "            elif child.name == 'inst_back':\n",
    "                print('  ' * indent + f\"Subtitulo: {child.get_text()}\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # print('  ' * indent + f\"Group: {rotulo}\")\n",
    "                    list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                print('  ' * indent + f\" key: {rotulo}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'p':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                print('  ' * indent + f\" val: {rotulo}\")\n",
    "                list_divs(child, indent)                \n",
    "            \n",
    "            # elif child.name == 'span':\n",
    "            #     rotulo = dividir(child.get_text())[0]\n",
    "            #     print('  ' * indent + f\" val: {rotulo}\")\n",
    "            #     list_divs(child, indent)   \n",
    "\n",
    "        else:\n",
    "            list_divs(child, indent)\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "list_divs(starting_div, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ainda com problemas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo chaves corretamente mas não valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_data(element, parent_dict, current_section=None):\n",
    "    key = None\n",
    "    \n",
    "    if isinstance(element, Tag):\n",
    "        if element.name == 'a' or element.name == 'h2':\n",
    "            current_section = element.text.strip()\n",
    "            parent_dict[current_section] = {}\n",
    "        \n",
    "        elif 'text-align-right' in element.get('class', []):\n",
    "            key = element.find('b').text.strip() if element.find('b') else None\n",
    "            if key and current_section:\n",
    "                parent_dict[current_section][key] = ''\n",
    "        \n",
    "        elif 'layout-cell-9 layout-cell-pad-5' in element.get('class', []):\n",
    "            value = element.find('a').text.strip() if element.find('a') else element.text.strip()\n",
    "            if value and current_section and key:\n",
    "                parent_dict[current_section][key] = value\n",
    "\n",
    "    if isinstance(element, Tag):\n",
    "        for child in element.children:\n",
    "            extract_data(child, parent_dict, current_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "initial_dict = {}\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Invoke the function\n",
    "extract_data(starting_div, initial_dict)\n",
    "initial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, json_output):\n",
    "    stack = deque([json_output])\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            current_dict = stack[-1]\n",
    "            \n",
    "            if child.name == 'h1':\n",
    "                key = dividir(child.get_text())[0]\n",
    "                current_dict[key] = {}\n",
    "                stack.append(current_dict[key])\n",
    "                \n",
    "            elif child.name == 'b':\n",
    "                key = dividir(child.get_text())[0]\n",
    "                current_dict[key] = {}\n",
    "                stack.append(current_dict[key])\n",
    "                \n",
    "            elif child.name == 'div':\n",
    "                class_name = child.get('class', [])\n",
    "                \n",
    "                # Ensure the class matches exactly\n",
    "                if 'layout-cell-pad-5' in class_name:\n",
    "                    value = dividir(child.get_text())\n",
    "                    \n",
    "                    # Additional safety check\n",
    "                    if len(stack) > 1:\n",
    "                        parent_dict = stack[-2]\n",
    "                        parent_key = list(parent_dict.keys())[-1]\n",
    "                        parent_dict[parent_key] = value\n",
    "                        stack.pop()\n",
    "                    else:\n",
    "                        print(\"Warning: Stack size insufficient.\")\n",
    "\n",
    "            # Debugging information\n",
    "            print(f\"Stack size: {len(stack)}, Current dict keys: {current_dict.keys()}\")\n",
    "\n",
    "            # Continue recursion\n",
    "            list_divs(child, current_dict)\n",
    "            \n",
    "    if len(stack) > 1:\n",
    "        stack.pop()\n",
    "\n",
    "# Initialize empty dictionary\n",
    "json_output = {}\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Assuming 'soup' contains your BeautifulSoup object\n",
    "list_divs(starting_div, json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Não monta os dicionários no Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, parent_dict):\n",
    "    stack = deque([(soup_element, parent_dict)])  # Initialize the stack with parent element and parent dictionary\n",
    "\n",
    "    while stack:\n",
    "        element, current_dict = stack.pop()  # Get the last tuple (element, dictionary) from the stack\n",
    "\n",
    "        for child in element.children:\n",
    "            if isinstance(child, Tag):\n",
    "                \n",
    "                if child.name == 'h1':\n",
    "                    key = dividir(child.get_text())[0]\n",
    "                    current_dict[key] = {}\n",
    "                    stack.append((child, current_dict[key]))  # Append child element and its dictionary to the stack\n",
    "                    \n",
    "                elif child.name == 'b':\n",
    "                    key = dividir(child.get_text())[0]\n",
    "                    current_dict[key] = {}\n",
    "                    stack.append((child, current_dict[key]))  # Append child element and its dictionary to the stack\n",
    "                \n",
    "                elif child.name == 'div':\n",
    "                    class_name = child.get('class', [])\n",
    "                    \n",
    "                    if 'layout-cell-pad-5' in class_name:\n",
    "                        value = dividir(child.get_text())\n",
    "                        if value:  # Only populate if value is non-empty\n",
    "                            current_dict.update(value)  # Add the value to the current dictionary\n",
    "                            \n",
    "                else:\n",
    "                    stack.append((child, current_dict))  # Append child element and current dictionary for other cases\n",
    "\n",
    "\n",
    "# Initialize empty dictionary\n",
    "json_output = {}\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Assuming 'soup' contains your BeautifulSoup object\n",
    "list_divs(starting_div, json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, dados, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    \n",
    "    ignorar = [None, ' ', 'Baixar Currículo', 'Imprimir Currículo']\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    valor = dividir(child.get_text())\n",
    "                    # Adicionar valor ao dicionário\n",
    "                    dados[f'Value_Level_{indent}'] = valor\n",
    "                \n",
    "                list_divs(child, dados, indent + 1)\n",
    "            \n",
    "            elif child.name == 'h2':\n",
    "                # Adicionar nome do nó ao dicionário\n",
    "                dados[f'Node_Name'] = child.string\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == \"foto\":\n",
    "                # Adicionar nome do nó ao dicionário\n",
    "                dados[f'Informações'] = child.string\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # Adicionar chave ao dicionário de primeiro nível\n",
    "                    dados[f'Properties'] = rotulo\n",
    "                    list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                # Adicionar subtítulo ao dicionário\n",
    "                dados[f'Subtitle_Level_{indent}'] = child.get_text()\n",
    "                list_divs(child, dados, indent + 1)\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # Adicionar grupo ao dicionário\n",
    "                    dados[f'Group_Level_{indent}'] = rotulo\n",
    "                    list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                # Adicionar chave ao dicionário\n",
    "                chave = dividir(child.get_text())[0]\n",
    "                dados[f'Group_Level_{indent}'] = chave\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "        else:\n",
    "            list_divs(child, dados, indent)\n",
    "\n",
    "# Iniciar a função principal\n",
    "if __name__ == '__main__':\n",
    "    # soup é uma variável que contém o objeto BeautifulSoup do seu HTML\n",
    "    dados = {}\n",
    "\n",
    "    # Encontra a div específica a partir da qual iniciar a travessia\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    # Chame a função list_divs passando o objeto BeautifulSoup e o dicionário vazio\n",
    "    list_divs(starting_div, dados)\n",
    "    \n",
    "    # Convertendo o dicionário para JSON\n",
    "    dados_json = json.dumps(dados, indent=4, ensure_ascii=False)\n",
    "    print(dados_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dados_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\", \"\")\n",
    "    string = string.replace(\"\\t\", '')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, graph_dict=None, indent=0, verbose=False):\n",
    "    if graph_dict is None:\n",
    "        graph_dict = {}\n",
    "    \n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return graph_dict\n",
    "    \n",
    "    ignorar = [None, ' ', 'Baixar Currículo', 'Imprimir Currículo']\n",
    "\n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if verbose:\n",
    "                conteudo = dividir(child.get_text())[0]\n",
    "                print(f\"Tag encontrada: {child.name}, Classe: {child.get('class', 'None')}, {conteudo}\")\n",
    "\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    conteudo = dividir(child.get_text())[0]\n",
    "                    graph_dict[\"values\"] = conteudo\n",
    "                \n",
    "                list_divs(child, graph_dict, indent + 1, verbose)\n",
    "\n",
    "            elif child.name == 'h2':\n",
    "                node_name = child.string\n",
    "                if \"nodes\" not in graph_dict:\n",
    "                    graph_dict[\"nodes\"] = []\n",
    "                graph_dict[\"nodes\"].append({\"name\": node_name})\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    graph_dict[\"properties\"] = {\"label\": rotulo}\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                graph_dict[\"subtitles\"] = child.get_text()\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    if \"groups\" not in graph_dict:\n",
    "                        graph_dict[\"groups\"] = []\n",
    "                    graph_dict[\"groups\"].append({\"label\": rotulo})\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                graph_dict[\"keys\"] = dividir(child.get_text())[0]\n",
    "\n",
    "        else:\n",
    "            list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "result = list_divs(starting_div, verbose=True)\n",
    "json.dumps(result, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    return string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "def extract_content(element, keys, key_classes, val_classes, verbose=False):\n",
    "    result = {}\n",
    "    try:\n",
    "        if verbose is True:\n",
    "            print(f\"Debug: Processing element of type {type(element)} with attributes {element.attrs}\")\n",
    "\n",
    "        for key_class in key_classes:\n",
    "            key_elements = element.find_all(class_=key_class, recursive=True)\n",
    "            for key_elem in key_elements:\n",
    "                key_text = key_elem.text.strip()\n",
    "                if verbose is True:\n",
    "                    print(f\"Debug: Found key element with text: {key_text}\")\n",
    "\n",
    "                value_elem = key_elem.find_next_sibling(class_=val_classes)\n",
    "                if value_elem:\n",
    "                    value_text = rep(value_elem.text.strip())\n",
    "                    if verbose is True:\n",
    "                        print(f\"Debug: Found value element with text: {value_text}\")\n",
    "                    result[key_text] = value_text\n",
    "                else:\n",
    "                    if verbose is True:\n",
    "                        print(f\"Debug: No value element found for the key '{key_text}' with attributes {key_elem.attrs}\")\n",
    "\n",
    "        for section_class in keys:\n",
    "            section_elements = element.find_all('div', {'class': section_class}, recursive=True)\n",
    "            for idx, section_elem in enumerate(section_elements):\n",
    "               if verbose is True:\n",
    "                print(f\"Debug: Found section element with attributes: {section_elem.attrs}\")\n",
    "\n",
    "                section_dict = extract_content(section_elem, keys, key_classes, val_classes)\n",
    "                section_key = section_elem.get('id', f'UnnamedSection{idx}')\n",
    "                result[section_key] = section_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in extract_content: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "    # keys = ['infpessoa', 'id','title_wraper','infpessoa','artigo-completo']\n",
    "    # key_classes = ['title_wraper','infpessoa','inst_back', 'id','layout-cell layout-cell-3 text-align-right','artigo-completo', 'layout-cell-1','cita-artigos']\n",
    "    # val_classes = ['layout-cell-9','data-cell','layout-cell-11']\n",
    "\n",
    "try:\n",
    "    # Initialize BeautifulSoup object (Assuming that 'soup' has been initialized)\n",
    "    keys = [\n",
    "            'b',\n",
    "            'id',\n",
    "            'infpessoa',\n",
    "            'title_wraper',\n",
    "            'artigo-completo',\n",
    "            'title_wraper',\n",
    "            'infpessoa',\n",
    "            'id',\n",
    "            'artigo-completo',\n",
    "            ]\n",
    "    key_classes = [\n",
    "                   'text-align-right',\n",
    "                   'title_wraper',        \n",
    "                   'inst_back',\n",
    "                #    'layout-cell-1', # Trabalhos em eventos\n",
    "                #    'cita-artigos', # Trabalhos completos e conclusão de curso de graduação\n",
    "                   ]\n",
    "    val_classes = [\n",
    "        'layout-cell-9',\n",
    "        'data-cell',\n",
    "        'layout-cell-11',\n",
    "        'title_wraper',\n",
    "        # 'infpessoa',\n",
    "        'id',\n",
    "        'artigo-completo',        \n",
    "        ]\n",
    "\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "    master_dict = {}\n",
    "    \n",
    "    if starting_div:\n",
    "        master_dict = extract_content(starting_div, keys, key_classes, val_classes)\n",
    "        print(json.dumps(master_dict, indent=4, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"Starting div not found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred in main: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    return string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "def extract_content(element, keys, key_classes, val_classes):\n",
    "    result = {}\n",
    "    \n",
    "    if isinstance(element, Tag):\n",
    "        for key in keys:\n",
    "            occurrences = element.find_all(key)\n",
    "            for c in occurrences:\n",
    "                if c is not None:\n",
    "                    result[key] = rep(c.text).strip()\n",
    "\n",
    "        for key_class in key_classes:\n",
    "            occurrences = element.find_all('div', {'class': key_class})\n",
    "            for idx, c in enumerate(occurrences):\n",
    "                if c is not None:\n",
    "                    key = c.text.strip() if c.text.strip() else f\"Unnamed-{idx}\"\n",
    "                    result[key] = {}\n",
    "                    \n",
    "                    sibling = c.find_next_sibling('div', {'class': val_classes})\n",
    "                    if sibling:\n",
    "                        result[key] = rep(sibling.text).strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "try:\n",
    "    # Initialize BeautifulSoup object (Assuming that 'soup' has been initialized)\n",
    "    keys = ['infpessoa', 'id','title_wraper','infpessoa','artigo-completo']\n",
    "    key_classes = ['title_wraper','infpessoa','inst_back', 'id','layout-cell layout-cell-3 text-align-right','artigo-completo', 'layout-cell-1','cita-artigos']\n",
    "    val_classes = ['layout-cell-9','data-cell','layout-cell-11']\n",
    "\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    master_dict = {}\n",
    "    for i in starting_div.children:\n",
    "        if isinstance(i, Tag):\n",
    "            master_dict.update(extract_content(i, keys, key_classes, val_classes))\n",
    "\n",
    "    print(json.dumps(master_dict, indent=4, ensure_ascii=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "def rep(string):\n",
    "    return string.replace(\"\\n\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\")\n",
    "\n",
    "keys = ['infpessoa','h2','a', 'b', 'ul', 'id', 'inst_back','layout-cell-3','layout-cell-9','layout-cell-12','layout-cell-pad-5']\n",
    "\n",
    "try:\n",
    "    # Assuming 'soup' has been defined and initialized with the HTML document\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})        \n",
    "\n",
    "    for i in starting_div.children:\n",
    "        if isinstance(i, Tag):  # Confirming that the child is an HTML Tag\n",
    "            for key in keys:\n",
    "                # Use find_all to get a list of all occurrences\n",
    "                occurrences = i.find_all(key)\n",
    "                \n",
    "                for c in occurrences:  # Iterate over each occurrence\n",
    "                    if c is not None:\n",
    "                        print(f'{key:10} {rep(c.text).strip()}')\n",
    "                        print('-' * 75)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(element):\n",
    "    \"\"\"Recursively extracts content and organizes it into nested dictionaries.\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Check for 'title-wrapper' class as the first-level dictionary key\n",
    "    if 'title-wrapper' in element.attrs.get('class', []):\n",
    "        key = element.text.strip()\n",
    "        result[key] = {}\n",
    "        \n",
    "        for child in element.findChildren('div', recursive=False):\n",
    "            if 'layout-cell-12' in child.attrs.get('class', []):\n",
    "                second_level_key = child.text.strip()\n",
    "                result[key][second_level_key] = {}\n",
    "                \n",
    "                for grandchild in child.findChildren('div', recursive=False):\n",
    "                    if 'layout-cell-3' in grandchild.attrs.get('class', []):\n",
    "                        third_level_key = grandchild.text.strip()\n",
    "                        \n",
    "                        for great_grandchild in grandchild.findChildren('div', recursive=False):\n",
    "                            if 'layout-cell-9' in great_grandchild.attrs.get('class', []):\n",
    "                                value = great_grandchild.text.strip()\n",
    "                                result[key][second_level_key][third_level_key] = value\n",
    "    \n",
    "    # Recursively process child elements\n",
    "    for child in element.findChildren('div', recursive=False):\n",
    "        result.update(extract_content(child))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "# print(layout_cell_main.text.strip())\n",
    "print(len(layout_cell_main.find_all('div', 'title-wrapper')))\n",
    "for title_wrapper in layout_cell_main.find_all('div', 'title-wrapper'):\n",
    "    for data in title_wrapper.find_all('div', 'layout-cell-3'):\n",
    "        key = data.find('div', 'layout-cell-3')\n",
    "        val = data.find('div', 'layout-cell-9')\n",
    "        if key is not None:\n",
    "            print(f'{key.text.strip()}: {val.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_structure = extract_content(soup)\n",
    "json_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_scrape(tag, result_dict):\n",
    "    classes = tag.get(\"class\", [])\n",
    "    \n",
    "    if \"title-wrapper\" in classes:\n",
    "        section_name = tag.text.strip()\n",
    "        result_dict[section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[section_name])\n",
    "\n",
    "    elif \"text-align-right\" in classes or \"subtit-1\" in classes:\n",
    "        sub_section_name = tag.text.strip()\n",
    "        result_dict[sub_section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[sub_section_name])\n",
    "            \n",
    "    elif \"layout-cell-9\" in classes or \"layout-cell-pad-5\" in classes:\n",
    "        value = tag.text.strip()\n",
    "        result_dict[\"value\"] = value\n",
    "    \n",
    "    elif \"layout-cell\" in classes:\n",
    "        for child in tag.children:\n",
    "            if child.name:\n",
    "                recursive_scrape(child, result_dict)\n",
    "\n",
    "def main(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Initial recursive call\n",
    "    for child in soup.find_all('div'):\n",
    "        recursive_scrape(child, result_dict)\n",
    "        \n",
    "    # Convert the result to JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(result_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_scrape(tag, result_dict):\n",
    "    classes = tag.get(\"class\", [])\n",
    "    \n",
    "    if \"title-wrapper\" in classes:\n",
    "        section_name = tag.text.strip()\n",
    "        result_dict[section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[section_name])\n",
    "\n",
    "    elif \"text-align-right\" in classes or \"subtit-1\" in classes:\n",
    "        sub_section_name = tag.text.strip()\n",
    "        result_dict[sub_section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[sub_section_name])\n",
    "            \n",
    "    elif \"layout-cell-9\" in classes or \"layout-cell-pad-5\" in classes:\n",
    "        value = tag.text.strip()\n",
    "        result_dict[\"value\"] = value\n",
    "    \n",
    "    elif \"layout-cell\" in classes:\n",
    "        for child in tag.children:\n",
    "            if child.name:\n",
    "                recursive_scrape(child, result_dict)\n",
    "\n",
    "def main(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Initial recursive call\n",
    "    for child in soup.find_all('div'):\n",
    "        recursive_scrape(child, result_dict)\n",
    "        \n",
    "    # Convert the result to JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(result_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados hierárquicos de um Soup de documento HTML e retorna um dicionário aninhado.\n",
    "\n",
    "    Parâmetros:\n",
    "        Objeto Soup do html_content (object): O conteúdo HTML como uma string.\n",
    "\n",
    "    Retorna:\n",
    "        dict: Um dicionário aninhado contendo os dados extraídos.\n",
    "    \"\"\"\n",
    "\n",
    "    def recursive_extraction(element):\n",
    "        children_data = {}\n",
    "        children = element.find_all(recursive=False)\n",
    "        \n",
    "        for child in children:\n",
    "            if child.has_attr('class') and 'text-align-right' in child['class']:\n",
    "                key = child.get_text().strip()\n",
    "                \n",
    "                value_container = child.find_next_sibling(class_='layout-cell-9')\n",
    "                if value_container:\n",
    "                    value = recursive_extraction(value_container)\n",
    "                    children_data[key] = value if value else value_container.get_text().strip()\n",
    "        \n",
    "        return children_data\n",
    "\n",
    "    # Instanciar um objeto BeautifulSoup\n",
    "    # soup = BeautifulSoup(html_content, 'html.parser', from_encoding='utf-8')\n",
    "  \n",
    "    # Iniciar a extração recursiva a partir do elemento raiz\n",
    "    # root = soup.body\n",
    "    # result_dict = recursive_extraction(root)\n",
    "\n",
    "    # Encontra a div específica a partir da qual iniciar a travessia\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    # Inicia a travessia a partir da div encontrada\n",
    "    result_dict = recursive_extraction(starting_div)\n",
    "\n",
    "    # Converter o dicionário em um objeto JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "  \n",
    "    # Salvar o JSON em um arquivo, especificando o encoding como UTF-8\n",
    "    with open('output_nested.json', 'w', encoding='utf-8') as file:\n",
    "        file.write(result_json)\n",
    "  \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = extrair_dados(soup, True)\n",
    "nome_no = 'Antonio Marcos Aires Barbosa'\n",
    "imprimir_informacoes({nome_no: json_data['Properties']}, nome_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "for title_wrapper in layout_cell_main.find_all('div.title-wrapper'):\n",
    "    index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_from_html(soup):\n",
    "    \"\"\"\n",
    "    Generate a JSON object from an Soup object for a Neo4j integration.\n",
    "\n",
    "    Parameters:\n",
    "        soup (object): Soup object from a HTML content.\n",
    "\n",
    "    Returns:\n",
    "        json_data (dict): A dictionary representing the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract node name\n",
    "    node_name = soup.select_one('div.infpessoa h2.nome').text.strip()\n",
    "\n",
    "    # Initialize the master dictionary\n",
    "    json_data = {node_name: {}}\n",
    "\n",
    "    # Locate the main layout cell\n",
    "    layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "    # Iterate over title-wrapper elements\n",
    "    for title_wrapper in layout_cell_main.select('div.title-wrapper'):\n",
    "        index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "        \n",
    "        # Initialize the child dictionary\n",
    "        json_data[node_name][index] = {}\n",
    "\n",
    "        # Iterate over layout cells\n",
    "        for layout_cell_3 in title_wrapper.select('div.layout-cell.layout-cell-3.text-align-right'):\n",
    "            grandchild_index = layout_cell_3.select_one('div.layout-cell-pad-5.text-align-right').text.strip()\n",
    "            \n",
    "            # Find the corresponding layout cell for values\n",
    "            layout_cell_9 = layout_cell_3.find_next_sibling('div', class_='layout-cell.layout-cell-9')\n",
    "            \n",
    "            values_text = layout_cell_9.select_one('div.layout-cell-pad-5').text\n",
    "            \n",
    "            # Create a list of values\n",
    "            values = values_text.split('<br class=\"clear\">' or '\\n\\n\\n')\n",
    "\n",
    "            # Add the grandchild dictionary\n",
    "            json_data[node_name][index][grandchild_index] = values\n",
    "\n",
    "    return json_data\n",
    "\n",
    "# Generate JSON data\n",
    "json_data = generate_json_from_html(soup)\n",
    "\n",
    "# Print or persist the generated JSON data\n",
    "print(json.dumps(json_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lattes = extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint(json_lattes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(json_data, parent_key='', separator='.'):\n",
    "    \"\"\"\n",
    "    Convert a nested dictionary into a flat dictionary, suitable for DataFrame conversion.\n",
    "    \n",
    "    Parameters:\n",
    "    - json_data (dict): The nested dictionary to flatten.\n",
    "    - parent_key (str, optional): The concatenated key used to represent nesting.\n",
    "    - separator (str, optional): The character to use for separating nested keys.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame representing the flattened dictionary.\n",
    "    \"\"\"\n",
    "    flat_dict = {}\n",
    "    \n",
    "    for k, v in json_data.items():\n",
    "        new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "        \n",
    "        if isinstance(v, dict):\n",
    "            flat_dict.update(dict_to_dataframe(v, new_key, separator=separator))\n",
    "        else:\n",
    "            flat_dict[new_key] = v\n",
    "            \n",
    "    return pd.DataFrame([flat_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming json_lattes is a JSON-formatted string\n",
    "json_lattes_dict = json.loads(json_lattes)\n",
    "\n",
    "# Then call the dict_to_dataframe function\n",
    "df = dict_to_dataframe(json_lattes_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    splpt = 'Currículo do Sistema de Currículos Lattes ('\n",
    "    string_title = soup.title.string if soup.title else \"Unknown\"\n",
    "    title = string_title.split(splpt)[1].strip(')')\n",
    "    \n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "    from py2neo import Graph, Node, Relationship\n",
    "    \n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    secoes = []\n",
    "    \n",
    "    print(f'{len(h1_elements[2:])} elementos encontrados')\n",
    "    for n,i in enumerate(h1_elements):\n",
    "        if n>1:\n",
    "            secao = i.text\n",
    "            print(f'    {secao}')\n",
    "            secoes.append(secao)\n",
    "    \n",
    "    for elem in h1_elements[2:]:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        \n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_parsoninfo(soup):\n",
    "    # Localizar o elemento de link que contém o título do currículo\n",
    "    link_element = soup.find(\"a\", {\"href\": lambda x: x and \"abreDetalhe\" in x})\n",
    "\n",
    "    # Extrair o texto do link para usar como título do nó\n",
    "    node_title = link_element.text if link_element else \"Unknown\"\n",
    "    print(f'Título do Nó: {node_title}')\n",
    "\n",
    "    # Localizar o elemento div contendo as propriedades\n",
    "    properties_div = soup.find(\"div\", {\"class\": \"resultado\"})\n",
    "    if properties_div:\n",
    "        print(f'Resultado: {properties_div.text}')\n",
    "    else:\n",
    "        print('Resultados não encontrados')\n",
    "\n",
    "    # Inicializar um dicionário para armazenar as propriedades\n",
    "    properties = {}\n",
    "    \n",
    "    # Localizar o elemento li que contém as informações do idlattes\n",
    "    li_element = soup.find(\"li\")\n",
    "    for i in li_element:\n",
    "        if 'http://lattes.cnpq.br/' in i:\n",
    "            idlattes = i.split('http://lattes.cnpq.br/')[1]\n",
    "            properties['Idlattes'] = idlattes\n",
    "            print(idlattes)\n",
    "\n",
    "    # Extrair e armazenar as propriedades relevantes\n",
    "    if properties_div:\n",
    "        properties['Nacionalidade'] = 'Brasil'\n",
    "        properties['Cargo'] = properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}).text if properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}) else 'Desconhecido'\n",
    "        properties['Titulação'] = properties_div.contents[-4] if len(properties_div.contents) > 4 else 'Desconhecido'\n",
    "\n",
    "        # Extração de nome e identificador único\n",
    "        a_element = li_element.find(\"a\")\n",
    "        properties[\"Nome\"] = a_element.text\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Títulos Acadêmicos e outras informações\n",
    "\n",
    "    return node_title, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_annotation_in_db(uri, user, password, annot_html, cv_id):\n",
    "    \"\"\"\n",
    "    Store the annotation HTML in a Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        uri (str): URI of the Neo4j database\n",
    "        user (str): Username for the Neo4j database\n",
    "        password (str): Password for the Neo4j database\n",
    "        annot_html (str): The HTML string containing annotations\n",
    "        cv_id (str): The unique identifier for the annotated CV\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize Neo4j driver\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    # Define Cypher query for adding an annotation\n",
    "    add_annotation_query = '''\n",
    "    MERGE (cv:CV {id: $cv_id})\n",
    "    CREATE (a:Annotation {html: $annot_html})\n",
    "    MERGE (cv)-[:HAS]->(a)\n",
    "    '''\n",
    "\n",
    "    # Execute query\n",
    "    with driver.session() as session:\n",
    "        session.run(add_annotation_query, cv_id=cv_id, annot_html=annot_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# from collections import Counter\n",
    "\n",
    "# def enumerate_tags(soup):\n",
    "#     # Extração de todos os marcadores (tags) no documento\n",
    "#     all_tags = [tag.name for tag in soup.find_all(True)]\n",
    "    \n",
    "#     # Contagem de ocorrências de cada marcador\n",
    "#     tag_count = Counter(all_tags)\n",
    "    \n",
    "#     # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "#     tag_dict = dict(tag_count)\n",
    "    \n",
    "#     return tag_dict\n",
    "\n",
    "# # Exemplo de uso\n",
    "# tag_dictionary = enumerate_tags(soup)\n",
    "# print(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div_elements = soup.find_all('div')\n",
    "# div_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_div_data(soup):\n",
    "#     \"\"\"\n",
    "#     Extrai dados das divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "#     Parâmetros:\n",
    "#     - html_document (str): String contendo o documento HTML.\n",
    "    \n",
    "#     Retorno:\n",
    "#     - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "#     \"\"\"\n",
    "#     # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "#     extracted_data = {}\n",
    "    \n",
    "#     # Localiza todas as divs com a classe 'layout-cell-pad-5 text-align-right'\n",
    "# #     divs_key = soup.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "#     divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "#     for div_key in divs_key:\n",
    "#         # Extrai o conteúdo da tag <b> dentro da div\n",
    "#         key = div_key.find('b').text if div_key.find('b') else None\n",
    "        \n",
    "#         # Encontra a div que segue imediatamente\n",
    "#         div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5'})\n",
    "        \n",
    "#         # Extrai o conteúdo da div\n",
    "#         value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "#         # Armazena no dicionário se ambas chave e valor existirem\n",
    "#         if key and value:\n",
    "#             extracted_data[key] = value\n",
    "    \n",
    "#     return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_div_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(extracted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        \n",
    "        data_dict = {}\n",
    "        \n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "            \n",
    "            current_higher_order_dict = None  # Initialize a variable to store the current higher-order dictionary\n",
    "            \n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                \n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    current_higher_order_dict = {}  # Create a new dictionary for this higher-order key\n",
    "                    data_dict[higher_order_key] = current_higher_order_dict  # Associate the new dictionary with the higher-order key\n",
    "                    \n",
    "                year_elems = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for year_elem, details_elem in zip(year_elems, details_elems):\n",
    "                    year_text = year_elem.text.strip() if year_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    if current_higher_order_dict is not None:\n",
    "                        # Insert the year-details pair into the current higher-order dictionary\n",
    "                        current_higher_order_dict[year_text] = details_text\n",
    "                    else:\n",
    "                        # If no higher-order key is present, associate the year-details pair directly with the title\n",
    "                        data_dict[year_text] = details_text\n",
    "                \n",
    "            result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(input_dict):\n",
    "    def recursive_descent(current_dict, parent_key='', separator='.'):\n",
    "        nonlocal flattened_dict\n",
    "        for k, v in current_dict.items():\n",
    "            new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                recursive_descent(v, new_key, separator=separator)\n",
    "            else:\n",
    "                flattened_dict[new_key] = v\n",
    "                \n",
    "    flattened_dict = {}\n",
    "    recursive_descent(input_dict)\n",
    "    \n",
    "    # Create DataFrame from the flattened dictionary\n",
    "    df = pd.DataFrame([flattened_dict])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dict = extract_data(soup)\n",
    "df = dict_to_dataframe(nested_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag, NavigableString\n",
    "\n",
    "html_content = '''\n",
    "<div class=\"layout-cell-pad-5\">\n",
    "    Doutorado em andamento em Informática Aplicada.\n",
    "    <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "    <br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde\n",
    "    <br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho.\n",
    "</div>\n",
    "'''\n",
    "\n",
    "soup_sample = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup_sample)\n",
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def extract_text_from_selectors(soup,select_path):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        elements = title_wrapper.select(select_path)\n",
    "#         print(len(elements))\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag, NavigableString\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=\"layout-cell.layout-cell-3.text-align-right\"\n",
    "vals=\"layout-cell layout-cell-9\"\n",
    "# <div class=\"layout-cell layout-cell-9\">\n",
    "# <div class=\"layout-cell-pad-5\">Doutorado em andamento em Informática Aplicada. <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "\t\t\n",
    "# \t<br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde<br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho. </div>\n",
    "# </div>\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "father     = 'div.title-wrapper'\n",
    "sons       = 'h1'\n",
    "grandchild = '' \n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        son_elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for grandchild in son_elements:\n",
    "            text_content = grandchild.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_titles(soup,father,sons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "father = 'div.title-wrapper'\n",
    "sons   = 'h1'\n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-12.data-cell'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "select_path='layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_to_dict(soup):\n",
    "    result_list = []\n",
    "    \n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        temp_dict = defaultdict(dict)\n",
    "        first_level_key = title_wrapper.select_one('div.layout-cell.layout-cell-12.data-cell').text.strip()\n",
    "        \n",
    "        for cell in title_wrapper.select('div.layout-cell.layout-cell-12.data-cell'):\n",
    "            second_level_keys = cell.select('div.layout-cell-pad-5.text-align-right')\n",
    "            values = cell.select('div.layout-cell.layout-cell-3.text-align-right')\n",
    "            \n",
    "            if len(second_level_keys) == len(values):\n",
    "                for key, value in zip(second_level_keys, values):\n",
    "                    second_level_key = key.text.strip()\n",
    "                    value_text = value.text.strip()\n",
    "                    temp_dict[first_level_key][second_level_key] = value_text\n",
    "        \n",
    "        result_list.append(temp_dict)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "# Execute the function\n",
    "result_list = extract_to_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['layout-cell', 'layout-cell-3', 'text-align-right']\n",
    "\n",
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "# 'text-align-right': 152,\n",
    "# 'layout-cell-pad-5': 152,\n",
    "\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-pad-main'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'text-align-right'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-9'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-3'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'data-cell'},\n",
    "#                                     {'class': 'layout-cell'},\n",
    "#                                     {'class': 'layout-cell-9},                                    \n",
    "#                                     {'class': 'layout-cell-12'},\n",
    "#                                     {'class': 'layout-cell-pad-5'},\n",
    "#                                     {'class': 'data-cell'}\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_text_from_hierarchy(tag, result_dict, parent_key=\"root\"):\n",
    "    div_elements = tag.find_all('div', recursive=False)\n",
    "    \n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        class_key = ', '.join(class_list) if class_list else \"None\"\n",
    "        \n",
    "        # Criando uma chave única que incorpora o caminho da raiz até esta div\n",
    "        full_key = f\"{parent_key} -> {class_key}\"\n",
    "\n",
    "        # Coleta o texto contido no elemento div atual\n",
    "        text_content = div.get_text(strip=True)\n",
    "\n",
    "        # Armazenar o conteúdo textual sob esta chave única\n",
    "        if full_key not in result_dict:\n",
    "            result_dict[full_key] = []\n",
    "        result_dict[full_key].append(text_content)\n",
    "\n",
    "        # Chamada recursiva para extrair textos dos filhos deste div\n",
    "        extract_text_from_hierarchy(div, result_dict, full_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)\n",
    "\n",
    "extract_text_from_hierarchy(soup.body, result_dict, 'text-align-right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.values():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes, key_stack=None):\n",
    "    if key_stack is None:\n",
    "        key_stack = []\n",
    "    \n",
    "    popped_key = None\n",
    "    if soup_element.name == \"div\":\n",
    "        class_list = soup_element.get('class', [])\n",
    "        \n",
    "        if any(k in class_list for k in key_classes):\n",
    "            new_key = soup_element.text.strip()\n",
    "#             print(f'Key found: {new_key}')  # Debugging line\n",
    "            key_stack.append(new_key)\n",
    "        \n",
    "        elif any(v in class_list for v in value_classes) and key_stack:\n",
    "            value = soup_element.text.strip()\n",
    "#             print(f'Value found: {value}')  # Debugging line\n",
    "            current_key = key_stack[-1]\n",
    "            if current_key in result_dict:\n",
    "                result_dict[current_key].append(value)\n",
    "            else:\n",
    "                result_dict[current_key] = [value]\n",
    "                \n",
    "#     print(f\"Current key_stack: {key_stack}\")  # Debugging line\n",
    "\n",
    "    for child in soup_element.find_all(\"div\", recursive=False):\n",
    "        extract_key_value_pairs(child, result_dict, key_classes, value_classes, key_stack)\n",
    "\n",
    "    if popped_key:\n",
    "        key_stack.append(popped_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'text-align-right'})\n",
    "# key_classes   = ['data-cell']\n",
    "# value_classes = ['text-align-right']\n",
    "\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    pprint(i)\n",
    "\n",
    "print()\n",
    "\n",
    "for i in result_dict.values():\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'layout-cell-pad-5'})\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "\n",
    "def extract_target_classes(soup,classe):\n",
    "    soup_element  = soup.find('div', {'class': classe})\n",
    "#     key_classes   = ['data-cell']\n",
    "#     value_classes = ['text-align-right']\n",
    "    key_classes   = ['layout-cell-9']\n",
    "    value_classes = ['layout-cell-pad-5']\n",
    "    \n",
    "    target_classes = [\n",
    "        'Produção '\n",
    "        'Bibliográfica',\n",
    "        'Produção '\n",
    "        'Técnica',\n",
    "        'Produção '\n",
    "        'Artística/Cultural'\n",
    "        'nome', \n",
    "        'resumo', \n",
    "        'artigo-completo', \n",
    "        'cita', \n",
    "        'cita-artigos', \n",
    "        'citacoes', \n",
    "        'detalhes', \n",
    "        'fator', \n",
    "        'foto', \n",
    "        'informacao-artigo', \n",
    "        'informacoes-autor', \n",
    "        'infpessoa', \n",
    "        'rodape-cv', \n",
    "        'science_cont', \n",
    "        'texto', \n",
    "        'trab'\n",
    "        ]\n",
    "    try:\n",
    "        extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "        for x,y in zip(result_dict.keys(),result_dict.values()):\n",
    "            if x in target_classes:\n",
    "                try:\n",
    "                    print(f\"{x:>12} | {y[0]}\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        filtered_dict = {k: v for k, v in result_dict.items() if k in target_classes}\n",
    "\n",
    "    except:\n",
    "        print(f'Classe \"{classe}\" não encontrada')\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variações das funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funciona mas achata o dicionário de Propriedades em um único e não está em UTF-8\n",
    "# from neo4j import GraphDatabase\n",
    "# import json\n",
    "\n",
    "# class InvalidPropertyError(ValueError):\n",
    "#     \"\"\"Custom Exception for Invalid Properties\"\"\"\n",
    "#     pass\n",
    "\n",
    "# def serialize_properties(input_dict):\n",
    "#     \"\"\"\n",
    "#     Serializes the properties in the dictionary that are not of primitive types.\n",
    "    \n",
    "#     :param input_dict: The input dictionary with properties.\n",
    "#     :return: A dictionary with properties serialized if necessary.\n",
    "#     \"\"\"\n",
    "#     return {k: json.dumps(v) if not isinstance(v, (str, int, float, bool, list)) else v for k, v in input_dict.items()}\n",
    "\n",
    "# def ensure_string_keys(input_dict):\n",
    "#     \"\"\"\n",
    "#     Ensures all keys in the dictionary are of string type.\n",
    "    \n",
    "#     :param input_dict: The input dictionary.\n",
    "#     :return: A dictionary with string keys.\n",
    "#     \"\"\"\n",
    "#     return {str(k): v for k, v in input_dict.items()}\n",
    "\n",
    "# def create_or_merge_node(tx, label, properties):\n",
    "#     \"\"\"\n",
    "#     Creates or merges a node in the Neo4j database.\n",
    "    \n",
    "#     :param tx: The transaction object.\n",
    "#     :param label: The label for the node.\n",
    "#     :param properties: The properties for the node.\n",
    "#     :return: The created or merged node.\n",
    "#     \"\"\"\n",
    "#     query = f\"MERGE (n:{label} {{name: $name}}) SET n += $properties RETURN n\"\n",
    "#     return tx.run(query, name=properties.get('name', 'UNKNOWN'), properties=properties).single()\n",
    "\n",
    "# def are_properties_primitive(input_dict):\n",
    "#     \"\"\"\n",
    "#     Validates if all properties in the dictionary are of primitive types or lists thereof.\n",
    "    \n",
    "#     :param input_dict: The input dictionary.\n",
    "#     :return: Boolean indicating the validation result.\n",
    "#     \"\"\"\n",
    "#     for key, value in input_dict.items():\n",
    "#         if not (isinstance(value, (str, int, float, bool)) or isinstance(value, list)):\n",
    "#             print(f\"Invalid property: {key} -> {value}\")\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "# def persist_to_neo4j(uri, user, password, node_data, label):\n",
    "#     \"\"\"\n",
    "#     Persists data to Neo4j database.\n",
    "    \n",
    "#     :param uri: The URI of the Neo4j database.\n",
    "#     :param user: The username for the Neo4j database.\n",
    "#     :param password: The password for the Neo4j database.\n",
    "#     :param node_data: The dictionary containing the properties for the node.\n",
    "#     :param label: The label for the node.\n",
    "#     \"\"\"\n",
    "#     driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "#     with driver.session() as session:\n",
    "#         converted_dict = ensure_string_keys(node_data)\n",
    "        \n",
    "#         # Serialize non-primitive properties\n",
    "#         serialized_dict = serialize_properties(converted_dict)\n",
    "        \n",
    "#         # Now validate the properties\n",
    "#         if not are_properties_primitive(serialized_dict):\n",
    "#             raise InvalidPropertyError(\"Properties should be of primitive types or arrays of primitive types.\")\n",
    "        \n",
    "#         session.write_transaction(create_or_merge_node, label, serialized_dict)\n",
    "\n",
    "#     driver.close()\n",
    "\n",
    "# # Example usage\n",
    "# uri = \"bolt://localhost:7687\"  # Replace with your Neo4j URI\n",
    "# user = \"neo4j\"  # Replace with your username\n",
    "# password = \"password\"  # Replace with your password\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample invocation\n",
    "# extracted_data = {\n",
    "#     'name': 'John Doe',\n",
    "#     'age': 30,\n",
    "#     None: 'This will be skipped',\n",
    "#     '': 'This key will be skipped',\n",
    "#     'Properties': {\n",
    "#         'Endereço Profissional': {\n",
    "#             'Rua': '123 Main St',\n",
    "#             'Cidade': 'City',\n",
    "#             'CEP': '12345'\n",
    "#         },\n",
    "#         'Outra Propriedade': {\n",
    "#             'Chave1': 'Valor1',\n",
    "#             'Chave2': 'Valor2'\n",
    "#         }\n",
    "#     },\n",
    "#     'nested_list': [1, 2, 3]\n",
    "# }\n",
    "\n",
    "# persist_to_neo4j(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# pprint(node_raimir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data = node_raimir\n",
    "\n",
    "# try:\n",
    "#     persist_to_neo4j(extracted_data)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para anotação de dados em HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_published_articles(soup, qualis_data):\n",
    "    # Localiza elementos contendo os artigos publicados\n",
    "    articles_elements = soup.find_all('div', class_='published-article')\n",
    "    \n",
    "    # Inicializa lista para conter dados dos artigos publicados\n",
    "    annotated_articles = []\n",
    "    \n",
    "    # Itera sobre cada elemento e anotando as informações necessárias\n",
    "    for article in articles_elements:\n",
    "        title = article.find('div', class_='article-title').text\n",
    "        issn = article.find('div', class_='article-issn').text\n",
    "        qualis = qualis_data.get(issn, 'N/A')  # Buscando o Qualis correspondente\n",
    "        \n",
    "        # Adiciona ao conjunto de artigos anotados\n",
    "        annotated_articles.append({\n",
    "            'title': title,\n",
    "            'issn': issn,\n",
    "            'qualis': qualis\n",
    "        })\n",
    "    \n",
    "    # Persiste em SQLite\n",
    "    conn = sqlite3.connect(\"lattes_data.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Cria tabela de artigos publicados, se não existir\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS published_articles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT,\n",
    "        issn TEXT,\n",
    "        qualis TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insere artigos anotados na tabela\n",
    "    for article in annotated_articles:\n",
    "        cursor.execute(\"INSERT INTO published_articles (title, issn, qualis) VALUES (?, ?, ?)\",\n",
    "                       (article['title'], article['issn'], article['qualis']))\n",
    "    \n",
    "    # Commit e fecha a conexão\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "async def annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info):\n",
    "    print(\"Procurando por publicações de periódicos...\")\n",
    "    \n",
    "    # Inicializa BeautifulSoup para analisar o conteúdo HTML da página\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extrai e anota informações dos artigos publicados\n",
    "    qualis_info = await annotate_published_articles(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup)\n",
    "    \n",
    "    return qualis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_annotation_message(images_urls, visualization_url, recent_updates_url, pub_count):\n",
    "    annot_header_html = ''\n",
    "    annot_buttons_html = ''\n",
    "    pub_count_string = ''\n",
    "    \n",
    "    if pub_count > 0:\n",
    "        s_char = 's' if pub_count > 1 else ''\n",
    "        pub_count_string = f'anotou o Qualis de {pub_count} artigo{s_char} em periódico{s_char} neste CV.'\n",
    "        annot_buttons_html = render_template_string(\"\"\"\n",
    "            <a href=\"#artigos-completos\">\n",
    "                <button> Ver anotações </button>\n",
    "            </a>\n",
    "        \"\"\")\n",
    "    else:\n",
    "        pub_count_string = 'não anotou nenhum artigo em periódico neste CV.'\n",
    "\n",
    "    annot_header_html = render_template_string(\"\"\"\n",
    "        <a href=\"{{visualization_url}}\" target=\"_blank\" id=\"qlattes-logo\">\n",
    "            <img src=\"{{images_urls['qlattesLogoURL']}}\" width=\"70\">\n",
    "        </a>{{pub_count_string}}\n",
    "        </br>\n",
    "    \"\"\", visualization_url=visualization_url, images_urls=images_urls, pub_count_string=pub_count_string)\n",
    "\n",
    "    # Aqui a informação seria armazenada em um banco de dados em vez de ser injetada em um elemento HTML\n",
    "    store_annotation_in_db(annot_header_html + annot_buttons_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_qualis_data(qualis_info):\n",
    "    pub_data = []\n",
    "    pub_data_year = []\n",
    "    curr_year = 0\n",
    "    \n",
    "    for i in range(len(qualis_info)):\n",
    "        if curr_year != qualis_info[i]['year']:\n",
    "            if curr_year > 0:\n",
    "                pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "                pub_data_year = []\n",
    "            \n",
    "            curr_year = qualis_info[i]['year']\n",
    "        \n",
    "        pub_data_item = {\n",
    "            'issn': qualis_info[i]['issn'],\n",
    "            'title': qualis_info[i]['title'],\n",
    "            'pubName': qualis_info[i]['pubName'],\n",
    "            'qualis': qualis_info[i]['qualisLabels']['qualis'],\n",
    "            'baseYear': qualis_info[i]['qualisLabels']['baseYear'],\n",
    "            'jcr': qualis_info[i]['jcrData']['jcr'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else 0,\n",
    "            'jcrYear': qualis_info[i]['jcrData']['baseYear'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else ''\n",
    "        }\n",
    "        \n",
    "        pub_data_year.append(pub_data_item)\n",
    "        \n",
    "    if len(qualis_info) > len(pub_data):\n",
    "        pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "    \n",
    "    return pub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_lattes_page(name_link, images_urls, recent_updates_url, qualis_data, data_source_info):\n",
    "    print(qualis_data)\n",
    "    \n",
    "    # Inicializando o WebDriver\n",
    "    driver_service = Service('path/to/chromedriver')\n",
    "    driver = webdriver.Chrome(service=driver_service)\n",
    "    driver.get(name_link['link'])\n",
    "    \n",
    "    # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "    visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "    # Limpar cache de dados de Qualis\n",
    "    qualis_data_cache = {}\n",
    "    \n",
    "    # Annotate Lattes page com informações de Qualis\n",
    "    lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "    if lattes_info:\n",
    "        await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "    # Consolidar dados de publicação a partir das informações de Lattes\n",
    "    pub_info = consolidate_qualis_data(lattes_info)\n",
    "    print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "    # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "    conn = sqlite3.connect('lattes_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "    existing_data = cursor.fetchall()\n",
    "    \n",
    "    lattes_data_array = []\n",
    "    \n",
    "    if existing_data:\n",
    "        # Filtrar dados existentes para evitar duplicatas\n",
    "        lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "    # Adicionar dados de Lattes atuais ao array\n",
    "    lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "    # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "    cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_link_html(text, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">{{text}}</a>\n",
    "    \"\"\", text=text, tooltip=tooltip, target_url=target_url)\n",
    "\n",
    "def create_icon_link_html(icon_url, icon_style, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">\n",
    "            <img src=\"{{icon_url}}\" style=\"{{icon_style}}\">\n",
    "        </a>\n",
    "    \"\"\", icon_url=icon_url, icon_style=icon_style, tooltip=tooltip, target_url=target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qualis_from_percentil(percentil):\n",
    "    if not percentil:\n",
    "        return 'N'\n",
    "\n",
    "    qualis_class_list     = ['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4']\n",
    "    qualis_threshold_list = [87.5, 75, 62.5, 50, 37.5, 25, 12.5, 0]\n",
    "\n",
    "    for i in range(len(qualis_threshold_list)):\n",
    "        if percentil >= qualis_threshold_list[i]:\n",
    "            return qualis_class_list[i]\n",
    "\n",
    "    return 'N'\n",
    "\n",
    "\n",
    "def get_alternative_issn(issn, capes_alt_data, scopus_data):\n",
    "    # Pesquisar ISSN nos dados complementares da CAPES\n",
    "    match = next((elem for elem in capes_alt_data if elem['issn'] == issn or elem['alt_issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        if 'alt_issn' in match and match['alt_issn'] != issn:\n",
    "            return match['alt_issn']\n",
    "        elif 'issn' in match and match['issn'] != issn:\n",
    "            return match['issn']\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        # Pesquisar ISSN nos dados do Scopus\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "        \n",
    "        if match:\n",
    "            if 'e-issn' in match and len(match['e-issn']) > 0 and match['e-issn'] != issn:\n",
    "                return match['e-issn']\n",
    "            elif 'issn' in match and len(match['issn']) > 0 and match['issn'] != issn:\n",
    "                return match['issn']\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualis_from_capes_data(issn, alt_issn, capes_data, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procurar pelo ISSN na base de dados da CAPES\n",
    "    match = next((elem for elem in capes_data if elem['issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        qualis_labels['source'] = 'capes'\n",
    "    elif alt_issn != '':\n",
    "        match = next((elem for elem in capes_data if elem['issn'] == alt_issn), None)\n",
    "        \n",
    "        if match:\n",
    "            qualis_labels['source'] = 'capes_alt'\n",
    "            \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = match['qualis']\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['baseYear'] = data_source_info[qualis_labels['source']]['baseYear']\n",
    "\n",
    "        qualis_labels_scopus = get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info)\n",
    "        \n",
    "        if qualis_labels_scopus['qualis'] != 'N':\n",
    "            qualis_labels['linkScopus'] = qualis_labels_scopus['linkScopus']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_pucrs_data(issn, alt_issn, pub_name, pucrs_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    labels_map = {\n",
    "        'pubName': 'periodico',\n",
    "        'qualis': 'Qualis_Final',\n",
    "        'percentil': 'percentil',\n",
    "        'linkScopus': 'link_scopus',\n",
    "        'adjusted': 'Ajuste_SBC'\n",
    "    }\n",
    "    \n",
    "    match = next((elem for elem in pucrs_data if elem['issn'] == issn or (alt_issn and elem['issn'] == alt_issn)), None)\n",
    "    \n",
    "    if match:\n",
    "        for key in labels_map.keys():\n",
    "            if labels_map[key] in match and match[labels_map[key]] != 'nulo':\n",
    "                qualis_labels[key] = match[labels_map[key]]\n",
    "                \n",
    "        qualis_labels['source'] = 'pucrs'\n",
    "        qualis_labels['baseYear'] = data_source_info['pucrs']['baseYear']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procura pelo ISSN nos dados da Scopus\n",
    "    match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "    \n",
    "    if not match and alt_issn != '':\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == alt_issn or elem['e-issn'] == alt_issn), None)\n",
    "        \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = calculate_qualis_from_percentil(match['percentil'])\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['percentil'] = match['percentil']\n",
    "        qualis_labels['linkScopus'] = match['source-id-url']\n",
    "        qualis_labels['source'] = 'scopus'\n",
    "        qualis_labels['baseYear'] = data_source_info['scopus']['baseYear']\n",
    "    \n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "async def get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    if not issn:\n",
    "        return qualis_labels\n",
    "\n",
    "    alt_issn = ''\n",
    "\n",
    "    # Verificar se o ISSN já está em cache\n",
    "    if issn in qualis_data_cache:\n",
    "        return qualis_data_cache[issn]\n",
    "    else:\n",
    "        # Verificar se um ISSN alternativo existe e está em cache\n",
    "        alt_issn = await get_alternative_issn(issn, qualis_data['capes_alt'], qualis_data['scopus'])\n",
    "        if alt_issn and alt_issn in qualis_data_cache:\n",
    "            return qualis_data_cache[alt_issn]\n",
    "\n",
    "    # Procurar pelo ISSN nos dados CAPES\n",
    "    qualis_labels = await get_qualis_from_capes_data(\n",
    "        issn, alt_issn, qualis_data['capes'], qualis_data['scopus'], data_source_info\n",
    "    )\n",
    "\n",
    "    # Se não encontrado\n",
    "    if qualis_labels['qualis'] == 'N':\n",
    "        # Procurar pelo ISSN nos dados PUC-RS\n",
    "        qualis_labels = await get_qualis_from_pucrs_data(\n",
    "            issn, alt_issn, pub_name, qualis_data['pucrs'], data_source_info\n",
    "        )\n",
    "\n",
    "        # Se ainda não encontrado\n",
    "        if qualis_labels['qualis'] == 'N':\n",
    "            # Procurar pelo ISSN nos dados Scopus\n",
    "            qualis_labels = await get_qualis_from_scopus_data(\n",
    "                issn, alt_issn, qualis_data['scopus'], data_source_info\n",
    "            )\n",
    "\n",
    "    # Adicionar rótulos ao cache Qualis\n",
    "    qualis_data_cache[issn] = qualis_labels\n",
    "\n",
    "    if alt_issn:\n",
    "        qualis_data_cache[alt_issn] = qualis_labels\n",
    "\n",
    "    return qualis_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_qualis_annotation(pub_info, images_URLs, data_source_info):\n",
    "    annotation_dict = {}\n",
    "    \n",
    "    # Create QLattes icon element\n",
    "    qlattes_img_elem = BeautifulSoup('<img>', 'html.parser')\n",
    "    qlattes_img_elem['src'] = images_URLs['qlattesIconURL']\n",
    "    qlattes_img_elem['style'] = 'margin-bottom:-4px'\n",
    "    \n",
    "    annotation_dict['qlattes_img_elem'] = str(qlattes_img_elem)\n",
    "    \n",
    "    # Create Qualis labels annotations\n",
    "    issn_label = f', ISSN {pub_info[\"issn\"]}' if pub_info.get('issn') else ''\n",
    "    \n",
    "    if pub_info['qualisLabels']['qualis'] == 'N':\n",
    "        qualis_annot = f' Não classificado{issn_label}'\n",
    "    else:\n",
    "        qualis_annot = f' {pub_info[\"qualisLabels\"][\"qualis\"]}{issn_label}'\n",
    "        \n",
    "        # add Data source and base year\n",
    "        source = pub_info['qualisLabels']['source']\n",
    "        source_info = data_source_info[source]\n",
    "        \n",
    "        data_source_label = f'{source_info[\"label\"]} ({source_info[\"baseYear\"]})'\n",
    "        qualis_annot += f', fonte {data_source_label}'\n",
    "    \n",
    "    annotation_dict['qualis_annot'] = qualis_annot\n",
    "    \n",
    "    # add icon with link to Google Scholar\n",
    "    base_url = 'https://scholar.google.com/scholar?q='\n",
    "    title_param = f'intitle%3A%22{pub_info[\"title\"].replace(\" \", \"+\")}%22'\n",
    "    link_scholar = f'{base_url}{title_param}'\n",
    "    \n",
    "    annotation_dict['link_scholar'] = link_scholar\n",
    "    \n",
    "    # add icon with link to Scopus (if available)\n",
    "    if pub_info['qualisLabels'].get('linkScopus'):\n",
    "        annotation_dict['link_scopus'] = pub_info['qualisLabels']['linkScopus']\n",
    "    \n",
    "    return annotation_dict\n",
    "\n",
    "def persist_annotation_div():\n",
    "    alert_div = BeautifulSoup('<div>', 'html.parser')\n",
    "    \n",
    "    alert_div['class'] = 'main-content max-width min-width'\n",
    "    alert_div['id'] = 'annot-div'\n",
    "    \n",
    "    print('Alert div persisted!')\n",
    "    \n",
    "    return str(alert_div)\n",
    "\n",
    "\n",
    "def persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo):\n",
    "    annot_dict = {}\n",
    "    \n",
    "    issnLabel = f\", ISSN {pubInfo['issn']}\" if pubInfo['issn'] else pubInfo['issn']\n",
    "    if pubInfo['qualisLabels']['qualis'] == 'N':\n",
    "        qualisAnnot = f\"Não classificado{issnLabel}\"\n",
    "    else:\n",
    "        qualisAnnot = f\"{pubInfo['qualisLabels']['qualis']}{issnLabel}\"\n",
    "        \n",
    "        dataSourceLabel = dataSourceInfo[pubInfo['qualisLabels']['source']]['label']\n",
    "        baseYear = dataSourceInfo[pubInfo['qualisLabels']['source']]['baseYear']\n",
    "        dataSourceLabel += f\" ({baseYear})\"\n",
    "        qualisAnnot += f\", fonte {dataSourceLabel}\"\n",
    "        \n",
    "    annot_dict['qualisAnnot'] = qualisAnnot\n",
    "    \n",
    "    titleParam = f\"intitle:\\\"{pubInfo['title']}\\\"\"\n",
    "    linkScholar = f\"https://scholar.google.com/scholar?q={titleParam}\"\n",
    "    annot_dict['linkScholar'] = linkScholar\n",
    "    \n",
    "    if pubInfo['qualisLabels'].get('linkScopus'):\n",
    "        annot_dict['linkScopus'] = pubInfo['qualisLabels']['linkScopus']\n",
    "    \n",
    "    elem['annotation'] = annot_dict\n",
    "\n",
    "def persist_annotation_div(imagesURLs, visualizationURL):\n",
    "    alert_div_dict = {}\n",
    "    alert_div_dict['id'] = \"annot-div\"\n",
    "    return alert_div_dict\n",
    "\n",
    "def persist_annotation_message(imagesURLs, visualizationURL, recentUpdatesURL, pubCount):\n",
    "    annot_dict = {}\n",
    "    if pubCount > 0:\n",
    "        sChar = 's' if pubCount > 1 else ''\n",
    "        pubCountString = f\"anotou o Qualis de {pubCount} artigo{sChar} em periódico{sChar}  neste CV.\"\n",
    "    else:\n",
    "        pubCountString = \"não anotou nenhum artigo em periódico neste CV.\"\n",
    "        \n",
    "    annot_dict['pubCountString'] = pubCountString\n",
    "    annot_dict['visualizationURL'] = visualizationURL\n",
    "    annot_dict['recentUpdatesURL'] = recentUpdatesURL\n",
    "    return annot_dict\n",
    "\n",
    "def set_attributes(elem, attrs):\n",
    "    for key, value in attrs.items():\n",
    "        elem[key] = value\n",
    "\n",
    "def consolidate_qualis_data(qualisInfo):\n",
    "    pubData = []\n",
    "    pubDataYear = []\n",
    "    currYear = 0\n",
    "    \n",
    "    for qInfo in qualisInfo:\n",
    "        if currYear != qInfo['year']:\n",
    "            if currYear > 0:\n",
    "                pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "                pubDataYear = []\n",
    "            currYear = qInfo['year']\n",
    "        \n",
    "        pubDataItem = {\n",
    "            'issn': qInfo['issn'],\n",
    "            'title': qInfo['title'],\n",
    "            'pubName': qInfo['pubName'],\n",
    "            'qualis': qInfo['qualisLabels']['qualis'],\n",
    "            'baseYear': qInfo['qualisLabels']['baseYear'],\n",
    "            'jcr': qInfo['jcrData']['jcr'] if 'jcr' in qInfo['jcrData'] else 0,\n",
    "            'jcrYear': qInfo['jcrData']['baseYear'] if 'baseYear' in qInfo['jcrData'] else ''\n",
    "        }\n",
    "        pubDataYear.append(pubDataItem)\n",
    "    \n",
    "    if len(qualisInfo) > len(pubData):\n",
    "        pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "        \n",
    "    return pubData\n",
    "\n",
    "# Suponhamos que 'html_content' seja o conteúdo HTML em que as anotações serão inseridas.\n",
    "# html_content = ...\n",
    "\n",
    "# Criamos um objeto BeautifulSoup\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Aqui você pode utilizar os métodos acima para persistir as informações.\n",
    "# Exemplo:\n",
    "# elem = {}\n",
    "# persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML annotation and CV ID\n",
    "annot_html = \"<a href='some_url'>Annotated Data</a>\"\n",
    "cv_id = \"cv_123\"\n",
    "\n",
    "# Store annotation\n",
    "store_annotation_in_db(uri, user, password, annot_html, cv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def process_lattes_page_v1(name_link, images_urls, recent_updates_url, qualis_data, data_source_info): \n",
    "#     print(qualis_data)\n",
    "    \n",
    "#     # Inicializando o WebDriver\n",
    "#     driver_service = Service('path/to/chromedriver')\n",
    "#     driver = webdriver.Chrome(service=driver_service)\n",
    "#     driver.get(name_link['link'])\n",
    "    \n",
    "#     # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "#     visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "#     # Limpar cache de dados de Qualis\n",
    "#     qualis_data_cache = {}\n",
    "    \n",
    "#     # Annotate Lattes page com informações de Qualis\n",
    "#     lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "#     if lattes_info:\n",
    "#         await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "#     # Consolidar dados de publicação a partir das informações de Lattes\n",
    "#     pub_info = consolidate_qualis_data(lattes_info)\n",
    "#     print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "#     # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "#     conn = sqlite3.connect('lattes_data.db')\n",
    "#     cursor = conn.cursor()\n",
    "#     cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "#     existing_data = cursor.fetchall()\n",
    "    \n",
    "#     lattes_data_array = []\n",
    "    \n",
    "#     if existing_data:\n",
    "#         # Filtrar dados existentes para evitar duplicatas\n",
    "#         lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "#     # Adicionar dados de Lattes atuais ao array\n",
    "#     lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "#     # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "#     cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "    \n",
    "#     print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def annotate_published_articles_v2(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup):\n",
    "#     # Localizar o primeiro elemento de artigo publicado\n",
    "#     start_elem = soup.find(\"div\", id=\"artigos-completos\")\n",
    "\n",
    "#     # Retornar uma lista vazia se não houver nenhum artigo publicado no CV\n",
    "#     if start_elem is None:\n",
    "#         return []\n",
    "\n",
    "#     qualis_info = []\n",
    "\n",
    "#     # Encontrar todos os artigos publicados\n",
    "#     pub_elems = start_elem.find_all(\"div\", class_=\"artigo-completo\")\n",
    "\n",
    "#     for pub_elem in pub_elems:\n",
    "#         qualis_pub_info = {\n",
    "#             'year': None,\n",
    "#             'issn': '',\n",
    "#             'title': '',\n",
    "#             'pubName': '',\n",
    "#             'qualisLabels': '',\n",
    "#             'jcrData': {}\n",
    "#         }\n",
    "#         # Obter o ano de publicação\n",
    "#         year_span = pub_elem.find(\"span\", class_=\"informacao-artigo\", attrs={\"data-tipo-ordenacao\": \"ano\"})\n",
    "#         if year_span:\n",
    "#             qualis_pub_info['year'] = int(year_span.text)\n",
    "        \n",
    "#         # Obter dados de publicação\n",
    "#         pub_elem_data = pub_elem.find(\"div\", attrs={\"cvuri\": True})\n",
    "\n",
    "#         if pub_elem_data:\n",
    "#             # Obter informações do periódico\n",
    "#             pub_info_string = pub_elem_data['cvuri']\n",
    "#             # Para fins de simplicidade, omitimos a função escapeHtml já que não é relevante para o BeautifulSoup\n",
    "\n",
    "#             # Obter ISSN, título e nome do periódico\n",
    "#             # Detalhes de implementação podem variar, pois o exemplo original usa JavaScript para manipular atributos DOM\n",
    "#             issn = ''  # Implemente a lógica para extrair o ISSN\n",
    "#             title = ''  # Implemente a lógica para extrair o título\n",
    "#             pub_name = ''  # Implemente a lógica para extrair o nome do periódico\n",
    "            \n",
    "#             qualis_pub_info['issn'] = issn\n",
    "#             qualis_pub_info['title'] = title\n",
    "#             qualis_pub_info['pubName'] = pub_name.upper()\n",
    "\n",
    "#             # Obter classificação Qualis do periódico\n",
    "#             qualis_labels = await get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info)\n",
    "#             qualis_pub_info['qualisLabels'] = qualis_labels\n",
    "\n",
    "#             # Obter dados JCR (omitido neste exemplo; pode ser implementado conforme a necessidade)\n",
    "            \n",
    "#             # Adicionar informações ao vetor qualis_info\n",
    "#             qualis_info.append(qualis_pub_info)\n",
    "            \n",
    "#     return qualis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def persist_to_neo4j(header_data):\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "#     header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "#     graph.create(header_node)\n",
    "#     return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERSÃO 01 de extrair dados\n",
    "# def extrair_dados(soup, verbose=False):\n",
    "#     nome_no = soup.select_one('div.infpessoa h2.nome').text if soup.select_one('div.infpessoa h2.nome') else None\n",
    "    \n",
    "#     if not nome_no:\n",
    "#         logging.error(\"Nome do nó não encontrado. Abortando.\")\n",
    "#         return\n",
    "    \n",
    "#     dados_json = {nome_no: {}}\n",
    "#     celula_principal = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "#     title_wrappers = celula_principal.select('div.title-wrapper')\n",
    "#     if verbose:\n",
    "#         logging.info(f'{len(title_wrappers)} seções de dados lidas com sucesso.')\n",
    "    \n",
    "#     for title_wrapper in title_wrappers:\n",
    "#         nome_secao = extrair_secao(title_wrapper)\n",
    "        \n",
    "#         if nome_secao is None:\n",
    "#             continue\n",
    "        \n",
    "#         if verbose:\n",
    "#             logging.info(f'Marcador de Seção: \"{nome_secao.text}\"')\n",
    "\n",
    "#         titulo = extrair_titulo(title_wrapper)\n",
    "#         if verbose:\n",
    "#             logging.info(f'Marcador de Título: \"{titulo}\"')\n",
    "                    \n",
    "#         chave = nome_secao.text\n",
    "#         dados_json[nome_no][chave] = {}\n",
    "\n",
    "#         # A seleção agora ocorre dentro do contexto de title_wrapper, e não de celula_principal.\n",
    "#         celulas_layout = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "        \n",
    "#         for celula_layout in celulas_layout:\n",
    "#             if celula_layout.find_all('div'):\n",
    "#                 indice, valores = extrair_indices(celula_layout)\n",
    "                \n",
    "#                 if indice and valores:\n",
    "#                     dados_json[nome_no][chave][indice] = valores\n",
    "\n",
    "#     if verbose:\n",
    "#         logging.info(f\"Total de índices extraídos: {len(dados_json[nome_no].keys())}\")\n",
    "#         # Outros blocos de código para depuração e verbosidade\n",
    "#     return dados_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ainda sem separador de seção\n",
    "# from bs4.element import Tag\n",
    "\n",
    "# def traverse(soup, parent_dict, current_section=None, root_properties=None):\n",
    "#     \"\"\"\n",
    "#     Traverses through the soup object recursively and populates the dictionary.\n",
    "#     root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "#     current_section: the current section being processed, allows to set subsections.\n",
    "#     \"\"\"\n",
    "#     section = None  \n",
    "#     node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "#     node_name = node_name_element.text if node_name_element else None\n",
    "#     if node_name is not None:\n",
    "#         parent_dict['name'] = node_name\n",
    "\n",
    "#     parag_elements = soup.find_all('p')\n",
    "#     for elem in parag_elements:\n",
    "#         class_name = elem.get('class', [None])[0]\n",
    "#         text_content = elem.get_text()\n",
    "#         if class_name:\n",
    "#             parent_dict[class_name] = text_content\n",
    "\n",
    "#     if root_properties is None:\n",
    "#         root_properties = parent_dict.setdefault('Properties', {})\n",
    "\n",
    "#     for child in soup.children:\n",
    "#         if isinstance(child, Tag):\n",
    "\n",
    "#             if child.get('id') == 'artigos-completos':\n",
    "#                 section_elem = child.findChild(\"div\", class_=\"inst_back\")\n",
    "#                 if section_elem:\n",
    "#                     section = section_elem.get_text().strip()\n",
    "#                     print(section)\n",
    "#                 subsection_elem = child.findChild(\"div\", class_=\"cita-artigos\")\n",
    "#                 if subsection_elem:\n",
    "#                     subsection = subsection_elem.get_text().strip()\n",
    "#                 jcr_articles_dict = parse_jcr_articles(soup)\n",
    "#                 if jcr_articles_dict:\n",
    "#                     root_properties.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "#             elif \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "\n",
    "#                     sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "#                     if sibling_cell:\n",
    "#                         cell_values = extract_data_from_cell(sibling_cell)\n",
    "\n",
    "#                         if current_section:\n",
    "#                             root_properties[current_section][cell_key] = cell_values\n",
    "#                         else:\n",
    "#                             root_properties[cell_key] = cell_values\n",
    "\n",
    "#             elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "#                     subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "#                     if subsection_elem:\n",
    "#                         subsection = subsection_elem.get_text().strip()\n",
    "\n",
    "#                         current_section_dict = root_properties.get(current_section, {})\n",
    "#                         # Convert to dictionary if it is a list\n",
    "#                         if isinstance(current_section_dict, list):\n",
    "#                             current_section_dict = {i: item for i, item in enumerate(current_section_dict)}\n",
    "#                             root_properties[current_section] = current_section_dict\n",
    "#                         subsection_dict = current_section_dict.setdefault(subsection, {})\n",
    "                        \n",
    "#                         sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "#                         if sibling_cell:\n",
    "#                             cell_values = extract_data_from_cell(sibling_cell)\n",
    "#                             subsection_dict[cell_key] = cell_values\n",
    "\n",
    "#             title = child.find(\"h1\")\n",
    "#             if title:\n",
    "#                 current_section = title.get_text().strip()\n",
    "#                 new_dict = {}\n",
    "#                 root_properties[current_section] = new_dict\n",
    "#                 traverse(child, new_dict, current_section, root_properties)\n",
    "#             else:\n",
    "#                 traverse(child, parent_dict, current_section, root_properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
