{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><center><img src=\"https://raw.githubusercontent.com/makaires77/fioce/master/assets/logo_fioce.png\" \n",
    "style=\"height:150px\" alt=\"Logo_Unifor\"></center></center>\n",
    "\n",
    "## <center>Explorar dados dos currículos Lattes para<br /> propor modelo de Grafo para análises futuras </center>\n",
    "\n",
    "    Antonio Marcos Aires Barbosa – Fiocruz Ceará\n",
    "\n",
    "**Introdução**\n",
    "\n",
    "A análise de Grafos permite obter insights como produtos de análises em contextos da realidade com base em modelos capazes de lidar dados heterogêneos e relações complexas.\n",
    "\n",
    "\n",
    "Neste trabalho propomos uma análise dos dados de pesquisa acadêmica tendo como fonte de dados os currículo Lattes de servidores da unidade Fiocruz Ceará.\n",
    "\n",
    "**Objetivo geral:**\n",
    "\n",
    "    Explorar dados dos currículos de servidores da Fiocruz Ceará.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "\n",
    "    1. Extrair dados dos currículos;\n",
    "    2. Propor modelo de grafo para análises futuras;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 0: Preparar e Testar Ambiente</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "        \n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_amb():\n",
    "    ## Visualizar versões dos principais componentes\n",
    "    import os\n",
    "    import pip\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # !pip3 install shutup\n",
    "    # import shutup; shutup.please()\n",
    "    \n",
    "    pyVer      = sys.version\n",
    "    pipVer     = pip.__version__\n",
    "    \n",
    "    print('\\nVERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT')\n",
    "    print('Interpretador em uso:', sys.executable)\n",
    "    print('    Ambiente ativado:',os.environ['CONDA_DEFAULT_ENV'])\n",
    "    print('     Python: '+pyVer, '\\n        Pip:', pipVer,'\\n'\n",
    "         )\n",
    "\n",
    "    !nvcc -V\n",
    "\n",
    "def try_gpu():\n",
    "    print('\\nVERSÕES DO PYTORCH E GPU DISPONÍVEIS')\n",
    "    try:\n",
    "        import torch\n",
    "        print('    PyTorch:',torch.__version__)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Dispositivo:',device)\n",
    "        print('Disponível :',device,torch.cuda.is_available(),' | Inicializado:',torch.cuda.is_initialized(),'| Capacidade:',torch.cuda.get_device_capability(device=None))\n",
    "        print('Nome GPU   :',torch.cuda.get_device_name(0),'         | Quantidade:',torch.cuda.device_count(),'\\n')\n",
    "    except Exception as e:\n",
    "        print('Erro ao configurar a GPU:',e,'\\n')\n",
    "\n",
    "def try_folders(drives,pastas,pastasraiz):\n",
    "    import os\n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    caminho_testado = drive+i+j\n",
    "                    if os.path.isfile(caminho_testado+'/chromedriver/chromedriver.exe'):\n",
    "                        print(f\"Listando arqivos em: {caminho_testado}\")\n",
    "                        print(os.listdir(caminho_testado))\n",
    "                        caminho = caminho_testado+'/'\n",
    "                except:\n",
    "                    caminho=''\n",
    "                    print('Não foi possível encontrar uma pasta de trabalho')\n",
    "    return caminho\n",
    "\n",
    "def try_browser(raiz):\n",
    "    print('\\nVERSÕES DO BROWSER E DO CHROMEDRIVER INSTALADAS')\n",
    "    from selenium import webdriver\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    try:\n",
    "        # Caminho para o seu chromedriver\n",
    "        driver_path=raiz+'chromedriver/chromedriver.exe'\n",
    "        print(driver_path)\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        str1 = driver.capabilities['browserVersion']\n",
    "        str2 = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]\n",
    "        print(f'     Versão do browser: {str1}')\n",
    "        print(f'Versão do chromedriver: {str2}')\n",
    "        driver.quit()\n",
    "\n",
    "        if str1[0:3] != str2[0:3]: \n",
    "            print(\"Versões incompatíveis, atualizar chromedriver!\")\n",
    "            print('  Baixar versão atualizada do Chromedriver em:')\n",
    "            print('  https://googlechromelabs.github.io/chrome-for-testing/#stable')\n",
    "            print('     Ex. Versão 116 PARA WINDOWS:')\n",
    "            print('\t    https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/win64/chromedriver-win64.zip')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def try_chromedriver(caminho):\n",
    "    try:\n",
    "        import os\n",
    "        os.listdir(caminho)\n",
    "    except Exception as e:\n",
    "        raiz=caminho\n",
    "\n",
    "    finally:\n",
    "        print(raiz)\n",
    "    return raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Erro ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "\n",
    "    caminho = drive+':/'+usuario+pastaraiz+'/'\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    import os\n",
    "\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastaraiz = 'fioce'\n",
    "caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "try_amb()\n",
    "try_gpu()\n",
    "try_browser(caminho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 1: Extrair DOM para objeto Soup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Importar, conectar e gerar driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "# !pip install py2neo --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from flask import render_template_string\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "delay = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Função 1: Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    \n",
    "\n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "    driver_path=caminho+'chromedriver/chromedriver.exe'\n",
    "    print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)    \n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "\n",
    "    # return browser, url, session_id\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Manipular HTML para chegar ao currículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_name(driver, delay, NOME):\n",
    "    '''\n",
    "    Função 2: passa o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def achar_busca(driver, delay):\n",
    "    '''\n",
    "    Função 3: clica no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = driver.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               #expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               #logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "def open_curriculum(driver,elm_vinculo):\n",
    "    link_nome     = achar_busca(driver, delay)\n",
    "    window_before = driver.current_window_handle\n",
    "\n",
    "    limite = 5\n",
    "    if str(elm_vinculo) == 'nan':\n",
    "        print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "        raise Exception\n",
    "    print('Vínculo encontrado no currículo de nome:',elm_vinculo.text)\n",
    "\n",
    "    ## Clicar no botão abrir currículo e mudar de aba\n",
    "    try:\n",
    "        ## Aguarda, encontra, clica em buscar nome\n",
    "        link_nome    = achar_busca(driver, delay)\n",
    "    except Exception as e:\n",
    "        print('Erro')\n",
    "        print(e)\n",
    "        \n",
    "    if link_nome.text == None:\n",
    "        xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "        # 'Stale file handle'\n",
    "        print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "        #    expected_ex_type=ZeroDivisionError, \n",
    "        wait_ms=200,\n",
    "        limit=limite, \n",
    "        #    logger=logger, \n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "    try:\n",
    "        ActionChains(driver).click(link_nome).perform()\n",
    "    except:\n",
    "        print(f'Currículo não encontrado.')\n",
    "\n",
    "    retry(WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "        #    expected_ex_type=ZeroDivisionError, \n",
    "        wait_ms=200,\n",
    "        limit=limite, \n",
    "        #    logger=logger, \n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "    # Clicar botão para abrir o currículo\n",
    "    btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "    ## Gerenciamento das janelas abertas no browser\n",
    "    WebDriverWait(driver, delay).until(EC.number_of_windows_to_be(2))\n",
    "    window_after = driver.window_handles\n",
    "    new_window   = [x for x in window_after if x != window_before][0]\n",
    "    driver.switch_to.window(new_window)\n",
    "    # time.sleep(0.2)\n",
    "\n",
    "    # Variável para rastrear se já passamos pelos elementos que vêm após o marcador\n",
    "    passou_artigos_completos = False\n",
    "\n",
    "    # Aguarde até que o elemento <div id=\"artigos-completos\"> seja encontrado\n",
    "    artigos_completos_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, 'artigos-completos'))\n",
    "    )\n",
    "    print('Extraindo dados das publicações...')\n",
    "    passou_artigos_completos = True\n",
    "\n",
    "    # Encontre todos os elementos sup com a classe ajaxJCR\n",
    "    sup_tags = driver.find_elements(By.CLASS_NAME, 'ajaxJCR jcrTip')\n",
    "\n",
    "    for sup_tag in sup_tags:\n",
    "        try:\n",
    "            # Verifique se o marcador foi encontrado\n",
    "            if not passou_artigos_completos:\n",
    "                # Verifique se o elemento atual contém o texto desejado\n",
    "                if \"Artigos completos publicados em periódicos\" in sup_tag.text:\n",
    "                    passou_artigos_completos = True\n",
    "                continue  # Continue para o próximo elemento\n",
    "\n",
    "            # Use o Selenium para ativar o tooltip\n",
    "            ActionChains(driver).move_to_element(sup_tag).perform()\n",
    "\n",
    "            # Aguarde até que o tooltip seja exibido (ajuste o tempo limite conforme necessário)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'jcrTip'))\n",
    "            )\n",
    "\n",
    "            # Obtenha o elemento do tooltip\n",
    "            tooltip_element = driver.find_element(By.CLASS_NAME, 'jcrTip')\n",
    "\n",
    "            # Verifique se o elemento do tooltip é visível\n",
    "            if tooltip_element.is_displayed():\n",
    "                # Obtenha o texto do tooltip\n",
    "                tooltip_text = tooltip_element.text\n",
    "\n",
    "                # Adicione o texto do tooltip como um atributo personalizado no elemento sup\n",
    "                driver.execute_script(\"arguments[0].setAttribute('data-tooltip', arguments[1]);\", sup_tag, tooltip_text)\n",
    "\n",
    "                # Aguarde até que o elemento do tooltip se torne obsoleto (stale)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.staleness_of(tooltip_element)\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ativar o tooltip: {e}\")\n",
    "\n",
    "    # Após adicionar os dados dos tooltips, obtenha o conteúdo HTML atualizado\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "    \n",
    "    # Usa BeautifulSoup para analisar\n",
    "    soup = BeautifulSoup(page_source, 'html.parser') \n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_terms(NOME, instituicao, unidade, termo, driver, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    \n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(driver)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            ## TO-DO inserir a crítica para o 'Stale file handle'\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(driver)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(driver)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, driver\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # driver.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, driver\n",
    "    \n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "def is_abreviation(substring):\n",
    "    pattern = r'(?: [a-zA-Z]\\.)|(?: [a-zA-Z] \\.)'\n",
    "    if re.search(pattern, substring):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_surname(substring):\n",
    "    return not is_abreviation(substring) and substring.endswith(', ')\n",
    "\n",
    "def compose_full_name(surname, parts, marker):\n",
    "    full_name = \"\"\n",
    "    for part in parts:\n",
    "        full_name += part + marker\n",
    "    full_name += ' '+surname\n",
    "    return full_name.strip()\n",
    "\n",
    "def split_authors(string, verbose=False):\n",
    "    authors_names = []\n",
    "    \n",
    "    m1a = \"; \"\n",
    "    m1b = \", \"\n",
    "    \n",
    "    if m1a in string and (string.count(m1a) <= string.count(m1b) or m1b not in string):\n",
    "        marker = m1a\n",
    "        authors_names = string.split(marker)\n",
    "        return [x.strip() for x in authors_names]\n",
    "    else:\n",
    "        marker = m1b\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Selected marker: \"{marker}\"')\n",
    "\n",
    "    parts_list = string.split(marker)\n",
    "    name    = \"\"\n",
    "    surname = \"\"\n",
    "    \n",
    "    for part in parts_list:\n",
    "        if is_abreviation(part):\n",
    "            classification = 'Abrev'\n",
    "            name += part + marker\n",
    "        else:\n",
    "            classification = 'Name'\n",
    "            if is_surname(part) or surname == \"\":\n",
    "                classification = 'SOBRENOME'\n",
    "                if surname:\n",
    "                    full_name = compose_full_name(surname.strip(), name.split(marker), ', ')\n",
    "                    if full_name not in authors_names:\n",
    "                        authors_names.append(full_name)\n",
    "                surname = part\n",
    "                name = part + marker\n",
    "            else:\n",
    "                name = part + marker\n",
    "        if verbose:\n",
    "            # print(f'Quantidade de m1a: {string.count(m1a)} \\nQuantidade de m1b: {string.count(m1b)}')\n",
    "            print(f'Autor: {part:40} | Forma: {classification}')\n",
    "\n",
    "        authors_names.append(part.strip(marker).strip())\n",
    "     \n",
    "    return [x.strip() for x in authors_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extrair_indices(layout_celula):\n",
    "    indice = layout_celula.select_one('div.layout-cell-pad-5.text-align-right')\n",
    "    valor = layout_celula.select_one('div.layout-cell.layout-cell-9 div.layout-cell-pad-5')\n",
    "    \n",
    "    if indice and valor:\n",
    "        valores_extraidos = valor.text.split('<br class=\"clear\">') if '<br class=\"clear\">' in valor.text else valor.text.split('\\n\\t\\t\\n\\t')\n",
    "        return indice.text, valores_extraidos\n",
    "    return None, None\n",
    "\n",
    "def extrair_secao(title_wrapper):\n",
    "    tags_relevantes = ['h1', 'ul', 'b']\n",
    "    tags_encontradas = []\n",
    "    for tag in tags_relevantes:\n",
    "        elemento_encontrado = title_wrapper.select_one(tag)\n",
    "        if elemento_encontrado:\n",
    "            tags_encontradas.append((tag, elemento_encontrado))\n",
    "    tags_ordenadas = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def encontrar_subchave(title_wrapper):\n",
    "    tags_relevantes  = ['ul', 'a', 'b']\n",
    "    tags_encontradas = [(tag, title_wrapper.find(tag)) for tag in tags_relevantes]\n",
    "    tags_ordenadas   = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def extrair_titulo(title_wrapper):\n",
    "    inst_back_texts = {}\n",
    "    inst_back_elements = title_wrapper.find_all('div', class_='inst_back')\n",
    "    for index, inst_back in enumerate(inst_back_elements):\n",
    "        b_tag = inst_back.find('b')\n",
    "        if b_tag:\n",
    "            inst_back_texts[index] = b_tag.text.strip()\n",
    "    return inst_back_texts if inst_back_texts else None\n",
    "\n",
    "def extrair_wraper(soup, json_data):\n",
    "    title_wrappers = soup.select('div.layout-cell-pad-main div.title-wrapper')\n",
    "    for title_wrapper in title_wrappers:\n",
    "        section_name = extrair_secao(title_wrapper)\n",
    "        if section_name:\n",
    "            section_name = section_name.text.strip()\n",
    "            \n",
    "            titles = extrair_titulo(title_wrapper)\n",
    "            json_data[\"Properties\"][section_name] = {}\n",
    "            \n",
    "            if titles:\n",
    "                for index, title in titles.items():\n",
    "                    json_data[\"Properties\"][section_name][title] = {}\n",
    "            \n",
    "            layout_cells = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "            for layout_celula in layout_cells:\n",
    "                indice, valores_extraidos = extrair_indices(layout_celula)\n",
    "                if indice and valores_extraidos:\n",
    "                    if titles and indice in titles.values():\n",
    "                        if len(titles) > 1:\n",
    "                            for title in titles.values():\n",
    "                                if title.strip() in indice:\n",
    "                                    json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                        else:\n",
    "                            title = list(titles.values())[0]\n",
    "                            json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                    else:\n",
    "                        json_data[\"Properties\"][section_name][indice] = valores_extraidos\n",
    "    return json_data\n",
    "\n",
    "def imprimir_informacoes(dados_json, nome_no, indent=0):\n",
    "    indentation = '    ' * indent  # Calculating the current indentation level\n",
    "\n",
    "    if dados_json and nome_no and dados_json.get(nome_no):\n",
    "        if indent == 0:  # Logging node-level information only at the root\n",
    "            logging.info(f\"{indentation}Node: {nome_no}\")\n",
    "            logging.info(f\"{indentation}Total keys extracted: {len(dados_json[nome_no].keys())}\")\n",
    "        \n",
    "        for key in dados_json[nome_no].keys():\n",
    "            logging.info(f\"{indentation}{key.strip() if key else ''}\")\n",
    "\n",
    "            if isinstance(dados_json[nome_no][key], dict):  # Check for nested dictionaries\n",
    "                # Recursive call to handle nested dictionaries\n",
    "                imprimir_informacoes(dados_json[nome_no], key, indent + 1)\n",
    "            else:\n",
    "                logging.info(f\"{indentation}    Values: {dados_json[nome_no][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função montar dicionário do DOM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para Titulos do grupo 02 a estrutura é diferente:\n",
    "\n",
    "dentro de class=title-wraper há uma nova div class=layout-cell-12 data-cell que contém como filhos uma div class=inst_back, e dentro dela uma tag 'b' (seção),\n",
    "seguindo ainda em siblings (filhos de div title-wrapper) há várias div class=cita-artigos (subseção) e \n",
    "cada div class=cita-artigos é seguida (em siblings) de vários pares de layout-cell-1 e layout-cell-11 (dados de interesse), há uma div id=artigos-completos...\n",
    "como filhos da div id=artigos-completos há uma div class=cita-artigos com nome da subseção seguida (em siblings) de várias divs class=artigo-completo\n",
    "só dentro de cada div class=artigo-completo é que há um ou vários pares de layout-cell-1 e layout-cell-11 com os dados de interesse\n",
    "dentro da div class=layout-cell-1 há os dados para chave do dicionário, dentro de uma tag 'b' que está dentro de uma div layout-cell-pad-5 text-align-right\n",
    "dentro da div class=layout-cell-11 há os dados para valores do dicionário, dentro de uma tag 'sup' que está dentro de uma div layout-cell-pad-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_normal(soup, verbose=False):\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "\n",
    "    info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "    name = info_list[0]\n",
    "\n",
    "    data_dict = {\"labels\": \"Person\",\"name\": name,\"InfPes\": [], \"Resumo\": []}\n",
    "    data_dict['InfPes'] = info_list\n",
    "    summary_text  = elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()\n",
    "    data_dict['Resumo'].append(summary_text)\n",
    "\n",
    "    tit1 = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar','Atuação Profissional','Linhas de pesquisa',\n",
    "            'Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento','Revisor de periódico','Revisor de projeto de fomento',\n",
    "            'Áreas de atuação','Idiomas','Inovação']\n",
    "    tit2 = ['Produções','Bancas','Orientações']\n",
    "    # Não extraindo ainda, semelhante ao tit2 mas não há subseções dados direto nas seções \"inst_back\"\n",
    "    tit3 = ['Eventos']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        data_cells_12 = div_title_wrapper.findChildren('div', class_='data-cell')\n",
    "\n",
    "        # print(len(data_cells_12), 'conteiners de dados')\n",
    "        titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        if titulo not in data_dict:\n",
    "            if titulo == '':\n",
    "                pass\n",
    "            else:\n",
    "                data_dict[titulo] = {}\n",
    "        if verbose:\n",
    "            print(f'{titulo}')\n",
    "\n",
    "        for data_cell in data_cells_12:\n",
    "            # as divs data_cells_12 englobam div class=infopessoa e várias divs class=title-wrapper\n",
    "\n",
    "            # Formações e demais seções que não tem subseções\n",
    "            if titulo in tit1:\n",
    "                ## Listagem das divisões e subdivisões\n",
    "                divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "                # print(len(divs_inst_back))\n",
    "                \n",
    "                for div_inst_back_cell in divs_inst_back:                                        \n",
    "                    inst_back = div_inst_back_cell.findChild('b')\n",
    "                    if inst_back:\n",
    "                        instback_text = inst_back.get_text().strip()\n",
    "                        data_dict[titulo][instback_text] = []\n",
    "                        \n",
    "                        if verbose:                            \n",
    "                            print(f'  {instback_text}') # nomes de seção com tipos de produção\n",
    "\n",
    "                    divs_subsection = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "                    for subsection in divs_subsection:\n",
    "                        subsection_text = subsection.get_text().strip()\n",
    "                        if subsection_text:\n",
    "                            data_dict[titulo][instback_text][subsection_text] = []\n",
    "                            if verbose:\n",
    "                                print(f'      {subsection_text}')   \n",
    "\n",
    "                elementos_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                elementos_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                keys=[]\n",
    "                vals=[]\n",
    "                for i,j in zip(elementos_layout_cell_3, elementos_layout_cell_9):\n",
    "                    if elementos_layout_cell_3 and elementos_layout_cell_9:\n",
    "                        key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                        key_text = key.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "                        keys.append(key_text)\n",
    "                        val = j.find('div', class_='layout-cell-pad-5')\n",
    "                        val_text = val.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "                        vals.append(val_text)\n",
    "                        if verbose:\n",
    "                            print(f'      {key_text:>3}: {val_text}') # impressão dos dados chave e valor\n",
    "                \n",
    "                agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                if titulo not in data_dict:\n",
    "                    data_dict[titulo] = {}\n",
    "                data_dict[titulo].update(agg_dict)                \n",
    "\n",
    "            # Produções, Orientações e Bancas (que contém subseções)\n",
    "            elif titulo in tit2:\n",
    "                ## Listagem das divisões e subdivisões\n",
    "                divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "                # print(len(divs_inst_back))\n",
    "                \n",
    "                for div_inst_back_cell in divs_inst_back:                                        \n",
    "                    inst_back = div_inst_back_cell.findChild('b')\n",
    "                    if inst_back:\n",
    "                        instback_text = inst_back.get_text().strip()\n",
    "                        if verbose:\n",
    "                            print(f'  {instback_text}') # nomes de seção com tipos de produção\n",
    "\n",
    "                        div_citacoes = div_inst_back_cell.find_next_sibling('div', class_='cita-artigos')\n",
    "                        if div_citacoes:\n",
    "                            cita_artigos_text = div_citacoes.findChild('b').get_text().strip()\n",
    "                            # print(f'    {cita_artigos_text}') # nomes de subseção como ocorrências \n",
    "                            \n",
    "                            if cita_artigos_text == 'Citações':\n",
    "                                ## Extrair dados das citações dos artigos publicados em bases indexadas\n",
    "                                sub_section_list = []\n",
    "                                    \n",
    "                                ## Extrair quantidade de citações e fator H através das divs de subseção que contém a classe lyout-cell-12\n",
    "                                next_siblings = div_citacoes.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "                                sub_section_list = []  # Inicialize a lista de dicionários fora do loop\n",
    "\n",
    "                                for i in next_siblings:\n",
    "                                    citation_counts = i.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que contém os Valores de Citações\n",
    "                                    if citation_counts:\n",
    "                                        for i in citation_counts:\n",
    "                                            database = i.get_text()\n",
    "                                            total_trab = i.find_next_sibling(\"div\", class_=\"trab\").get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                            total_cite = i.find_next_sibling(\"div\", class_=\"cita\").get_text().split(\"Total de citações:\")[1]\n",
    "                                            fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                            num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                            data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\").get_text().split(\"Data:\")[1].strip()\n",
    "\n",
    "                                            # Converta os valores para tipos de dados adequados\n",
    "                                            total_trab = int(total_trab)\n",
    "                                            total_cite = int(total_cite)\n",
    "\n",
    "                                            citation_numbers = {\n",
    "                                                \"Database\": database,\n",
    "                                                \"Total de trabalhos\": total_trab,\n",
    "                                                \"Total de citações\": total_cite,\n",
    "                                                \"Índice_H\": num_fator_h,\n",
    "                                                \"Data\": data_wos\n",
    "                                            }\n",
    "\n",
    "                                            sub_section_list.append(citation_numbers)\n",
    "\n",
    "                                            if verbose:\n",
    "                                                print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "\n",
    "                        # Extração dos Artigos publicados em periódicos\n",
    "                        div_artigos = div_inst_back_cell.find_next_siblings('div', id='artigos-completos')\n",
    "                        for div_artigo in div_artigos:\n",
    "                            if div_artigo:\n",
    "                                divs_cita_artigos = div_artigo.find_all('div', class_='cita-artigos')\n",
    "                                for cita_artigos in divs_cita_artigos:\n",
    "                                    if cita_artigos:\n",
    "                                        cita_artigos_text = cita_artigos.findChild('b').get_text().strip()\n",
    "                                        if verbose:\n",
    "                                            print(f'    {cita_artigos_text}') # nomes de subseção como ocorrências          \n",
    "                                        \n",
    "                                        elementos_layout_cell_1 = div_artigo.find_all('div', class_='layout-cell-1')\n",
    "                                        elementos_layout_cell_11 = div_artigo.find_all('div', class_='layout-cell-11')\n",
    "                                    \n",
    "                                        # extract(instback_text, elementos_layout_cell_1,elementos_layout_cell_11)\n",
    "                                        vals_jcr=[]\n",
    "                                        keys=[]\n",
    "                                        vals=[]\n",
    "                                        for i,j in zip(elementos_layout_cell_1, elementos_layout_cell_11):\n",
    "                                            if elementos_layout_cell_1 and elementos_layout_cell_11:\n",
    "                                                key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                                                key_text = key.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "                                                val = j.find('div', class_='layout-cell-pad-5')\n",
    "                                                info_dict = {\n",
    "                                                    'data-issn': 'NULL',\n",
    "                                                    'impact-factor': 'NULL',  \n",
    "                                                    'jcr-year': 'NULL',\n",
    "                                                    'journal': 'NULL',\n",
    "                                                    'raw_data': 'NULL',\n",
    "                                                }\n",
    "                                                # Remova as tags span da div\n",
    "                                                for span in val.find_all('span'):\n",
    "                                                    span.extract()\n",
    "                                                \n",
    "                                                val_text = val.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "\n",
    "                                                keys.append(key_text)\n",
    "                                                vals.append(val_text)\n",
    "                                                if verbose:\n",
    "                                                    print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "                                                if cita_artigos_text == 'Artigos completos publicados em periódicos':\n",
    "                                                    sup_element = j.find('sup')\n",
    "                                                    raw_jcr_data = sup_element.get_text()\n",
    "                                                    # print('sup_element:',sup_element)\n",
    "                                                    img_element = sup_element.find('img')\n",
    "                                                    # print('img_element:',img_element)\n",
    "\n",
    "                                                    if sup_element:\n",
    "                                                        if img_element:\n",
    "                                                            original_title = img_element.get('original-title')\n",
    "                                                            raw_jcr_data = img_element.get_text()\n",
    "                                                            if original_title:\n",
    "                                                                info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                                \n",
    "                                                                if info_list != 'NULL':\n",
    "                                                                    info_dict = {\n",
    "                                                                        'data-issn': img_element.get('data-issn'),\n",
    "                                                                        'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                        'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ',''),\n",
    "                                                                        'journal': info_list[0],\n",
    "                                                                        'raw_data': raw_jcr_data,\n",
    "                                                                    }\n",
    "                                                            else:\n",
    "                                                                info_dict = {\n",
    "                                                                    'data-issn': img_element.get('data-issn'),\n",
    "                                                                    'impact-factor': 'NULL',\n",
    "                                                                    'jcr-year': 'NULL',\n",
    "                                                                    'journal': img_element.get('original-title'),\n",
    "                                                                    'raw_data': raw_jcr_data,\n",
    "                                                                }\n",
    "                                                    else:\n",
    "                                                        info_dict = {\n",
    "                                                            'data-issn': 'NULL',\n",
    "                                                            'original-title': 'NULL',\n",
    "                                                            'Fator de impacto': 'NULL',\n",
    "                                                            'ano_apuração': 'NULL',\n",
    "                                                            'raw_data': raw_jcr_data, \n",
    "                                                        }                                                                \n",
    "                                                        \n",
    "                                                    vals_jcr.append(info_dict)\n",
    "                                                    if verbose:\n",
    "                                                        print(f'         {info_dict}')\n",
    "\n",
    "                                        agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                                        if titulo not in data_dict:\n",
    "                                            data_dict[titulo] = {}\n",
    "                                        data_dict[titulo].update(agg_dict)\n",
    "                                        \n",
    "                                        if 'JCR' not in data_dict:\n",
    "                                            data_dict['JCR'] = []\n",
    "                                        data_dict['JCR'].extend(vals_jcr)\n",
    "\n",
    "                    extracted = ['Citações', 'Artigos completos publicados em periódicos']\n",
    "                    divs = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "                    # print('subsections', len(divs))\n",
    "                    for div_publicacoes in divs:\n",
    "                        tags_b = div_publicacoes.find('b')\n",
    "                        if tags_b:\n",
    "                            current_section = div_publicacoes.find_previous_sibling('div', class_='inst_back')\n",
    "                            current_section_text = current_section.find('b').get_text()\n",
    "                            # print(current_section_text)\n",
    "                            if current_section_text not in data_dict[titulo]:\n",
    "                                data_dict[titulo][current_section_text] = {}  # Inicializa dicionário vazio se não existir para seções\n",
    "                            current_subection_text = tags_b.get_text().strip() if tags_b.get_text().strip() else None\n",
    "                            if current_subection_text not in data_dict[titulo][current_section_text]:\n",
    "                                data_dict[titulo][current_section_text][current_subection_text] = {}  # Inicializa dicionário vazio se não existir para subseções\n",
    "                            \n",
    "                            if current_subection_text not in extracted:\n",
    "                                if verbose:\n",
    "                                    print(f'    {current_subection_text}')\n",
    "                                elementos_layout_cell_1 = div_publicacoes.find_next_siblings('div', class_='layout-cell-1')\n",
    "                                elementos_layout_cell_11 = div_publicacoes.find_next_siblings('div', class_='layout-cell-11')\n",
    "                                \n",
    "                                keys=[]\n",
    "                                vals=[]\n",
    "                                for i, j in zip(elementos_layout_cell_1, elementos_layout_cell_11):                                            \n",
    "                                    key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                                    key_text = key.get_text().strip() if key else \"\"\n",
    "                                    val = j.find('div', class_='layout-cell-pad-5')\n",
    "                                    val_text = val.get_text().strip().replace('\\n',' ').replace('\\t','') if val else \"\"\n",
    "                                    keys.append(key_text)\n",
    "                                    vals.append(val_text) \n",
    "                                    if verbose:\n",
    "                                        print(f'      {key_text:>3}: {val_text}')  # dados chave e valor  \n",
    "\n",
    "                                    #condição de parada\n",
    "                                    try:\n",
    "                                        # next_class = j.find_next_sibling().get('class', [])[0]\n",
    "                                        # print(f'      {next_class}')\n",
    "                                        next_next_class = j.find_next_sibling().find_next_sibling().get('class', [])[0]\n",
    "                                        # print(f'      {next_next_class}')\n",
    "                                        if next_next_class:\n",
    "                                            if next_next_class == 'cita-artigos' or next_next_class == 'clear':                                                \n",
    "                                                extracted.append(current_subection_text)\n",
    "                                                break\n",
    "                                    except:\n",
    "                                        break\n",
    "                        \n",
    "                                    agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                                    data_dict[titulo][current_section_text][current_subection_text] = agg_dict\n",
    "                                    data_dict['Produções']['Produção bibliográfica']['Citações'] = sub_section_list\n",
    "\n",
    "            # Eventos (não contém subseções)\n",
    "            elif titulo in tit3:\n",
    "                secoes = ['Participação em eventos, congressos, exposições e feiras','Organização de eventos, congressos, exposições e feiras']\n",
    "                ## Listagem das divisões e subdivisões\n",
    "                divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "                # print(len(divs_inst_back))\n",
    "                          \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_normal(soup, verbose=False):\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "\n",
    "    info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "    name = info_list[0]\n",
    "\n",
    "    data_dict = {\"labels\": \"Person\",\"name\": name,\"InfPes\": [], \"Resumo\": []}\n",
    "    data_dict['InfPes'] = info_list\n",
    "    summary_text  = elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()\n",
    "    data_dict['Resumo'].append(summary_text)\n",
    "\n",
    "    tit1 = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar','Atuação Profissional','Linhas de pesquisa',\n",
    "            'Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento','Revisor de periódico','Revisor de projeto de fomento',\n",
    "            'Áreas de atuação','Idiomas','Inovação']\n",
    "    tit2 = ['Produções','Bancas','Orientações']\n",
    "    tit3 = ['Eventos']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        data_cells_12 = div_title_wrapper.findChildren('div', class_='data-cell')\n",
    "\n",
    "        titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        if titulo not in data_dict:\n",
    "            if titulo == '':\n",
    "                pass\n",
    "            else:\n",
    "                data_dict[titulo] = {}\n",
    "        if verbose:\n",
    "            print(f'{titulo}')\n",
    "\n",
    "        for data_cell in data_cells_12:\n",
    "\n",
    "            # Formações e demais seções que não têm subseções\n",
    "            if titulo in tit1:\n",
    "                divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "                \n",
    "                for div_inst_back_cell in divs_inst_back:                                        \n",
    "                    inst_back = div_inst_back_cell.findChild('b')\n",
    "                    if inst_back:\n",
    "                        instback_text = inst_back.get_text().strip()\n",
    "                        data_dict[titulo][instback_text] = []\n",
    "                        \n",
    "                        if verbose:                            \n",
    "                            print(f'  {instback_text}')\n",
    "\n",
    "                    divs_subsection = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "                    for subsection in divs_subsection:\n",
    "                        subsection_text = subsection.get_text().strip()\n",
    "                        if subsection_text:\n",
    "                            data_dict[titulo][instback_text].append(subsection_text)\n",
    "                            if verbose:\n",
    "                                print(f'      {subsection_text}')\n",
    "\n",
    "                elementos_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                elementos_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                keys=[]\n",
    "                vals=[]\n",
    "                for i,j in zip(elementos_layout_cell_3, elementos_layout_cell_9):\n",
    "                    if elementos_layout_cell_3 and elementos_layout_cell_9:\n",
    "                        key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                        key_text = key.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "                        keys.append(key_text)\n",
    "                        val = j.find('div', class_='layout-cell-pad-5')\n",
    "                        val_text = val.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "                        vals.append(val_text)\n",
    "                        if verbose:\n",
    "                            print(f'      {key_text:>3}: {val_text}')\n",
    "                \n",
    "                agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                if titulo not in data_dict:\n",
    "                    data_dict[titulo] = {}\n",
    "                data_dict[titulo].update(agg_dict)                \n",
    "\n",
    "            # Produções, Orientações e Bancas (que contém subseções)\n",
    "            elif titulo in tit2:\n",
    "                divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "                for div_inst_back_cell in divs_inst_back:\n",
    "                    inst_back = div_inst_back_cell.findChild('b')\n",
    "                    if inst_back:\n",
    "                        instback_text = inst_back.get_text().strip()\n",
    "                        if instback_text not in data_dict[titulo]:\n",
    "                            data_dict[titulo][instback_text] = {}\n",
    "                        if verbose:\n",
    "                            print(f'  {instback_text}')\n",
    "\n",
    "                    divs_cita_artigos = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "                    for div_cita in divs_cita_artigos:\n",
    "                        cita_artigos_text = div_cita.findChild('b').get_text().strip() if div_cita.findChild('b') else None\n",
    "                        if cita_artigos_text:\n",
    "                            data_dict[titulo][instback_text][cita_artigos_text] = []\n",
    "                            if verbose:\n",
    "                                print(f'      {cita_artigos_text}')\n",
    "                            \n",
    "                            # Aqui você pode adicionar a extração de informações específicas para cada subseção.\n",
    "\n",
    "                    elementos_layout_cell_1 = div_inst_back_cell.find_all('div', class_='layout-cell-1')\n",
    "                    elementos_layout_cell_11 = div_inst_back_cell.find_all('div', class_='layout-cell-11')\n",
    "                    keys = []\n",
    "                    vals = []\n",
    "                    for i, j in zip(elementos_layout_cell_1, elementos_layout_cell_11):\n",
    "                        key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                        key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '') if key else None\n",
    "                        val = j.find('div', class_='layout-cell-pad-5')\n",
    "                        val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '') if val else None\n",
    "                        if key_text and val_text:\n",
    "                            keys.append(key_text)\n",
    "                            vals.append(val_text)\n",
    "                            if verbose:\n",
    "                                print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "                    agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                    if instback_text and agg_dict:\n",
    "                        data_dict[titulo][instback_text].update(agg_dict)\n",
    "\n",
    "            # Seção \"Eventos\"\n",
    "            elif titulo in tit3:\n",
    "                # Coleta de dados para a seção \"Eventos\" conforme a lógica original\n",
    "                divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "                for div_inst_back_cell in divs_inst_back:                                        \n",
    "                    inst_back = div_inst_back_cell.findChild('b')\n",
    "                    if inst_back:\n",
    "                        instback_text = inst_back.get_text().strip()\n",
    "                        if verbose:\n",
    "                            print(f'  {instback_text}')\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 2: Exportar DOM para Dicionários</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalization of the DOM extraction from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digital representation of the HTML DOM (Document Object Model) in question follow a consistent class-based structure, where the division of information into various classes within 'div' elements serves as an important taxonomy for organizing and categorizing the information.\n",
    "\n",
    "The nested structure predominantly consists of HTML div elements differentiated by their CSS classes. The div elements appear in a tree-like organization, hierarchically grouped under recursive presence of the div elements within the class 'title-wrapper', followed by the div elements marked with 'layout-cell' and hierarquically organized until reaching the more detailed levels where the data of interest is, contained in the classes such 'data-cell', 'text-align-right' or 'layout-cell-pad-5' and tags like 'a', 'b'.\n",
    "\n",
    "The extraction of data from this intricate nested architecture necessitates a recursive methodology that maintains the hierarchical fidelity of the original data. Thus, one approach to transforming this data into a structured JSON object would be to employ depth-first search (DFS) algorithms to traverse through each node in this tree-like structure. Each traversal would examine the class attributes and potentially the text content within each div. \n",
    "\n",
    "**Formalization:**\n",
    "\n",
    "In the formal language of computational theory, let \\( T \\) be the DOM tree with each node \\( n \\) containing a list of attributes \\( A(n) \\) and a text content \\( C(n) \\). Let \\( JSON(n) \\) be the JSON representation of the node \\( n \\). The recursive function to extract data can be described as:\n",
    "\n",
    "\n",
    "JSON(n) = \n",
    "\\begin{cases} \n",
    "\\{ \"type\": A(n), \"content\": C(n), \"children\": \\{ JSON(c) \\,|\\, c \\in \\text{children of } n \\} \\} & \\text{if } n \\text{ has children} \\\\\n",
    "\\{ \"type\": A(n), \"content\": C(n) \\} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "**Python implementation:**\n",
    " \n",
    "In terms of practical implementation, Python's Beautiful Soup library can be particularly effective for this task, allowing for a relatively straightforward traversal of each div element to construct the JSON object.\n",
    "\n",
    "The end result would be a JSON object where each entry corresponds to a 'div' element in the original HTML structure, represented by a dictionary containing the attributes and content of the div, and potentially another dictionary (or list of dictionaries) representing any nested child div elements. This would effectively capture the data within each div while maintaining the hierarchical structure of the original HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drives=['C:/Users/','E:/','./home/']\n",
    "pastas=['marcos.aires/', 'marco/']\n",
    "pastasraiz=['kgfioce','fioce']\n",
    "caminho=try_folders(drives,pastas,pastasraiz)\n",
    "\n",
    "preparar_pastas(caminho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarqia básica das classes nas divs recorrentes para dados básicos e Publicação de Artigos é:\n",
    "# N00> elem_principal \"layout-cell-pad-main\" | \"p\" (v: resumo)                          [elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")]\n",
    "#   N01> \"infpessoa\"                                                                    [dict_infopessoa = elm_main_cell.find(\"div\", class_=\"infpessoa\")]\n",
    "#   N01> \"title-wrapper\" | \"h1\" (k: Títulos)                                            [for t in elm_main_cell.fid_all(\"div\", class_=\"title-wrapper\")]\n",
    "#       N02> \"layout-cell-12 data-cell\"                                                 [for title in titles: t.find(\"h1\") e i.get_text() para pegar nome de cada Título ] \n",
    "#           N03> \"int_back\" | \"b\" (k: Seções)                                           [USAR FIND_ALL PARA CAPTAR TODAS ENTRADAS RECORRENTES e b.get_text() para pegar Nome de Seção]\n",
    "#               N04> \"layout-cell-12\" >>> \"web_s\" (v: ValoresCitações)  \n",
    "#           N03> \"artigos-completos\"                                                    [i.find(\"div\", class_=\"artigos-completos\")]\n",
    "#               N04> \"cita-artigos\" | \"b\" (k: Subseção)                                 [USAR FIND_ALL PARA CAPTAR TODAS ENTRADAS RECORRENTES e b.get_text() para pegar Nome de Subseção]\n",
    "#               N04> \"artigo-completo\"                                                  [34 Artigos USAR FIND_ALL PARA CAPTAR TODAS ENTRADAS RECORRENTES DE DICIONÁRIO CONFORME ABAIXO]\n",
    "##                   N05> \"layout-cell layout-cell-1 text-align-right\" \n",
    "##                       N06> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do artigo]\n",
    "##                   N05> \"layout-cell layout-cell-11\" \n",
    "#                       N06> \"layout-cell-pad-5\" | \"span\" \"informacao-artigo\" ({k:JCR v:ValoresJCR}, {k:autor v:NomeAutor}, {k:ano v:AnoPub}, {k:doi v:LinkDOI})\n",
    "#                                                | .get_text() \n",
    "#                                                | <sup><... data-issn=\"18685137\" original-title=\"Journal of Ambient Intelligence and Humanized Computing (1868-5137)<br />Fator de impacto (JCR 2021): 3.662\"></sup>\n",
    "#                                                | <div class=\"citado\" cvuri=\"/buscatextual/servletcitacoes?doi=10.1007/s12652-023-04555-3&amp;issn=18685137&amp;volume=&amp;issue=&amp;paginaInicial=1&amp;titulo=Hybrid model for early identification post-Covid-19 sequelae&amp;sequencial=1&amp;nomePeriodico=Journal of Ambient Intelligence and Humanized Computing\" tooltip=\"Citações a partir de 1996\"></div>\n",
    "#\n",
    "# CONTEÚDO PARA EXTRAÇÃO ITERATIVA E RECORRENTE COM FUNÇÃO: extract_pairs()\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [02 Livros publicados/organizados ou edições]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosLivro)           [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [24 Capítulos de Livros]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [74 Trabalhos completos publicados em anais de congressos]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "##           N03> <br class=\"clear\">\n",
    "#         Bancas\n",
    "#          Participação em bancas de trabalhos de conclusão  \n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [22 Mestrado]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [1 Teses de doutorado]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#           N03> \"cita-artigos\" | \"b\" (k: Subseção)                              [17 Trabalhos de conclusão de curso de graduação]\n",
    "##           N03> \"layout-cell layout-cell-1 text-align-right\"\n",
    "##               N04> \"layout-cell-pad-5 text-align-right\" | \"b\" (k: N.Ordem)    [b.get_text() para pegar Número de ordem do Livro]\n",
    "##           N03> \"layout-cell layout-cell-11\"\n",
    "##               N04> \"layout-cell-pad-5\" | \"b\" (k:Autor)                        [b.get_text() para pegar Autor do Livro]\n",
    "##                                        | .get_text() (v:DadosCapítulo)        [.get_text() para pegar Dados do Livro]\n",
    "##           N03> <br class=\"clear\">\n",
    "#          Participação em bancas de comissões julgadoras\n",
    "# EVENTOS \n",
    "#  (33 Participação em eventos, congressos, exposições e feiras)\n",
    "#  (03 Organização de eventos, congressos, exposições e feiras)\n",
    "# ORIENTAÇÕES (22 Dissertações, 05 Teses, 09 TCC)\n",
    "#  Orientações e supervisões em andamento\n",
    "#    (01 Dissertação de Mestrado, 05 Tese de doutorado)\n",
    "# INOVAÇÃO (02 Projetos de Pesquisa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair DOM para Dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = connect_driver(caminho)\n",
    "NOME = ['Raimir Holanda Filho']\n",
    "fill_name(driver, delay, NOME)\n",
    "\n",
    "limite=3\n",
    "instituicao = 'Fundação Oswaldo Cruz'\n",
    "unidade     = 'Fiocruz Ceará'\n",
    "termo       = 'Ministerio da Saude'\n",
    "\n",
    "elm_vinculo, np.NaN, np.NaN, np.NaN, driver = find_terms(NOME, instituicao, unidade, termo, driver, delay, limite)\n",
    "soup = open_curriculum(driver,elm_vinculo)\n",
    "\n",
    "caracteres = len(soup.text)\n",
    "linhas = len(soup.text.split('\\n'))\n",
    "print(f'Total de caracteres extraídos: {caracteres:6d}')\n",
    "print(f'Quantidade extraída de linhas: {linhas:6d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tit1_soup(soup, verbose=False):\n",
    "#     data_dict = {}\n",
    "\n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "#     # Títulos contendo subseções\n",
    "#     tit1 = ['Identificação', 'Endereço', 'Formação acadêmica/titulação', 'Pós-doutorado', 'Formação Complementar',\n",
    "#             'Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão',\n",
    "#             'Projetos de desenvolvimento', 'Revisor de periódico', 'Revisor de projeto de fomento', 'Áreas de atuação',\n",
    "#             'Idiomas', 'Inovação']\n",
    "\n",
    "#     tit2 = ['Atuação Profissional'] # dados com subseções\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         # Encontre o título do bloco\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "#         # Verifique se o título está na lista 'tit1'\n",
    "#         if titulo in tit1:\n",
    "#             data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "#             if verbose:\n",
    "#                 print(titulo)\n",
    "            \n",
    "#             for data_cell in data_cells:\n",
    "#                 divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "#                 divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "#                 keys = []\n",
    "#                 vals = []\n",
    "\n",
    "#                 for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "#                     if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "#                         key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                         key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "#                         keys.append(key_text)\n",
    "#                         val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                         val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "#                         vals.append(val_text)\n",
    "#                         if verbose:\n",
    "#                             print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "#                 agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "#                 data_dict[titulo] = agg_dict\n",
    "        \n",
    "#         if titulo in tit2:\n",
    "#             data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "#             if verbose:\n",
    "#                 print(titulo)\n",
    "            \n",
    "#             for data_cell in data_cells:\n",
    "#                 sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "#                 if verbose:\n",
    "#                     print(len(sections), 'seções')\n",
    "\n",
    "#                 for section in sections:\n",
    "#                     section_name = section.find('b').get_text().strip()\n",
    "#                     data_dict[titulo][section_name] = []\n",
    "#                     if verbose:\n",
    "#                         print(section_name)\n",
    "\n",
    "#                     sibling = section.find_next_sibling()\n",
    "#                     current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "#                     while sibling:\n",
    "#                         classes = sibling.get('class', [])\n",
    "\n",
    "#                         if 'layout-cell-3' in classes:  # Data key\n",
    "#                             key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "#                             sibling = sibling.find_next_sibling()\n",
    "\n",
    "#                             if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "#                                 val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "#                                 current_data[key] = val\n",
    "#                                 if verbose:\n",
    "#                                     print(len(current_data.values()), key, val)\n",
    "\n",
    "#                         elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "#                             next_sibling = sibling.find_next_sibling()\n",
    "#                             if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "#                                 sibling = None\n",
    "#                             else:\n",
    "#                                 if current_data:\n",
    "#                                     data_dict[titulo][section_name] = current_data  # Armazenamos os dados em uma lista\n",
    "\n",
    "#                         if sibling:\n",
    "#                             sibling = sibling.find_next_sibling()\n",
    "#     return data_dict\n",
    "\n",
    "\n",
    "# def extract_tit2_soup(soup, verbose=False):\n",
    "#     data_dict = {}\n",
    "                \n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')    \n",
    "    \n",
    "#     tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         # Encontre o título do bloco\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "#         # Verifique se o título está na lista 'tit2'\n",
    "#         if titulo in tit2:\n",
    "#             data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "#             if verbose:\n",
    "#                 print(titulo)\n",
    "\n",
    "#             for data_cell in data_cells:\n",
    "#                 sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "#                 if verbose:\n",
    "#                     print(len(sections), 'seções')\n",
    "\n",
    "#                 for section in sections:\n",
    "#                     section_name = section.find('b').get_text().strip()\n",
    "#                     data_dict[titulo][section_name] = {}\n",
    "\n",
    "#                     sibling = section.find_next_sibling()\n",
    "#                     current_subsection = None\n",
    "#                     current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "#                     div_citacoes = section.find_next_sibling('div', class_='cita-artigos')\n",
    "#                     if verbose:\n",
    "#                         print(len(div_citacoes), 'próxima cita-artigos')\n",
    "#                     if div_citacoes:\n",
    "#                         cita_artigos_text = div_citacoes.findChild('b').get_text().strip()\n",
    "#                         if verbose:\n",
    "#                             print(f'    {cita_artigos_text}') # nomes de subseção como ocorrências \n",
    "                        \n",
    "#                         if cita_artigos_text == 'Citações':\n",
    "#                             current_subsection = cita_artigos_text\n",
    "#                             data_dict[titulo][section_name]['Citações'] = {}\n",
    "#                             ## Extrair dados das citações dos artigos publicados em bases indexadas\n",
    "#                             sub_section_list = []\n",
    "                                \n",
    "#                             ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "#                             next_siblings = div_citacoes.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "#                             for i in next_siblings:\n",
    "#                                 citation_counts = i.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que contém os Valores de Citações\n",
    "#                                 if citation_counts:\n",
    "#                                     for i in citation_counts:\n",
    "#                                         database = i.get_text()\n",
    "#                                         total_trab = i.find_next_sibling(\"div\", class_=\"trab\").get_text().split(\"Total de trabalhos:\")[1]\n",
    "#                                         total_cite = i.find_next_sibling(\"div\", class_=\"cita\").get_text().split(\"Total de citações:\")[1]\n",
    "#                                         fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "#                                         num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "#                                         data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\").get_text().split(\"Data:\")[1].strip()\n",
    "\n",
    "#                                         # Converta os valores para tipos de dados adequados\n",
    "#                                         total_trab = int(total_trab)\n",
    "#                                         total_cite = int(total_cite)\n",
    "\n",
    "#                                         citation_numbers = {\n",
    "#                                             \"Database\": database,\n",
    "#                                             \"Total de trabalhos\": total_trab,\n",
    "#                                             \"Total de citações\": total_cite,\n",
    "#                                             \"Índice_H\": num_fator_h,\n",
    "#                                             \"Data\": data_wos\n",
    "#                                         }\n",
    "\n",
    "#                                         # Verifique se a subseção atual já existe no dicionário\n",
    "#                                         if 'Citações' not in data_dict[titulo][section_name]:\n",
    "#                                             data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "\n",
    "#                                         data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "\n",
    "#                                         if verbose:\n",
    "#                                             print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "#                         else:\n",
    "#                             while sibling:\n",
    "#                                 classes = sibling.get('class', [])\n",
    "\n",
    "#                                 if 'cita-artigos' in classes:  # Subsection start\n",
    "#                                     subsection_name = sibling.find('b').get_text().strip()\n",
    "#                                     current_subsection = subsection_name\n",
    "#                                     data_dict[titulo][section_name][current_subsection] = {}\n",
    "#                                     current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "\n",
    "#                                 elif 'layout-cell-1' in classes:  # Data key\n",
    "#                                     key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "#                                     sibling = sibling.find_next_sibling()\n",
    "\n",
    "#                                     if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "#                                         val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "#                                         current_data[key] = val\n",
    "\n",
    "#                                 elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "#                                     next_sibling = sibling.find_next_sibling()\n",
    "#                                     if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "#                                         sibling = None\n",
    "#                                     else:\n",
    "#                                         if current_subsection:\n",
    "#                                             data_dict[titulo][section_name][current_subsection] = current_data  # Armazenamos os dados da subseção atual\n",
    "\n",
    "#                                 if sibling:\n",
    "#                                     sibling = sibling.find_next_sibling()\n",
    "\n",
    "#     return data_dict\n",
    "\n",
    "\n",
    "# def extract_tit3_soup(soup, verbose=False):\n",
    "#     data_dict = {}\n",
    "\n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "#     # Títulos da seção 'Eventos'\n",
    "#     tit3 = ['Eventos']\n",
    "\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         # Encontre o título do bloco\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "#         # Verifique se o título está na lista 'tit3'\n",
    "#         if titulo in tit3:\n",
    "#             data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "#             if verbose:\n",
    "#                 print(titulo)\n",
    "            \n",
    "#             for data_cell in data_cells:\n",
    "#                 sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "#                 if verbose:\n",
    "#                     print(len(sections), 'seções')\n",
    "\n",
    "#                 for section in sections:\n",
    "#                     section_name = section.find('b').get_text().strip()\n",
    "#                     data_dict[titulo][section_name] = []\n",
    "#                     if verbose:\n",
    "#                         print(section_name)\n",
    "\n",
    "#                     sibling = section.find_next_sibling()\n",
    "#                     current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "#                     while sibling:\n",
    "#                         classes = sibling.get('class', [])\n",
    "\n",
    "#                         if 'layout-cell-1' in classes:  # Data key\n",
    "#                             key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "#                             sibling = sibling.find_next_sibling()\n",
    "\n",
    "#                             if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "#                                 val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "#                                 current_data[key] = val\n",
    "#                                 if verbose:\n",
    "#                                     print(len(current_data.values()), key, val)\n",
    "\n",
    "#                         elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "#                             next_sibling = sibling.find_next_sibling()\n",
    "#                             if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "#                                 sibling = None\n",
    "#                             else:\n",
    "#                                 if current_data:\n",
    "#                                     data_dict[titulo][section_name] = current_data  # Armazenamos os dados em uma lista\n",
    "\n",
    "#                         if sibling:\n",
    "#                             sibling = sibling.find_next_sibling()\n",
    "\n",
    "#     return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(input_data, dict):\n",
    "            for key, value in input_data.items():\n",
    "                if isinstance(value, dict):  # Se um valor ainda é um dicionário, converte em string JSON\n",
    "                    input_data[key] = json.dumps(Neo4jPersister.convert_to_primitives(value), ensure_ascii=False)\n",
    "                else:\n",
    "                    input_data[key] = Neo4jPersister.convert_to_primitives(value)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, list):\n",
    "            return [Neo4jPersister.convert_to_primitives(item) for item in input_data]\n",
    "        \n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        \n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return Neo4jPersister.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def persist_data(self, data_dict, label):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"CREATE (node:{label}) SET node = $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "    def update_data(self, node_id, data_dict):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MATCH (node) WHERE id(node) = {node_id} SET node += $props\"\n",
    "            session.run(query, props=data_dict_primitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tit1_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "    # Títulos contendo subseções\n",
    "    tit1 = ['Identificação', 'Endereço', 'Formação acadêmica/titulação', 'Pós-doutorado', 'Formação Complementar',\n",
    "            'Linhas de pesquisa', 'Projetos de pesquisa', 'Projetos de extensão',\n",
    "            'Projetos de desenvolvimento', 'Revisor de periódico', 'Revisor de projeto de fomento', 'Áreas de atuação',\n",
    "            'Idiomas', 'Inovação']\n",
    "\n",
    "    tit2 = ['Atuação Profissional'] # dados com subseções\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "        # Verifique se o título está na lista 'tit1'\n",
    "        if titulo in tit1:\n",
    "            if verbose:\n",
    "                print(titulo)\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                keys = []\n",
    "                vals = []\n",
    "\n",
    "                for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "                    if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "                        key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                        key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                        keys.append(key_text)\n",
    "                        val = j.find('div', class_='layout-cell-pad-5')\n",
    "                        val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                        vals.append(val_text)\n",
    "                        if verbose:\n",
    "                            print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "                agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                data_dict[titulo] = Neo4jPersister.convert_to_primitives(agg_dict)\n",
    "\n",
    "        \n",
    "        if titulo in tit2:\n",
    "            if verbose:\n",
    "                print(titulo)\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = []\n",
    "                    if verbose:\n",
    "                        print(section_name)\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    while sibling:\n",
    "                        classes = sibling.get('class', [])\n",
    "\n",
    "                        if 'layout-cell-3' in classes:  # Data key\n",
    "                            key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                            if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "                                val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                current_data[key] = val\n",
    "                                if verbose:\n",
    "                                    print(len(current_data.values()), key, val)\n",
    "\n",
    "                        elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                            next_sibling = sibling.find_next_sibling()\n",
    "                            if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                sibling = None\n",
    "                            else:\n",
    "                                if current_data:\n",
    "                                    data_dict[titulo][section_name].append(current_data)  # Armazenamos os dados em uma lista\n",
    "\n",
    "                        if sibling:\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def extract_tit2_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')    \n",
    "    \n",
    "    tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        \n",
    "        # Verifique se o título está na lista 'tit2'\n",
    "        if titulo in tit2:\n",
    "            if verbose:\n",
    "                print(f'Título: {titulo}')\n",
    "\n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = {}\n",
    "                    if verbose:\n",
    "                        print(f'Seção: {section_name}')\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_subsection = None\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    if section_name == 'Produção bibliográfica':\n",
    "                        subsections = section.find_next_siblings('div', class_='cita-artigos')\n",
    "                        if verbose:\n",
    "                            print(len(subsections), 'subseções')                       \n",
    "                        for subsection in subsections:                            \n",
    "                            if subsection:\n",
    "                                subsection_name = subsection.find('b').get_text().strip()\n",
    "                                if verbose:\n",
    "                                    print(f'    Subseção: {subsection_name}') # nomes de subseção como ocorrências \n",
    "                                    print(f'    {len(subsection)} divs na subseção {subsection_name}')                                \n",
    "                                if subsection_name == 'Citações':\n",
    "                                    current_subsection = subsection_name\n",
    "                                    data_dict[titulo][section_name]['Citações'] = {}\n",
    "                                    sub_section_list = []\n",
    "                                        \n",
    "                                    ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "                                    next_siblings = subsection.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "                                    for sibling in next_siblings:\n",
    "                                        citation_counts = sibling.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que contém os Valores de Citações\n",
    "                                        if citation_counts:\n",
    "                                            for i in citation_counts:\n",
    "                                                database = i.get_text()\n",
    "                                                total_trab = i.find_next_sibling(\"div\", class_=\"trab\").get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                                total_cite = i.find_next_sibling(\"div\", class_=\"cita\").get_text().split(\"Total de citações:\")[1]\n",
    "                                                fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                                num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                                data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\").get_text().split(\"Data:\")[1].strip()\n",
    "\n",
    "                                                # Converta os valores para tipos de dados adequados\n",
    "                                                total_trab = int(total_trab)\n",
    "                                                total_cite = int(total_cite)\n",
    "\n",
    "                                                citation_numbers = {\n",
    "                                                    \"Database\": database,\n",
    "                                                    \"Total de trabalhos\": total_trab,\n",
    "                                                    \"Total de citações\": total_cite,\n",
    "                                                    \"Índice_H\": num_fator_h,\n",
    "                                                    \"Data\": data_wos\n",
    "                                                }\n",
    "\n",
    "                                                # Verifique se a subseção atual já existe no dicionário\n",
    "                                                if 'Citações' not in data_dict[titulo][section_name]:\n",
    "                                                    data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "\n",
    "                                                data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "\n",
    "                                                if verbose:\n",
    "                                                    print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "                            \n",
    "                        ## Encontrar a div irmã de div subseção com classe layout-cell-12 com artigos\n",
    "                        vals_jcr = []\n",
    "                        div_artigo_geral = data_cell.findChild(\"div\", id=\"artigos-completos\")\n",
    "                        if verbose:\n",
    "                            print(f'Encontrada {len(div_artigo_geral)} div geral de artigos')  \n",
    "                        divs_artigos = div_artigo_geral.find_all('div', class_='artigo-completo')\n",
    "                        if verbose:\n",
    "                            print(len(divs_artigos), 'divs de artigos')\n",
    "                        if divs_artigos:                              \n",
    "                            for div_artigo in divs_artigos:                                   \n",
    "                                    ## Extrair filhos da classes de artigos completos que estão à frente\n",
    "                                sibling = div_artigo.findChild()\n",
    "                                current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                                while sibling:\n",
    "                                    classes = sibling.get('class', [])\n",
    "\n",
    "                                    if 'layout-cell-1' in classes:  # Data key\n",
    "                                        key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                        sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                        if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                            val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                            info_dict = {\n",
    "                                                'data-issn': 'NULL',\n",
    "                                                'impact-factor': 'NULL',  \n",
    "                                                'jcr-year': 'NULL',\n",
    "                                                'journal': 'NULL',\n",
    "                                                'raw_data': 'NULL',\n",
    "                                            }\n",
    "                                            # Remova as tags span da div\n",
    "                                            for span in sibling.find_all('span'):\n",
    "                                                span.extract()\n",
    "                                            \n",
    "                                            val_text = sibling.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "\n",
    "                                            current_data[key] = val_text\n",
    "                                            if verbose:\n",
    "                                                print(len(current_data.values()), key, val)\n",
    "\n",
    "                                            sup_element = sibling.find('sup')\n",
    "                                            raw_jcr_data = sup_element.get_text()\n",
    "                                            # print('sup_element:',sup_element)\n",
    "                                            img_element = sup_element.find('img')\n",
    "                                            # print('img_element:',img_element)\n",
    "\n",
    "                                            if sup_element:\n",
    "                                                if img_element:\n",
    "                                                    original_title = img_element.get('original-title')\n",
    "                                                    raw_jcr_data = img_element.get_text()\n",
    "                                                    if original_title:\n",
    "                                                        info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                        \n",
    "                                                        if info_list != 'NULL':\n",
    "                                                            info_dict = {\n",
    "                                                                'data-issn': img_element.get('data-issn'),\n",
    "                                                                'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ',''),\n",
    "                                                                'journal': info_list[0],\n",
    "                                                                'raw_data': raw_jcr_data,\n",
    "                                                            }\n",
    "                                                    else:\n",
    "                                                        info_dict = {\n",
    "                                                            'data-issn': img_element.get('data-issn'),\n",
    "                                                            'impact-factor': 'NULL',\n",
    "                                                            'jcr-year': 'NULL',\n",
    "                                                            'journal': img_element.get('original-title'),\n",
    "                                                            'raw_data': raw_jcr_data,\n",
    "                                                        }\n",
    "                                            else:\n",
    "                                                info_dict = {\n",
    "                                                    'data-issn': 'NULL',\n",
    "                                                    'original-title': 'NULL',\n",
    "                                                    'Fator de impacto': 'NULL',\n",
    "                                                    'ano_apuração': 'NULL',\n",
    "                                                    'raw_data': raw_jcr_data, \n",
    "                                                }                                                                \n",
    "                                                \n",
    "                                            vals_jcr.append(info_dict)\n",
    "                                            if verbose:\n",
    "                                                print(f'         {info_dict}')\n",
    "\n",
    "                                        if 'JCR' not in data_dict:\n",
    "                                            data_dict['JCR'] = []\n",
    "                                        \n",
    "                                        if verbose:\n",
    "                                            print(len(vals_jcr))\n",
    "                                        data_dict['JCR'] = vals_jcr\n",
    "\n",
    "                                    elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                        next_sibling = sibling.find_next_sibling()\n",
    "                                        if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                            sibling = None\n",
    "                                        else:\n",
    "                                            if current_data:\n",
    "                                                converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                                data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = converted_data\n",
    "\n",
    "                                    if sibling:\n",
    "                                        sibling = sibling.find_next_sibling()\n",
    "\n",
    "                    else:\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "\n",
    "                            if 'cita-artigos' in classes:  # Subsection start\n",
    "                                subsection_name = sibling.find('b').get_text().strip()\n",
    "                                current_subsection = subsection_name\n",
    "                                if verbose:\n",
    "                                    print(f'    Subseção: {subsection_name}')\n",
    "                                data_dict[titulo][section_name][current_subsection] = {}\n",
    "                                current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "\n",
    "                            elif 'layout-cell-1' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_subsection:\n",
    "                                        data_dict[titulo][section_name][current_subsection] = Neo4jPersister.convert_to_primitives(current_data)  # Armazenamos os dados da subseção atual\n",
    "                                        \n",
    "\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def extract_tit3_soup(soup, data_dict=None, verbose=False):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "    \n",
    "    # Títulos da seção 'Eventos'\n",
    "    tit3 = ['Eventos']\n",
    "\n",
    "    for div_title_wrapper in divs_title_wrapper:\n",
    "        # Encontre o título do bloco\n",
    "        titulo = div_title_wrapper.find('h1').text.strip()\n",
    "        data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "        # Verifique se o título está na lista 'tit3'\n",
    "        if titulo in tit3:\n",
    "            if verbose:\n",
    "                print(f'Título: {titulo}')\n",
    "            \n",
    "            data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "            for data_cell in data_cells:\n",
    "                sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                if verbose:\n",
    "                    print(len(sections), 'seções')\n",
    "\n",
    "                for section in sections:\n",
    "                    section_name = section.find('b').get_text().strip()\n",
    "                    data_dict[titulo][section_name] = []\n",
    "                    if verbose:\n",
    "                        print(section_name)\n",
    "\n",
    "                    sibling = section.find_next_sibling()\n",
    "                    current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                    while sibling:\n",
    "                        classes = sibling.get('class', [])\n",
    "\n",
    "                        if 'layout-cell-1' in classes:  # Data key\n",
    "                            key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "                            if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                current_data[key] = val\n",
    "                                if verbose:\n",
    "                                    print(len(current_data.values()), key, val)\n",
    "\n",
    "                        elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                            next_sibling = sibling.find_next_sibling()\n",
    "                            if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                sibling = None\n",
    "                            else:\n",
    "                                if current_data:\n",
    "                                    converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                    data_dict[titulo][section_name] = converted_data\n",
    "\n",
    "                        if sibling:\n",
    "                            sibling = sibling.find_next_sibling()\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_tit1_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tit2_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tit3_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def aggregate_data_dicts(soup):\n",
    "    # Crie um dicionário vazio para armazenar os dados agregados\n",
    "\n",
    "    elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "    name = info_list[0]\n",
    "\n",
    "    aggregated_data = {\"labels\": \"Person\",\"name\": name,\"InfPes\": [], \"Resumo\": []}\n",
    "    aggregated_data['InfPes'] = info_list\n",
    "    summary_text  = elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()\n",
    "    aggregated_data['Resumo'].append(summary_text)\n",
    "\n",
    "    tit1_data = extract_tit1_soup(soup, verbose=False)\n",
    "    tit2_data = extract_tit2_soup(soup, verbose=False)\n",
    "    tit3_data = extract_tit3_soup(soup, verbose=False)\n",
    "\n",
    "    # Função para mesclar dicionários aninhados\n",
    "    def merge_dict(d1, d2):\n",
    "        for key, value in d2.items():\n",
    "            if key in d1 and isinstance(d1[key], dict) and isinstance(value, dict):\n",
    "                merge_dict(d1[key], value)\n",
    "            else:\n",
    "                d1[key] = value\n",
    "\n",
    "    # Adicione os dados de tit1\n",
    "    for title, data in tit1_data.items():\n",
    "        if title not in aggregated_data:\n",
    "            aggregated_data[title] = {}\n",
    "        merge_dict(aggregated_data[title], data)\n",
    "\n",
    "    # Adicione os dados de tit2\n",
    "    for title, data in tit2_data.items():\n",
    "        if title not in aggregated_data:\n",
    "            aggregated_data[title] = {}\n",
    "        merge_dict(aggregated_data[title], data)\n",
    "\n",
    "    # Adicione os dados de tit3\n",
    "    for title, data in tit3_data.items():\n",
    "        if title not in aggregated_data:\n",
    "            aggregated_data[title] = {}\n",
    "        merge_dict(aggregated_data[title], data)\n",
    "\n",
    "    # Converta o dicionário agregado em JSON\n",
    "    # aggregated_json = json.dumps(aggregated_data, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_full_data(soup, verbose=False):\n",
    "#     data_dict = {}\n",
    "    \n",
    "#     elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "\n",
    "#     info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "#     name = info_list[0]\n",
    "\n",
    "#     data_dict = {\"labels\": \"Person\",\"name\": name,\"InfPes\": [], \"Resumo\": []}\n",
    "#     data_dict['InfPes'] = info_list\n",
    "#     summary_text  = elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()\n",
    "#     data_dict['Resumo'].append(summary_text)\n",
    "\n",
    "#     tit1 = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar','Atuação Profissional','Linhas de pesquisa',\n",
    "#             'Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento','Revisor de periódico','Revisor de projeto de fomento',\n",
    "#             'Áreas de atuação','Idiomas','Inovação']\n",
    "#     tit2 = ['Produções','Bancas','Orientações']\n",
    "#     # Não extraindo ainda, semelhante ao tit2 mas não há subseções dados direto nas seções \"inst_back\"\n",
    "#     tit3 = ['Eventos']\n",
    "\n",
    "#     for div_title_wrapper in divs_title_wrapper:\n",
    "#         data_cells_12 = div_title_wrapper.findChildren('div', class_='data-cell')\n",
    "\n",
    "#         titulo = div_title_wrapper.find('h1').text.strip()\n",
    "#         if titulo not in data_dict:\n",
    "#             if titulo == '':\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 data_dict[titulo] = {}\n",
    "#         if verbose:\n",
    "#             print(f'{titulo}')\n",
    "\n",
    "#         for data_cell in data_cells_12:\n",
    "#             if titulo in tit1 or titulo in tit3:\n",
    "#                 divs_inst_back = div_title_wrapper.findChildren('div', class_='inst_back')\n",
    "\n",
    "#                 for div_inst_back_cell in divs_inst_back:\n",
    "#                     inst_back = div_inst_back_cell.findChild('b')\n",
    "#                     if inst_back:\n",
    "#                         instback_text = inst_back.get_text().strip()\n",
    "#                         data_dict.setdefault(titulo, {}).setdefault(instback_text, {})\n",
    "\n",
    "#                         divs_subsection = div_inst_back_cell.find_next_siblings('div', class_='cita-artigos')\n",
    "#                         for subsection in divs_subsection:\n",
    "#                             subsection_text = subsection.get_text().strip()\n",
    "#                             if subsection_text:\n",
    "#                                 data_dict[titulo][instback_text][subsection_text] = extract_partial_data(subsection, verbose=verbose)\n",
    "\n",
    "#                 elementos_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "#                 elementos_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "#                 keys = []\n",
    "#                 vals = []\n",
    "#                 for i, j in zip(elementos_layout_cell_3, elementos_layout_cell_9):\n",
    "#                     if elementos_layout_cell_3 and elementos_layout_cell_9:\n",
    "#                         key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                         key_text = key.get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "#                         keys.append(key_text)\n",
    "#                         val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                         val_text = val.get_text().strip().replace('\\n', '').replace('\\t', '')\n",
    "#                         vals.append(val_text)\n",
    "#                         if verbose:\n",
    "#                             print(f'      {key_text:>3}: {val_text}')\n",
    "                \n",
    "#                 agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "#                 if titulo not in data_dict:\n",
    "#                     data_dict[titulo] = {}\n",
    "#                 data_dict[titulo].update(agg_dict)\n",
    "\n",
    "#             elif titulo in tit2:\n",
    "#                 divs_inst_back = data_cell.find_all('div', class_='inst_back')\n",
    "\n",
    "#                 for div_inst_back_cell in divs_inst_back:\n",
    "#                     inst_back = div_inst_back_cell.findChild('b')\n",
    "#                     if inst_back:\n",
    "#                         instback_text = inst_back.get_text().strip()\n",
    "#                         if verbose:\n",
    "#                             print(f'  {instback_text}')\n",
    "\n",
    "#                         div_citacoes = div_inst_back_cell.find_next_sibling('div', class_='cita-artigos')\n",
    "#                         if div_citacoes:\n",
    "#                             cita_artigos_text = div_citacoes.findChild('b').get_text().strip()\n",
    "#                             if cita_artigos_text == 'Citações':\n",
    "#                                 partial_data = extract_tit2(div_citacoes, verbose=verbose)\n",
    "#                                 data_dict.setdefault(titulo, {}).setdefault(instback_text, {})[cita_artigos_text] = partial_data\n",
    "\n",
    "#                         div_artigos = div_inst_back_cell.find_next_siblings('div', id='artigos-completos')\n",
    "#                         for div_artigo in div_artigos:\n",
    "#                             if div_artigo:\n",
    "#                                 divs_cita_artigos = div_artigo.find_all('div', class_='cita-artigos')\n",
    "#                                 for cita_artigos in divs_cita_artigos:\n",
    "#                                     if cita_artigos:\n",
    "#                                         cita_artigos_text = cita_artigos.findChild('b').get_text().strip()\n",
    "#                                         if verbose:\n",
    "#                                             print(f'    {cita_artigos_text}')\n",
    "\n",
    "#                                         elementos_layout_cell_1 = div_artigo.find_all('div', class_='layout-cell-1')\n",
    "#                                         elementos_layout_cell_11 = div_artigo.find_all('div', class_='layout-cell-11')\n",
    "                                        \n",
    "#                                         keys = []\n",
    "#                                         vals = []\n",
    "#                                         for i, j in zip(elementos_layout_cell_1, elementos_layout_cell_11):\n",
    "#                                             key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#                                             key_text = key.get_text().strip().replace('\\n', '').replace('\\t', '')\n",
    "#                                             val = j.find('div', class_='layout-cell-pad-5')\n",
    "#                                             val_text = val.get_text().strip().replace('\\n', '').replace('\\t', '')\n",
    "#                                             keys.append(key_text)\n",
    "#                                             vals.append(val_text)\n",
    "#                                             if verbose:\n",
    "#                                                 print(f'      {key_text:>3}: {val_text}')\n",
    "                                            \n",
    "#                                             # Condição de parada\n",
    "#                                             try:\n",
    "#                                                 next_next_class = j.find_next_sibling().find_next_sibling().get('class', [])[0]\n",
    "#                                                 if next_next_class:\n",
    "#                                                     if next_next_class == 'cita-artigos' or next_next_class == 'clear':\n",
    "#                                                         break\n",
    "#                                             except:\n",
    "#                                                 break\n",
    "\n",
    "#                                         partial_data = extract_tit1(cita_artigos, verbose=verbose)\n",
    "#                                         data_dict[titulo][instback_text][cita_artigos_text] = partial_data\n",
    "\n",
    "#     return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montar dicionários aninhados para Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = aggregate_data_dicts(soup)\n",
    "# pprint(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 3: Persistir em Neo4j</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Persistir todos dados como propriedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j_persister import Neo4jPersister  # Supondo que o nome do arquivo da classe seja neo4j_persister.py\n",
    "\n",
    "# Create a Neo4jPersister instance and persist the data\n",
    "neo4j_persister = Neo4jPersister(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "neo4j_persister.persist_data(data_dict, \"Person\")\n",
    "\n",
    "# Close the Neo4j connection\n",
    "neo4j_persister.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar subnós a partir de propriedades persistidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeo4j:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def _consultar_propriedades_jcr(self, tx, name):\n",
    "        query = (\n",
    "            \"MATCH (p:Person {name: $name})\"\n",
    "            \"RETURN p.JCR AS jcr\"\n",
    "        )\n",
    "        result = tx.run(query, name=name)\n",
    "        return [record[\"jcr\"] for record in result]\n",
    "\n",
    "    ## Versão para usar com criação de nós secundários retorna lista\n",
    "    def consultar_lista_propriedades_jcr(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            query = (\n",
    "                \"MATCH (p:Person {name: $name})\"\n",
    "                \"RETURN p.JCR AS jcr\"\n",
    "            )\n",
    "            result = session.run(query, name=name)\n",
    "            jcr_properties_list = [record[\"jcr\"] for record in result]\n",
    "\n",
    "        return jcr_properties_list\n",
    "    \n",
    "    ## Versão para usar com criação de nós secundários retorna JSON\n",
    "    def consultar_propriedades_jcr(self, name):\n",
    "        with self._driver.session() as session:\n",
    "            query = (\n",
    "                \"MATCH (p:Person {name: $name})\"\n",
    "                \"RETURN p.JCR AS jcr\"\n",
    "            )\n",
    "            result = session.run(query, name=name)\n",
    "            jcr_data = result.single()[\"jcr\"]\n",
    "            jcr_properties_list = json.loads(jcr_data)\n",
    "            return jcr_properties_list\n",
    "\n",
    "\n",
    "    def create_person_with_jcr(self, name, jcr_properties):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._create_person_with_jcr, name, jcr_properties)\n",
    "\n",
    "    # def criar_nos_secundarios_e_relacionamentos(self, name, jcr_properties_list):\n",
    "    #     with self._driver.session() as session:\n",
    "    #         for properties in jcr_properties_list:\n",
    "    #             # Extraia as propriedades do dicionário\n",
    "    #             data_issn = properties.get(\"data-issn\", \"NULL\")\n",
    "    #             impact_factor = properties.get(\"impact-factor\", \"NULL\")\n",
    "    #             journal_name = properties.get(\"journal\", \"NULL\")\n",
    "\n",
    "    #             # Verifique se o nó Journals já existe com base no data_issn, e crie se não existir\n",
    "    #             create_query = (\n",
    "    #                 \"MERGE (journal:Journals {data_issn: $data_issn}) \"\n",
    "    #                 \"ON CREATE SET journal.impact_factor = $impact_factor, journal.name = $journal_name \"\n",
    "    #                 \"ON MATCH SET journal.impact_factor = $impact_factor, journal.name = $journal_name\"\n",
    "    #             )\n",
    "\n",
    "\n",
    "    #             session.run(create_query, data_issn=data_issn, impact_factor=impact_factor, journal_name=journal_name)\n",
    "\n",
    "    #             # Estabeleça o relacionamento PUBLICOU_EM entre o nó Person e o nó Journals\n",
    "    #             relation_query = (\n",
    "    #                 \"MATCH (person:Person {name: $name}), \"\n",
    "    #                 \"(journal:Journals {data_issn: $data_issn}) \"\n",
    "    #                 \"CREATE (person)-[:PUBLICOU_EM {data_issn_count: 1, impact_factor_sum: $impact_factor}]->(journal)\"\n",
    "    #             )\n",
    "\n",
    "    #             session.run(relation_query, name=name, data_issn=data_issn, impact_factor=impact_factor)\n",
    "\n",
    "    #         session.close()\n",
    "\n",
    "    def criar_nos_secundarios_e_relacionamentos(self, name, jcr_properties):\n",
    "        with self._driver.session() as session:\n",
    "            # Percorra as propriedades JCR e crie os nós e relacionamentos\n",
    "            for properties in jcr_properties:\n",
    "                data_issn = properties.get(\"data-issn\", \"NULL\")\n",
    "                impact_factor = properties.get(\"impact-factor\", \"NULL\")\n",
    "                journal_name = properties.get(\"journal\", \"NULL\")\n",
    "\n",
    "                # Verifique se o nó Journals já existe com base no data_issn, e crie se não existir\n",
    "                create_query = (\n",
    "                    \"MERGE (journal:Journals {data_issn: $data_issn}) \"\n",
    "                    \"ON CREATE SET journal.impact_factor = $impact_factor, journal.name = $journal_name \"\n",
    "                    \"ON MATCH SET journal.impact_factor = $impact_factor, journal.name = $journal_name\"\n",
    "                )\n",
    "                session.run(create_query, data_issn=data_issn, impact_factor=impact_factor, journal_name=journal_name)\n",
    "\n",
    "                # Estabeleça o relacionamento PUBLICOU_EM entre o nó Person e o nó Journals\n",
    "                relation_query = (\n",
    "                    \"MATCH (person:Person {name: $name}), \"\n",
    "                    \"(journal:Journals {data_issn: $data_issn}) \"\n",
    "                    \"MERGE (person)-[rel:PUBLICOU_EM]->(journal) \"\n",
    "                    \"ON CREATE SET rel.data_issn_count = 1, rel.impact_factor = $impact_factor \"\n",
    "                    \"ON MATCH SET rel.impact_factor = $impact_factor \"\n",
    "                    \n",
    "                    ## Exemplo de como seria utilizar uma soma\n",
    "                    # \"ON CREATE SET rel.data_issn_count = 1, rel.impact_factor_sum = $impact_factor \"                    \n",
    "                    # \"ON MATCH SET rel.data_issn_count = rel.data_issn_count + 1, rel.impact_factor_sum = rel.impact_factor_sum + $impact_factor\"\n",
    "                )\n",
    "                session.run(relation_query, name=name, data_issn=data_issn, impact_factor=impact_factor)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_person_with_jcr(tx, name, jcr_properties):\n",
    "        # Cria o nó Person\n",
    "        person_query = (\n",
    "            \"CREATE (p:Person {name: $name}) \"\n",
    "            \"RETURN p\"\n",
    "        )\n",
    "        person_result = tx.run(person_query, name=name)\n",
    "        person_node = person_result.single()[0]\n",
    "\n",
    "        # Cria os nós secundários para cada valor único de data-issn\n",
    "        data_issn_values = set(prop.get(\"data-issn\") for prop in jcr_properties)\n",
    "        for data_issn in data_issn_values:\n",
    "            if data_issn:\n",
    "                secondary_node_query = (\n",
    "                    \"CREATE (s:SecondaryNode {data_issn: $data_issn}) \"\n",
    "                    \"RETURN s\"\n",
    "                )\n",
    "                tx.run(secondary_node_query, data_issn=data_issn)\n",
    "\n",
    "                # Cria a relação entre o nó Person e o nó secundário\n",
    "                relation_query = (\n",
    "                    \"MATCH (p:Person {name: $name}), (s:SecondaryNode {data_issn: $data_issn}) \"\n",
    "                    \"CREATE (p)-[:HAS_JCR]->(s)\"\n",
    "                )\n",
    "                tx.run(relation_query, name=name, data_issn=data_issn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chame a função passando a lista de propriedades JCR\n",
    "name = 'Raimir Holanda Filho'\n",
    "neo4j = MyNeo4j(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "jcr_properties_list = neo4j.consultar_propriedades_jcr(name)\n",
    "neo4j.criar_nos_secundarios_e_relacionamentos(name, jcr_properties_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jcr_properties_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "\n",
    "def format_issn_ticks(x, pos, unique_data_issn):\n",
    "    if x < len(unique_data_issn):\n",
    "        if unique_data_issn[x] == 'NULL':\n",
    "            return 'NULL'\n",
    "        # Formatar o valor para exibição com hífen\n",
    "        return f\"{unique_data_issn[x][:4]}-{unique_data_issn[x][4:]}\"\n",
    "    return ''\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "    # Filtrar os valores \"NULL\" e extrair os valores de data-issn\n",
    "    data_issn_values = [entry[\"data-issn\"] for entry in jcr_properties_list if entry[\"data-issn\"] != \"NULL\"]\n",
    "\n",
    "    # Contar as ocorrências únicas de data-issn\n",
    "    data_issn_counts = Counter(data_issn_values)\n",
    "\n",
    "    # Obter as ocorrências únicas de data-issn e suas contagens\n",
    "    unique_data_issn, counts = zip(*data_issn_counts.items())\n",
    "\n",
    "    # Criar o gráfico de dispersão\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(range(len(unique_data_issn)), counts, alpha=0.5)\n",
    "    plt.title('Gráfico de Dispersão de Impact Factor por Data ISSN')\n",
    "    plt.xlabel('Data ISSN')\n",
    "    plt.ylabel('Contagem de Ocorrências')\n",
    "\n",
    "    # Formatar os rótulos no eixo x usando a função personalizada\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: format_issn_ticks(int(x), pos, unique_data_issn)))\n",
    "    \n",
    "    # Rotacionar rótulos do eixo x para melhor visibilidade\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Chame a função para plotar o gráfico\n",
    "plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "\n",
    "def format_issn_ticks(x, pos, unique_data_issn):\n",
    "    if x < len(unique_data_issn):\n",
    "        if unique_data_issn[x] == 'NULL':\n",
    "            return 'NULL'\n",
    "        # Formatar o valor para exibição com hífen\n",
    "        return f\"{unique_data_issn[x][:4]}-{unique_data_issn[x][4:]}\"\n",
    "    return ''\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "    # Filtrar os valores \"NULL\" e extrair os valores de data-issn e impact-factor\n",
    "    data_issn_values = []\n",
    "    impact_factor_values = []\n",
    "    for entry in jcr_properties_list:\n",
    "        data_issn = entry[\"data-issn\"]\n",
    "        impact_factor = entry[\"impact-factor\"]\n",
    "\n",
    "        if data_issn != \"NULL\" and impact_factor != \"NULL\":\n",
    "            data_issn_values.append(data_issn)\n",
    "            impact_factor_values.append(float(impact_factor))\n",
    "\n",
    "    # Contar as ocorrências únicas de data-issn\n",
    "    data_issn_counts = Counter(data_issn_values)\n",
    "\n",
    "    # Obter as ocorrências únicas de data-issn e suas contagens\n",
    "    unique_data_issn, counts = zip(*data_issn_counts.items())\n",
    "\n",
    "    # Contar a quantidade de valores \"NULL\"\n",
    "    null_count = jcr_properties_list.count({\"data-issn\": \"NULL\", \"impact-factor\": \"NULL\"})\n",
    "\n",
    "    # Inserir a contagem de \"NULL\" como uma entrada extra\n",
    "    unique_data_issn = list(unique_data_issn)\n",
    "    counts = list(counts)\n",
    "    unique_data_issn.append('NULL')\n",
    "    counts.append(null_count)\n",
    "\n",
    "    # Criar o gráfico de dispersão\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(range(len(unique_data_issn)), counts, alpha=0.5)\n",
    "    plt.title('Gráfico de Dispersão de Impact Factor por Data ISSN')\n",
    "    plt.xlabel('Data ISSN')\n",
    "    plt.ylabel('Contagem de Ocorrências')\n",
    "\n",
    "    # Adicionar rótulos de dados com os valores de impact-factor (exceto para a entrada \"NULL\")\n",
    "    for i, count in enumerate(counts[:-1]):\n",
    "        plt.annotate(f'Impact: {impact_factor_values[i]:.3f}', (i, count), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "\n",
    "    # Adicionar rótulo para a entrada \"NULL\" com a contagem correta\n",
    "    plt.annotate(f'NULL: {null_count}', (len(unique_data_issn) - 1, null_count), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "\n",
    "    # Formatar os rótulos no eixo x usando a função personalizada\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: format_issn_ticks(int(x), pos, unique_data_issn)))\n",
    "    \n",
    "    # Rotacionar rótulos do eixo x para melhor visibilidade\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Chame a função para plotar o gráfico\n",
    "plot_scatter(jcr_properties_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "\n",
    "def format_issn_ticks(x, pos, unique_data_issn):\n",
    "    if x < len(unique_data_issn):\n",
    "        # Formatar o valor para exibição com hífen, incluindo \"NULL\"\n",
    "        return f\"{unique_data_issn[int(x)][:4]}-{unique_data_issn[int(x)][4:]}\"\n",
    "    return ''\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "    # Filtrar os valores \"NULL\" e extrair os valores de data-issn e impact-factor\n",
    "    data_issn_values = []\n",
    "    impact_factor_values = []\n",
    "    for entry in jcr_properties_list:\n",
    "        data_issn = entry[\"data-issn\"]\n",
    "        impact_factor = entry[\"impact-factor\"]\n",
    "\n",
    "        if data_issn != \"NULL\" and impact_factor != \"NULL\":\n",
    "            data_issn_values.append(data_issn)\n",
    "            impact_factor_values.append(float(impact_factor))\n",
    "\n",
    "    # Contar as ocorrências únicas de data-issn\n",
    "    data_issn_counts = Counter(data_issn_values)\n",
    "\n",
    "    # Obter as ocorrências únicas de data-issn e suas contagens\n",
    "    unique_data_issn, counts = zip(*data_issn_counts.items())\n",
    "\n",
    "    # Adicionar \"NULL\" às ocorrências únicas, mesmo que não haja ocorrências\n",
    "    unique_data_issn = list(unique_data_issn)\n",
    "    counts = list(counts)\n",
    "    unique_data_issn.append('NULL')\n",
    "    counts.append(0)  # Adicionar contagem zero para \"NULL\"\n",
    "\n",
    "    # Criar uma lista de valores de impacto correspondentes às contagens e valores \"NULL\"\n",
    "    impact_values = []\n",
    "    for data in unique_data_issn:\n",
    "        if data == 'NULL':\n",
    "            impact_values.append(None)  # Use None para valores \"NULL\"\n",
    "        else:\n",
    "            # Encontre o índice correspondente em data_issn_values\n",
    "            idx = data_issn_values.index(data)\n",
    "            impact_values.append(impact_factor_values[idx])\n",
    "\n",
    "    # Criar o gráfico de dispersão\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(counts, impact_values, alpha=0.5)\n",
    "    plt.title('Gráfico de Dispersão de Impact Factor por Contagem de Ocorrências')\n",
    "    plt.xlabel('Contagem de Ocorrências')\n",
    "    plt.ylabel('Impact Factor')\n",
    "\n",
    "    # Adicionar rótulos de dados com as contagens\n",
    "    for i, count in enumerate(counts):\n",
    "        if unique_data_issn[i] == 'NULL':\n",
    "            plt.annotate(f'Count: {count}', (count, 0), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "        else:\n",
    "            plt.annotate(f'Count: {count}', (count, impact_values[i]), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "\n",
    "    # Formatar os rótulos no eixo x usando a função personalizada\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: format_issn_ticks(x, pos, unique_data_issn)))\n",
    "    \n",
    "    # Rotacionar rótulos do eixo x para melhor visibilidade\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Chame a função para plotar o gráfico\n",
    "plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "\n",
    "def format_issn_ticks(x, pos, unique_data_issn):\n",
    "    if x < len(unique_data_issn):\n",
    "        # Formatar o valor para exibição com hífen, incluindo \"NULL\"\n",
    "        return f\"{unique_data_issn[int(x)][:4]}-{unique_data_issn[int(x)][4:]}\"\n",
    "    return ''\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "    # Extrair os valores de data-issn e impact-factor\n",
    "    data_issn_values = []\n",
    "    impact_factor_values = []\n",
    "    for entry in jcr_properties_list:\n",
    "        data_issn = entry[\"data-issn\"]\n",
    "        impact_factor = entry[\"impact-factor\"]\n",
    "\n",
    "        if impact_factor != \"NULL\":\n",
    "            data_issn_values.append(data_issn)\n",
    "            impact_factor_values.append(float(impact_factor))\n",
    "\n",
    "    # Contar as ocorrências únicas de data-issn, incluindo \"NULL\"\n",
    "    data_issn_counts = Counter(data_issn_values)\n",
    "    unique_data_issn = list(data_issn_counts.keys())\n",
    "    unique_data_issn.append('NULL')  # Adicionar \"NULL\" à lista única de data-issn\n",
    "    counts = [data_issn_counts[data] for data in unique_data_issn]\n",
    "\n",
    "    # Criar o gráfico de dispersão\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Criar uma lista correspondente de valores x para cada valor em y\n",
    "    x_values = [unique_data_issn.index(data_issn) for data_issn in data_issn_values]\n",
    "\n",
    "    plt.scatter(x_values, impact_factor_values, alpha=0.5)\n",
    "    plt.title('Gráfico de Dispersão de Impact Factor por Data ISSN')\n",
    "    plt.xlabel('Data ISSN')\n",
    "    plt.ylabel('Impact Factor')\n",
    "\n",
    "    # Formatar os rótulos no eixo x usando a função personalizada\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: format_issn_ticks(x, pos, unique_data_issn)))\n",
    "    \n",
    "    # Rotacionar rótulos do eixo x para melhor visibilidade\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Chame a função para plotar o gráfico\n",
    "plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "\n",
    "def format_issn_ticks(x, pos, unique_data_issn):\n",
    "    if x < len(unique_data_issn):\n",
    "        return f\"{unique_data_issn[int(x)][:4]}-{unique_data_issn[int(x)][4:]}\"\n",
    "    return ''\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "    data_issn_values = []\n",
    "    impact_factor_values = []\n",
    "    journal_values = []\n",
    "    for entry in jcr_properties_list:\n",
    "        data_issn = entry[\"data-issn\"]\n",
    "        impact_factor = entry[\"impact-factor\"]\n",
    "        journal = entry[\"journal\"]\n",
    "\n",
    "        if impact_factor != \"NULL\":\n",
    "            data_issn_values.append(data_issn)\n",
    "            impact_factor_values.append(float(impact_factor))\n",
    "            journal_values.append(journal)\n",
    "\n",
    "    data_issn_counts = Counter(data_issn_values)\n",
    "    unique_data_issn = list(data_issn_counts.keys())\n",
    "    unique_data_issn.append('NULL')\n",
    "    counts = [data_issn_counts[data] for data in unique_data_issn]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 9))\n",
    "\n",
    "    x_values = [unique_data_issn.index(data_issn) for data_issn in data_issn_values]\n",
    "\n",
    "    plt.scatter(x_values, impact_factor_values, alpha=0.5)\n",
    "    plt.title('Gráfico de Dispersão de Impact Factor por ISSN')\n",
    "    plt.xlabel('ISSN')\n",
    "    plt.ylabel('Fator de Impacto')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: format_issn_ticks(x, pos, unique_data_issn)))\n",
    "\n",
    "    for i, (journal, count) in enumerate(zip(journal_values, counts)):\n",
    "        plt.annotate(f'{journal}\\nCount: {count}', (x_values[i], impact_factor_values[i]), fontsize=10, ha='left', va='bottom')\n",
    "\n",
    "    total_count = sum(counts)\n",
    "    max_impact = max(impact_factor_values)\n",
    "    \n",
    "    # Adicionar texto no centro superior do gráfico\n",
    "    plt.text(len(unique_data_issn) / 2, max_impact + 0.05, f'Total Count: {total_count}', ha='center', fontsize=10)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
    "\n",
    "    # Adicionar legenda com os valores de contagem fora da área do gráfico\n",
    "    legend_labels = [f'{data_issn}: {count}' for data_issn, count in zip(unique_data_issn, counts)]\n",
    "    legend = ax.legend(legend_labels, title='Contagem', fontsize=8, loc='upper right', bbox_to_anchor=(0.99, 0.9))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=1.1)  # Ajusta a posição superior do gráfico para acomodar a legenda\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "\n",
    "def format_issn_ticks(x, pos, unique_data_issn):\n",
    "    if x < len(unique_data_issn):\n",
    "        return f\"{unique_data_issn[int(x)][:4]}-{unique_data_issn[int(x)][4:]}\"\n",
    "    return ''\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "    data_issn_values = []\n",
    "    impact_factor_values = []\n",
    "    journal_values = []\n",
    "    for entry in jcr_properties_list:\n",
    "        data_issn = entry[\"data-issn\"]\n",
    "        impact_factor = entry[\"impact-factor\"]\n",
    "        journal = entry[\"journal\"]\n",
    "\n",
    "        if impact_factor != \"NULL\":\n",
    "            data_issn_values.append(data_issn)\n",
    "            impact_factor_values.append(float(impact_factor))\n",
    "            journal_values.append(journal)\n",
    "        else:\n",
    "            data_issn_values.append(data_issn)\n",
    "            impact_factor_values.append(0)\n",
    "            journal_values.append(journal)            \n",
    "\n",
    "    data_issn_counts = Counter(data_issn_values)\n",
    "    print(data_issn_counts)\n",
    "    unique_data_issn = list(data_issn_counts.keys())\n",
    "    unique_data_issn.append('NULL')\n",
    "    print(unique_data_issn)\n",
    "    counts = [data_issn_counts[data] for data in unique_data_issn]\n",
    "    print(counts)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    x_values = [unique_data_issn.index(data_issn) for data_issn in data_issn_values]\n",
    "\n",
    "    plt.scatter(x_values, impact_factor_values, alpha=0.5)\n",
    "    plt.title('Gráfico de Dispersão de Impact Factor por Data ISSN')\n",
    "    plt.xlabel('Data ISSN')\n",
    "    plt.ylabel('Impact Factor')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: format_issn_ticks(x, pos, unique_data_issn)))\n",
    "\n",
    "    for i, (journal, count) in enumerate(zip(journal_values, counts)):\n",
    "        plt.annotate(f'{journal}\\nCount: {count}', (x_values[i], impact_factor_values[i]), fontsize=8, ha='left', va='bottom')\n",
    "\n",
    "    total_count = sum(counts)\n",
    "    max_impact = max(impact_factor_values)\n",
    "    \n",
    "    # Adicionar texto no centro superior do gráfico com contagem total de artigos\n",
    "    plt.text(len(unique_data_issn) / 2, max_impact + 0.05, f'Total Count: {total_count}', ha='right', fontsize=10)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "\n",
    "    # Adicionar legenda com os valores de contagem fora da área do gráfico\n",
    "    legend_labels = [f'{ISSN}: {count}' for ISSN, count in zip(unique_data_issn, counts)]\n",
    "    legend = ax.legend(legend_labels, title='Contagem', fontsize=6, loc='upper right')\n",
    "    \n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.subplots_adjust(top=1.2)  # Ajusta a posição superior do gráfico para acomodar a legenda\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "    data = pd.DataFrame(jcr_properties_list)\n",
    "\n",
    "    # Filtrar valores de impact factor diferentes de 'NULL'\n",
    "    data = data[data['impact-factor'] != 'NULL']\n",
    "\n",
    "    # Contagem de ocorrências por data ISSN\n",
    "    data_counts = data['data-issn'].value_counts().reset_index()\n",
    "    data_counts.columns = ['data-issn', 'count']\n",
    "\n",
    "    # Converter 'impact-factor' para float\n",
    "    data['impact-factor'] = data['impact-factor'].astype(float)\n",
    "\n",
    "    # Criar o gráfico de dispersão com Plotly Express\n",
    "    fig = px.scatter(data, x='data-issn', y='impact-factor', color='data-issn',\n",
    "                     title='Gráfico de Dispersão de Fator de Impacto versus ISSN',\n",
    "                     labels={'data-issn': 'ISSN', 'impact-factor': 'Impact Factor'},\n",
    "                     hover_data=['journal'])\n",
    "\n",
    "    # Adicionar contagem como tamanho dos pontos\n",
    "    fig.update_traces(marker=dict(size=data_counts['count'] * 10))\n",
    "\n",
    "    # Personalizar layout\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    fig.update_traces(marker=dict(opacity=1))\n",
    "\n",
    "    # Exibir o gráfico\n",
    "    fig.show()\n",
    "\n",
    "# Chame a função com sua lista de dados jcr_properties_list\n",
    "plot_scatter(jcr_properties_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(jcr_properties_list)\n",
    "\n",
    "# Filtrar valores de impact factor diferentes de 'NULL'\n",
    "data_filtered = data[data['impact-factor'] != 'NULL']\n",
    "\n",
    "# Contagem de ocorrências por data ISSN\n",
    "data_counts = data_filtered['data-issn'].value_counts().reset_index()\n",
    "data_counts.columns = ['data-issn', 'count']\n",
    "\n",
    "# Calcular a contagem de valores NULL\n",
    "null_count = len(data) - len(data_filtered)\n",
    "\n",
    "# Criar um novo DataFrame com a linha para valores NULL\n",
    "null_row    = pd.DataFrame({'data-issn': ['NULL'], 'count': [null_count]})\n",
    "data_counts = pd.concat([data_counts, null_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://developer.clarivate.com/\n",
    "## https://developer.clarivate.com/apis?filter=on&Web+of+Science=on\n",
    "# Subscription and access: Access to this API requires a paid license. The license is available as an add-on subscription to Journal Citation Reports™ or InCites Benchmarking & Analytics™. Don't hesitate to get in touch with us with any questions you may have. This API uses API Key access. All APIs in our Developer Portal require registering an application.\n",
    "# Plans: This API has only one plan (Journals API Plan) and allows a maximum of 5 requests per second.\n",
    "\n",
    "# !pip install git+https://github.com/clarivate/wosjournals-python-client.git\n",
    "# !pip install git+https://github.com/Clarivate-SAR/woslite_py_client.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import woslite_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clarivate.wos_journals.client\n",
    "# from __future__ import print_function\n",
    "import time\n",
    "import woslite_client\n",
    "from woslite_client.rest import ApiException\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def plot_scatter(jcr_properties_list):\n",
    "    data = pd.DataFrame(jcr_properties_list)\n",
    "\n",
    "    # Filtrar valores de impact factor diferentes de 'NULL'\n",
    "    data_filtered = data[data['impact-factor'] != 'NULL']\n",
    "\n",
    "    # Converter 'impact-factor' para float\n",
    "    data_filtered['impact-factor'] = data_filtered['impact-factor'].astype(float)\n",
    "\n",
    "    # Contagem de ocorrências por data ISSN\n",
    "    data_counts = data_filtered['data-issn'].value_counts().reset_index()\n",
    "    data_counts.columns = ['data-issn', 'count']\n",
    "\n",
    "    # Calcular a contagem de valores NULL\n",
    "    null_count = len(data) - len(data_filtered)\n",
    "\n",
    "    # Criar um novo DataFrame com a linha para valores NULL\n",
    "    null_row    = pd.DataFrame({'data-issn': ['NULL'], 'count': [null_count]})\n",
    "    data_counts = pd.concat([data_counts, null_row], ignore_index=True)\n",
    "\n",
    "    # Criar o gráfico de dispersão com Plotly Express\n",
    "    fig = px.scatter(data_filtered, x='data-issn', y='impact-factor', color='data-issn',\n",
    "                     title='Gráfico de Dispersão de Impact Factor por Data ISSN',\n",
    "                     labels={'data-issn': 'Data ISSN', 'impact-factor': 'Impact Factor'},\n",
    "                     hover_data=['journal'])\n",
    "\n",
    "    # Adicionar contagem como tamanho dos pontos\n",
    "    fig.update_traces(marker=dict(size=data_counts['count'] * 3))  # Aumente o tamanho dos marcadores\n",
    "\n",
    "    # Adicionar rótulos de dados com o número de ocorrências\n",
    "    for i, row in data_counts.iterrows():\n",
    "        fig.add_annotation(\n",
    "            text=f'Count: {row[\"count\"]}',\n",
    "            x=row['data-issn'],\n",
    "            y=data_filtered['impact-factor'].max() + 10,  # Ajuste a posição vertical\n",
    "            showarrow=False,\n",
    "            font=dict(size=10)\n",
    "        )\n",
    "\n",
    "    # Personalizar layout\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    fig.update_traces(marker=dict(opacity=0.5))\n",
    "\n",
    "    # Exibir o gráfico\n",
    "    fig.show()\n",
    "    return data, data_counts, data_filtered\n",
    "\n",
    "# Chame a função com sua lista de dados jcr_properties_list\n",
    "data, data_counts, data_filtered = plot_scatter(jcr_properties_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_issn_values = []\n",
    "impact_factor_values = []\n",
    "journal_values = []\n",
    "for entry in jcr_properties_list:\n",
    "    data_issn = entry[\"data-issn\"]\n",
    "    impact_factor = entry[\"impact-factor\"]\n",
    "    journal = entry[\"journal\"]\n",
    "\n",
    "    if impact_factor != \"NULL\":\n",
    "        data_issn_values.append(data_issn)\n",
    "        impact_factor_values.append(float(impact_factor))\n",
    "        journal_values.append(journal)\n",
    "    else:\n",
    "        data_issn_values.append(data_issn)\n",
    "        impact_factor_values.append(0)\n",
    "        journal_values.append('NULL')            \n",
    "\n",
    "data_issn_counts = Counter(data_issn_values)\n",
    "print(data_issn_counts)\n",
    "unique_data_issn = list(data_issn_counts.keys())\n",
    "\n",
    "print(unique_data_issn)\n",
    "counts = [data_issn_counts[data] for data in unique_data_issn]\n",
    "print(counts)\n",
    "legend_labels = [f'{ISSN}: {count}' for ISSN, count in zip(unique_data_issn, counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras formas de persistir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para criar parâmetros dinâmicos e persistir níveis de detales usa-se APOC\n",
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    items = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_dict(v, new_key, sep=sep))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "def persist_to_neo4j(extracted_data):\n",
    "    driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            name = extracted_data.get('name', 'UNKNOWN')\n",
    "            session.write_transaction(lambda tx: tx.run(\n",
    "                \"MERGE (n:Person {name: $name}) RETURN n\",\n",
    "                name=name,\n",
    "            ))\n",
    "            \n",
    "            # Flatten the dictionary\n",
    "            flat_data = flatten_dict(extracted_data)\n",
    "\n",
    "            # Persist each flattened key-value pair\n",
    "            for key, value in flat_data.items():\n",
    "                if key is None or key == '':\n",
    "                    continue\n",
    "                \n",
    "                query = f\"MATCH (n:Person {{name: $name}}) SET n.`{key}` = $value RETURN n\"\n",
    "                session.write_transaction(lambda tx, query=query, value=value: tx.run(query, name=name, value=value))\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_to_neo4j(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j import GraphDatabase\n",
    "\n",
    "# def recursive_persist(tx, parent_node_id, properties):\n",
    "#     for key, value in properties.items():\n",
    "#         if key is None or key == '':\n",
    "#             continue\n",
    "\n",
    "#         if isinstance(value, dict):\n",
    "#             # Create a new node for the nested dictionary and link it to the parent\n",
    "#             result = tx.run(\n",
    "#                 \"MATCH (p) WHERE id(p) = $parent_id \"\n",
    "#                 \"CREATE (p)-[:HAS]->(m:SubNode {name: $key}) \"\n",
    "#                 \"RETURN id(m)\",\n",
    "#                 parent_id=parent_node_id,\n",
    "#                 key=key\n",
    "#             )\n",
    "#             new_node_id = result.single()[0]\n",
    "\n",
    "#             # Recursive call to set properties for the new node\n",
    "#             recursive_persist(tx, new_node_id, value)\n",
    "#         else:\n",
    "#             # Directly set the primitive property on the parent node\n",
    "#             tx.run(\n",
    "#                 \"MATCH (n) WHERE id(n) = $id \"\n",
    "#                 f\"SET n.`{key}` = $value\",\n",
    "#                 id=parent_node_id,\n",
    "#                 value=value\n",
    "#             )\n",
    "\n",
    "# def persist_to_neo4j(extracted_data):\n",
    "#     driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     try:\n",
    "#         with driver.session() as session:\n",
    "#             def transactional_work(tx):\n",
    "#                 name = extracted_data.get('name', 'UNKNOWN')\n",
    "#                 properties = {k: v for k, v in extracted_data.items() if k != 'name'}\n",
    "#                 result = tx.run(\n",
    "#                     \"MERGE (n:MyLabel {name: $name}) \"\n",
    "#                     \"RETURN id(n)\",\n",
    "#                     name=name\n",
    "#                 )\n",
    "#                 root_node_id = result.single()[0]\n",
    "#                 recursive_persist(tx, root_node_id, properties)\n",
    "\n",
    "#             session.write_transaction(transactional_work)\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {e}\")\n",
    "#     finally:\n",
    "#         driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with your `extracted_data`\n",
    "# persist_to_neo4j(extracted_data)\n",
    "# # Sample invocation\n",
    "# extracted_data = {\n",
    "#     'name': 'João Doé',\n",
    "#     'age': 30,\n",
    "#     None: 'This will be ignored',\n",
    "#     '': 'This will be ignored',\n",
    "#     'nested_dict': {\n",
    "#         'key1': 'value1',\n",
    "#         'key2': None\n",
    "#     },\n",
    "#     'nested_list': [1, 2, 3]  # This will now be converted to a JSON string\n",
    "# }\n",
    "# persist_to_neo4j(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teste funcional\n",
    "# def main():\n",
    "#     uri = \"bolt://localhost:7687\"\n",
    "#     driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "#     production_dict = {\n",
    "#         'name': 'John Doe',\n",
    "#         'Properties': {\n",
    "#             'Produções': [\n",
    "#                 {'ano': '2023', 'revista': 'Journal A', 'jcr': '3.662', 'doi': 'some_doi'},\n",
    "#                 {'ano': '2022', 'revista': 'Journal B', 'jcr': None, 'doi': 'another_doi'}\n",
    "#             ]\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     with driver.session() as session:\n",
    "#         session.write_transaction(add_producao, production_dict)\n",
    "\n",
    "#     driver.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Persistir produções como nós secundários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "## Persiste as produções como nós secundários ligados ao nó principal\n",
    "def add_producao(tx, production_dict):\n",
    "    # Extract person's name and properties sub-dictionary\n",
    "    person_name = production_dict.get('name', 'UNKNOWN')\n",
    "    properties_dict = production_dict.get('Properties', {})\n",
    "\n",
    "    # First, merge the Person node based on the name\n",
    "    tx.run(\"MERGE (a:Person {name: $person_name})\", person_name=person_name)\n",
    "\n",
    "    # Initialize an empty dictionary to hold the cleaned production data\n",
    "    producoes = properties_dict.get('Produções', [])\n",
    "    \n",
    "    for producao in producoes:\n",
    "        clean_producao = {}\n",
    "        \n",
    "        # Replace None with 'NULL' and ignore empty strings\n",
    "        for key, value in producao.items():\n",
    "            if value == '':\n",
    "                continue\n",
    "            if value is None:\n",
    "                value = 'NULL'\n",
    "            clean_producao[key] = value\n",
    "\n",
    "        # Create or merge the Producao node based on unique keys ('ano' and 'revista' here)\n",
    "        tx.run(\n",
    "            \"MERGE (p:Producao {ano: $ano, revista: $revista}) \"\n",
    "            \"SET p += $props\",\n",
    "            ano=clean_producao.get('ano'),\n",
    "            revista=clean_producao.get('revista'),\n",
    "            props=clean_producao\n",
    "        )\n",
    "\n",
    "        # Connect the Person node to the Producao node\n",
    "        tx.run(\n",
    "            \"MATCH (a:Person {name: $person_name}), (p:Producao {ano: $ano, revista: $revista}) \"\n",
    "            \"MERGE (a)-[r:HAS_PRODUCOES]->(p)\",\n",
    "            person_name=person_name,\n",
    "            ano=clean_producao.get('ano'),\n",
    "            revista=clean_producao.get('revista')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = {\"Label\": \"Person\", \"name\": {}, \"Properties\": {}}\n",
    "articles_dict = extrair_normal(soup)\n",
    "# articles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create a session\n",
    "session = driver.session()\n",
    "\n",
    "# Call the function within a transaction\n",
    "session.write_transaction(add_producao, articles_dict)\n",
    "\n",
    "# Optionally, close the session and driver\n",
    "session.close()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Persiste todod dados como propriedades de um nó\n",
    "main_cell     = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "generic_dict = {\"Label\": \"Person\", \"Properties\": {}}\n",
    "if main_cell:\n",
    "    traverse(main_cell, generic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_to_neo4j(generic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento de Entidades Nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    import spacy\n",
    "    \n",
    "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "    nlp_pt = spacy.load(\"pt_core_news_lg\")\n",
    "    doc = nlp_pt(text)\n",
    "    entities = {'ORG': [], 'GPE': [], 'NORP': [], 'PERSON': [], 'PRODUCT': [], 'WORK_OF_ART': []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModel:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def add_entity(self, entity_type, entity_value):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._add_entity, entity_type, entity_value)\n",
    "            \n",
    "    @staticmethod\n",
    "    def _add_entity(tx, entity_type, entity_value):\n",
    "        query = f\"MERGE (a:{entity_type} {{name: $name}})\"\n",
    "        tx.run(query, name=entity_value)\n",
    "    \n",
    "    def add_relation(self, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "        with self._driver.session() as session:\n",
    "            session.write_transaction(self._add_relation, src_type, src_name, rel_type, tgt_type, tgt_name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_relation(tx, src_type, src_name, rel_type, tgt_type, tgt_name):\n",
    "        query = (\n",
    "            f\"MATCH (a:{src_type} {{name: $src_name}}), (b:{tgt_type} {{name: $tgt_name}}) \"\n",
    "            f\"MERGE (a)-[:{rel_type}]->(b)\"\n",
    "        )\n",
    "        tx.run(query, src_name=src_name, tgt_name=tgt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    def expand(key, value):\n",
    "        if isinstance(value, dict):\n",
    "            return [(str(key) + '.' + str(k), v) for k, v in flatten_dict(value).items()]\n",
    "        else:\n",
    "            return [(str(key), value)]\n",
    "        \n",
    "    items = [item for k, v in d.items() for item in expand(k, v)]\n",
    "    return dict(items)\n",
    "\n",
    "def main(data_dict):\n",
    "    graph_model = GraphModel(\"bolt://localhost:7687\", \"neo4j\", \"password\")    \n",
    "    flattened_data = flatten_dict(data_dict['Properties']['Identificação'])\n",
    "    person_name = flattened_data.get('Nome', None)[0]\n",
    "    print(f'Nome: {person_name}')\n",
    "    if person_name:\n",
    "        graph_model.add_entity(\"Person\", person_name)\n",
    "    \n",
    "    institutional = flatten_dict(data_dict['Properties'])\n",
    "    for key, value in institutional.items():\n",
    "        if key in ['Endereço.Endereço Profissional']:\n",
    "            text = ' '.join(value)\n",
    "            print(text)\n",
    "            entities = extract_named_entities(text)\n",
    "            print(f'Entidades reconhecidas: {entities}')\n",
    "            entities\n",
    "            \n",
    "            for org in entities.get('ORG', []):\n",
    "                graph_model.add_entity(\"Organization\", org)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"AFFILIATED_WITH\", \"Organization\", org)\n",
    "                \n",
    "            for gpe in entities.get('GPE', []):\n",
    "                graph_model.add_entity(\"Location\", gpe)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "    projetos = flatten_dict(data_dict['Properties'])\n",
    "    for key, value in projetos.items():\n",
    "        if key in ['Projetos de pesquisa', 'Projetos de extensão', 'Projetos de desenvolvimento']:\n",
    "            entities = extract_named_entities(value)\n",
    "            \n",
    "            for org in entities.get('ORG', []):\n",
    "                graph_model.add_entity(\"Organization\", org)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"HAS_PROJECT_IN\", \"Organization\", org)\n",
    "                \n",
    "            for gpe in entities.get('GPE', []):\n",
    "                graph_model.add_entity(\"Location\", gpe)\n",
    "                graph_model.add_relation(\"Person\", person_name, \"LOCATED_IN\", \"Location\", gpe)\n",
    "\n",
    "    graph_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Processamento para eliminar strings duplicadas e vazias\n",
    "def remove_duplicates(entities):\n",
    "    for key in entities.keys():\n",
    "        seen = set()\n",
    "        unique_values = []\n",
    "        \n",
    "        for value in entities[key]:\n",
    "            lower_value = value.lower()\n",
    "            if lower_value not in seen and value.strip():\n",
    "                seen.add(lower_value)\n",
    "                unique_values.append(value)\n",
    "        \n",
    "        # Atualização do dicionário\n",
    "        entities[key] = unique_values\n",
    "    return entities\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    nlp = spacy.load('en_core_web_lg')  # Assuming the use of the small Portuguese model\n",
    "    doc = nlp(text)\n",
    "    entities = {'ORG': [], 'GPE': [], 'PHONE': [], 'URL': []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities.keys():\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    \n",
    "    unique_entities = remove_duplicates(entities)\n",
    "    \n",
    "    return unique_entities\n",
    "\n",
    "text = 'Pós-Doutor em Computação pela Sorbonne Université-Pierre et Marie Curie (França - 2020). Doutor em Ciência da Computação pela Universitat Politecnica de Catalunya (Espanha - 2005). Atualmente é professor titular da Universidade de Fortaleza - UNIFOR na qual é membro permanente dos programas de pós-graduação em Informática Aplicada (mestrado e doutorado) e do mestrado profissional em administração. Possui mais de 100 trabalhos publicados em conferências e periódicos nacionais e internacionais. Tem experiência na área de Ciência da Computação, com ênfase em Ciência de Dados e Teleinformática, atuando principalmente nos seguintes temas: Redes Complexas, Grafos de Conhecimento, Aprendizagem de Máquina, Internet das Coisas, Segurança e Blockchain. (Texto informado pelo autor)\\n'\n",
    "entities = extract_named_entities(text)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = node_raimir\n",
    "print(data_dict['Properties'].keys())\n",
    "\n",
    "main(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 3: Extrair Lista de Currículos Lattes</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de extração de lista de currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  #seletorcss=\"div.paginacao:nth-child(4) > a:nth-child(2)\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas\n",
    "\n",
    "def achar_busca(browser, delay):\n",
    "    '''\n",
    "    Função para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = browser.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)\n",
    "\n",
    "def preencher_busca(browser, delay, NOME):\n",
    "    '''\n",
    "    Função para passar o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)\n",
    "\n",
    "def definir_filtros(browser, delay, mestres=True, assunto=False):\n",
    "    '''\n",
    "    Clica nos check-boxes para definir os filtros de buscas\n",
    "    Para buscar por Assuntos usar parâmetro True, caso omitido fará busca por Nome por default\n",
    "    '''\n",
    "    from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import traceback   \n",
    "    \n",
    "    ## Aguardar carregar e clicar em checkbox de Assunto\n",
    "    try:\n",
    "        if mestres == True:\n",
    "            limite=2\n",
    "            ## Aguardar opção dropdown ser carregada e clicar em sua checkbox\n",
    "            css_buscar_demais = '#buscarDemais'\n",
    "            checkbox_buscar_demais = browser.find_element(By.CSS_SELECTOR, css_buscar_demais)\n",
    "            \n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, css_buscar_demais))),\n",
    "                   wait_ms=150,\n",
    "                   limit=limite, \n",
    "                   on_exhaust=(f'Problema clicar em {checkbox_buscar_demais}, {limite} tentativas sem sucesso.'))\n",
    "            \n",
    "            time.sleep(1)\n",
    "            checkbox_buscar_demais.click()\n",
    "            print(f'Clique efetuado em {checkbox_buscar_demais}')\n",
    "\n",
    "        # css_estrangeiros  = '#buscarEstrangeiros'\n",
    "\n",
    "        if assunto == True:\n",
    "            ## Implementar número de retentativas para casos de conexão muito instável\n",
    "            limite=5\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#buscaAssunto\"))).click(),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "        \n",
    "            ## Aguardar opção Atuação Profissional ser carregada e clicar em sua checkbox\n",
    "            xpath_atuacaoprofissional = \".//*[contains(text(), 'Atuação profissional')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_atuacaoprofissional))).click()\n",
    "\n",
    "            ## Aguardar opção Ciências da Saúde ser carregada e clicar em sua checkbox\n",
    "            xpath_cienciassaude = \".//*[contains(text(), 'Ciências da Saúde')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_cienciassaude))).click()\n",
    "            #browser.find_element_by_xpath(xpath_cienciassaude).click()        \n",
    "\n",
    "            ## Aguardar opção Enfermagem ser carregada e clicar em sua checkbox\n",
    "            xpath_enfermagem    = \".//*[contains(text(), 'Enfermagem')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_enfermagem))).click()\n",
    "            #browser.find_element_by_xpath(xpath_enfermagem).click()\n",
    "            aplicar_link  = browser.find_element(By.LINK_TEXT, 'Aplicar')\n",
    "            aplicar_link.click()\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f'Erro na função definir_filtros()')\n",
    "        print(e)\n",
    "        # traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        # print(e, traceback_str) \n",
    "\n",
    "def procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(browser)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            ## TO-DO inserir a crítica para o 'Stale file handle'\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(browser)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(browser)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(browser, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', browser\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, browser\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # browser.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = browser.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', browser\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, browser\n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, browser\n",
    "\n",
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        import re\n",
    "\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    title = soup.title.string if soup.title else \"Unknown\"\n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "\n",
    "\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    return header_node\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_data[key] = value\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that contains the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_lista(lista, mestres=True, assunto=False):\n",
    "    sucesso=[]\n",
    "    falhas=[]\n",
    "    duvidas=[]\n",
    "    tipo_erro=[]\n",
    "    curriculos=[]\n",
    "    rotulos=[]\n",
    "    conteudos=[]\n",
    "\n",
    "    delay=10\n",
    "    limite=3\n",
    "    instituicao = 'Fundação Oswaldo Cruz'\n",
    "    unidade     = 'Fiocruz Ceará'\n",
    "    termo       = 'Ministerio da Saude'\n",
    "\n",
    "    t0 = time.time()\n",
    "    browser = connect_driver(caminho)\n",
    "    for NOME in lista:\n",
    "        try:\n",
    "            preencher_busca(browser, delay, NOME)\n",
    "            elemento_achado, nome_falha, duvida, erro, browser = procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite)\n",
    "            link_nome     = achar_busca(browser, delay)\n",
    "            window_before = browser.current_window_handle\n",
    "            \n",
    "            if str(elemento_achado) == 'nan':\n",
    "                print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "                falhas.append(nome_falha)\n",
    "                duvidas.append(duvida)\n",
    "                tipo_erro.append(erro)\n",
    "                # print(nome_falha)\n",
    "                # print(erro)\n",
    "                # clear_output(wait=True)\n",
    "                raise Exception\n",
    "            print('Vínculo encontrado no currículo de nome:',elemento_achado.text)\n",
    "\n",
    "            ## Clicar no botão abrir currículo e mudar de aba\n",
    "            try:\n",
    "                ## Aguarda, encontra, clica em buscar nome\n",
    "                link_nome    = achar_busca(browser, delay)\n",
    "                nome_buscado = []\n",
    "                nome_achado  = []\n",
    "                nome_buscado.append(NOME)\n",
    "                \n",
    "                if link_nome.text == None:\n",
    "                    xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "                    # 'Stale file handle'\n",
    "                    print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "                    retry(WebDriverWait(browser, delay).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "                try:\n",
    "                    ActionChains(browser).click(link_nome).perform()\n",
    "                    nome_achado.append(link_nome.text)\n",
    "                except:\n",
    "                    print(f'Currículo não encontrado para: {NOME}.')\n",
    "                    return\n",
    "                \n",
    "                retry(WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "                \n",
    "                # Clicar botão para abrir o currículo\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                time.sleep(0.2)\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "\n",
    "                # Pega o código fonte da página\n",
    "                page_source = browser.page_source\n",
    "\n",
    "                # Usa BeautifulSoup para analisar\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                # Extração e Persistência do cabeçalho\n",
    "                dict_header = parse_header(soup)\n",
    "                header_node = persist_to_neo4j(dict_header)\n",
    "                \n",
    "                # Extração e Persistência dos elementos H1\n",
    "                graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "                parse_h1_elements(soup, header_node, graph)\n",
    "                graph, cv_node, properties = parse_parsoninfo(soup)                \n",
    "\n",
    "            except Exception as e:\n",
    "                print('Erro',e)\n",
    "                print('Tentando nova requisição ao servidor')\n",
    "                time.sleep(1)\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "                time.sleep(1)\n",
    "\n",
    "            sucesso.append(NOME)\n",
    "\n",
    "        except:\n",
    "            print(f'Currículo não encontrado: {NOME}')\n",
    "            browser.back()\n",
    "            continue\n",
    "\n",
    "    df_dados =pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(curriculos),\n",
    "        'ROTULOS': pd.Series(rotulos),\n",
    "        'CONTEUDOS': pd.Series(conteudos),\n",
    "            })\n",
    "    \n",
    "    t6=time.time()\n",
    "    print('='*95)\n",
    "    # print(f' {len(sucesso)} currículos extraídos com sucesso')\n",
    "    print(f' Tempo total para extrair {len(conteudos)} seções dos currículos: {tempo(t0,t6)}')\n",
    "    # print('='*95)\n",
    "    browser.quit()\n",
    "    \n",
    "    return df_dados, sucesso  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>FASE 4: Funções montar dicionários</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_data[key] = value\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that contains the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        data_dict = {}\n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "\n",
    "            higher_order_key = None  # Inicializa variável para armazenar a chave de ordem superior\n",
    "            data_list = []  # Inicialize lista para armazenar entradas de índices de dataframe\n",
    "            \n",
    "            parag_elements = parent_div.find_all('p')\n",
    "            if parag_elements:\n",
    "                for idx, elem in enumerate(parag_elements):\n",
    "                    class_name = elem.get('class', [None])[0]  # Assumes only one class; otherwise, join them into a single string\n",
    "                    higher_order_key = class_name\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "                    data_entry = {'rotulos': class_name, 'conteudos': elem.text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "\n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                kdict_elements = cell.find_all('b')\n",
    "                # print(len(kdict_elements))\n",
    "\n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    data_list = []  # Redefine a lista para a nova chave de ordem superior\n",
    "                    data_dict[higher_order_key] = data_list  # Cria nova lista para esta chave de ordem superior\n",
    "\n",
    "                index_elems   = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for key_elem, details_elem in zip(index_elems, details_elems):\n",
    "                    key_text     = key_elem.text.strip() if key_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    data_entry = {'rotulos': key_text, 'conteudos': details_text}\n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "                \n",
    "            if higher_order_key is None:\n",
    "                # Se nenhuma chave de ordem superior for encontrada, associa a lista de dados diretamente ao título\n",
    "                result_dict[title_text] = data_list\n",
    "            else:\n",
    "                # Caso contrário, associa o data_dict contendo chaves de ordem superior ao título\n",
    "                result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def generate_dataframe_and_neo4j_dict(data_dict):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame and a dictionary for Neo4j persistence, incorporating section and subsection names.\n",
    "\n",
    "    Parameters:\n",
    "        data_dict (dict): A nested dictionary containing section and subsection data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame aggregating all sections and subsections, with additional columns specifying their names.\n",
    "        dict: A dictionary intended for Neo4j persistence, formatted according to the Neo4j data model.\n",
    "    \"\"\"\n",
    "\n",
    "    all_frames = []  # List to store DataFrames corresponding to each section and subsection\n",
    "    neo4j_dict = {}  # Dictionary for Neo4j persistence\n",
    "\n",
    "    for section, items in data_dict.items():\n",
    "        if section:  # Exclude empty sections\n",
    "            neo4j_dict[section] = {}\n",
    "            if isinstance(items, list):  # If items is a list, convert to DataFrame\n",
    "                df = pd.DataFrame(items)\n",
    "                df['Section'] = section  # Append a column for the section name\n",
    "                all_frames.append(df)\n",
    "                neo4j_dict[section] = items  # For list items, add them as they are\n",
    "\n",
    "            elif isinstance(items, dict):  # If items is a dictionary, explore subsections\n",
    "                for subsection, subitems in items.items():\n",
    "                    if subitems:  # Exclude empty subsections\n",
    "                        df = pd.DataFrame(subitems)\n",
    "                        df['Subsection'] = subsection  # Append a column for the subsection name\n",
    "                        df['Section'] = section  # Append a column for the section name\n",
    "                        all_frames.append(df)\n",
    "                        neo4j_dict[section][subsection] = subitems  # Store subsection data\n",
    "\n",
    "    # Concatenate all DataFrames vertically to form one unified DataFrame\n",
    "    dataframe = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "    return dataframe, neo4j_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução das extrações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair todas seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = extract_data(soup)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair seções específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_articles = extrair_artigos(soup)\n",
    "dict_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_dict.items():\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe, neo4j_dict = generate_dataframe_and_neo4j_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exibição dos dataframes por seção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(neo4j_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Identificação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Endereço']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Formação acadêmica/titulação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Formação Complementar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Linhas de pesquisa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Projetos de desenvolvimento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Áreas de atuação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Idiomas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Atuação Profissional_Fundação Oswaldo Cruz, FIOCRUZ, Brasil.'\n",
    " 'Produções_Produção bibliográfica',\n",
    " 'Bancas_Participação em bancas de trabalhos de conclusão',\n",
    " 'Eventos_Participação em eventos, congressos, exposições e feiras',\n",
    " 'Orientações_Orientações e supervisões concluídas',\n",
    " 'Inovação_Projeto de desenvolvimento tecnológico']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa estruturação funcionou bem para as seções que tem anos como chaves, mas falha para em seções com subníveis de detalhamento, como nas produções e no resumo, onde os dados de valores não foram populados no dicionário embora as chaves tenham sido extraídas com sucesso.\n",
    "\n",
    "Pode ser por falta de algum elemento, classe ou marcador não apontada ou não hierarquizada corretamente na função de extração.\n",
    "\n",
    "Deve-se escolher outros marcadores para complementar a extração, ou usar o atual somente para seção 'Formação acadêmica/titulação'.\n",
    "\n",
    "Falhas não capturou corretamente:\n",
    "- subchaves de Atuação Profissional\n",
    "- subchaves de Orientações\n",
    "- subchaves de Produções pra Livros e Capítulos de livros\n",
    "\n",
    "Falhas não capturoud e forma alguma:\n",
    "- Projetos de Pesquisa\n",
    "- Inovação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não funcionou tão bem para a Atuação profissional pois não capturou chaves corretas\n",
    "atuaprof_df = neo4j_dict['Atuação Profissional_Fundação Oswaldo Cruz, FIOCRUZ, Brasil.']\n",
    "atuaprof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_df = neo4j_dict['Linhas de pesquisa']\n",
    "linhas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projdesv_df = neo4j_dict['Projetos de desenvolvimento']\n",
    "projdesv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projpesq_df = neo4j_dict['Áreas de atuação']\n",
    "projpesq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_dict['Produção bibliográfica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair marcadores CSS específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_classes_content(soup_element, target_classes):\n",
    "    result_dict = defaultdict(list)\n",
    "    \n",
    "    for t_class in target_classes:\n",
    "        for element in soup_element.find_all(class_=t_class):\n",
    "            text_content = element.text.strip()\n",
    "            result_dict[t_class].append(text_content)\n",
    "            \n",
    "    return dict(result_dict)\n",
    "\n",
    "target_classes = [\n",
    "        # 'infpessoa', \n",
    "        'nome', \n",
    "        'resumo', \n",
    "        # 'artigo-completo', \n",
    "        # 'cita-artigos', \n",
    "        # 'citacoes', \n",
    "        # 'detalhes', \n",
    "        # 'fator', \n",
    "        # 'foto', \n",
    "        # 'informacao-artigo', \n",
    "        # 'informacoes-autor', \n",
    "        # 'rodape-cv', \n",
    "        # 'science_cont', \n",
    "        # 'texto',\n",
    "        #  \n",
    "        # 'cita', \n",
    "        # 'trab'\n",
    "        ]\n",
    "\n",
    "result_dict = extract_classes_content(soup.body, target_classes)\n",
    "from pprint import pprint\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções para interagir com Neo4j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database credentials and URI\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    try:\n",
    "        graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    except:\n",
    "        print('Erro ao conectar ao Neo4j')\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'].split('(')[1].strip(')'), meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    print(type(header_node))\n",
    "\n",
    "    return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict, List\n",
    "\n",
    "def create_or_update_publications(graph: Graph, publications_dict: Dict[str, Dict[str, List[Dict[str, str]]]]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its associated publications in the Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        publications_dict (Dict[str, Dict[str, List[Dict[str, str]]]]): Dictionary containing publication information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    publications_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Search for existing researcher node by name\n",
    "    existing_node = matcher.match(\"Researcher\", name=publications_dict['Node Name']).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding publications.\")\n",
    "\n",
    "    # Process publications\n",
    "    for publication in publications_dict['Properties']['Produções']:\n",
    "        # Check if a similar publication already exists\n",
    "        existing_pub_node = matcher.match(\"Publication\", doi=publication['doi']).first()\n",
    "\n",
    "        if not existing_pub_node:\n",
    "            pub_node = Node(\"Publication\", **publication)\n",
    "            graph.create(pub_node)\n",
    "            publications_created += 1\n",
    "        else:\n",
    "            for key, value in publication.items():\n",
    "                if key not in existing_pub_node or existing_pub_node[key] != value:\n",
    "                    existing_pub_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            pub_node = existing_pub_node\n",
    "            graph.push(pub_node)\n",
    "\n",
    "        # Create or update relationship between researcher and publication\n",
    "        rel = Relationship(existing_node, \"PUBLISHED\", pub_node)\n",
    "        graph.merge(rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Publications created: {publications_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    if 'nome' in researcher_dict and 'resumo' in researcher_dict:\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['nome'][0] if isinstance(researcher_dict['nome'], list) else researcher_dict['nome']\n",
    "        summary = researcher_dict['resumo'][0] if isinstance(researcher_dict['resumo'], list) else researcher_dict['resumo']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, resumo=summary)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'nome' and 'resumo'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'nome': ['John Doe'], 'resumo': ['This is a summary.']}\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "# create_researcher_node(graph, researcher_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infopessoa(result_dict):\n",
    "    nome,_,_,endereco_lattes,_,idlattes,_,_,atualizacao = result_dict['infpessoa'][0].split('\\n')\n",
    "\n",
    "    dict_info = {\n",
    "        'NOME' : nome,\n",
    "        # 'link_lattes' : endereco_lattes.split(': ')[1],\n",
    "        'IDLATTES' : idlattes.split(': ')[1],\n",
    "        'ATUALIZAÇÃO' : atualizacao.split('Última atualização do currículo em ')[1],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(dict_info, index=[0]).T\n",
    "    df.columns = ['DADOS_CURRICULO']\n",
    "    return df, dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "\n",
    "def generate_lattes_dict(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a dictionary from a BeautifulSoup object to be persisted in Neo4j.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML/XML data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the relevant information for Neo4j persistence.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold the extracted data\n",
    "    lattes_data = {}\n",
    "    \n",
    "    # Extracting researcher's name as an example\n",
    "    name_section = soup.find('div', {'id': 'name-section'})\n",
    "    if name_section:\n",
    "        lattes_data['researcher_name'] = name_section.text.strip()\n",
    "    \n",
    "    # Extracting list of publications as an example\n",
    "    publications = []\n",
    "    for pub in soup.find_all('div', {'class': 'publication'}):\n",
    "        publication_data = {}\n",
    "        title = pub.find('span', {'class': 'title'})\n",
    "        authors = pub.find('span', {'class': 'authors'})\n",
    "        \n",
    "        if title:\n",
    "            publication_data['title'] = title.text.strip()\n",
    "        \n",
    "        if authors:\n",
    "            publication_data['authors'] = authors.text.strip().split(',')\n",
    "        \n",
    "        publications.append(publication_data)\n",
    "    \n",
    "    lattes_data['publications'] = publications\n",
    "    \n",
    "    # Additional extractions can be performed as per the requirements\n",
    "    \n",
    "    return lattes_data\n",
    "\n",
    "# Example usage (Assuming 'some_html_content' contains the HTML content)\n",
    "# soup = BeautifulSoup(some_html_content, 'html.parser')\n",
    "# lattes_dict = generate_lattes_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infopessoa, dict_pessoa = infopessoa(result_dict)\n",
    "df_infopessoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pessoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "from typing import Dict\n",
    "\n",
    "def create_researcher_node_from_dict(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "        \n",
    "        # Create a new node of type 'Researcher'\n",
    "        researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "        \n",
    "        # Add the node to the Neo4j database\n",
    "        graph.create(researcher_node)\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create a Researcher node in Neo4j based on the dictionary\n",
    "create_researcher_node_from_dict(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_researcher_node(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    nodes_created = 0\n",
    "    nodes_updated = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Validate the existence of required keys in the dictionary\n",
    "    required_keys = ['NOME', 'IDLATTES', 'ATUALIZAÇÃO']\n",
    "    if all(key in researcher_dict for key in required_keys):\n",
    "        # Extracting data from the dictionary\n",
    "        name = researcher_dict['NOME']\n",
    "        id_lattes = researcher_dict['IDLATTES']\n",
    "        last_updated = researcher_dict['ATUALIZAÇÃO']\n",
    "\n",
    "        # Create a NodeMatcher object\n",
    "        matcher = NodeMatcher(graph)\n",
    "\n",
    "        # Look for existing nodes with the same name\n",
    "        existing_node = matcher.match(\"Researcher\", name=name).first()\n",
    "\n",
    "        if existing_node:\n",
    "            # Update properties of existing node\n",
    "            for key, value in researcher_dict.items():\n",
    "                if key.lower() not in existing_node or existing_node[key.lower()] != value:\n",
    "                    existing_node[key.lower()] = value\n",
    "                    properties_updated += 1\n",
    "\n",
    "            # Push the changes to the database\n",
    "            graph.push(existing_node)\n",
    "            nodes_updated += 1\n",
    "\n",
    "        else:\n",
    "            # Create a new node of type 'Researcher'\n",
    "            researcher_node = Node(\"Researcher\", name=name, id_lattes=id_lattes, last_updated=last_updated)\n",
    "            # Add the node to the Neo4j database\n",
    "            graph.create(researcher_node)\n",
    "            nodes_created += 1\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"Nodes created: {nodes_created}\")\n",
    "        print(f\"Nodes updated: {nodes_updated}\")\n",
    "        print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "    else:\n",
    "        raise KeyError(\"The researcher dictionary must contain keys 'NOME', 'IDLATTES', and 'ATUALIZAÇÃO'.\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given researcher dictionary\n",
    "# researcher_dict = {'NOME': 'John Doe', 'IDLATTES': '0000000000000000', 'ATUALIZAÇÃO': '31/12/2023'}\n",
    "\n",
    "# Create or update a Researcher node in Neo4j based on the dictionary\n",
    "create_or_update_researcher_node(graph, dict_pessoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vinculo = extract_academic(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher, Relationship\n",
    "from typing import Dict\n",
    "\n",
    "def create_or_update_professional_links(graph: Graph, researcher_dict: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Create or update a researcher node and its professional links in the Neo4j database based on the given dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        researcher_dict (Dict[str, str]): A dictionary containing the researcher's professional information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters\n",
    "    relationships_created = 0\n",
    "    properties_updated = 0\n",
    "\n",
    "    # Create a NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "\n",
    "    # Look for existing nodes with the same name\n",
    "    existing_node = matcher.match(\"Researcher\", name=researcher_dict.get('Nome')).first()\n",
    "\n",
    "    if not existing_node:\n",
    "        raise ValueError(\"Researcher node must exist before adding professional links.\")\n",
    "\n",
    "    # Process professional affiliations and activities\n",
    "    for key, value in researcher_dict.items():\n",
    "        if key not in ['Nome', 'Endereço Profissional']:\n",
    "            # Create or find the organization/affiliation node\n",
    "            org_node = matcher.match(\"Organization\", name=key).first()\n",
    "            if not org_node:\n",
    "                org_node = Node(\"Organization\", name=key)\n",
    "                graph.create(org_node)\n",
    "\n",
    "            # Create or update the relationship\n",
    "            rel_type = \"AFFILIATED_WITH\" if 'Atual' in value else \"HAS_BEEN_AFFILIATED_WITH\"\n",
    "            rel = Relationship(existing_node, rel_type, org_node, details=value)\n",
    "\n",
    "            # Check if a similar relationship already exists\n",
    "            existing_rel = None\n",
    "            for r in graph.match((existing_node, org_node), r_type=rel_type):\n",
    "                if r['details'] == value:\n",
    "                    existing_rel = r\n",
    "                    break\n",
    "\n",
    "            # Create new or update existing relationship\n",
    "            if not existing_rel:\n",
    "                graph.create(rel)\n",
    "                relationships_created += 1\n",
    "            else:\n",
    "                for property_name, property_value in rel.items():\n",
    "                    if property_name not in existing_rel or existing_rel[property_name] != property_value:\n",
    "                        existing_rel[property_name] = property_value\n",
    "                        properties_updated += 1\n",
    "                graph.push(existing_rel)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Relationships created: {relationships_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update professional links in Neo4j based on the dictionary\n",
    "create_or_update_professional_links(graph, dict_vinculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('./../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_path = './../data/classificações_publicadas_todas_as_areas_avaliacao1672761192111.xlsx'\n",
    "xlsx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, NodeMatcher\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "def persist_journals_from_xlsx(graph: Graph, xlsx_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Persist journal information into a Neo4j database from an Excel (.xlsx) file.\n",
    "\n",
    "    Parameters:\n",
    "        graph (Graph): The py2neo Graph object connected to the Neo4j database.\n",
    "        xlsx_path (str): The path to the Excel (.xlsx) file containing the journal information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize counters for tracking\n",
    "    nodes_created = 0\n",
    "    properties_updated = 0\n",
    "    \n",
    "    # Create NodeMatcher object\n",
    "    matcher = NodeMatcher(graph)\n",
    "    \n",
    "    # Read the Excel file into a pandas DataFrame\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract journal properties\n",
    "        properties = {'ISSN': row['ISSN'],\n",
    "                      'Título': row['Título'],\n",
    "                      'Área de Avaliação': row['Área de Avaliação'],\n",
    "                      'Estrato': row['Estrato']}\n",
    "        \n",
    "        # Check if a node with the same ISSN already exists\n",
    "        existing_node = matcher.match(\"Journal\", ISSN=row['ISSN']).first()\n",
    "        \n",
    "        if existing_node:\n",
    "            for key, value in properties.items():\n",
    "                if key not in existing_node or existing_node[key] != value:\n",
    "                    existing_node[key] = value\n",
    "                    properties_updated += 1\n",
    "            graph.push(existing_node)\n",
    "        else:\n",
    "            new_node = Node(\"Journal\", **properties)\n",
    "            graph.create(new_node)\n",
    "            nodes_created += 1\n",
    "    \n",
    "    # Print statistical data\n",
    "    print(f\"Nodes created: {nodes_created}\")\n",
    "    print(f\"Properties updated: {properties_updated}\")\n",
    "\n",
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Given the path to the Excel file\n",
    "# xlsx_path = \"path/to/excel/file.xlsx\"\n",
    "\n",
    "# Persist the journal information from the Excel file\n",
    "persist_journals_from_xlsx(graph, xlsx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = pd.DataFrame([extract_academic(soup)]).T\n",
    "df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_projects = extract_research_project(soup)\n",
    "extracted_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência do cabeçalho\n",
    "header_data = parse_header(soup)\n",
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_node = \n",
    "# header_node = persist_to_neo4j(header_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_article = result_dict['artigo-completo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados=[]\n",
    "for i in result_dict['artigo-completo']:\n",
    "    dados.append(i.split('\\n\\n'))\n",
    "\n",
    "def quebra(linha):\n",
    "    return [x.split('\\n') for x in linha]\n",
    "\n",
    "for i in dados:\n",
    "    print(quebra(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\n",
    "        'artigo-completo', 'cita', 'cita-artigos', 'citacoes', 'detalhes', 'fator', \n",
    "        'foto', 'informacao-artigo', 'informacoes-autor', 'infpessoa', 'nome', \n",
    "        'resumo', 'rodape-cv', 'science_cont', 'texto', 'trab'\n",
    "        ]\n",
    "\n",
    "def extract_selected_classes(soup, target_classes):\n",
    "    \"\"\"\n",
    "    Extrai conteúdos de classes específicas de um objeto soup extraído de documento HTML.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Objeto soup e lista de classes a serem extraídas.\n",
    "\n",
    "    Retorno:\n",
    "    - Um dicionário que mapeia o nome da classe à lista de conteúdos extraídos.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Dicionário para armazenar os conteúdos\n",
    "    content_dict = defaultdict(list)\n",
    "    \n",
    "    # Iteração através das classes alvo para extração de conteúdo\n",
    "    for target_class in target_classes:\n",
    "        elements = soup.find_all(class_=target_class)\n",
    "        for element in elements:\n",
    "            print(element.text)\n",
    "            dado = element.text.split('\\n')\n",
    "\n",
    "            for i in dado:\n",
    "                if i != '':\n",
    "                    content_dict[target_class].append(dado)\n",
    "            \n",
    "    return dict(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\n",
    "        'informacao-artigo'\n",
    "        ]\n",
    "extract_selected_classes(soup, target_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outras formas de extrair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_tooltip_data(driver, element):\n",
    "#     # Use a biblioteca ActionChains para simular uma ação de mouse hover\n",
    "#     hover = ActionChains(driver).move_to_element(element)\n",
    "#     hover.perform()\n",
    "\n",
    "#     # Aguarde até que o tooltip seja exibido (ajuste o tempo limite conforme necessário)\n",
    "#     tooltip = WebDriverWait(driver, 10).until(\n",
    "#         EC.presence_of_element_located((By.CLASS_NAME, 'jcrTip'))\n",
    "#     )\n",
    "\n",
    "#     # Extraia o texto do tooltip\n",
    "#     tooltip_text = tooltip.text\n",
    "\n",
    "#     # Faça o parsing do texto do tooltip conforme necessário\n",
    "#     # Por exemplo, você pode dividir as informações usando '\\n' ou ':'\n",
    "#     info_list = tooltip_text.split('\\n')\n",
    "#     info_dict = {\n",
    "#         'data-issn': element.get_attribute('data-issn'),\n",
    "#         'original-title': info_list[0],\n",
    "#         'Fator de impacto (JCR 2022)': info_list[1].split(': ')[1]\n",
    "#     }\n",
    "\n",
    "#     return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_jcr_data(soup, result_list = []):\n",
    "#     from pprint import pprint\n",
    "\n",
    "#     sup_elements = soup.find_all('sup')\n",
    "#     print(len(sup_elements))\n",
    "\n",
    "#     for sup_element in sup_elements:\n",
    "#         img_element = sup_element.find('img')\n",
    "#         if img_element:\n",
    "#             data_issn = img_element.get('data-issn')\n",
    "#             original_title = img_element.get('original-title', '')\n",
    "            \n",
    "#             # Inicialize as informações como em branco\n",
    "#             data_issn_value = ''\n",
    "#             fator_de_impacto_value = ''\n",
    "#             apuracao_jcr_value = ''\n",
    "            \n",
    "#             # Verifique se as informações desejadas estão presentes e extraia-as\n",
    "#             if 'Fator de impacto (JCR 2021):' in original_title:\n",
    "#                 fator_de_impacto_start = original_title.find('Fator de impacto (JCR 2021): ') + len('Fator de impacto (JCR 2021): ')\n",
    "#                 fator_de_impacto_value = original_title[fator_de_impacto_start:]\n",
    "                \n",
    "#                 # Verifique se existe uma quebra de linha para separar a apuração do JCR\n",
    "#                 if '<br />' in fator_de_impacto_value:\n",
    "#                     fator_de_impacto_parts = fator_de_impacto_value.split('<br />')\n",
    "#                     fator_de_impacto_value = fator_de_impacto_parts[0]\n",
    "#                     apuracao_jcr_value = fator_de_impacto_parts[1]\n",
    "            \n",
    "#             # Crie um dicionário com as informações extraídas\n",
    "#             info_dict = {\n",
    "#                 'data-issn': data_issn,\n",
    "#                 'original-title': original_title,\n",
    "#                 'Fator de impacto': fator_de_impacto_value,\n",
    "#                 'Apuração JCR': apuracao_jcr_value\n",
    "#             }\n",
    "\n",
    "#             # Adicione o dicionário à lista de resultados\n",
    "#             result_list.append(info_dict)\n",
    "\n",
    "#     # Exiba a lista de dicionários\n",
    "#     return result_list\n",
    "\n",
    "\n",
    "# def extract_jcr_tag(sup_element, result_list = []):\n",
    "\n",
    "#     img_element = sup_element.find('img')\n",
    "#     if img_element:\n",
    "#         data_issn = img_element.get('data-issn')\n",
    "#         original_title = img_element.get('original-title', '')\n",
    "        \n",
    "#         # Inicialize as informações como em branco\n",
    "#         data_issn_value = ''\n",
    "#         fator_de_impacto_value = ''\n",
    "#         apuracao_jcr_value = ''\n",
    "        \n",
    "#         # Verifique se as informações desejadas estão presentes e extraia-as\n",
    "#         if 'Fator de impacto (JCR 2021):' in original_title:\n",
    "#             fator_de_impacto_start = original_title.find('Fator de impacto (JCR 2021): ') + len('Fator de impacto (JCR 2021): ')\n",
    "#             fator_de_impacto_value = original_title[fator_de_impacto_start:]\n",
    "            \n",
    "#             # Verifique se existe uma quebra de linha para separar a apuração do JCR\n",
    "#             if '<br />' in fator_de_impacto_value:\n",
    "#                 fator_de_impacto_parts = fator_de_impacto_value.split('<br />')\n",
    "#                 fator_de_impacto_value = fator_de_impacto_parts[0]\n",
    "#                 apuracao_jcr_value = fator_de_impacto_parts[1]\n",
    "        \n",
    "#         # Crie um dicionário com as informações extraídas\n",
    "#         info_dict = {\n",
    "#             'data-issn': data_issn,\n",
    "#             'original-title': original_title,\n",
    "#             'Fator de impacto': fator_de_impacto_value,\n",
    "#             'Apuração JCR': apuracao_jcr_value\n",
    "#         }\n",
    "\n",
    "#         # Adicione o dicionário à lista de resultados\n",
    "#         # result_list.append(info_dict)\n",
    "\n",
    "#     return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract(instback_text, elementos_layout_cell_1,elementos_layout_cell_11):\n",
    "#     for i,j in zip(elementos_layout_cell_1, elementos_layout_cell_11):\n",
    "#         if elementos_layout_cell_1 and elementos_layout_cell_11:\n",
    "#             key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "#             key_text = key.get_text().strip().replace('\\n',' ').replace('\\t','')\n",
    "    \n",
    "#             val = j.find('div', class_='layout-cell-pad-5')\n",
    "#             val_text = val.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "#             print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "#             if instback_text == 'Produção bibliográfica':\n",
    "#                 sup_element = j.find('sup')\n",
    "\n",
    "#                 if sup_element:\n",
    "#                     img_element = sup_element.find('img')\n",
    "#                     if img_element:\n",
    "#                         data_issn = img_element.get('data-issn')\n",
    "#                         original_title = img_element.get('original-title', '')\n",
    "                        \n",
    "#                         # Inicialize as informações como em branco\n",
    "#                         data_issn_value = ''\n",
    "#                         fator_de_impacto_value = ''\n",
    "#                         apuracao_jcr_value = ''\n",
    "                        \n",
    "#                         # Verifique se as informações desejadas estão presentes e extraia-as\n",
    "#                         if 'Fator de impacto (JCR 2021):' in original_title:\n",
    "#                             fator_de_impacto_start = original_title.find('Fator de impacto (JCR 2021): ') + len('Fator de impacto (JCR 2021): ')\n",
    "#                             fator_de_impacto_value = original_title[fator_de_impacto_start:]\n",
    "                            \n",
    "#                             # Verifique se existe uma quebra de linha para separar a apuração do JCR\n",
    "#                             if '<br />' in fator_de_impacto_value:\n",
    "#                                 fator_de_impacto_parts = fator_de_impacto_value.split('<br />')\n",
    "#                                 fator_de_impacto_value = fator_de_impacto_parts[0]\n",
    "#                                 apuracao_jcr_value = fator_de_impacto_parts[1]\n",
    "                        \n",
    "#                         # Crie um dicionário com as informações extraídas\n",
    "#                         detail_dict = {\n",
    "#                             'data-issn': data_issn,\n",
    "#                             'original-title': original_title,\n",
    "#                             'Fator de impacto': fator_de_impacto_value,\n",
    "#                             'Apuração JCR': apuracao_jcr_value\n",
    "#                         }\n",
    "#                         print(f'          {detail_dict}')\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_pairs(section, looking_class_key, looking_class_val, classes_to_ignore, aggregated_data_dict, verbose=False):\n",
    "#     \"\"\"Extraction of key-value pairs from HTML subsections for aggregation into a dictionary object.\n",
    "    \n",
    "#     Parameters:\n",
    "#         section (bs4.element.Tag): Root HTML tag from which traversal begins.\n",
    "#         looking_class_key (str): Class name to identify HTML elements containing keys.\n",
    "#         looking_class_val (str): Class name to identify HTML elements containing values.\n",
    "#         classes_to_ignore (str): Class name to identify HTML elements that mark the end of a subsection.\n",
    "#         aggregated_data_dict (dict): Aggregated data in dictionary format.\n",
    "    \n",
    "#     Returns:\n",
    "#         aggregated_data_dict (dict): Dictionary populated with key-value pairs from the HTML subsection.\n",
    "#     \"\"\"\n",
    "#     initial_class = section.find_next()\n",
    "#     class_found_0 = initial_class.get('class', [])\n",
    "#     class_found_1 = initial_class.find_next_sibling().get('class', [])\n",
    "#     class_found_2 = initial_class.find_next_sibling().find_next_sibling().get('class', [])\n",
    "#     class_found_3 = initial_class.find_next_sibling().find_next_sibling().find_next_sibling()\n",
    "    \n",
    "#     iteracao=0\n",
    "\n",
    "#     next_node = initial_class\n",
    "#     while True:\n",
    "#         next_node = next_node.find_next_sibling() if next_node.find_next_sibling() else None\n",
    "#         iteracao+=1\n",
    "#         # if verbose:\n",
    "#             # print('    \\nBuscando:',looking_class_key)\n",
    "#             # print('    Achados 0:',class_found_0)\n",
    "#             # print('    Achados 1:',class_found_1)\n",
    "#             # print('    Achados 2:',class_found_2)   \n",
    "#             # print('    Achados 3:',class_found_3)    \n",
    "#             # print(f'    Iteração: {iteracao:02}')\n",
    "\n",
    "#         if next_node is None:\n",
    "#             break \n",
    "#         if verbose:\n",
    "#             align_right = next_node.find('div', class_=\"text-align-right\") if next_node else None\n",
    "#             if align_right:\n",
    "#                 b_text = align_right.findChild('b') if align_right.findChild('b') else None\n",
    "#                 print('\\nClasses da próxima div:', b_text.get_text())\n",
    "                \n",
    "#         next_node_child = next_node.findChild() if next_node.findChild() else None\n",
    "#         if verbose:\n",
    "#             if next_node_child:\n",
    "#                 div_val  = next_node_child\n",
    "#                 cellpad5 = next_node_child.get('class', []) if next_node else None\n",
    "\n",
    "#                 if cellpad5:\n",
    "#                     # print('Classes div Filho da próxima div:',cellpad5)\n",
    "\n",
    "#                     # Remova as tags span da div\n",
    "#                     for span in div_val.find_all('span'):\n",
    "#                         span.extract()\n",
    "\n",
    "#                     # Obtenha o texto restante na div\n",
    "#                     texto = div_val.get_text(strip=True) \n",
    "\n",
    "#                     print('Valores div:',texto.strip().replace('\\n', ''))   \n",
    "#                     print(extract_jcr_tag(div_val.findChild('sup')))\n",
    "#                     print('-'*125)\n",
    "    \n",
    "#         if next_node_child is None:\n",
    "#             break\n",
    "    \n",
    "#         if next_node_child.has_attr('class') and classes_to_ignore in next_node['class']:\n",
    "#             classes1=next_node_child.get('class', [])\n",
    "#             print(f'Continuar: {classes1}')\n",
    "#             continue\n",
    "        \n",
    "#         if next_node_child.has_attr('class') and looking_class_key in next_node['class']:\n",
    "#             if verbose:\n",
    "#                 print('Entrou na condição de extração')\n",
    "#             key_node = next_node.findChildren('div').find('b')\n",
    "#             if verbose:\n",
    "#                 print('\\nKey:',key_node)        \n",
    "#             if key_node:\n",
    "#                 key_text = key_node.get_text().strip()\n",
    "#                 val_node = key_node.find_next_sibling('div', class_=looking_class_val)\n",
    "#                 if val_node:\n",
    "#                     val_text = val_node.get_text().strip()\n",
    "#                     if verbose:\n",
    "#                         print('\\Val:',val_text)   \n",
    "#                     aggregated_data_dict[key_text] = val_text\n",
    "    \n",
    "#     print(val_text)\n",
    "#     return aggregated_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def extract_data_from_cell(cell):\n",
    "    \"\"\"\n",
    "    Extracts relevant text data from a layout cell.\n",
    "    \"\"\"\n",
    "    texts = cell.stripped_strings\n",
    "    list_texts = [x.replace('\\n\\t\\t\\t\\t\\t\\t\\t',' ') for x in texts]\n",
    "    list_texts = [x.replace('\\t\\t\\t\\t\\t\\t',' ') for x in list_texts]\n",
    "    list_texts = [x.replace('\\t\\t\\t\\t\\t',' ') for x in list_texts]\n",
    "    list_texts = [x.replace('\\n',' ') for x in list_texts]\n",
    "    \n",
    "    return list_texts\n",
    "\n",
    "def parse_resume(soup):\n",
    "    result_dict = {}\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None \n",
    "    json_data['name'] = node_name\n",
    "\n",
    "    parag_elements = soup.find_all('p')\n",
    "    for elem in parag_elements:\n",
    "        # Retrieve the class attribute, returning None if not found\n",
    "        class_name = elem.get('class', [None])[0]\n",
    "        text_content = elem.text\n",
    "        \n",
    "        # Update the dictionary\n",
    "        if class_name:  # Only update if class_name is not None\n",
    "            result_dict[class_name] = text_content\n",
    "\n",
    "        json_data[\"Properties\"] = result_dict\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def parse_cita_artigos(soup):\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    json_data['name'] = node_name\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    div_cita = producao_bibliografica_div.find_patent()\n",
    "    producoes = []\n",
    "    for artigo_div in div_cita.find_all('div', {'class': 'cita-artigos'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        \n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['OutrasProduções'] = producoes\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def parse_jcr_articles(soup):\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    json_data['name'] = node_name\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    producoes = []\n",
    "    for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        \n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        # str_autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        # autores = split_authors(str_autores)\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        artigo_dict['ano']     = ano\n",
    "        # artigo_dict['autores'] = autores\n",
    "        artigo_dict['revista'] = revista\n",
    "        artigo_dict['titulo']  = titulo\n",
    "        artigo_dict['jcr']     = jcr\n",
    "        artigo_dict['doi']     = doi\n",
    "        \n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = producoes\n",
    "   \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sep_divs(soup):\n",
    "    titles_list=[]\n",
    "    h1 = soup.find_all('h1')\n",
    "    ignore=['','Currículo Lattes - Busca Textual - Visualização do Currículo']\n",
    "    for i in h1:\n",
    "        if i:\n",
    "            h1_text = i.get_text().strip()\n",
    "            if h1_text not in ignore:\n",
    "                titles_list.append(h1_text)\n",
    "\n",
    "    return titles_list\n",
    "\n",
    "def clean_texts(tag):\n",
    "    tag_text = tag.get_text().strip()\n",
    "    return tag_text\n",
    "\n",
    "def get_section_next(div):\n",
    "    inst_back = div.find('div', _class='inst_back')\n",
    "    inst_back_text = inst_back.get_text().strip()\n",
    "    return inst_back_text\n",
    "\n",
    "def get_section_prev(div):\n",
    "    inst_back = div.find_previous_sibling('div', _class='inst_back')\n",
    "    inst_back_text = inst_back.get_text().strip()\n",
    "    return inst_back_text\n",
    "\n",
    "def get_subsection_next(div):\n",
    "    cita_artigos = div.find('div', _class='cita-artigos')\n",
    "    cita_artigos_text = cita_artigos.get_text().strip()\n",
    "    return cita_artigos_text\n",
    "\n",
    "def get_subsection_prev(div):\n",
    "    cita_artigos = div.find_previous_sibling('div', _class='cita-artigos')\n",
    "    cita_artigos_text = cita_artigos.get_text().strip()\n",
    "    return cita_artigos_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articledata(data_div):\n",
    "    import numpy as np\n",
    "    if data_div:\n",
    "        info_dict = {}\n",
    "        \n",
    "        # Extracting data from span tags\n",
    "        year = data_div.find('span', {'data-tipo-ordenacao': 'ano'})\n",
    "        if year:\n",
    "            info_dict['Year'] = year.text\n",
    "\n",
    "        jcr = data_div.find('span', {'data-tipo-ordenacao': 'jcr'})\n",
    "        if jcr:\n",
    "            info_dict['JCR'] = jcr.text\n",
    "        else:\n",
    "            info_dict['JCR'] = np.NaN\n",
    "\n",
    "        numero_citacao = data_div.find('span', {'class': 'numero-citacao'})\n",
    "        numero_citacao = numero_citacao.text if numero_citacao else np.NaN\n",
    "        if numero_citacao:\n",
    "            info_dict['Cites'] = numero_citacao\n",
    "\n",
    "        author = data_div.find('span', {'data-tipo-ordenacao': 'autor'})\n",
    "        if author:\n",
    "            info_dict['Author'] = author.text\n",
    "\n",
    "        bold_author = data_div.find('b')\n",
    "        if bold_author:\n",
    "            info_dict['Bold Author'] = bold_author.text\n",
    "\n",
    "        # Extracting additional data like DOIs and other authors\n",
    "        doi_link = data_div.find('a', {'class': 'icone-producao icone-doi'})\n",
    "        if doi_link:\n",
    "            info_dict['DOI Link'] = doi_link.get('href')\n",
    "\n",
    "        # importance = data_div.find('span', {'data-tipo-ordenacao': 'importancia'})\n",
    "        # if importance:\n",
    "        #     info_dict['Importance'] = importance.text\n",
    "\n",
    "        # Displaying the extracted data\n",
    "        return info_dict\n",
    "    \n",
    "# def extract_pairs(subsection_div, looking_class_key, looking_class_val):\n",
    "#     data_dict = {}\n",
    "#     divs_cell1 = subsection_div.find_next_sibling(\"div\", class_=looking_class_key)\n",
    "#     print(f'        Quantidade de livros: {len(divs_cell1)}')\n",
    "#     key_orders_div  = divs_cell1.find('b')\n",
    "#     key_orders_txt  = key_orders_div.get_text().strip()\n",
    "#     values_div      = divs_cell1.find_next_sibling(\"div\", class_=looking_class_val)\n",
    "#     values_txt      = values_div.findChild().get_text().strip()\n",
    "#     data_dict[key_orders_txt] = values_txt\n",
    "#     next_div       = values_div.find_next_sibling().find_next_sibling()\n",
    "#     next_div_class = next_div.get('class')[1]\n",
    "#     if next_div_class == looking_class_key:\n",
    "#         extract_pairs(next_div, looking_class_key, looking_class_val)\n",
    "#     else:\n",
    "#         return data_dict\n",
    "\n",
    "# def extract_pairs(subsection_div, looking_class_key, looking_class_val, data_dict=None):\n",
    "#     if data_dict is None:\n",
    "#         data_dict = {}\n",
    "    \n",
    "#     divs_cell1 = subsection_div.find_next_sibling(\"div\", class_=looking_class_key)\n",
    "#     if divs_cell1 is None:\n",
    "#         return data_dict, None\n",
    "    \n",
    "#     key_orders_div  = divs_cell1.find('b')\n",
    "#     if key_orders_div:\n",
    "#         key_orders_txt  = key_orders_div.get_text().strip()\n",
    "#         values_div      = divs_cell1.find_next_sibling(\"div\", class_=looking_class_val)\n",
    "        \n",
    "#         if values_div:\n",
    "#             values_txt = values_div.findChild().get_text().strip() if values_div.findChild() else \"\"\n",
    "#             data_dict[key_orders_txt] = values_txt\n",
    "    \n",
    "#     next_div = divs_cell1.find_next_sibling()\n",
    "#     if next_div:\n",
    "#         next_div_class = ' '.join(next_div.get('class', []))\n",
    "        \n",
    "#         if looking_class_key in next_div_class.split():\n",
    "#             return extract_pairs(next_div, looking_class_key, looking_class_val, data_dict)\n",
    "    \n",
    "#     return data_dict, next_div_class        \n",
    "\n",
    "\n",
    "# def extract_pairs(current_div, looking_class_key, looking_class_val, aggregated_data_dict):\n",
    "#     if current_div is None:\n",
    "#         return aggregated_data_dict\n",
    "    \n",
    "#     next_div = current_div.find_next_sibling(\"div\", class_=looking_class_key)    \n",
    "#     if next_div is None or looking_class_key not in next_div.get('class', []):\n",
    "#         return aggregated_data_dict\n",
    "\n",
    "#     key_orders_div = next_div.find('b')\n",
    "#     if key_orders_div:\n",
    "#         key_orders_txt = key_orders_div.get_text().strip()\n",
    "#         values_div = next_div.find_next_sibling(\"div\", class_=looking_class_val)\n",
    "#         if values_div:\n",
    "#             values_txt = values_div.findChild().get_text().strip()\n",
    "#             aggregated_data_dict[key_orders_txt] = values_txt\n",
    "\n",
    "#     next_sibling_div = next_div.find_next_sibling()\n",
    "#     while next_sibling_div and 'clear' in next_sibling_div.get('class', []):\n",
    "#         next_sibling_div = next_sibling_div.find_next_sibling()\n",
    "\n",
    "#     return extract_pairs(next_sibling_div, looking_class_key, looking_class_val, aggregated_data_dict)\n",
    "\n",
    "\n",
    "def extract_pairs(main_div, recurrent_classes, looking_class_key, looking_class_val, classes_to_ignore, aggregated_data_dict):\n",
    "    \"\"\"\n",
    "    Function to iteratively extract key-value pairs from sibling div elements in the DOM.\n",
    "    \n",
    "    Parameters:\n",
    "        recurrent_classes (str): The class which indicates a new recurrent section in the DOM.\n",
    "        looking_class_key (str): The class whose first child's 'b' tag content serves as dictionary key.\n",
    "        looking_class_val (str): The class whose text content serves as dictionary value.\n",
    "        classes_to_ignore (str): The class which should interrupt the sibling iteration.\n",
    "        aggregated_data_dict (dict): A pre-existing dictionary to append the extracted data.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The aggregated dictionary containing the extracted data.\n",
    "    \"\"\"\n",
    "    sections = main_div.find_all('div', class_=recurrent_classes)\n",
    "    \n",
    "    for section in sections:\n",
    "        next_node = section\n",
    "        while True:\n",
    "            next_node = next_node.find_next_sibling()\n",
    "            if next_node is None:\n",
    "                break\n",
    "            if next_node.has_attr('class') and classes_to_ignore in next_node['class']:\n",
    "                break\n",
    "            if next_node.has_attr('class') and looking_class_key in next_node['class']:\n",
    "                key_node = next_node.find('div').find('b')\n",
    "                if key_node:\n",
    "                    key_text = key_node.get_text().strip()\n",
    "                    val_node = next_node.find_next_sibling('div', class_=looking_class_val)\n",
    "                    if val_node:\n",
    "                        val_text = val_node.get_text().strip()\n",
    "                        aggregated_data_dict[key_text] = val_text\n",
    "    return aggregated_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag\n",
    "\n",
    "def traverse_up(soup, parent_dict, current_section=None, root_properties=None):\n",
    "    \"\"\"\n",
    "    Traverses through the soup object recursively and populates the dictionary.\n",
    "    root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "    current_section: the current section being processed, allows to set subsections.\n",
    "    \"\"\"\n",
    "    section = None\n",
    "    subsection = None\n",
    "    section_dict = {}\n",
    "    subsection_dict = {}\n",
    "    \n",
    "    # Existing code for name extraction\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    if node_name is not None:\n",
    "        parent_dict['name'] = node_name\n",
    "\n",
    "    # Existing code for paragraph extraction\n",
    "    parag_elements = soup.find_all('p')\n",
    "    for elem in parag_elements:\n",
    "        class_name = elem.get('class', [None])[0]\n",
    "        text_content = elem.get_text()\n",
    "        if class_name:\n",
    "            parent_dict[class_name] = text_content\n",
    "\n",
    "    # Initialization of root_properties\n",
    "    if root_properties is None:\n",
    "        root_properties = parent_dict.setdefault('Properties', {})\n",
    "    \n",
    "    for child in soup.children:\n",
    "        if isinstance(child, Tag):\n",
    "            ## Segmento para extrair os dados de JCR dos artigos completos em periódicos\n",
    "            # if child.get('id') == 'artigos-completos':\n",
    "            #     section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")          \n",
    "            #     if section_elem:\n",
    "            #         section = section_elem.get_text().strip()\n",
    "            #         section_dict = root_properties.setdefault(section, {})\n",
    "\n",
    "            #     subsection_elem = child.find('b')\n",
    "            #     if subsection_elem:\n",
    "            #         subsection = subsection_elem.get_text().strip()\n",
    "            #         if section:  \n",
    "            #             subsection_dict = section_dict.setdefault(subsection, {})\n",
    "\n",
    "            #     jcr_articles_dict = parse_jcr_articles(soup)\n",
    "            #     if jcr_articles_dict:\n",
    "            #         if subsection:  \n",
    "            #             subsection_dict.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "            ## Segmento para extrair até Produções (não tem subseções)\n",
    "            if \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)\n",
    "\n",
    "                    sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "                    if sibling_cell:\n",
    "                        cell_values = extract_data_from_cell(sibling_cell)\n",
    "\n",
    "                        if current_section:\n",
    "                            root_properties[current_section][cell_key] = cell_values\n",
    "                        else:\n",
    "                            root_properties[cell_key] = cell_values\n",
    "\n",
    "            ## Segmento para extrair de Produções até o final\n",
    "            elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                section_elem = child.findParent().findParent().find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                if section_elem:\n",
    "                    section = section_elem.get_text().strip()\n",
    "                    section_dict = root_properties.setdefault(section, {})\n",
    "                elif section_elem == None:\n",
    "                    list_sub = ['Dissertação de mestrado','Tese de doutorado']\n",
    "                    subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        if subsection_elem.get_text().strip() in list_sub:\n",
    "                            section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                    if section_elem:\n",
    "                        section = section_elem.get_text().strip()\n",
    "                        section_dict = root_properties.setdefault(section, {})                \n",
    "                else:\n",
    "                    print('Erro ao seção')     \n",
    "\n",
    "                subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                if subsection_elem:\n",
    "                    subsection = subsection_elem.get_text().strip()\n",
    "                elif subsection_elem == None:\n",
    "                    subsection_elem = child.findParent().find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        subsection = subsection_elem.get_text().strip()                \n",
    "                else:\n",
    "                    print('Erro ao definir subseção')\n",
    "                \n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)    \n",
    "\n",
    "                if section:\n",
    "                    current_section_dict = section_dict.get(current_section, {})\n",
    "                else:\n",
    "                    current_section_dict = root_properties.get(current_section, {})\n",
    "                    \n",
    "                # Converte par dictionário se for lista\n",
    "                if isinstance(current_section_dict, list):\n",
    "                    current_section_dict = {i: item for i, item in enumerate(current_section_dict)}\n",
    "                    if section:\n",
    "                        section_dict[current_section] = current_section_dict\n",
    "                    else:\n",
    "                        root_properties[current_section] = current_section_dict\n",
    "                        \n",
    "                subsection_dict = current_section_dict.setdefault(subsection, {})\n",
    "                \n",
    "                # Retira valores da próxima div irmã que contém classe layout-cell-11\n",
    "                sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "                if sibling_cell:\n",
    "                    cell_values = extract_data_from_cell(sibling_cell)\n",
    "                    subsection_dict[cell_key] = cell_values\n",
    "\n",
    "            # Code for headers\n",
    "            title = child.find(\"h1\")\n",
    "            if title:\n",
    "                current_section = title.get_text().strip()\n",
    "                new_dict = {}\n",
    "                if section:\n",
    "                    section_dict[current_section] = new_dict\n",
    "                else:\n",
    "                    root_properties[current_section] = new_dict\n",
    "                \n",
    "                traverse_up(child, new_dict, current_section, root_properties)\n",
    "            else:\n",
    "                traverse_up(child, parent_dict, current_section, root_properties)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag\n",
    "\n",
    "def traverse_bottom(soup, parent_dict, current_section=None, root_properties=None):\n",
    "    \"\"\"\n",
    "    Traverses through the soup object recursively and populates the dictionary.\n",
    "    root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "    current_section: the current section being processed, allows to set subsections.\n",
    "    \"\"\"\n",
    "    section = None\n",
    "    subsection = None\n",
    "    section_dict = {}\n",
    "    subsection_dict = {}\n",
    "    \n",
    "    # Existing code for name extraction\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    if node_name is not None:\n",
    "        parent_dict['name'] = node_name\n",
    "\n",
    "    # Existing code for paragraph extraction\n",
    "    parag_elements = soup.find_all('p')\n",
    "    for elem in parag_elements:\n",
    "        class_name = elem.get('class', [None])[0]\n",
    "        text_content = elem.get_text()\n",
    "        if class_name:\n",
    "            parent_dict[class_name] = text_content\n",
    "\n",
    "    # Initialization of root_properties\n",
    "    if root_properties is None:\n",
    "        root_properties = parent_dict.setdefault('Properties', {})\n",
    "    \n",
    "    for child in soup.children:\n",
    "        if isinstance(child, Tag):\n",
    "            ## Segmento para extrair os dados de JCR dos artigos completos em periódicos\n",
    "            # if child.get('id') == 'artigos-completos':\n",
    "            #     section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")          \n",
    "            #     if section_elem:\n",
    "            #         section = section_elem.get_text().strip()\n",
    "            #         section_dict = root_properties.setdefault(section, {})\n",
    "\n",
    "            #     subsection_elem = child.find('b')\n",
    "            #     if subsection_elem:\n",
    "            #         subsection = subsection_elem.get_text().strip()\n",
    "            #         if section:  \n",
    "            #             subsection_dict = section_dict.setdefault(subsection, {})\n",
    "\n",
    "            #     jcr_articles_dict = parse_jcr_articles(soup)\n",
    "            #     if jcr_articles_dict:\n",
    "            #         if subsection:  \n",
    "            #             subsection_dict.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "            ## Segmento para extrair até Produções (não tem subseções)\n",
    "            if \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)\n",
    "\n",
    "                    sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "                    if sibling_cell:\n",
    "                        cell_values = extract_data_from_cell(sibling_cell)\n",
    "\n",
    "                        if current_section:\n",
    "                            root_properties[current_section][cell_key] = cell_values\n",
    "                        else:\n",
    "                            root_properties[cell_key] = cell_values\n",
    "\n",
    "            ## Segmento para extrair de Produções até o final\n",
    "            elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "                section_elem = child.findParent().findParent().find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                if section_elem:\n",
    "                    section = section_elem.get_text().strip()\n",
    "                    section_dict = root_properties.setdefault(section, {})\n",
    "                elif section_elem == None:\n",
    "                    list_sub = ['Dissertação de mestrado','Tese de doutorado']\n",
    "                    subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        if subsection_elem.get_text().strip() in list_sub:\n",
    "                            section_elem = child.find_previous_sibling(\"div\", class_=\"inst_back\")\n",
    "                    if section_elem:\n",
    "                        section = section_elem.get_text().strip()\n",
    "                        section_dict = root_properties.setdefault(section, {})                \n",
    "                else:\n",
    "                    print('Erro ao seção')     \n",
    "\n",
    "                subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                if subsection_elem:\n",
    "                    subsection = subsection_elem.get_text().strip()\n",
    "                elif subsection_elem == None:\n",
    "                    subsection_elem = child.findParent().find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "                    if subsection_elem:\n",
    "                        subsection = subsection_elem.get_text().strip()                \n",
    "                else:\n",
    "                    print('Erro ao definir subseção')\n",
    "                \n",
    "                cell_key = extract_data_from_cell(child)\n",
    "                if cell_key:\n",
    "                    cell_key = ' '.join(cell_key)    \n",
    "\n",
    "                if section:\n",
    "                    current_section_dict = section_dict.get(current_section, {})\n",
    "                else:\n",
    "                    current_section_dict = root_properties.get(current_section, {})\n",
    "                    \n",
    "                # Converte par dictionário se for lista\n",
    "                if isinstance(current_section_dict, list):\n",
    "                    current_section_dict = {i: item for i, item in enumerate(current_section_dict)}\n",
    "                    if section:\n",
    "                        section_dict[current_section] = current_section_dict\n",
    "                    else:\n",
    "                        root_properties[current_section] = current_section_dict\n",
    "                        \n",
    "                subsection_dict = current_section_dict.setdefault(subsection, {})\n",
    "                \n",
    "                # Retira valores da próxima div irmã que contém classe layout-cell-11\n",
    "                sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "                if sibling_cell:\n",
    "                    cell_values = extract_data_from_cell(sibling_cell)\n",
    "                    subsection_dict[cell_key] = cell_values\n",
    "\n",
    "            # Code for headers\n",
    "            title = child.find(\"h1\")\n",
    "            if title:\n",
    "                current_section = title.get_text().strip()\n",
    "                new_dict = {}\n",
    "                if section:\n",
    "                    section_dict[current_section] = new_dict\n",
    "                else:\n",
    "                    root_properties[current_section] = new_dict\n",
    "                \n",
    "                traverse_bottom(child, new_dict, current_section, root_properties)\n",
    "            else:\n",
    "                traverse_bottom(child, parent_dict, current_section, root_properties)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_empty_list(value):\n",
    "#     return isinstance(value, list) and not value\n",
    "\n",
    "# def convert_to_hashable(value):\n",
    "#     if is_empty_list(value):\n",
    "#         return None\n",
    "#     elif isinstance(value, list):\n",
    "#         return tuple(value)\n",
    "#     return value\n",
    "\n",
    "# def print_dictionary(dictionary, indent=0):\n",
    "#     \"\"\"\n",
    "#     Recursively prints nested dictionaries.\n",
    "\n",
    "#     Parameters:\n",
    "#         dictionary (dict): The dictionary to print.\n",
    "#         indent (int): The current indentation level for nested dictionaries.\n",
    "#     \"\"\"\n",
    "#     for key, value in dictionary.items():\n",
    "#         if isinstance(value, dict):\n",
    "#             print('  ' * indent + str(key) + ': ')\n",
    "#             print_dictionary(value, indent + 1)\n",
    "#         else:\n",
    "#             print('  ' * indent + str(key) + ': ' + str(value))\n",
    "\n",
    "# def append_to_key(dictionary, key, value):\n",
    "#     \"\"\"Appends a value to a list associated with a given key in a dictionary.\n",
    "#     If the key does not exist, a new list is created and the value is appended to it.\n",
    "#     \"\"\"\n",
    "#     if key in dictionary:\n",
    "#         if isinstance(dictionary[key], list):\n",
    "#             dictionary[key].append(value)\n",
    "#         else:\n",
    "#             dictionary[key] = [dictionary[key], value]\n",
    "#     else:\n",
    "#         dictionary[key] = [value]\n",
    "\n",
    "# def find_sibling_cell(cell):\n",
    "#     # Your logic here to find sibling cells\n",
    "#     return None\n",
    "\n",
    "# def find_main_cell(soup):\n",
    "#     main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "#     return main_cell\n",
    "\n",
    "# def traverse(child, parent_dict, current_section=None, root_properties=None):\n",
    "#     if root_properties is None:\n",
    "#         root_properties = {}\n",
    "    \n",
    "#     new_dict = {\"Label\": None, \"Properties\": {}}\n",
    "#     cell_key = convert_to_hashable(extract_data_from_cell(child))\n",
    "    \n",
    "#     if cell_key is not None:\n",
    "#         cell_values = extract_data_from_cell(find_sibling_cell(child))\n",
    "#         parent_dict[cell_key] = convert_to_hashable(cell_values)\n",
    "        \n",
    "#     for sub_child in child.find_all_next():\n",
    "#         traverse(sub_child, new_dict, current_section, root_properties)\n",
    "        \n",
    "#     root_properties[current_section] = merge_properties_ordered(root_properties.get(current_section, {}), new_dict)\n",
    "#     return root_properties\n",
    "\n",
    "# def parse_soup(soup):\n",
    "#     main_cell = find_main_cell(soup)\n",
    "#     merged_dict = {\"Label\": \"Person\", \"Properties\": {}}\n",
    "#     if main_cell:\n",
    "#         parsed_data = traverse(main_cell, merged_dict)\n",
    "#         return merge_properties_ordered(merged_dict, parsed_data)\n",
    "#     return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_properties_ordered(dic1, dic2):\n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    \"\"\"Merges the 'Properties' subdictionaries from dic1 and dic2 into a new ordered dictionary.\"\"\"\n",
    "    merged_dict = OrderedDict()\n",
    "    merged_dict['Label'] = 'Person'\n",
    "    merged_dict['name'] = dic1.get('name', '')\n",
    "    merged_dict['Properties'] = OrderedDict()\n",
    "\n",
    "    # Explicitly extract the 'Properties' subdictionaries from dic1 and dic2\n",
    "    prop1 = dic1.get('Properties', {})\n",
    "    prop2 = dic2.get('Properties', {})\n",
    "    \n",
    "    # Iterate over the keys of dic1 first to preserve their order\n",
    "    for key in prop1.keys():\n",
    "        merged_dict['Properties'][key] = prop1[key]\n",
    "\n",
    "    # Next, iterate over the keys of dic2, updating values and potentially inserting new keys\n",
    "    for key in prop2.keys():\n",
    "        val1 = merged_dict['Properties'].get(key)\n",
    "        val2 = prop2[key]\n",
    "        \n",
    "        # Merge logic: concatenate if both are lists, otherwise use the value from dic2\n",
    "        if isinstance(val1, list) and isinstance(val2, list):\n",
    "            merged_dict['Properties'][key] = val1 + val2\n",
    "        else:\n",
    "            merged_dict['Properties'][key] = val2\n",
    "\n",
    "    return merged_dict\n",
    "\n",
    "def parse_soup(soup):\n",
    "    resume_dict   = parse_resume(soup)\n",
    "    articles_dict = {\"Label\": \"Person\", \"name\":{}, \"Properties\": {}}\n",
    "    articles_dict = parse_jcr_articles(soup)\n",
    "    merged_dict   = merge_properties_ordered(resume_dict, articles_dict)\n",
    "\n",
    "    main_cell     = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "    generic_dict = {\"Label\": \"Person\", \"Properties\": {}}\n",
    "    if main_cell:\n",
    "        traverse_bottom(main_cell, generic_dict)\n",
    "\n",
    "    merged_dict = merge_properties_ordered(merged_dict, generic_dict)\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = parse_soup(soup)\n",
    "print(data_dict.keys())\n",
    "print(data_dict['Properties'].keys())\n",
    "print(data_dict['Properties']['Orientações'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['Properties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_subsection(section_title, parent_dict, data):\n",
    "#     sub_key = f\"Produções-{section_title}\"\n",
    "#     parent_dict[sub_key] = data\n",
    "\n",
    "# def traverse(soup, parent_dict, current_section=None, root_properties=None):\n",
    "#     \"\"\"\n",
    "#     Traverses through the soup object recursively and populates the dictionary.\n",
    "#     root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "#     current_section: the current section being processed, allows to set subsections.\n",
    "#     \"\"\"\n",
    "#     subsections = ['Artigos completos publicados em periódicos',\n",
    "#                    'Livros publicados/organizados ou edições',\n",
    "#                    'Trabalhos completos publicados em anais de congressos',\n",
    "#                    'Trabalhos de conclusão de curso de graduação',\n",
    "#                    'Participação em eventos, congressos, exposições e feiras',\n",
    "#                    'Trabalho de conclusão de curso de graduação'\n",
    "#                    ]\n",
    "\n",
    "#     if root_properties is None:\n",
    "#         root_properties = parent_dict.setdefault('Properties', {})\n",
    "\n",
    "#     for child in soup.children:\n",
    "#         if isinstance(child, Tag):\n",
    "#             # Additional case for JCR articles extraction\n",
    "#             if child.get('id') == 'artigos-completos':\n",
    "#                 jcr_articles_dict = parse_jcr_articles(soup)\n",
    "#                 if jcr_articles_dict:\n",
    "#                     root_properties.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "#             # Standard extraction for cells\n",
    "#             elif \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "\n",
    "#                     sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "#                     if sibling_cell:\n",
    "#                         cell_values = extract_data_from_cell(sibling_cell)\n",
    "#                         if current_section:\n",
    "#                             root_properties[current_section][cell_key] = cell_values\n",
    "#                         else:\n",
    "#                             root_properties[cell_key] = cell_values\n",
    "\n",
    "#             # Non-Standard extraction\n",
    "#             elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "\n",
    "#                     sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "#                     if sibling_cell:\n",
    "#                         cell_values = extract_data_from_cell(sibling_cell)\n",
    "#                         clean_values = [x.replace('\\n\\t\\t\\t\\t\\t\\t\\t',' ') for x in cell_values]\n",
    "\n",
    "#                         if current_section:\n",
    "#                             if isinstance(root_properties[current_section], list):\n",
    "#                                 converted_dict = {i: item for i, item in enumerate(root_properties[current_section])}\n",
    "#                                 root_properties[current_section] = converted_dict\n",
    "#                             root_properties[current_section][cell_key] = cell_values\n",
    "#                         else:\n",
    "#                             root_properties[cell_key] = clean_values\n",
    "\n",
    "#             # Handling headers to define sections\n",
    "#             title = child.find(\"h1\")\n",
    "#             if title:\n",
    "#                 current_section = title.get_text().strip()\n",
    "#                 new_dict = {}\n",
    "#                 root_properties[current_section] = new_dict\n",
    "#                 traverse(child, new_dict, current_section, root_properties)\n",
    "\n",
    "#             # Detecting the subsection\n",
    "#             subsection = child.find_previous(\"div\", class_=\"cita-artigos\").get_text().strip() if child.find_previous(\"div\", class_=\"cita-artigos\") else None\n",
    "#             if subsection in subsections:\n",
    "#                 print(subsection)\n",
    "\n",
    "#                 if isinstance(root_properties.get(current_section), list):\n",
    "#                     root_properties[current_section] = {i: item for i, item in enumerate(root_properties[current_section])}\n",
    "                    \n",
    "#                 new_key = f\"Produções-{subsection}\"\n",
    "\n",
    "#                 if current_section and isinstance(root_properties.get(current_section), dict):\n",
    "#                     # Check if subsection exists before attempting to pop it\n",
    "#                     if subsection in root_properties[current_section]:\n",
    "#                         root_properties[new_key] = root_properties[current_section].pop(subsection)\n",
    "#                 elif not current_section:\n",
    "#                     print(\"current_section is not defined.\")\n",
    "#                 else:\n",
    "#                     print(f\"Unexpected type for root_properties[{current_section}]: {type(root_properties[current_section])}\")\n",
    "            \n",
    "#             else:\n",
    "#                 traverse(child, parent_dict, current_section, root_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(soup): \n",
    "    # Extração de todas as classes no documento, armazenadas em uma lista\n",
    "    all_classes = [value for element in soup.find_all(class_=True) for value in element[\"class\"]]\n",
    "    \n",
    "    # Contagem de ocorrências de cada classe\n",
    "    class_count = Counter(all_classes)\n",
    "    \n",
    "    # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "    class_dict = dict(class_count)\n",
    "    \n",
    "    return class_dict\n",
    "\n",
    "def list_divs(soup):\n",
    "    div_elements = soup.find_all('div')\n",
    "    unique_classes = set()\n",
    "\n",
    "    for div in div_elements[1:]:\n",
    "        div_id = div.get('id', 'N/A')\n",
    "        class_list = div.get('class')\n",
    "        print(f'{div_id} {class_list}')\n",
    "        \n",
    "        if class_list:\n",
    "            for i in class_list:\n",
    "                cont = div.find('div',{'class': i})\n",
    "                try:\n",
    "                    text = cont.text.strip().replace('/n/n/n','/n').replace('/n/n','/n')\n",
    "                    print(f'  {text}')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            print('-'*50)\n",
    "            unique_classes.update(class_list)\n",
    "    \n",
    "    return unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_divs_with_hierarchy(tag, indent_level=0, verbose=False):\n",
    "    indent = \" \" * indent_level * 4  # Four spaces for each level of indentation\n",
    "    div_elements = tag.find_all('div', recursive=False)  # Find direct children only\n",
    "    unique_classes = set()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"{indent}Inspecting level {indent_level}, found {len(div_elements)} divs.\")\n",
    "\n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        if class_list:\n",
    "            unique_classes.update(class_list)\n",
    "            print(f\"{indent} {', '.join(class_list)}\")\n",
    "        else:\n",
    "            print(f\"{indent} None\")\n",
    "\n",
    "        # Recursive call to explore the children of this div\n",
    "        list_divs_with_hierarchy(div, indent_level + 1)\n",
    "\n",
    "    if indent_level == 0:  # Print unique classes only once, at the end of the initial call\n",
    "        print(\"\\nUnique classes:\")\n",
    "        for i in unique_classes:\n",
    "            print(f'    {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de classes únicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize BeautifulSoup with a sample HTML content\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_classes(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_divs(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achar atributos de classes e divs definidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# keys_dicts = soup.find_all('div', {'class':'layout-cell-pad-5'})\n",
    "# for i in keys_dicts:\n",
    "#     pprint(i.attrs)\n",
    "#     text = i.get_text().replace('\\n\\n\\n\\n','').replace('\\t\\t\\n\\t','').replace('\\t\\t\\t','').strip()\n",
    "#     pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achar classe específica (dicionário class) em elemento (div)\n",
    "# from pprint import pprint\n",
    "\n",
    "# # query0 = soup.select_one('div', {'class':'layout-cell-1'}).attrs\n",
    "# keys_dicts = soup.select('div', {'class':'text-align-right'})\n",
    "# for i in keys_dicts:\n",
    "#     pprint(i.attrs)\n",
    "#     pprint(i.get_text().replace('\\n\\n\\n\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achar classe específica (dicionário class) em elemento (div)\n",
    "# from pprint import pprint\n",
    "\n",
    "# # query0 = soup.select_one('div', {'class':'layout-cell-1'}).attrs\n",
    "# keys_dicts = soup.find_all('div', {'class':'text-align-right'})\n",
    "# for i in keys_dicts:\n",
    "#     pprint(i.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar a hierarquia com identação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "# body_tag = soup.body\n",
    "\n",
    "# # Then try calling list_divs_with_hierarchy on the <body> tag\n",
    "# list_divs_with_hierarchy(body_tag, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicial_element = soup.select_one('div', {'class':'title-wrapper'})\n",
    "# inicial_element = soup.find('div', {'class':'layout-cell-pad-main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_divs_with_hierarchy(inicial_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar atributos de elemento e classe especificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query1 = soup.select_one('img', {'class':'foto'}).attrs\n",
    "# pprint(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Extrair dados e seções específicas\n",
    "\n",
    "- extrair_dados(soup, verbose=False)\n",
    "- extrair_artigos(soup, verbose=False)\n",
    "- extrair_orientacoes(soup, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume(soup):\n",
    "    # <h2 class=\"nome\" tabindex=\"0\">Antonio Marcos Aires Barbosa</h2>\n",
    "    name_element = soup.find('h2', {'class': 'nome'})\n",
    "    node_title = name_element.text if name_element else \"Unknown\"\n",
    "    found_list = []\n",
    "    nodes = {}\n",
    "    properties = {}\n",
    "    recurrent_div = soup.find_all('div', {'title-wrapper'})\n",
    "\n",
    "    for div in recurrent_div:\n",
    "        parag_element = div.find('p')\n",
    "\n",
    "        if parag_element:\n",
    "            key   = parag_element.get('class')[0]\n",
    "            value = parag_element.text.strip('\\n')\n",
    "            properties[f'{key}'] = value\n",
    "            found_list.append(properties)\n",
    "            \n",
    "    nodes[node_title] = properties\n",
    "                       \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_resume(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "node_name = node_name_element.text if node_name_element else None\n",
    "node_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair estrutura de árvore recursivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "import json\n",
    "\n",
    "def extract_tree_structure(soup, level=0):\n",
    "    \"\"\" \n",
    "    Extrai recursivamente a estrutura de árvore de um objeto soup.\n",
    "    \n",
    "    Parâmetros:\n",
    "        soup (bs4.BeautifulSoup | bs4.Tag): O objeto soup ou tag para explorar.\n",
    "        level (int): Nível atual da árvore para fins de aninhamento.\n",
    "        \n",
    "    Retorna:\n",
    "        dict: Dicionário que representa a estrutura da árvore.\n",
    "    \"\"\"\n",
    "    \n",
    "    if soup is None:\n",
    "        return None\n",
    "    \n",
    "    tree_structure = {}\n",
    "    \n",
    "    if isinstance(soup, Tag):\n",
    "        tree_structure['name'] = soup.name\n",
    "        tree_structure['classes'] = soup.get('class', [])\n",
    "    \n",
    "    children_structure = []\n",
    "    for child in soup.children:\n",
    "        if isinstance(child, Tag):\n",
    "            child_structure = extract_tree_structure(child, level + 1)\n",
    "            children_structure.append(child_structure)\n",
    "    \n",
    "    if children_structure:\n",
    "        tree_structure['children'] = children_structure\n",
    "    \n",
    "    return tree_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicial_element = soup.find('div',{'class': 'content-wrapper'})\n",
    "# tree_structure = extract_tree_structure(inicial_element)\n",
    "# print(json.dumps(tree_structure, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_texts(soup):\n",
    "#     json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "    \n",
    "#     # Extract name from 'h2' tag as the first entry of the main dictionary\n",
    "#     name_tag = soup.find('h2', {'class': 'nome'})\n",
    "#     if name_tag:\n",
    "#         json_data['Properties']['name'] = name_tag.get_text().strip()\n",
    "    \n",
    "#     main_layout_cells = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "#     for main_cell in main_layout_cells:\n",
    "#         current_section = None\n",
    "#         current_subsection = None\n",
    "#         current_subtit = None\n",
    "\n",
    "#         # Handle 'h1' tags for sections\n",
    "#         h1_tags = main_cell.find_all('h1')\n",
    "#         for h1_tag in h1_tags:\n",
    "#             current_section = h1_tag.get_text().strip()\n",
    "#             json_data['Properties'].setdefault(current_section, {})\n",
    "        \n",
    "#         # Handle 'inst_back' tags for subsections\n",
    "#         inst_back_tags = main_cell.find_all('div', {'class': 'inst_back'})\n",
    "#         for inst_back_tag in inst_back_tags:\n",
    "#             current_subsection = inst_back_tag.get_text().strip()\n",
    "#             if current_section:\n",
    "#                 json_data['Properties'][current_section].setdefault(current_subsection, {})\n",
    "\n",
    "#         # # Handle 'subtit-1' tags for sub-subsections\n",
    "#         # subtit_tags = main_cell.find_all('div', {'class': 'subtit-1'})\n",
    "#         # for subtit_tag in subtit_tags:\n",
    "#         #     current_subtit = subtit_tag.get_text().strip()\n",
    "#         #     if current_section and current_subsection:\n",
    "#         #         json_data['Properties'][current_section][current_subsection].setdefault(current_subtit, {})\n",
    "        \n",
    "#         # Handle key-value pairs\n",
    "#         keys = main_cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "#         vals = main_cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "#         for key_elem, val_elem in zip(keys, vals):\n",
    "#             key_text = key_elem.get_text().strip()\n",
    "#             val_text = val_elem.get_text().strip()\n",
    "\n",
    "#             if current_subtit and current_subsection and current_section:\n",
    "#                 json_data['Properties'][current_section][current_subsection][current_subtit][key_text] = val_text\n",
    "#             elif current_subsection and current_section:\n",
    "#                 json_data['Properties'][current_section][current_subsection][key_text] = val_text\n",
    "#             elif current_section:\n",
    "#                 json_data['Properties'][current_section][key_text] = val_text\n",
    "\n",
    "#     return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_texts(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from py2neo import Node\n",
    "\n",
    "def list_secoes(soup, tag):\n",
    "    elements = soup.find_all(tag, {'tabindex': '0'})\n",
    "    for elem in elements:\n",
    "        text = elem.get_text().strip()\n",
    "        node = Node(\"H1Element\", text=text)\n",
    "        print(f'{text}')\n",
    "\n",
    "def list_subsection(soup, classe):\n",
    "    class_elements = soup.find_all('div', {classe})\n",
    "    for elem in class_elements:\n",
    "        class_text = elem.get_text().strip()\n",
    "        class_node = Node(\"Class_Element\", text=class_text)\n",
    "        print(f'{class_text}')\n",
    "\n",
    "def list_b(soup):\n",
    "    # Find all 'b' elements without filtering by tabindex\n",
    "    b_elements = soup.find_all('b')\n",
    "    for elem in b_elements:\n",
    "        # Retrieve the text content within the 'b' element and strip any leading/trailing white space\n",
    "        b_text = elem.get_text().strip()\n",
    "\n",
    "        # Locate the parent 'div' of the current 'b' element\n",
    "        parent_div = elem.find_parent('div')\n",
    "        \n",
    "        # Initialize variables to store class and id attributes\n",
    "        parent_div_class = None\n",
    "        parent_div_id = None\n",
    "\n",
    "        # Retrieve the class and id attributes if they exist\n",
    "        if parent_div is not None:\n",
    "            parent_div_class = parent_div.get('class', [None])[0]  # Retrieve the first class name if multiple are present\n",
    "            # parent_div_id = parent_div.get('id', None)  # Retrieve the id attribute\n",
    "\n",
    "        # The Node object creation step has been retained for further use\n",
    "        b_node = Node(\"BElement\", text=b_text)\n",
    "        \n",
    "        # Print the extracted text, along with the parent div's class and id\n",
    "        print(f'Text: {b_text:40} | Div: {parent_div_class}')\n",
    "\n",
    "def list_ascendants(soup, tag):\n",
    "    # Find all 'b' elements\n",
    "    b_elements = soup.find_all(tag)\n",
    "    for elem in b_elements:\n",
    "        # Retrieve the text content within the 'b' element, stripping any leading/trailing white space\n",
    "        b_text = elem.get_text().strip()\n",
    "\n",
    "        # Locate the parent element of the current 'b' element\n",
    "        parent_elem = elem.find_parent()\n",
    "        \n",
    "        # Locate the grandparent element of the current 'b' element\n",
    "        grandparent_elem = parent_elem.find_parent() if parent_elem is not None else None\n",
    "\n",
    "        # Retrieve the class names of parent and grandparent elements, returning None if not found\n",
    "        parent_class_name = parent_elem.get('class', [None])[0] if parent_elem is not None else None\n",
    "        grandparent_class_name = grandparent_elem.get('class', [None])[0] if grandparent_elem is not None else None\n",
    "\n",
    "        # Create a Node object for further use\n",
    "        b_node = Node(\"BElement\", text=b_text)\n",
    "\n",
    "        # Print the extracted text along with the class names of the parent and grandparent elements\n",
    "        print(f'{grandparent_class_name} | {parent_class_name} | Text: {b_text}')\n",
    "\n",
    "def parse_h1(soup, parent_node, graph):\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <h2 class=\"nome\" tabindex=\"0\">Antonio Marcos Aires Barbosa</h2>\n",
    "name_element = soup.find('h2', {'class': 'nome'})\n",
    "node_title = name_element.text if name_element else \"Unknown\"\n",
    "print(node_title)\n",
    "found_list = []\n",
    "nodes = {}\n",
    "properties = {}\n",
    "recurrent_div = soup.find_all('div', {'title-wrapper'})\n",
    "# print(len(recurrent_div))\n",
    "\n",
    "print('Valores de p')\n",
    "for div in recurrent_div:\n",
    "    parag_element = div.find('p')\n",
    "    if parag_element:\n",
    "        key   = parag_element.get('class', [None])[0]\n",
    "        value = parag_element.get_text().split('\\n')        \n",
    "        print(f'{key:22}: {value}')\n",
    "\n",
    "    sections = div.find_all('inst_back')\n",
    "    for section in sections:\n",
    "        title_element = section.get_text().strip()\n",
    "        if title_element:\n",
    "            print('\\nValores de inst_back')\n",
    "            print(len(title_element))\n",
    "\n",
    "for n,div in enumerate(recurrent_div):\n",
    "    print('\\nValores de h1')            \n",
    "    section_element = div.find('h1')\n",
    "    if section_element and n>0:\n",
    "        key = section_element.get_text().strip()\n",
    "        value = section_element.find_next().get_text().split('\\n\\n')\n",
    "        print(f'{key:22}: {value}')\n",
    "\n",
    "print('\\nValores de b')\n",
    "for div in recurrent_div:\n",
    "    subsections = div.find_all('b')\n",
    "    for element in subsections:\n",
    "        if element:\n",
    "            key = element.get_text().strip()\n",
    "            filtered_list = [x.strip() for x in element.find_next().get_text().split('\\n') if x.strip()]\n",
    "            value = filtered_list\n",
    "            print(f'{key:22}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "def extract_sections(soup):\n",
    "    master_dict = {}  # Initialize an empty dictionary to store the hierarchical data\n",
    "    \n",
    "    # Loop through sections designated by 'title-wrapper'\n",
    "    for section_elem in soup.find_all('div', {'class': 'title-wrapper'}):\n",
    "        section_title_elem = section_elem.find('h1')\n",
    "        section_title = section_title_elem.get_text().strip() if section_title_elem else None\n",
    "        \n",
    "        if section_title:\n",
    "            master_dict[section_title] = {}\n",
    "       \n",
    "            layouts = soup.find_all('div', {'class': ['layout-cell']})\n",
    "            print(len(layouts), 'células de layout')\n",
    "            # Loop to extract keys and values from specific classes\n",
    "            for layout in layouts:\n",
    "                subsection_title = layout.get_text().strip()\n",
    "                master_dict[section_title][subsection_title] = {}                    \n",
    "                tags_b = layout.find_all('b')\n",
    "                sections = layout.find_all('div', {'inst_back'})\n",
    "                subsections = layout.find_all('h1')\n",
    "                tags_a = layout.find_all('a')\n",
    "                cell05 = layout.find_all('div', {'layout-cell-pad-5 text-align-right'})\n",
    "                rights = layout.find_all('div', {'text-align-right'})\n",
    "                cell12 = layout.find_all('div', {'layout-cell-12'})\n",
    "                cell03 = layout.find_all('div', {'layout-cell-3'})\n",
    "                cell09 = layout.find_all('div', {'layout-cell-9'})\n",
    "                # print(len(tags_b), len(rights), len(sections), len(subsections), len(tags_a), len(cell12), len(cell05), len(cell03), len(cell09))\n",
    "\n",
    "                content_key = layout.get_text().replace('\\n', '').strip()\n",
    "                print('Classe:',layout.get('class', [None]))\n",
    "                print(' Dados:',content_key)\n",
    "                content_value = {}\n",
    "                master_dict[section_title][subsection_title][content_key] = content_value\n",
    "    return master_dict\n",
    "\n",
    "result_dict = extract_sections(soup)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts(soup, element):\n",
    "    json_data = {\"Label\": 'Person', \"Properties\": {}}\n",
    "\n",
    "    # Find the parent element that contains relevant sections\n",
    "    parent_element = element.find_parent() if element.find_parent() else soup\n",
    "    \n",
    "    current_section = None\n",
    "    current_subsection = None\n",
    "\n",
    "    # Iterate over each child of the parent element\n",
    "    for child in parent_element.children:\n",
    "        # Handling section tags (Level 1)\n",
    "        if child.name == 'h1':\n",
    "            current_section = child.get_text().strip()\n",
    "            json_data[\"Properties\"][current_section] = {}\n",
    "            current_subsection = None\n",
    "        # Handling subsection tags (Level 2)\n",
    "        elif child.get('class') == ['inst_back']:\n",
    "            if current_section:\n",
    "                current_subsection = child.find('b').get_text().strip()\n",
    "                json_data[\"Properties\"][current_section][current_subsection] = {}\n",
    "        # Handling layout cells for keys and values (Level 3)\n",
    "        elif child.get('class') == ['layout-cell']:\n",
    "            keys = child.find_all('div', {'class': 'layout-cell-3'}, limit=1)\n",
    "            vals = child.find_all('div', {'class': 'layout-cell-9'}, limit=1)\n",
    "            for key_elem, value_elem in zip(keys, vals):\n",
    "                key_text = key_elem.get_text().strip()\n",
    "                val_text = value_elem.get_text().strip()\n",
    "                target_dict = json_data[\"Properties\"]\n",
    "                if current_section:\n",
    "                    target_dict = target_dict[current_section]\n",
    "                    if current_subsection:\n",
    "                        target_dict = target_dict[current_subsection]\n",
    "                target_dict[key_text] = val_text\n",
    "                \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaves Dicionários Nível01: Seções, Nível02: Subseções, Nível03: Títulos\n",
    "def find_class_ascendants(soup,classe):\n",
    "    div_sec = soup.find_all('div',{'class':classe})\n",
    "    for i in div_sec:\n",
    "        level0 = i.find_parent().find_parent().find_parent().find_parent()\n",
    "        level1 = i.find_parent().find_parent().find_parent()\n",
    "        level2 = i.find_parent().find_parent()\n",
    "        level3 = i.find_parent()\n",
    "        print(f'{level0.name} {level0.get(\"class\", [None])}')\n",
    "        print(f'  {level1.name} {level1.get(\"class\", [None])}')\n",
    "        print(f'    {level2.name} {level2.get(\"class\", [None])}')\n",
    "        print(f'      {level3.name} {level3.get(\"class\", [None])}')\n",
    "        print(f'        {i.name} {i.get(\"class\", [None])}')\n",
    "        print(f'        {i.get_text().strip()}')\n",
    "\n",
    "def find_tag_ascendants(soup,tag):\n",
    "    div_sec = soup.find_all(tag)\n",
    "    for i in div_sec:\n",
    "        level0 = i.find_parent().find_parent().find_parent().find_parent()\n",
    "        level1 = i.find_parent().find_parent().find_parent()\n",
    "        level2 = i.find_parent().find_parent()\n",
    "        level3 = i.find_parent()\n",
    "        print(f'{level0.name} {level0.get(\"class\", [None])}')\n",
    "        print(f'  {level1.name} {level1.get(\"class\", [None])}')\n",
    "        print(f'    {level2.name} {level2.get(\"class\", [None])}')\n",
    "        print(f'      {level3.name} {level3.get(\"class\", [None])}')\n",
    "        print(f'        {i.name} {i.get(\"class\", [None])}')\n",
    "        print(f'        {i.get_text().strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_class_ascendants(soup, 'subtit-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão as chave de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'layout-cell-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'layout-cell-pad-5 text-align-right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'layout-cell-9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_tag_ascendants(soup,'h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_tag_ascendants(soup,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'subtit-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe onde estão os valores de dicionário em máximo nível de detalhe\n",
    "# find_class_ascendants(soup,'inst_back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.select_one('layout-cell layout-cell-12 layout-cell-pad-main'))\n",
    "# print(soup.select_one('div.layout-cell.layout-cell-12'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaves Dicionários Nível 02 - Subseções\n",
    "div_tit = soup.find_all('h1')\n",
    "for i in div_tit:\n",
    "    tit_text = i.get_text().strip()\n",
    "    print(tit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaves Dicionários Nível 02 - Subseções\n",
    "div_tit = soup.find_all('div', 'inst_back')\n",
    "for i in div_tit:\n",
    "    tit_text = i.find('b').get_text().strip()\n",
    "    print(tit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    master_dict = {}\n",
    "    \n",
    "    # Extract sections\n",
    "    section_elements = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    for section_elem in section_elements:\n",
    "        section_title_elem = section_elem.find('h1')\n",
    "        section_title = section_title_elem.get_text().strip() if section_title_elem else None\n",
    "        \n",
    "        if section_title:\n",
    "            master_dict[section_title] = {}\n",
    "            \n",
    "            # Extract subsections\n",
    "            subsection_elements = section_elem.find_all('div', {'class': 'inst_back'})\n",
    "            for subsection_elem in subsection_elements:\n",
    "                subsection_title = subsection_elem.get_text().strip()\n",
    "                master_dict[section_title][subsection_title] = {}\n",
    "                \n",
    "                # Extract key-value pairs using classes you've defined for the last level\n",
    "                layout_cells = subsection_elem.find_parent().find_all('div', {'layout-cell'})\n",
    "                \n",
    "                for layout_cell in layout_cells:\n",
    "                    data_dict = extract_texts(layout_cell)\n",
    "                    master_dict[section_title][subsection_title] = data_dict\n",
    "                        \n",
    "    return master_dict\n",
    "\n",
    "result_dict = extract_data(soup)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_sections(soup):\n",
    "    # Initialize an empty dictionary to hold the hierarchical data\n",
    "    master_dict = {}\n",
    "    \n",
    "    # Find all elements that are considered as sections\n",
    "    section_elements = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    for section_elem in section_elements:\n",
    "        # Extract section title if available\n",
    "        section_title_elem = section_elem.find('h1')\n",
    "        section_title = section_title_elem.get_text().strip() if section_title_elem else None\n",
    "        \n",
    "        if section_title:\n",
    "            master_dict[section_title] = {}\n",
    "            \n",
    "            # Extract subsections within each section\n",
    "            subsection_elements = section_elem.find_all('div', {'class': 'inst_back'})\n",
    "            for subsection_elem in subsection_elements:\n",
    "                subsection_title = subsection_elem.get_text().strip()\n",
    "                master_dict[section_title][subsection_title] = {}\n",
    "                \n",
    "                # Extract content within each subsection and store as key-value pairs\n",
    "                content_elements_par = subsection_elem.find_all('p')\n",
    "                content_elements_b = subsection_elem.find_all('b')\n",
    "                for content_elem in content_elements_par:\n",
    "                    content_key = content_elem.get('class')[0] if content_elem.get('class') else None\n",
    "                    content_value = content_elem.get_text().strip()\n",
    "                    \n",
    "                    if content_key and content_value:\n",
    "                        master_dict[section_title][subsection_title][content_key] = content_value\n",
    "\n",
    "                for content_elem in content_elements_b:\n",
    "                    content_key = content_elem.get('class')[0] if content_elem.get('class') else None\n",
    "                    content_value = content_elem.get_text().strip()\n",
    "                    \n",
    "                    if content_key and content_value:\n",
    "                        master_dict[section_title][subsection_title][content_key] = content_value\n",
    "                        \n",
    "    return master_dict\n",
    "\n",
    "result_dict = extract_sections(soup)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = soup.find_all('div', {'title-wrapper'})\n",
    "\n",
    "for elem in elements:\n",
    "    par = elem.find('p')\n",
    "    if par:\n",
    "        key = par.get('class')[0]\n",
    "        val = par.get_text().strip()\n",
    "        print(f\"{key}: {val}\")  \n",
    "    \n",
    "    tit = elem.find('h1')\n",
    "    if tit:\n",
    "        section = tit.get_text()\n",
    "        if section != '':\n",
    "            key = tit.get_text().strip()\n",
    "            if tit.find_next('b'):\n",
    "                for i in tit.find_next('b'):\n",
    "                    val = i.get_text()\n",
    "                    if val.strip() != '':\n",
    "                        print(val)\n",
    "            lin = tit.find('b')\n",
    "            print(f\"{key:35}: {val}\")\n",
    "        subsect = soup.find('inst_back')\n",
    "        if subsect:\n",
    "            print(len(subsect))\n",
    "            print(f\"{subsect.get_text().strip()}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_secoes(soup,'h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subsection(soup, 'inst_back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ascendants(soup,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alldata(soup):\n",
    "    name_element = soup.find('h2', {'class': 'nome'})\n",
    "    node_title = name_element.text if name_element else \"Unknown\"\n",
    "    found_list = []\n",
    "    nodes = {}\n",
    "    properties = {}\n",
    "    recurrent_div = soup.find_all('div', {'title-wrapper'})\n",
    "\n",
    "    for div in recurrent_div:\n",
    "        parag_element = div.find('p')\n",
    "        title_element = div.find_all('inst_back')\n",
    "\n",
    "        if parag_element:\n",
    "            key   = parag_element.get('class')[0]\n",
    "            value = parag_element.text.strip('\\n')\n",
    "            properties[f'{key}'] = value\n",
    "            found_list.append(properties)\n",
    "\n",
    "        if title_element:\n",
    "            key   = title_element.get('class')\n",
    "            value = title_element.text.strip('\\n')\n",
    "            properties[f'{key}'] = value\n",
    "            found_list.append(properties)\n",
    "            \n",
    "    nodes[node_title] = properties\n",
    "                       \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(get_alldata(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes de extração de seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_dados(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node\": node_name, \"Properties\": {}}\n",
    "    \n",
    "    # Step 3: Traverse to Find Sections\n",
    "    json_data = extrair_wraper(soup, json_data)\n",
    "\n",
    "    # Step 4: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    producoes = []\n",
    "    for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        artigo_dict['ano']     = ano\n",
    "        artigo_dict['autores'] = autores\n",
    "        artigo_dict['revista'] = revista\n",
    "        artigo_dict['titulo']  = titulo\n",
    "        artigo_dict['jcr']     = jcr\n",
    "        artigo_dict['doi']     = doi\n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = producoes\n",
    "\n",
    "    projdevtec=[]\n",
    "    for projdevtec_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        projdevtec_dict = {}\n",
    "        projdevtec.append(projdevtec_dict)\n",
    "        json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projdevtec\n",
    "\n",
    "    # projpesq=[]\n",
    "    # for projpesq_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "    #     projpesq_dict = {}\n",
    "    \n",
    "    \n",
    "    #     projpesq.append(projdevtec_dict)\n",
    "    #     json_data[\"Properties\"][\"Inovação\"][\"Projeto de desenvolvimento tecnológico\"] = projpesq\n",
    "\n",
    "    orientacoes=[]\n",
    "    for orientacoes_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        orientacoes_dict = {}   \n",
    "        orientacoes.append(orientacoes_dict)\n",
    "        json_data[\"Properties\"][\"Orientações\"] = orientacoes\n",
    "\n",
    "    bancas=[]\n",
    "    for bancas_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        bancas_dict = {}\n",
    "        bancas.append(bancas_dict)\n",
    "        json_data[\"Properties\"][\"Bancas\"] = bancas\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_orientacoes(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node\": node_name, \"Properties\": {}}\n",
    "\n",
    "    # Step 3: Information extraction: Bibliographic Production Section\n",
    "    general_div = soup.find('div', {'class': 'Orientações e supervisões concluídas'})\n",
    "    if verbose:\n",
    "        if general_div:\n",
    "            print(f\"Qte.Divs: {len(general_div)}\")\n",
    "        else:\n",
    "            print('Nenhuma div encontrada com essa hierarquia')\n",
    "            return\n",
    "    orientacoes=[]\n",
    "    for selected_div in general_div.find_all('div', {'class': 'identity'}):\n",
    "        orientacoes_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = selected_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = selected_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = selected_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = selected_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = selected_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        \n",
    "        dados   = selected_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        str_autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        autores = split_authors(str_autores)\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        orientacoes_dict['ano']     = ano\n",
    "        orientacoes_dict['autores'] = autores\n",
    "        orientacoes_dict['revista'] = revista\n",
    "        orientacoes_dict['titulo']  = titulo\n",
    "        orientacoes_dict['jcr']     = jcr\n",
    "        orientacoes_dict['doi']     = doi\n",
    "        orientacoes.append(orientacoes_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = orientacoes\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "node_name = node_name_element.text if node_name_element else 'Unknown'\n",
    "\n",
    "text_contents = []\n",
    "elements_list = soup.find_all('b')\n",
    "print(len(elements_list))\n",
    "for i in elements_list:\n",
    "    # Check if text content matches the targeted string\n",
    "    if i.text == 'Orientações e supervisões concluídas':\n",
    "        # Retrieve the parent element\n",
    "        parent_element = i.find_parent()\n",
    "        \n",
    "        # Retrieve the name of the parent element\n",
    "        parent_name = parent_element.name if parent_element else 'Unknown'\n",
    "        \n",
    "        # Retrieve and store the text contents of all child elements of the parent\n",
    "        for child in parent_element.find_all(True):\n",
    "            text_contents.append(child.text)\n",
    "        \n",
    "        print(f\"Found in parent element: {parent_name}\")\n",
    "        print(\"Texts of all child elements:\", text_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in elements_list:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrair_orientacoes(soup, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções: Extrair do DOM Dados Identificação\n",
    "\n",
    "- parse_personinfo(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_personinfo(soup):\n",
    "    # Step 1: Identify Node Name\n",
    "    properties = {}\n",
    "\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    # div_informacoesautor = soup.find(\"ul\", {\"class\": \"informacoes-autor\"})\n",
    "    if node_name:\n",
    "        properties['name'] = node_name\n",
    "    for li in soup.find_all('li'):\n",
    "        text_content = li.text  # Extract the text content of the 'li' element\n",
    "        span_class = li.span['class'][0] if li.span else 'Unknown'  # Extract the class of the span within the 'li', if present\n",
    "\n",
    "        # Populate dictionary based on span class\n",
    "        if span_class == 'img_link':\n",
    "            properties['CV_URL'] = text_content.replace('Endereço para acessar este CV: ', '').strip()\n",
    "        elif span_class == 'img_identy':\n",
    "            properties['ID_Lattes'] = li.find('span', {'style': 'font-weight: bold; color: #326C99;'}).text.strip() if li.find('span', {'style': 'font-weight: bold; color: #326C99;'}) else 'Unknown'\n",
    "        elif span_class == 'img_cert':\n",
    "            date_str = text_content.replace('Última atualização do currículo em ', '').strip()\n",
    "            \n",
    "            # Assuming that the date string is in the format 'DD/MM/YYYY', parse it into a datetime object\n",
    "            # Modify the format string according to the actual format of date_str\n",
    "            parsed_date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "            # Convert the datetime object to ISO 8601 format (YYYY-MM-DD)\n",
    "            iso_date = parsed_date.strftime('%Y-%m-%d')\n",
    "            properties['Last_Updated'] = iso_date\n",
    "\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Initialize the Neo4j driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "n4j_driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "def create_unique_composite_constraint(tx, label, properties):\n",
    "    # Build the updated query string for creating the unique composite constraint\n",
    "    # Note: The 'properties' parameter is expected to be a list of property names.\n",
    "    prop_string = \", \".join([f\"n.{prop}\" for prop in properties])\n",
    "    constraint_name = \"_\".join([label.lower()] + [prop.lower() for prop in properties])\n",
    "    \n",
    "    query = (\n",
    "        f\"CREATE CONSTRAINT {constraint_name} \"\n",
    "        f\"IF NOT EXISTS FOR (n:{label}) REQUIRE ({prop_string}) IS NODE KEY\"\n",
    "    )\n",
    "    # Execute the transaction to create the constraint\n",
    "    tx.run(query)\n",
    "\n",
    "def create_or_update_node(tx, node_label, properties):\n",
    "    find_query = f\"MATCH (n:{node_label} {{name: $name}}) RETURN n\"\n",
    "    existing_nodes = list(tx.run(find_query, name=properties['name']))\n",
    "\n",
    "    if existing_nodes:\n",
    "        existing_node = existing_nodes[0]['n']\n",
    "        if existing_node['Last_Updated'] < properties['Last_Updated']:\n",
    "            update_query = (\n",
    "                f\"MATCH (n:{node_label} {{name: $name}}) \"\n",
    "                f\"SET n += $properties\"\n",
    "            )\n",
    "            tx.run(update_query, properties=properties, name=properties['name'])\n",
    "    else:\n",
    "        create_query = f\"CREATE (n:{node_label} $properties)\"\n",
    "        tx.run(create_query, properties=properties)\n",
    "\n",
    "def update(n4j_driver, person_properties, verbose=False):\n",
    "    with n4j_driver.session() as session:\n",
    "        try:\n",
    "            session.write_transaction(create_or_update_node, 'Person', person_properties)\n",
    "            if verbose:\n",
    "                print(\"Operation completed:\")\n",
    "                print(f\"{person_properties.values()} has been evaluated for creation or update.\")\n",
    "        except Exception as e:\n",
    "            pprint(f\"An error occurred: {e}\")\n",
    "\n",
    "def augment_node(tx, node_label, node_name, properties_to_add):\n",
    "    query = (\n",
    "        f\"MATCH (n:{node_label} {{name: $node_name}}) \"\n",
    "        f\"SET n += $properties_to_add \"\n",
    "        f\"RETURN n\"\n",
    "    )\n",
    "    result = tx.run(query, node_name=node_name, properties_to_add=properties_to_add)\n",
    "    return [record['n'] for record in result]\n",
    "\n",
    "def update_node_with_new_properties(n4j_driver, node_label, node_name, properties_to_add, verbose=False):\n",
    "    with n4j_driver.session() as session:\n",
    "        try:\n",
    "            updated_nodes = session.write_transaction(augment_node, node_label, node_name, properties_to_add)\n",
    "            if verbose:\n",
    "                print(\"Operation completed:\")\n",
    "                for node in updated_nodes:\n",
    "                    print(f\"Updated Node: {node}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Function for parsing resume and returning JSON\n",
    "def parse_resume(soup, verbose=False):\n",
    "    # Existing parsing logic...\n",
    "    # Assuming json_data contains the parsed data\n",
    "    json_data = {\"Node\": \"node_name\", \"Properties\": {\"property_1\": \"value_1\"}}\n",
    "\n",
    "    # Update the Node with new properties\n",
    "    update_node_with_new_properties(\n",
    "        n4j_driver, \n",
    "        'Person', \n",
    "        json_data['Node'], \n",
    "        json_data['Properties'], \n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar a restrição nome-data_atualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session and Transaction for creating unique composite constraint\n",
    "with n4j_driver.session() as session:\n",
    "    session.write_transaction(create_unique_composite_constraint, 'Person', ['name', 'Last_Updated'])\n",
    "\n",
    "# Close the Neo4j driver\n",
    "n4j_driver.close()\n",
    "\n",
    "# Log the completion of constraint creation\n",
    "print(\"Unique composite constraint has been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_properties = parse_personinfo(soup)\n",
    "person_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_to_add = parse_resume(soup)\n",
    "\n",
    "# Invoke the function to update the node\n",
    "update_node_with_new_properties(n4j_driver, 'Person', 'John Doe', properties_to_add, verbose=True)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "n4j_driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(n4j_driver, person_properties, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '12/09/2023'\n",
    "# Assuming that the date string is in the format 'DD/MM/YYYY', parse it into a datetime object\n",
    "# Modify the format string according to the actual format of date_str\n",
    "parsed_date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "# Convert the datetime object to ISO 8601 format (YYYY-MM-DD)\n",
    "iso_date = parsed_date.strftime('%Y-%m-%d')\n",
    "person_properties['Last_Updated'] = iso_date\n",
    "person_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução da persistência em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or update publications in Neo4j based on the dictionary\n",
    "create_or_update_publications(graph, dict_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Initialize the Neo4j graph (Replace 'neo4j' and 'password' with your database credentials)\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create or update publications in Neo4j based on the dictionary\n",
    "create_or_update_publications(graph, dict_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar Grafo em Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência dos elementos H1\n",
    "try:\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "except Exception as e:\n",
    "    print('Erro ao conectar ao Neo4j')\n",
    "    print(e)\n",
    "try:    \n",
    "    header_node = persist_to_neo4j(header_data)\n",
    "    print({type(header_node)})\n",
    "    parse_h1_elements(soup, header_node, graph)\n",
    "    cv_node, properties = parse_parsoninfo(soup)\n",
    "except Exception as e:\n",
    "    print('Erro ao persistir nó')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node(\"Curriculum\", \n",
    "     title=header_data['title'].split('(')[1].strip(')'), \n",
    "     meta_keywords=header_data['meta_keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes em desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', {'class': 'layout-cell-pad-main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def list_divs(soup_element, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):  # check if the child is an instance of Tag\n",
    "            # Check for 'div' tags and list classes\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')  # Default to 'None' if class is not present\n",
    "                print('  ' * indent + f\"div {class_name}\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            # # Check for 'h1' tags\n",
    "            # elif child.name == 'h1':\n",
    "            #     print('  ' * indent + f\"h1 {child.text.strip()}\")\n",
    "            #     list_divs(child, indent)\n",
    "\n",
    "            # Check for 'h2' tags\n",
    "            elif child.name == 'h2':\n",
    "                print('  ' * indent + f\"h2 {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            # Check for 'a' tags\n",
    "            elif child.name == 'a':\n",
    "                print('  ' * indent + f\"a {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            # Check for 'ul' tags\n",
    "            elif child.name == 'ul':\n",
    "                print('  ' * indent + \"ul\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            # Check for 'li' tags\n",
    "            elif child.name == 'li':\n",
    "                print('  ' * indent + \"li: \" + child.text.strip())\n",
    "                list_divs(child, indent + 1)\n",
    "                            \n",
    "            # Check for 'inst_back' as class in any tag\n",
    "            elif child.has_attr('class') and 'inst_back' in child['class']:\n",
    "                print('  ' * indent + f\"{child.name} inst_back\")\n",
    "                list_divs(child, indent + 1)\n",
    "            \n",
    "            # Check for 'b' tags\n",
    "            elif child.name == 'b':\n",
    "                print('  ' * indent + f\"b {child.text.strip()}\")\n",
    "                list_divs(child, indent)\n",
    "        else:\n",
    "            list_divs(child, indent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "list_divs(starting_div, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrando dados extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    \n",
    "    ignorar = [None, ' ','Baixar Currículo','Imprimir Currículo']\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "\n",
    "                # Checar classe 'layout-cell-pad-5' com os valores dos dicionários\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    print('  ' * (indent + 1) + f\" val: {dividir(child.get_text())}\")                    \n",
    "                \n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            elif child.name == 'h2':\n",
    "                print('  ' * indent + f\"Node Name: {child.string}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    print('  ' * indent + f\"Properties: {rotulo}\")\n",
    "                    list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                print('  ' * indent + f\"Subtitulo: {child.get_text()}\")\n",
    "                list_divs(child, indent + 1)\n",
    "                \n",
    "            elif child.name == 'inst_back':\n",
    "                print('  ' * indent + f\"Subtitulo: {child.get_text()}\")\n",
    "                list_divs(child, indent + 1)\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # print('  ' * indent + f\"Group: {rotulo}\")\n",
    "                    list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                print('  ' * indent + f\" key: {rotulo}\")\n",
    "                list_divs(child, indent)\n",
    "\n",
    "            elif child.name == 'p':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                print('  ' * indent + f\" val: {rotulo}\")\n",
    "                list_divs(child, indent)                \n",
    "            \n",
    "            # elif child.name == 'span':\n",
    "            #     rotulo = dividir(child.get_text())[0]\n",
    "            #     print('  ' * indent + f\" val: {rotulo}\")\n",
    "            #     list_divs(child, indent)   \n",
    "\n",
    "        else:\n",
    "            list_divs(child, indent)\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "list_divs(starting_div, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ainda com problemas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo chaves corretamente mas não valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_data(element, parent_dict, current_section=None):\n",
    "    key = None\n",
    "    \n",
    "    if isinstance(element, Tag):\n",
    "        if element.name == 'a' or element.name == 'h2':\n",
    "            current_section = element.text.strip()\n",
    "            parent_dict[current_section] = {}\n",
    "        \n",
    "        elif 'text-align-right' in element.get('class', []):\n",
    "            key = element.find('b').text.strip() if element.find('b') else None\n",
    "            if key and current_section:\n",
    "                parent_dict[current_section][key] = ''\n",
    "        \n",
    "        elif 'layout-cell-9 layout-cell-pad-5' in element.get('class', []):\n",
    "            value = element.find('a').text.strip() if element.find('a') else element.text.strip()\n",
    "            if value and current_section and key:\n",
    "                parent_dict[current_section][key] = value\n",
    "\n",
    "    if isinstance(element, Tag):\n",
    "        for child in element.children:\n",
    "            extract_data(child, parent_dict, current_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "initial_dict = {}\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Invoke the function\n",
    "extract_data(starting_div, initial_dict)\n",
    "initial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, json_output):\n",
    "    stack = deque([json_output])\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            current_dict = stack[-1]\n",
    "            \n",
    "            if child.name == 'h1':\n",
    "                key = dividir(child.get_text())[0]\n",
    "                current_dict[key] = {}\n",
    "                stack.append(current_dict[key])\n",
    "                \n",
    "            elif child.name == 'b':\n",
    "                key = dividir(child.get_text())[0]\n",
    "                current_dict[key] = {}\n",
    "                stack.append(current_dict[key])\n",
    "                \n",
    "            elif child.name == 'div':\n",
    "                class_name = child.get('class', [])\n",
    "                \n",
    "                # Ensure the class matches exactly\n",
    "                if 'layout-cell-pad-5' in class_name:\n",
    "                    value = dividir(child.get_text())\n",
    "                    \n",
    "                    # Additional safety check\n",
    "                    if len(stack) > 1:\n",
    "                        parent_dict = stack[-2]\n",
    "                        parent_key = list(parent_dict.keys())[-1]\n",
    "                        parent_dict[parent_key] = value\n",
    "                        stack.pop()\n",
    "                    else:\n",
    "                        print(\"Warning: Stack size insufficient.\")\n",
    "\n",
    "            # Debugging information\n",
    "            print(f\"Stack size: {len(stack)}, Current dict keys: {current_dict.keys()}\")\n",
    "\n",
    "            # Continue recursion\n",
    "            list_divs(child, current_dict)\n",
    "            \n",
    "    if len(stack) > 1:\n",
    "        stack.pop()\n",
    "\n",
    "# Initialize empty dictionary\n",
    "json_output = {}\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Assuming 'soup' contains your BeautifulSoup object\n",
    "list_divs(starting_div, json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Não monta os dicionários no Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, parent_dict):\n",
    "    stack = deque([(soup_element, parent_dict)])  # Initialize the stack with parent element and parent dictionary\n",
    "\n",
    "    while stack:\n",
    "        element, current_dict = stack.pop()  # Get the last tuple (element, dictionary) from the stack\n",
    "\n",
    "        for child in element.children:\n",
    "            if isinstance(child, Tag):\n",
    "                \n",
    "                if child.name == 'h1':\n",
    "                    key = dividir(child.get_text())[0]\n",
    "                    current_dict[key] = {}\n",
    "                    stack.append((child, current_dict[key]))  # Append child element and its dictionary to the stack\n",
    "                    \n",
    "                elif child.name == 'b':\n",
    "                    key = dividir(child.get_text())[0]\n",
    "                    current_dict[key] = {}\n",
    "                    stack.append((child, current_dict[key]))  # Append child element and its dictionary to the stack\n",
    "                \n",
    "                elif child.name == 'div':\n",
    "                    class_name = child.get('class', [])\n",
    "                    \n",
    "                    if 'layout-cell-pad-5' in class_name:\n",
    "                        value = dividir(child.get_text())\n",
    "                        if value:  # Only populate if value is non-empty\n",
    "                            current_dict.update(value)  # Add the value to the current dictionary\n",
    "                            \n",
    "                else:\n",
    "                    stack.append((child, current_dict))  # Append child element and current dictionary for other cases\n",
    "\n",
    "\n",
    "# Initialize empty dictionary\n",
    "json_output = {}\n",
    "\n",
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Assuming 'soup' contains your BeautifulSoup object\n",
    "list_divs(starting_div, json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    string = string.replace(\"\\t\",'')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, dados, indent=0):\n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return\n",
    "    \n",
    "    ignorar = [None, ' ', 'Baixar Currículo', 'Imprimir Currículo']\n",
    "    \n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    valor = dividir(child.get_text())\n",
    "                    # Adicionar valor ao dicionário\n",
    "                    dados[f'Value_Level_{indent}'] = valor\n",
    "                \n",
    "                list_divs(child, dados, indent + 1)\n",
    "            \n",
    "            elif child.name == 'h2':\n",
    "                # Adicionar nome do nó ao dicionário\n",
    "                dados[f'Node_Name'] = child.string\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == \"foto\":\n",
    "                # Adicionar nome do nó ao dicionário\n",
    "                dados[f'Informações'] = child.string\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # Adicionar chave ao dicionário de primeiro nível\n",
    "                    dados[f'Properties'] = rotulo\n",
    "                    list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                # Adicionar subtítulo ao dicionário\n",
    "                dados[f'Subtitle_Level_{indent}'] = child.get_text()\n",
    "                list_divs(child, dados, indent + 1)\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    # Adicionar grupo ao dicionário\n",
    "                    dados[f'Group_Level_{indent}'] = rotulo\n",
    "                    list_divs(child, dados, indent)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                # Adicionar chave ao dicionário\n",
    "                chave = dividir(child.get_text())[0]\n",
    "                dados[f'Group_Level_{indent}'] = chave\n",
    "                list_divs(child, dados, indent)\n",
    "\n",
    "        else:\n",
    "            list_divs(child, dados, indent)\n",
    "\n",
    "# Iniciar a função principal\n",
    "if __name__ == '__main__':\n",
    "    # soup é uma variável que contém o objeto BeautifulSoup do seu HTML\n",
    "    dados = {}\n",
    "\n",
    "    # Encontra a div específica a partir da qual iniciar a travessia\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    # Chame a função list_divs passando o objeto BeautifulSoup e o dicionário vazio\n",
    "    list_divs(starting_div, dados)\n",
    "    \n",
    "    # Convertendo o dicionário para JSON\n",
    "    dados_json = json.dumps(dados, indent=4, ensure_ascii=False)\n",
    "    print(dados_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dados_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\", \"\")\n",
    "    string = string.replace(\"\\t\", '')\n",
    "    string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    return string.split('\\n')\n",
    "\n",
    "def list_divs(soup_element, graph_dict=None, indent=0, verbose=False):\n",
    "    if graph_dict is None:\n",
    "        graph_dict = {}\n",
    "    \n",
    "    if not isinstance(soup_element, Tag):\n",
    "        return graph_dict\n",
    "    \n",
    "    ignorar = [None, ' ', 'Baixar Currículo', 'Imprimir Currículo']\n",
    "\n",
    "    for child in soup_element.children:\n",
    "        if isinstance(child, Tag):\n",
    "            if verbose:\n",
    "                conteudo = dividir(child.get_text())[0]\n",
    "                print(f\"Tag encontrada: {child.name}, Classe: {child.get('class', 'None')}, {conteudo}\")\n",
    "\n",
    "            if child.name == 'div':\n",
    "                class_name = child.get('class', 'None')\n",
    "\n",
    "                if class_name == ['layout-cell-pad-5']:\n",
    "                    conteudo = dividir(child.get_text())[0]\n",
    "                    graph_dict[\"values\"] = conteudo\n",
    "                \n",
    "                list_divs(child, graph_dict, indent + 1, verbose)\n",
    "\n",
    "            elif child.name == 'h2':\n",
    "                node_name = child.string\n",
    "                if \"nodes\" not in graph_dict:\n",
    "                    graph_dict[\"nodes\"] = []\n",
    "                graph_dict[\"nodes\"].append({\"name\": node_name})\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'h1':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    graph_dict[\"properties\"] = {\"label\": rotulo}\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'inst_back':\n",
    "                graph_dict[\"subtitles\"] = child.get_text()\n",
    "\n",
    "            elif child.name == 'a':\n",
    "                rotulo = dividir(child.get_text())[0]\n",
    "                if rotulo not in ignorar:\n",
    "                    if \"groups\" not in graph_dict:\n",
    "                        graph_dict[\"groups\"] = []\n",
    "                    graph_dict[\"groups\"].append({\"label\": rotulo})\n",
    "\n",
    "                list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "            elif child.name == 'b':\n",
    "                graph_dict[\"keys\"] = dividir(child.get_text())[0]\n",
    "\n",
    "        else:\n",
    "            list_divs(child, graph_dict, indent, verbose)\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra a div específica a partir da qual iniciar a travessia\n",
    "starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "# Inicia a travessia a partir da div encontrada\n",
    "result = list_divs(starting_div, verbose=True)\n",
    "json.dumps(result, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    return string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "def extract_content(element, keys, key_classes, val_classes, verbose=False):\n",
    "    result = {}\n",
    "    try:\n",
    "        if verbose is True:\n",
    "            print(f\"Debug: Processing element of type {type(element)} with attributes {element.attrs}\")\n",
    "\n",
    "        for key_class in key_classes:\n",
    "            key_elements = element.find_all(class_=key_class, recursive=True)\n",
    "            for key_elem in key_elements:\n",
    "                key_text = key_elem.text.strip()\n",
    "                if verbose is True:\n",
    "                    print(f\"Debug: Found key element with text: {key_text}\")\n",
    "\n",
    "                value_elem = key_elem.find_next_sibling(class_=val_classes)\n",
    "                if value_elem:\n",
    "                    value_text = rep(value_elem.text.strip())\n",
    "                    if verbose is True:\n",
    "                        print(f\"Debug: Found value element with text: {value_text}\")\n",
    "                    result[key_text] = value_text\n",
    "                else:\n",
    "                    if verbose is True:\n",
    "                        print(f\"Debug: No value element found for the key '{key_text}' with attributes {key_elem.attrs}\")\n",
    "\n",
    "        for section_class in keys:\n",
    "            section_elements = element.find_all('div', {'class': section_class}, recursive=True)\n",
    "            for idx, section_elem in enumerate(section_elements):\n",
    "               if verbose is True:\n",
    "                print(f\"Debug: Found section element with attributes: {section_elem.attrs}\")\n",
    "\n",
    "                section_dict = extract_content(section_elem, keys, key_classes, val_classes)\n",
    "                section_key = section_elem.get('id', f'UnnamedSection{idx}')\n",
    "                result[section_key] = section_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in extract_content: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "    # keys = ['infpessoa', 'id','title_wraper','infpessoa','artigo-completo']\n",
    "    # key_classes = ['title_wraper','infpessoa','inst_back', 'id','layout-cell layout-cell-3 text-align-right','artigo-completo', 'layout-cell-1','cita-artigos']\n",
    "    # val_classes = ['layout-cell-9','data-cell','layout-cell-11']\n",
    "\n",
    "try:\n",
    "    # Initialize BeautifulSoup object (Assuming that 'soup' has been initialized)\n",
    "    keys = [\n",
    "            'b',\n",
    "            'id',\n",
    "            'infpessoa',\n",
    "            'title_wraper',\n",
    "            'artigo-completo',\n",
    "            'title_wraper',\n",
    "            'infpessoa',\n",
    "            'id',\n",
    "            'artigo-completo',\n",
    "            ]\n",
    "    key_classes = [\n",
    "                   'text-align-right',\n",
    "                   'title_wraper',        \n",
    "                   'inst_back',\n",
    "                #    'layout-cell-1', # Trabalhos em eventos\n",
    "                #    'cita-artigos', # Trabalhos completos e conclusão de curso de graduação\n",
    "                   ]\n",
    "    val_classes = [\n",
    "        'layout-cell-9',\n",
    "        'data-cell',\n",
    "        'layout-cell-11',\n",
    "        'title_wraper',\n",
    "        # 'infpessoa',\n",
    "        'id',\n",
    "        'artigo-completo',        \n",
    "        ]\n",
    "\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "    master_dict = {}\n",
    "    \n",
    "    if starting_div:\n",
    "        master_dict = extract_content(starting_div, keys, key_classes, val_classes)\n",
    "        print(json.dumps(master_dict, indent=4, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"Starting div not found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred in main: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(string):\n",
    "    string = string.replace(\"\\t\\t\\t\\t\\t\\t\\t\",\"\")\n",
    "    return string.replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "def extract_content(element, keys, key_classes, val_classes):\n",
    "    result = {}\n",
    "    \n",
    "    if isinstance(element, Tag):\n",
    "        for key in keys:\n",
    "            occurrences = element.find_all(key)\n",
    "            for c in occurrences:\n",
    "                if c is not None:\n",
    "                    result[key] = rep(c.text).strip()\n",
    "\n",
    "        for key_class in key_classes:\n",
    "            occurrences = element.find_all('div', {'class': key_class})\n",
    "            for idx, c in enumerate(occurrences):\n",
    "                if c is not None:\n",
    "                    key = c.text.strip() if c.text.strip() else f\"Unnamed-{idx}\"\n",
    "                    result[key] = {}\n",
    "                    \n",
    "                    sibling = c.find_next_sibling('div', {'class': val_classes})\n",
    "                    if sibling:\n",
    "                        result[key] = rep(sibling.text).strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "try:\n",
    "    # Initialize BeautifulSoup object (Assuming that 'soup' has been initialized)\n",
    "    keys = ['infpessoa', 'id','title_wraper','infpessoa','artigo-completo']\n",
    "    key_classes = ['title_wraper','infpessoa','inst_back', 'id','layout-cell layout-cell-3 text-align-right','artigo-completo', 'layout-cell-1','cita-artigos']\n",
    "    val_classes = ['layout-cell-9','data-cell','layout-cell-11']\n",
    "\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    master_dict = {}\n",
    "    for i in starting_div.children:\n",
    "        if isinstance(i, Tag):\n",
    "            master_dict.update(extract_content(i, keys, key_classes, val_classes))\n",
    "\n",
    "    print(json.dumps(master_dict, indent=4, ensure_ascii=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "def rep(string):\n",
    "    return string.replace(\"\\n\\n\\n\",\"\\n\").replace(\"\\n\\n\",\"\\n\")\n",
    "\n",
    "keys = ['infpessoa','h2','a', 'b', 'ul', 'id', 'inst_back','layout-cell-3','layout-cell-9','layout-cell-12','layout-cell-pad-5']\n",
    "\n",
    "try:\n",
    "    # Assuming 'soup' has been defined and initialized with the HTML document\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})        \n",
    "\n",
    "    for i in starting_div.children:\n",
    "        if isinstance(i, Tag):  # Confirming that the child is an HTML Tag\n",
    "            for key in keys:\n",
    "                # Use find_all to get a list of all occurrences\n",
    "                occurrences = i.find_all(key)\n",
    "                \n",
    "                for c in occurrences:  # Iterate over each occurrence\n",
    "                    if c is not None:\n",
    "                        print(f'{key:10} {rep(c.text).strip()}')\n",
    "                        print('-' * 75)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(element):\n",
    "    \"\"\"Recursively extracts content and organizes it into nested dictionaries.\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Check for 'title-wrapper' class as the first-level dictionary key\n",
    "    if 'title-wrapper' in element.attrs.get('class', []):\n",
    "        key = element.text.strip()\n",
    "        result[key] = {}\n",
    "        \n",
    "        for child in element.findChildren('div', recursive=False):\n",
    "            if 'layout-cell-12' in child.attrs.get('class', []):\n",
    "                second_level_key = child.text.strip()\n",
    "                result[key][second_level_key] = {}\n",
    "                \n",
    "                for grandchild in child.findChildren('div', recursive=False):\n",
    "                    if 'layout-cell-3' in grandchild.attrs.get('class', []):\n",
    "                        third_level_key = grandchild.text.strip()\n",
    "                        \n",
    "                        for great_grandchild in grandchild.findChildren('div', recursive=False):\n",
    "                            if 'layout-cell-9' in great_grandchild.attrs.get('class', []):\n",
    "                                value = great_grandchild.text.strip()\n",
    "                                result[key][second_level_key][third_level_key] = value\n",
    "    \n",
    "    # Recursively process child elements\n",
    "    for child in element.findChildren('div', recursive=False):\n",
    "        result.update(extract_content(child))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "# print(layout_cell_main.text.strip())\n",
    "print(len(layout_cell_main.find_all('div', 'title-wrapper')))\n",
    "for title_wrapper in layout_cell_main.find_all('div', 'title-wrapper'):\n",
    "    for data in title_wrapper.find_all('div', 'layout-cell-3'):\n",
    "        key = data.find('div', 'layout-cell-3')\n",
    "        val = data.find('div', 'layout-cell-9')\n",
    "        if key is not None:\n",
    "            print(f'{key.text.strip()}: {val.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_structure = extract_content(soup)\n",
    "json_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_scrape(tag, result_dict):\n",
    "    classes = tag.get(\"class\", [])\n",
    "    \n",
    "    if \"title-wrapper\" in classes:\n",
    "        section_name = tag.text.strip()\n",
    "        result_dict[section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[section_name])\n",
    "\n",
    "    elif \"text-align-right\" in classes or \"subtit-1\" in classes:\n",
    "        sub_section_name = tag.text.strip()\n",
    "        result_dict[sub_section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[sub_section_name])\n",
    "            \n",
    "    elif \"layout-cell-9\" in classes or \"layout-cell-pad-5\" in classes:\n",
    "        value = tag.text.strip()\n",
    "        result_dict[\"value\"] = value\n",
    "    \n",
    "    elif \"layout-cell\" in classes:\n",
    "        for child in tag.children:\n",
    "            if child.name:\n",
    "                recursive_scrape(child, result_dict)\n",
    "\n",
    "def main(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Initial recursive call\n",
    "    for child in soup.find_all('div'):\n",
    "        recursive_scrape(child, result_dict)\n",
    "        \n",
    "    # Convert the result to JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(result_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_scrape(tag, result_dict):\n",
    "    classes = tag.get(\"class\", [])\n",
    "    \n",
    "    if \"title-wrapper\" in classes:\n",
    "        section_name = tag.text.strip()\n",
    "        result_dict[section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[section_name])\n",
    "\n",
    "    elif \"text-align-right\" in classes or \"subtit-1\" in classes:\n",
    "        sub_section_name = tag.text.strip()\n",
    "        result_dict[sub_section_name] = {}\n",
    "        for child in tag.find_next_siblings():\n",
    "            recursive_scrape(child, result_dict[sub_section_name])\n",
    "            \n",
    "    elif \"layout-cell-9\" in classes or \"layout-cell-pad-5\" in classes:\n",
    "        value = tag.text.strip()\n",
    "        result_dict[\"value\"] = value\n",
    "    \n",
    "    elif \"layout-cell\" in classes:\n",
    "        for child in tag.children:\n",
    "            if child.name:\n",
    "                recursive_scrape(child, result_dict)\n",
    "\n",
    "def main(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Initial recursive call\n",
    "    for child in soup.find_all('div'):\n",
    "        recursive_scrape(child, result_dict)\n",
    "        \n",
    "    # Convert the result to JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(result_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados hierárquicos de um Soup de documento HTML e retorna um dicionário aninhado.\n",
    "\n",
    "    Parâmetros:\n",
    "        Objeto Soup do html_content (object): O conteúdo HTML como uma string.\n",
    "\n",
    "    Retorna:\n",
    "        dict: Um dicionário aninhado contendo os dados extraídos.\n",
    "    \"\"\"\n",
    "\n",
    "    def recursive_extraction(element):\n",
    "        children_data = {}\n",
    "        children = element.find_all(recursive=False)\n",
    "        \n",
    "        for child in children:\n",
    "            if child.has_attr('class') and 'text-align-right' in child['class']:\n",
    "                key = child.get_text().strip()\n",
    "                \n",
    "                value_container = child.find_next_sibling(class_='layout-cell-9')\n",
    "                if value_container:\n",
    "                    value = recursive_extraction(value_container)\n",
    "                    children_data[key] = value if value else value_container.get_text().strip()\n",
    "        \n",
    "        return children_data\n",
    "\n",
    "    # Instanciar um objeto BeautifulSoup\n",
    "    # soup = BeautifulSoup(html_content, 'html.parser', from_encoding='utf-8')\n",
    "  \n",
    "    # Iniciar a extração recursiva a partir do elemento raiz\n",
    "    # root = soup.body\n",
    "    # result_dict = recursive_extraction(root)\n",
    "\n",
    "    # Encontra a div específica a partir da qual iniciar a travessia\n",
    "    starting_div = soup.find('div', {'class': 'layout-cell-pad-main'})\n",
    "\n",
    "    # Inicia a travessia a partir da div encontrada\n",
    "    result_dict = recursive_extraction(starting_div)\n",
    "\n",
    "    # Converter o dicionário em um objeto JSON\n",
    "    result_json = json.dumps(result_dict, indent=4, ensure_ascii=False)\n",
    "  \n",
    "    # Salvar o JSON em um arquivo, especificando o encoding como UTF-8\n",
    "    with open('output_nested.json', 'w', encoding='utf-8') as file:\n",
    "        file.write(result_json)\n",
    "  \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = extrair_dados(soup, True)\n",
    "nome_no = 'Antonio Marcos Aires Barbosa'\n",
    "imprimir_informacoes({nome_no: json_data['Properties']}, nome_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "for title_wrapper in layout_cell_main.find_all('div.title-wrapper'):\n",
    "    index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_from_html(soup):\n",
    "    \"\"\"\n",
    "    Generate a JSON object from an Soup object for a Neo4j integration.\n",
    "\n",
    "    Parameters:\n",
    "        soup (object): Soup object from a HTML content.\n",
    "\n",
    "    Returns:\n",
    "        json_data (dict): A dictionary representing the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract node name\n",
    "    node_name = soup.select_one('div.infpessoa h2.nome').text.strip()\n",
    "\n",
    "    # Initialize the master dictionary\n",
    "    json_data = {node_name: {}}\n",
    "\n",
    "    # Locate the main layout cell\n",
    "    layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "    # Iterate over title-wrapper elements\n",
    "    for title_wrapper in layout_cell_main.select('div.title-wrapper'):\n",
    "        index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "        \n",
    "        # Initialize the child dictionary\n",
    "        json_data[node_name][index] = {}\n",
    "\n",
    "        # Iterate over layout cells\n",
    "        for layout_cell_3 in title_wrapper.select('div.layout-cell.layout-cell-3.text-align-right'):\n",
    "            grandchild_index = layout_cell_3.select_one('div.layout-cell-pad-5.text-align-right').text.strip()\n",
    "            \n",
    "            # Find the corresponding layout cell for values\n",
    "            layout_cell_9 = layout_cell_3.find_next_sibling('div', class_='layout-cell.layout-cell-9')\n",
    "            \n",
    "            values_text = layout_cell_9.select_one('div.layout-cell-pad-5').text\n",
    "            \n",
    "            # Create a list of values\n",
    "            values = values_text.split('<br class=\"clear\">' or '\\n\\n\\n')\n",
    "\n",
    "            # Add the grandchild dictionary\n",
    "            json_data[node_name][index][grandchild_index] = values\n",
    "\n",
    "    return json_data\n",
    "\n",
    "# Generate JSON data\n",
    "json_data = generate_json_from_html(soup)\n",
    "\n",
    "# Print or persist the generated JSON data\n",
    "print(json.dumps(json_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lattes = extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint(json_lattes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(json_data, parent_key='', separator='.'):\n",
    "    \"\"\"\n",
    "    Convert a nested dictionary into a flat dictionary, suitable for DataFrame conversion.\n",
    "    \n",
    "    Parameters:\n",
    "    - json_data (dict): The nested dictionary to flatten.\n",
    "    - parent_key (str, optional): The concatenated key used to represent nesting.\n",
    "    - separator (str, optional): The character to use for separating nested keys.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame representing the flattened dictionary.\n",
    "    \"\"\"\n",
    "    flat_dict = {}\n",
    "    \n",
    "    for k, v in json_data.items():\n",
    "        new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "        \n",
    "        if isinstance(v, dict):\n",
    "            flat_dict.update(dict_to_dataframe(v, new_key, separator=separator))\n",
    "        else:\n",
    "            flat_dict[new_key] = v\n",
    "            \n",
    "    return pd.DataFrame([flat_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming json_lattes is a JSON-formatted string\n",
    "json_lattes_dict = json.loads(json_lattes)\n",
    "\n",
    "# Then call the dict_to_dataframe function\n",
    "df = dict_to_dataframe(json_lattes_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    splpt = 'Currículo do Sistema de Currículos Lattes ('\n",
    "    string_title = soup.title.string if soup.title else \"Unknown\"\n",
    "    title = string_title.split(splpt)[1].strip(')')\n",
    "    \n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "    from py2neo import Graph, Node, Relationship\n",
    "    \n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    secoes = []\n",
    "    \n",
    "    print(f'{len(h1_elements[2:])} elementos encontrados')\n",
    "    for n,i in enumerate(h1_elements):\n",
    "        if n>1:\n",
    "            secao = i.text\n",
    "            print(f'    {secao}')\n",
    "            secoes.append(secao)\n",
    "    \n",
    "    for elem in h1_elements[2:]:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        \n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_parsoninfo(soup):\n",
    "    # Localizar o elemento de link que contém o título do currículo\n",
    "    link_element = soup.find(\"a\", {\"href\": lambda x: x and \"abreDetalhe\" in x})\n",
    "\n",
    "    # Extrair o texto do link para usar como título do nó\n",
    "    node_title = link_element.text if link_element else \"Unknown\"\n",
    "    print(f'Título do Nó: {node_title}')\n",
    "\n",
    "    # Localizar o elemento div contendo as propriedades\n",
    "    properties_div = soup.find(\"div\", {\"class\": \"resultado\"})\n",
    "    if properties_div:\n",
    "        print(f'Resultado: {properties_div.text}')\n",
    "    else:\n",
    "        print('Resultados não encontrados')\n",
    "\n",
    "    # Inicializar um dicionário para armazenar as propriedades\n",
    "    properties = {}\n",
    "    \n",
    "    # Localizar o elemento li que contém as informações do idlattes\n",
    "    li_element = soup.find(\"li\")\n",
    "    for i in li_element:\n",
    "        if 'http://lattes.cnpq.br/' in i:\n",
    "            idlattes = i.split('http://lattes.cnpq.br/')[1]\n",
    "            properties['Idlattes'] = idlattes\n",
    "            print(idlattes)\n",
    "\n",
    "    # Extrair e armazenar as propriedades relevantes\n",
    "    if properties_div:\n",
    "        properties['Nacionalidade'] = 'Brasil'\n",
    "        properties['Cargo'] = properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}).text if properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}) else 'Desconhecido'\n",
    "        properties['Titulação'] = properties_div.contents[-4] if len(properties_div.contents) > 4 else 'Desconhecido'\n",
    "\n",
    "        # Extração de nome e identificador único\n",
    "        a_element = li_element.find(\"a\")\n",
    "        properties[\"Nome\"] = a_element.text\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Títulos Acadêmicos e outras informações\n",
    "\n",
    "    return node_title, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_annotation_in_db(uri, user, password, annot_html, cv_id):\n",
    "    \"\"\"\n",
    "    Store the annotation HTML in a Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        uri (str): URI of the Neo4j database\n",
    "        user (str): Username for the Neo4j database\n",
    "        password (str): Password for the Neo4j database\n",
    "        annot_html (str): The HTML string containing annotations\n",
    "        cv_id (str): The unique identifier for the annotated CV\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize Neo4j driver\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    # Define Cypher query for adding an annotation\n",
    "    add_annotation_query = '''\n",
    "    MERGE (cv:CV {id: $cv_id})\n",
    "    CREATE (a:Annotation {html: $annot_html})\n",
    "    MERGE (cv)-[:HAS]->(a)\n",
    "    '''\n",
    "\n",
    "    # Execute query\n",
    "    with driver.session() as session:\n",
    "        session.run(add_annotation_query, cv_id=cv_id, annot_html=annot_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# from collections import Counter\n",
    "\n",
    "# def enumerate_tags(soup):\n",
    "#     # Extração de todos os marcadores (tags) no documento\n",
    "#     all_tags = [tag.name for tag in soup.find_all(True)]\n",
    "    \n",
    "#     # Contagem de ocorrências de cada marcador\n",
    "#     tag_count = Counter(all_tags)\n",
    "    \n",
    "#     # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "#     tag_dict = dict(tag_count)\n",
    "    \n",
    "#     return tag_dict\n",
    "\n",
    "# # Exemplo de uso\n",
    "# tag_dictionary = enumerate_tags(soup)\n",
    "# print(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div_elements = soup.find_all('div')\n",
    "# div_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_div_data(soup):\n",
    "#     \"\"\"\n",
    "#     Extrai dados das divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "#     Parâmetros:\n",
    "#     - html_document (str): String contendo o documento HTML.\n",
    "    \n",
    "#     Retorno:\n",
    "#     - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "#     \"\"\"\n",
    "#     # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "#     extracted_data = {}\n",
    "    \n",
    "#     # Localiza todas as divs com a classe 'layout-cell-pad-5 text-align-right'\n",
    "# #     divs_key = soup.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "#     divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "#     for div_key in divs_key:\n",
    "#         # Extrai o conteúdo da tag <b> dentro da div\n",
    "#         key = div_key.find('b').text if div_key.find('b') else None\n",
    "        \n",
    "#         # Encontra a div que segue imediatamente\n",
    "#         div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5'})\n",
    "        \n",
    "#         # Extrai o conteúdo da div\n",
    "#         value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "#         # Armazena no dicionário se ambas chave e valor existirem\n",
    "#         if key and value:\n",
    "#             extracted_data[key] = value\n",
    "    \n",
    "#     return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_div_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(extracted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        \n",
    "        data_dict = {}\n",
    "        \n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "            \n",
    "            current_higher_order_dict = None  # Initialize a variable to store the current higher-order dictionary\n",
    "            \n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                \n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    current_higher_order_dict = {}  # Create a new dictionary for this higher-order key\n",
    "                    data_dict[higher_order_key] = current_higher_order_dict  # Associate the new dictionary with the higher-order key\n",
    "                    \n",
    "                year_elems = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for year_elem, details_elem in zip(year_elems, details_elems):\n",
    "                    year_text = year_elem.text.strip() if year_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    if current_higher_order_dict is not None:\n",
    "                        # Insert the year-details pair into the current higher-order dictionary\n",
    "                        current_higher_order_dict[year_text] = details_text\n",
    "                    else:\n",
    "                        # If no higher-order key is present, associate the year-details pair directly with the title\n",
    "                        data_dict[year_text] = details_text\n",
    "                \n",
    "            result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(input_dict):\n",
    "    def recursive_descent(current_dict, parent_key='', separator='.'):\n",
    "        nonlocal flattened_dict\n",
    "        for k, v in current_dict.items():\n",
    "            new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                recursive_descent(v, new_key, separator=separator)\n",
    "            else:\n",
    "                flattened_dict[new_key] = v\n",
    "                \n",
    "    flattened_dict = {}\n",
    "    recursive_descent(input_dict)\n",
    "    \n",
    "    # Create DataFrame from the flattened dictionary\n",
    "    df = pd.DataFrame([flattened_dict])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dict = extract_data(soup)\n",
    "df = dict_to_dataframe(nested_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag, NavigableString\n",
    "\n",
    "html_content = '''\n",
    "<div class=\"layout-cell-pad-5\">\n",
    "    Doutorado em andamento em Informática Aplicada.\n",
    "    <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "    <br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde\n",
    "    <br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho.\n",
    "</div>\n",
    "'''\n",
    "\n",
    "soup_sample = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup_sample)\n",
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def extract_text_from_selectors(soup,select_path):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        elements = title_wrapper.select(select_path)\n",
    "#         print(len(elements))\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag, NavigableString\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=\"layout-cell.layout-cell-3.text-align-right\"\n",
    "vals=\"layout-cell layout-cell-9\"\n",
    "# <div class=\"layout-cell layout-cell-9\">\n",
    "# <div class=\"layout-cell-pad-5\">Doutorado em andamento em Informática Aplicada. <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "\t\t\n",
    "# \t<br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde<br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho. </div>\n",
    "# </div>\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "father     = 'div.title-wrapper'\n",
    "sons       = 'h1'\n",
    "grandchild = '' \n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        son_elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for grandchild in son_elements:\n",
    "            text_content = grandchild.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_titles(soup,father,sons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "father = 'div.title-wrapper'\n",
    "sons   = 'h1'\n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-12.data-cell'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "select_path='layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_to_dict(soup):\n",
    "    result_list = []\n",
    "    \n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        temp_dict = defaultdict(dict)\n",
    "        first_level_key = title_wrapper.select_one('div.layout-cell.layout-cell-12.data-cell').text.strip()\n",
    "        \n",
    "        for cell in title_wrapper.select('div.layout-cell.layout-cell-12.data-cell'):\n",
    "            second_level_keys = cell.select('div.layout-cell-pad-5.text-align-right')\n",
    "            values = cell.select('div.layout-cell.layout-cell-3.text-align-right')\n",
    "            \n",
    "            if len(second_level_keys) == len(values):\n",
    "                for key, value in zip(second_level_keys, values):\n",
    "                    second_level_key = key.text.strip()\n",
    "                    value_text = value.text.strip()\n",
    "                    temp_dict[first_level_key][second_level_key] = value_text\n",
    "        \n",
    "        result_list.append(temp_dict)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "# Execute the function\n",
    "result_list = extract_to_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['layout-cell', 'layout-cell-3', 'text-align-right']\n",
    "\n",
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "# 'text-align-right': 152,\n",
    "# 'layout-cell-pad-5': 152,\n",
    "\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-pad-main'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'text-align-right'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-9'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-3'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'data-cell'},\n",
    "#                                     {'class': 'layout-cell'},\n",
    "#                                     {'class': 'layout-cell-9},                                    \n",
    "#                                     {'class': 'layout-cell-12'},\n",
    "#                                     {'class': 'layout-cell-pad-5'},\n",
    "#                                     {'class': 'data-cell'}\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_text_from_hierarchy(tag, result_dict, parent_key=\"root\"):\n",
    "    div_elements = tag.find_all('div', recursive=False)\n",
    "    \n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        class_key = ', '.join(class_list) if class_list else \"None\"\n",
    "        \n",
    "        # Criando uma chave única que incorpora o caminho da raiz até esta div\n",
    "        full_key = f\"{parent_key} -> {class_key}\"\n",
    "\n",
    "        # Coleta o texto contido no elemento div atual\n",
    "        text_content = div.get_text(strip=True)\n",
    "\n",
    "        # Armazenar o conteúdo textual sob esta chave única\n",
    "        if full_key not in result_dict:\n",
    "            result_dict[full_key] = []\n",
    "        result_dict[full_key].append(text_content)\n",
    "\n",
    "        # Chamada recursiva para extrair textos dos filhos deste div\n",
    "        extract_text_from_hierarchy(div, result_dict, full_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)\n",
    "\n",
    "extract_text_from_hierarchy(soup.body, result_dict, 'text-align-right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.values():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes, key_stack=None):\n",
    "    if key_stack is None:\n",
    "        key_stack = []\n",
    "    \n",
    "    popped_key = None\n",
    "    if soup_element.name == \"div\":\n",
    "        class_list = soup_element.get('class', [])\n",
    "        \n",
    "        if any(k in class_list for k in key_classes):\n",
    "            new_key = soup_element.text.strip()\n",
    "#             print(f'Key found: {new_key}')  # Debugging line\n",
    "            key_stack.append(new_key)\n",
    "        \n",
    "        elif any(v in class_list for v in value_classes) and key_stack:\n",
    "            value = soup_element.text.strip()\n",
    "#             print(f'Value found: {value}')  # Debugging line\n",
    "            current_key = key_stack[-1]\n",
    "            if current_key in result_dict:\n",
    "                result_dict[current_key].append(value)\n",
    "            else:\n",
    "                result_dict[current_key] = [value]\n",
    "                \n",
    "#     print(f\"Current key_stack: {key_stack}\")  # Debugging line\n",
    "\n",
    "    for child in soup_element.find_all(\"div\", recursive=False):\n",
    "        extract_key_value_pairs(child, result_dict, key_classes, value_classes, key_stack)\n",
    "\n",
    "    if popped_key:\n",
    "        key_stack.append(popped_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'text-align-right'})\n",
    "# key_classes   = ['data-cell']\n",
    "# value_classes = ['text-align-right']\n",
    "\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    pprint(i)\n",
    "\n",
    "print()\n",
    "\n",
    "for i in result_dict.values():\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'layout-cell-pad-5'})\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "\n",
    "def extract_target_classes(soup,classe):\n",
    "    soup_element  = soup.find('div', {'class': classe})\n",
    "#     key_classes   = ['data-cell']\n",
    "#     value_classes = ['text-align-right']\n",
    "    key_classes   = ['layout-cell-9']\n",
    "    value_classes = ['layout-cell-pad-5']\n",
    "    \n",
    "    target_classes = [\n",
    "        'Produção '\n",
    "        'Bibliográfica',\n",
    "        'Produção '\n",
    "        'Técnica',\n",
    "        'Produção '\n",
    "        'Artística/Cultural'\n",
    "        'nome', \n",
    "        'resumo', \n",
    "        'artigo-completo', \n",
    "        'cita', \n",
    "        'cita-artigos', \n",
    "        'citacoes', \n",
    "        'detalhes', \n",
    "        'fator', \n",
    "        'foto', \n",
    "        'informacao-artigo', \n",
    "        'informacoes-autor', \n",
    "        'infpessoa', \n",
    "        'rodape-cv', \n",
    "        'science_cont', \n",
    "        'texto', \n",
    "        'trab'\n",
    "        ]\n",
    "    try:\n",
    "        extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "        for x,y in zip(result_dict.keys(),result_dict.values()):\n",
    "            if x in target_classes:\n",
    "                try:\n",
    "                    print(f\"{x:>12} | {y[0]}\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        filtered_dict = {k: v for k, v in result_dict.items() if k in target_classes}\n",
    "\n",
    "    except:\n",
    "        print(f'Classe \"{classe}\" não encontrada')\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variações das funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funciona mas achata o dicionário de Propriedades em um único e não está em UTF-8\n",
    "# from neo4j import GraphDatabase\n",
    "# import json\n",
    "\n",
    "# class InvalidPropertyError(ValueError):\n",
    "#     \"\"\"Custom Exception for Invalid Properties\"\"\"\n",
    "#     pass\n",
    "\n",
    "# def serialize_properties(input_dict):\n",
    "#     \"\"\"\n",
    "#     Serializes the properties in the dictionary that are not of primitive types.\n",
    "    \n",
    "#     :param input_dict: The input dictionary with properties.\n",
    "#     :return: A dictionary with properties serialized if necessary.\n",
    "#     \"\"\"\n",
    "#     return {k: json.dumps(v) if not isinstance(v, (str, int, float, bool, list)) else v for k, v in input_dict.items()}\n",
    "\n",
    "# def ensure_string_keys(input_dict):\n",
    "#     \"\"\"\n",
    "#     Ensures all keys in the dictionary are of string type.\n",
    "    \n",
    "#     :param input_dict: The input dictionary.\n",
    "#     :return: A dictionary with string keys.\n",
    "#     \"\"\"\n",
    "#     return {str(k): v for k, v in input_dict.items()}\n",
    "\n",
    "# def create_or_merge_node(tx, label, properties):\n",
    "#     \"\"\"\n",
    "#     Creates or merges a node in the Neo4j database.\n",
    "    \n",
    "#     :param tx: The transaction object.\n",
    "#     :param label: The label for the node.\n",
    "#     :param properties: The properties for the node.\n",
    "#     :return: The created or merged node.\n",
    "#     \"\"\"\n",
    "#     query = f\"MERGE (n:{label} {{name: $name}}) SET n += $properties RETURN n\"\n",
    "#     return tx.run(query, name=properties.get('name', 'UNKNOWN'), properties=properties).single()\n",
    "\n",
    "# def are_properties_primitive(input_dict):\n",
    "#     \"\"\"\n",
    "#     Validates if all properties in the dictionary are of primitive types or lists thereof.\n",
    "    \n",
    "#     :param input_dict: The input dictionary.\n",
    "#     :return: Boolean indicating the validation result.\n",
    "#     \"\"\"\n",
    "#     for key, value in input_dict.items():\n",
    "#         if not (isinstance(value, (str, int, float, bool)) or isinstance(value, list)):\n",
    "#             print(f\"Invalid property: {key} -> {value}\")\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "# def persist_to_neo4j(uri, user, password, node_data, label):\n",
    "#     \"\"\"\n",
    "#     Persists data to Neo4j database.\n",
    "    \n",
    "#     :param uri: The URI of the Neo4j database.\n",
    "#     :param user: The username for the Neo4j database.\n",
    "#     :param password: The password for the Neo4j database.\n",
    "#     :param node_data: The dictionary containing the properties for the node.\n",
    "#     :param label: The label for the node.\n",
    "#     \"\"\"\n",
    "#     driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "#     with driver.session() as session:\n",
    "#         converted_dict = ensure_string_keys(node_data)\n",
    "        \n",
    "#         # Serialize non-primitive properties\n",
    "#         serialized_dict = serialize_properties(converted_dict)\n",
    "        \n",
    "#         # Now validate the properties\n",
    "#         if not are_properties_primitive(serialized_dict):\n",
    "#             raise InvalidPropertyError(\"Properties should be of primitive types or arrays of primitive types.\")\n",
    "        \n",
    "#         session.write_transaction(create_or_merge_node, label, serialized_dict)\n",
    "\n",
    "#     driver.close()\n",
    "\n",
    "# # Example usage\n",
    "# uri = \"bolt://localhost:7687\"  # Replace with your Neo4j URI\n",
    "# user = \"neo4j\"  # Replace with your username\n",
    "# password = \"password\"  # Replace with your password\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample invocation\n",
    "# extracted_data = {\n",
    "#     'name': 'John Doe',\n",
    "#     'age': 30,\n",
    "#     None: 'This will be skipped',\n",
    "#     '': 'This key will be skipped',\n",
    "#     'Properties': {\n",
    "#         'Endereço Profissional': {\n",
    "#             'Rua': '123 Main St',\n",
    "#             'Cidade': 'City',\n",
    "#             'CEP': '12345'\n",
    "#         },\n",
    "#         'Outra Propriedade': {\n",
    "#             'Chave1': 'Valor1',\n",
    "#             'Chave2': 'Valor2'\n",
    "#         }\n",
    "#     },\n",
    "#     'nested_list': [1, 2, 3]\n",
    "# }\n",
    "\n",
    "# persist_to_neo4j(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# pprint(node_raimir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data = node_raimir\n",
    "\n",
    "# try:\n",
    "#     persist_to_neo4j(extracted_data)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para anotação de dados em HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_published_articles(soup, qualis_data):\n",
    "    # Localiza elementos contendo os artigos publicados\n",
    "    articles_elements = soup.find_all('div', class_='published-article')\n",
    "    \n",
    "    # Inicializa lista para conter dados dos artigos publicados\n",
    "    annotated_articles = []\n",
    "    \n",
    "    # Itera sobre cada elemento e anotando as informações necessárias\n",
    "    for article in articles_elements:\n",
    "        title = article.find('div', class_='article-title').text\n",
    "        issn = article.find('div', class_='article-issn').text\n",
    "        qualis = qualis_data.get(issn, 'N/A')  # Buscando o Qualis correspondente\n",
    "        \n",
    "        # Adiciona ao conjunto de artigos anotados\n",
    "        annotated_articles.append({\n",
    "            'title': title,\n",
    "            'issn': issn,\n",
    "            'qualis': qualis\n",
    "        })\n",
    "    \n",
    "    # Persiste em SQLite\n",
    "    conn = sqlite3.connect(\"lattes_data.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Cria tabela de artigos publicados, se não existir\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS published_articles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT,\n",
    "        issn TEXT,\n",
    "        qualis TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insere artigos anotados na tabela\n",
    "    for article in annotated_articles:\n",
    "        cursor.execute(\"INSERT INTO published_articles (title, issn, qualis) VALUES (?, ?, ?)\",\n",
    "                       (article['title'], article['issn'], article['qualis']))\n",
    "    \n",
    "    # Commit e fecha a conexão\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "async def annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info):\n",
    "    print(\"Procurando por publicações de periódicos...\")\n",
    "    \n",
    "    # Inicializa BeautifulSoup para analisar o conteúdo HTML da página\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extrai e anota informações dos artigos publicados\n",
    "    qualis_info = await annotate_published_articles(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup)\n",
    "    \n",
    "    return qualis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_annotation_message(images_urls, visualization_url, recent_updates_url, pub_count):\n",
    "    annot_header_html = ''\n",
    "    annot_buttons_html = ''\n",
    "    pub_count_string = ''\n",
    "    \n",
    "    if pub_count > 0:\n",
    "        s_char = 's' if pub_count > 1 else ''\n",
    "        pub_count_string = f'anotou o Qualis de {pub_count} artigo{s_char} em periódico{s_char} neste CV.'\n",
    "        annot_buttons_html = render_template_string(\"\"\"\n",
    "            <a href=\"#artigos-completos\">\n",
    "                <button> Ver anotações </button>\n",
    "            </a>\n",
    "        \"\"\")\n",
    "    else:\n",
    "        pub_count_string = 'não anotou nenhum artigo em periódico neste CV.'\n",
    "\n",
    "    annot_header_html = render_template_string(\"\"\"\n",
    "        <a href=\"{{visualization_url}}\" target=\"_blank\" id=\"qlattes-logo\">\n",
    "            <img src=\"{{images_urls['qlattesLogoURL']}}\" width=\"70\">\n",
    "        </a>{{pub_count_string}}\n",
    "        </br>\n",
    "    \"\"\", visualization_url=visualization_url, images_urls=images_urls, pub_count_string=pub_count_string)\n",
    "\n",
    "    # Aqui a informação seria armazenada em um banco de dados em vez de ser injetada em um elemento HTML\n",
    "    store_annotation_in_db(annot_header_html + annot_buttons_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_qualis_data(qualis_info):\n",
    "    pub_data = []\n",
    "    pub_data_year = []\n",
    "    curr_year = 0\n",
    "    \n",
    "    for i in range(len(qualis_info)):\n",
    "        if curr_year != qualis_info[i]['year']:\n",
    "            if curr_year > 0:\n",
    "                pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "                pub_data_year = []\n",
    "            \n",
    "            curr_year = qualis_info[i]['year']\n",
    "        \n",
    "        pub_data_item = {\n",
    "            'issn': qualis_info[i]['issn'],\n",
    "            'title': qualis_info[i]['title'],\n",
    "            'pubName': qualis_info[i]['pubName'],\n",
    "            'qualis': qualis_info[i]['qualisLabels']['qualis'],\n",
    "            'baseYear': qualis_info[i]['qualisLabels']['baseYear'],\n",
    "            'jcr': qualis_info[i]['jcrData']['jcr'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else 0,\n",
    "            'jcrYear': qualis_info[i]['jcrData']['baseYear'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else ''\n",
    "        }\n",
    "        \n",
    "        pub_data_year.append(pub_data_item)\n",
    "        \n",
    "    if len(qualis_info) > len(pub_data):\n",
    "        pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "    \n",
    "    return pub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_lattes_page(name_link, images_urls, recent_updates_url, qualis_data, data_source_info):\n",
    "    print(qualis_data)\n",
    "    \n",
    "    # Inicializando o WebDriver\n",
    "    driver_service = Service('path/to/chromedriver')\n",
    "    driver = webdriver.Chrome(service=driver_service)\n",
    "    driver.get(name_link['link'])\n",
    "    \n",
    "    # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "    visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "    # Limpar cache de dados de Qualis\n",
    "    qualis_data_cache = {}\n",
    "    \n",
    "    # Annotate Lattes page com informações de Qualis\n",
    "    lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "    if lattes_info:\n",
    "        await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "    # Consolidar dados de publicação a partir das informações de Lattes\n",
    "    pub_info = consolidate_qualis_data(lattes_info)\n",
    "    print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "    # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "    conn = sqlite3.connect('lattes_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "    existing_data = cursor.fetchall()\n",
    "    \n",
    "    lattes_data_array = []\n",
    "    \n",
    "    if existing_data:\n",
    "        # Filtrar dados existentes para evitar duplicatas\n",
    "        lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "    # Adicionar dados de Lattes atuais ao array\n",
    "    lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "    # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "    cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_link_html(text, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">{{text}}</a>\n",
    "    \"\"\", text=text, tooltip=tooltip, target_url=target_url)\n",
    "\n",
    "def create_icon_link_html(icon_url, icon_style, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">\n",
    "            <img src=\"{{icon_url}}\" style=\"{{icon_style}}\">\n",
    "        </a>\n",
    "    \"\"\", icon_url=icon_url, icon_style=icon_style, tooltip=tooltip, target_url=target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qualis_from_percentil(percentil):\n",
    "    if not percentil:\n",
    "        return 'N'\n",
    "\n",
    "    qualis_class_list     = ['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4']\n",
    "    qualis_threshold_list = [87.5, 75, 62.5, 50, 37.5, 25, 12.5, 0]\n",
    "\n",
    "    for i in range(len(qualis_threshold_list)):\n",
    "        if percentil >= qualis_threshold_list[i]:\n",
    "            return qualis_class_list[i]\n",
    "\n",
    "    return 'N'\n",
    "\n",
    "\n",
    "def get_alternative_issn(issn, capes_alt_data, scopus_data):\n",
    "    # Pesquisar ISSN nos dados complementares da CAPES\n",
    "    match = next((elem for elem in capes_alt_data if elem['issn'] == issn or elem['alt_issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        if 'alt_issn' in match and match['alt_issn'] != issn:\n",
    "            return match['alt_issn']\n",
    "        elif 'issn' in match and match['issn'] != issn:\n",
    "            return match['issn']\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        # Pesquisar ISSN nos dados do Scopus\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "        \n",
    "        if match:\n",
    "            if 'e-issn' in match and len(match['e-issn']) > 0 and match['e-issn'] != issn:\n",
    "                return match['e-issn']\n",
    "            elif 'issn' in match and len(match['issn']) > 0 and match['issn'] != issn:\n",
    "                return match['issn']\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualis_from_capes_data(issn, alt_issn, capes_data, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procurar pelo ISSN na base de dados da CAPES\n",
    "    match = next((elem for elem in capes_data if elem['issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        qualis_labels['source'] = 'capes'\n",
    "    elif alt_issn != '':\n",
    "        match = next((elem for elem in capes_data if elem['issn'] == alt_issn), None)\n",
    "        \n",
    "        if match:\n",
    "            qualis_labels['source'] = 'capes_alt'\n",
    "            \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = match['qualis']\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['baseYear'] = data_source_info[qualis_labels['source']]['baseYear']\n",
    "\n",
    "        qualis_labels_scopus = get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info)\n",
    "        \n",
    "        if qualis_labels_scopus['qualis'] != 'N':\n",
    "            qualis_labels['linkScopus'] = qualis_labels_scopus['linkScopus']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_pucrs_data(issn, alt_issn, pub_name, pucrs_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    labels_map = {\n",
    "        'pubName': 'periodico',\n",
    "        'qualis': 'Qualis_Final',\n",
    "        'percentil': 'percentil',\n",
    "        'linkScopus': 'link_scopus',\n",
    "        'adjusted': 'Ajuste_SBC'\n",
    "    }\n",
    "    \n",
    "    match = next((elem for elem in pucrs_data if elem['issn'] == issn or (alt_issn and elem['issn'] == alt_issn)), None)\n",
    "    \n",
    "    if match:\n",
    "        for key in labels_map.keys():\n",
    "            if labels_map[key] in match and match[labels_map[key]] != 'nulo':\n",
    "                qualis_labels[key] = match[labels_map[key]]\n",
    "                \n",
    "        qualis_labels['source'] = 'pucrs'\n",
    "        qualis_labels['baseYear'] = data_source_info['pucrs']['baseYear']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procura pelo ISSN nos dados da Scopus\n",
    "    match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "    \n",
    "    if not match and alt_issn != '':\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == alt_issn or elem['e-issn'] == alt_issn), None)\n",
    "        \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = calculate_qualis_from_percentil(match['percentil'])\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['percentil'] = match['percentil']\n",
    "        qualis_labels['linkScopus'] = match['source-id-url']\n",
    "        qualis_labels['source'] = 'scopus'\n",
    "        qualis_labels['baseYear'] = data_source_info['scopus']['baseYear']\n",
    "    \n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "async def get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    if not issn:\n",
    "        return qualis_labels\n",
    "\n",
    "    alt_issn = ''\n",
    "\n",
    "    # Verificar se o ISSN já está em cache\n",
    "    if issn in qualis_data_cache:\n",
    "        return qualis_data_cache[issn]\n",
    "    else:\n",
    "        # Verificar se um ISSN alternativo existe e está em cache\n",
    "        alt_issn = await get_alternative_issn(issn, qualis_data['capes_alt'], qualis_data['scopus'])\n",
    "        if alt_issn and alt_issn in qualis_data_cache:\n",
    "            return qualis_data_cache[alt_issn]\n",
    "\n",
    "    # Procurar pelo ISSN nos dados CAPES\n",
    "    qualis_labels = await get_qualis_from_capes_data(\n",
    "        issn, alt_issn, qualis_data['capes'], qualis_data['scopus'], data_source_info\n",
    "    )\n",
    "\n",
    "    # Se não encontrado\n",
    "    if qualis_labels['qualis'] == 'N':\n",
    "        # Procurar pelo ISSN nos dados PUC-RS\n",
    "        qualis_labels = await get_qualis_from_pucrs_data(\n",
    "            issn, alt_issn, pub_name, qualis_data['pucrs'], data_source_info\n",
    "        )\n",
    "\n",
    "        # Se ainda não encontrado\n",
    "        if qualis_labels['qualis'] == 'N':\n",
    "            # Procurar pelo ISSN nos dados Scopus\n",
    "            qualis_labels = await get_qualis_from_scopus_data(\n",
    "                issn, alt_issn, qualis_data['scopus'], data_source_info\n",
    "            )\n",
    "\n",
    "    # Adicionar rótulos ao cache Qualis\n",
    "    qualis_data_cache[issn] = qualis_labels\n",
    "\n",
    "    if alt_issn:\n",
    "        qualis_data_cache[alt_issn] = qualis_labels\n",
    "\n",
    "    return qualis_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_qualis_annotation(pub_info, images_URLs, data_source_info):\n",
    "    annotation_dict = {}\n",
    "    \n",
    "    # Create QLattes icon element\n",
    "    qlattes_img_elem = BeautifulSoup('<img>', 'html.parser')\n",
    "    qlattes_img_elem['src'] = images_URLs['qlattesIconURL']\n",
    "    qlattes_img_elem['style'] = 'margin-bottom:-4px'\n",
    "    \n",
    "    annotation_dict['qlattes_img_elem'] = str(qlattes_img_elem)\n",
    "    \n",
    "    # Create Qualis labels annotations\n",
    "    issn_label = f', ISSN {pub_info[\"issn\"]}' if pub_info.get('issn') else ''\n",
    "    \n",
    "    if pub_info['qualisLabels']['qualis'] == 'N':\n",
    "        qualis_annot = f' Não classificado{issn_label}'\n",
    "    else:\n",
    "        qualis_annot = f' {pub_info[\"qualisLabels\"][\"qualis\"]}{issn_label}'\n",
    "        \n",
    "        # add Data source and base year\n",
    "        source = pub_info['qualisLabels']['source']\n",
    "        source_info = data_source_info[source]\n",
    "        \n",
    "        data_source_label = f'{source_info[\"label\"]} ({source_info[\"baseYear\"]})'\n",
    "        qualis_annot += f', fonte {data_source_label}'\n",
    "    \n",
    "    annotation_dict['qualis_annot'] = qualis_annot\n",
    "    \n",
    "    # add icon with link to Google Scholar\n",
    "    base_url = 'https://scholar.google.com/scholar?q='\n",
    "    title_param = f'intitle%3A%22{pub_info[\"title\"].replace(\" \", \"+\")}%22'\n",
    "    link_scholar = f'{base_url}{title_param}'\n",
    "    \n",
    "    annotation_dict['link_scholar'] = link_scholar\n",
    "    \n",
    "    # add icon with link to Scopus (if available)\n",
    "    if pub_info['qualisLabels'].get('linkScopus'):\n",
    "        annotation_dict['link_scopus'] = pub_info['qualisLabels']['linkScopus']\n",
    "    \n",
    "    return annotation_dict\n",
    "\n",
    "def persist_annotation_div():\n",
    "    alert_div = BeautifulSoup('<div>', 'html.parser')\n",
    "    \n",
    "    alert_div['class'] = 'main-content max-width min-width'\n",
    "    alert_div['id'] = 'annot-div'\n",
    "    \n",
    "    print('Alert div persisted!')\n",
    "    \n",
    "    return str(alert_div)\n",
    "\n",
    "\n",
    "def persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo):\n",
    "    annot_dict = {}\n",
    "    \n",
    "    issnLabel = f\", ISSN {pubInfo['issn']}\" if pubInfo['issn'] else pubInfo['issn']\n",
    "    if pubInfo['qualisLabels']['qualis'] == 'N':\n",
    "        qualisAnnot = f\"Não classificado{issnLabel}\"\n",
    "    else:\n",
    "        qualisAnnot = f\"{pubInfo['qualisLabels']['qualis']}{issnLabel}\"\n",
    "        \n",
    "        dataSourceLabel = dataSourceInfo[pubInfo['qualisLabels']['source']]['label']\n",
    "        baseYear = dataSourceInfo[pubInfo['qualisLabels']['source']]['baseYear']\n",
    "        dataSourceLabel += f\" ({baseYear})\"\n",
    "        qualisAnnot += f\", fonte {dataSourceLabel}\"\n",
    "        \n",
    "    annot_dict['qualisAnnot'] = qualisAnnot\n",
    "    \n",
    "    titleParam = f\"intitle:\\\"{pubInfo['title']}\\\"\"\n",
    "    linkScholar = f\"https://scholar.google.com/scholar?q={titleParam}\"\n",
    "    annot_dict['linkScholar'] = linkScholar\n",
    "    \n",
    "    if pubInfo['qualisLabels'].get('linkScopus'):\n",
    "        annot_dict['linkScopus'] = pubInfo['qualisLabels']['linkScopus']\n",
    "    \n",
    "    elem['annotation'] = annot_dict\n",
    "\n",
    "def persist_annotation_div(imagesURLs, visualizationURL):\n",
    "    alert_div_dict = {}\n",
    "    alert_div_dict['id'] = \"annot-div\"\n",
    "    return alert_div_dict\n",
    "\n",
    "def persist_annotation_message(imagesURLs, visualizationURL, recentUpdatesURL, pubCount):\n",
    "    annot_dict = {}\n",
    "    if pubCount > 0:\n",
    "        sChar = 's' if pubCount > 1 else ''\n",
    "        pubCountString = f\"anotou o Qualis de {pubCount} artigo{sChar} em periódico{sChar}  neste CV.\"\n",
    "    else:\n",
    "        pubCountString = \"não anotou nenhum artigo em periódico neste CV.\"\n",
    "        \n",
    "    annot_dict['pubCountString'] = pubCountString\n",
    "    annot_dict['visualizationURL'] = visualizationURL\n",
    "    annot_dict['recentUpdatesURL'] = recentUpdatesURL\n",
    "    return annot_dict\n",
    "\n",
    "def set_attributes(elem, attrs):\n",
    "    for key, value in attrs.items():\n",
    "        elem[key] = value\n",
    "\n",
    "def consolidate_qualis_data(qualisInfo):\n",
    "    pubData = []\n",
    "    pubDataYear = []\n",
    "    currYear = 0\n",
    "    \n",
    "    for qInfo in qualisInfo:\n",
    "        if currYear != qInfo['year']:\n",
    "            if currYear > 0:\n",
    "                pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "                pubDataYear = []\n",
    "            currYear = qInfo['year']\n",
    "        \n",
    "        pubDataItem = {\n",
    "            'issn': qInfo['issn'],\n",
    "            'title': qInfo['title'],\n",
    "            'pubName': qInfo['pubName'],\n",
    "            'qualis': qInfo['qualisLabels']['qualis'],\n",
    "            'baseYear': qInfo['qualisLabels']['baseYear'],\n",
    "            'jcr': qInfo['jcrData']['jcr'] if 'jcr' in qInfo['jcrData'] else 0,\n",
    "            'jcrYear': qInfo['jcrData']['baseYear'] if 'baseYear' in qInfo['jcrData'] else ''\n",
    "        }\n",
    "        pubDataYear.append(pubDataItem)\n",
    "    \n",
    "    if len(qualisInfo) > len(pubData):\n",
    "        pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "        \n",
    "    return pubData\n",
    "\n",
    "# Suponhamos que 'html_content' seja o conteúdo HTML em que as anotações serão inseridas.\n",
    "# html_content = ...\n",
    "\n",
    "# Criamos um objeto BeautifulSoup\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Aqui você pode utilizar os métodos acima para persistir as informações.\n",
    "# Exemplo:\n",
    "# elem = {}\n",
    "# persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML annotation and CV ID\n",
    "annot_html = \"<a href='some_url'>Annotated Data</a>\"\n",
    "cv_id = \"cv_123\"\n",
    "\n",
    "# Store annotation\n",
    "store_annotation_in_db(uri, user, password, annot_html, cv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def process_lattes_page_v1(name_link, images_urls, recent_updates_url, qualis_data, data_source_info): \n",
    "#     print(qualis_data)\n",
    "    \n",
    "#     # Inicializando o WebDriver\n",
    "#     driver_service = Service('path/to/chromedriver')\n",
    "#     driver = webdriver.Chrome(service=driver_service)\n",
    "#     driver.get(name_link['link'])\n",
    "    \n",
    "#     # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "#     visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "#     # Limpar cache de dados de Qualis\n",
    "#     qualis_data_cache = {}\n",
    "    \n",
    "#     # Annotate Lattes page com informações de Qualis\n",
    "#     lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "#     if lattes_info:\n",
    "#         await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "#     # Consolidar dados de publicação a partir das informações de Lattes\n",
    "#     pub_info = consolidate_qualis_data(lattes_info)\n",
    "#     print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "#     # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "#     conn = sqlite3.connect('lattes_data.db')\n",
    "#     cursor = conn.cursor()\n",
    "#     cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "#     existing_data = cursor.fetchall()\n",
    "    \n",
    "#     lattes_data_array = []\n",
    "    \n",
    "#     if existing_data:\n",
    "#         # Filtrar dados existentes para evitar duplicatas\n",
    "#         lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "#     # Adicionar dados de Lattes atuais ao array\n",
    "#     lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "#     # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "#     cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "    \n",
    "#     print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def annotate_published_articles_v2(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup):\n",
    "#     # Localizar o primeiro elemento de artigo publicado\n",
    "#     start_elem = soup.find(\"div\", id=\"artigos-completos\")\n",
    "\n",
    "#     # Retornar uma lista vazia se não houver nenhum artigo publicado no CV\n",
    "#     if start_elem is None:\n",
    "#         return []\n",
    "\n",
    "#     qualis_info = []\n",
    "\n",
    "#     # Encontrar todos os artigos publicados\n",
    "#     pub_elems = start_elem.find_all(\"div\", class_=\"artigo-completo\")\n",
    "\n",
    "#     for pub_elem in pub_elems:\n",
    "#         qualis_pub_info = {\n",
    "#             'year': None,\n",
    "#             'issn': '',\n",
    "#             'title': '',\n",
    "#             'pubName': '',\n",
    "#             'qualisLabels': '',\n",
    "#             'jcrData': {}\n",
    "#         }\n",
    "#         # Obter o ano de publicação\n",
    "#         year_span = pub_elem.find(\"span\", class_=\"informacao-artigo\", attrs={\"data-tipo-ordenacao\": \"ano\"})\n",
    "#         if year_span:\n",
    "#             qualis_pub_info['year'] = int(year_span.text)\n",
    "        \n",
    "#         # Obter dados de publicação\n",
    "#         pub_elem_data = pub_elem.find(\"div\", attrs={\"cvuri\": True})\n",
    "\n",
    "#         if pub_elem_data:\n",
    "#             # Obter informações do periódico\n",
    "#             pub_info_string = pub_elem_data['cvuri']\n",
    "#             # Para fins de simplicidade, omitimos a função escapeHtml já que não é relevante para o BeautifulSoup\n",
    "\n",
    "#             # Obter ISSN, título e nome do periódico\n",
    "#             # Detalhes de implementação podem variar, pois o exemplo original usa JavaScript para manipular atributos DOM\n",
    "#             issn = ''  # Implemente a lógica para extrair o ISSN\n",
    "#             title = ''  # Implemente a lógica para extrair o título\n",
    "#             pub_name = ''  # Implemente a lógica para extrair o nome do periódico\n",
    "            \n",
    "#             qualis_pub_info['issn'] = issn\n",
    "#             qualis_pub_info['title'] = title\n",
    "#             qualis_pub_info['pubName'] = pub_name.upper()\n",
    "\n",
    "#             # Obter classificação Qualis do periódico\n",
    "#             qualis_labels = await get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info)\n",
    "#             qualis_pub_info['qualisLabels'] = qualis_labels\n",
    "\n",
    "#             # Obter dados JCR (omitido neste exemplo; pode ser implementado conforme a necessidade)\n",
    "            \n",
    "#             # Adicionar informações ao vetor qualis_info\n",
    "#             qualis_info.append(qualis_pub_info)\n",
    "            \n",
    "#     return qualis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def persist_to_neo4j(header_data):\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "#     header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "#     graph.create(header_node)\n",
    "#     return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERSÃO 01 de extrair dados\n",
    "# def extrair_dados(soup, verbose=False):\n",
    "#     nome_no = soup.select_one('div.infpessoa h2.nome').text if soup.select_one('div.infpessoa h2.nome') else None\n",
    "    \n",
    "#     if not nome_no:\n",
    "#         logging.error(\"Nome do nó não encontrado. Abortando.\")\n",
    "#         return\n",
    "    \n",
    "#     dados_json = {nome_no: {}}\n",
    "#     celula_principal = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "#     title_wrappers = celula_principal.select('div.title-wrapper')\n",
    "#     if verbose:\n",
    "#         logging.info(f'{len(title_wrappers)} seções de dados lidas com sucesso.')\n",
    "    \n",
    "#     for title_wrapper in title_wrappers:\n",
    "#         nome_secao = extrair_secao(title_wrapper)\n",
    "        \n",
    "#         if nome_secao is None:\n",
    "#             continue\n",
    "        \n",
    "#         if verbose:\n",
    "#             logging.info(f'Marcador de Seção: \"{nome_secao.text}\"')\n",
    "\n",
    "#         titulo = extrair_titulo(title_wrapper)\n",
    "#         if verbose:\n",
    "#             logging.info(f'Marcador de Título: \"{titulo}\"')\n",
    "                    \n",
    "#         chave = nome_secao.text\n",
    "#         dados_json[nome_no][chave] = {}\n",
    "\n",
    "#         # A seleção agora ocorre dentro do contexto de title_wrapper, e não de celula_principal.\n",
    "#         celulas_layout = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "        \n",
    "#         for celula_layout in celulas_layout:\n",
    "#             if celula_layout.find_all('div'):\n",
    "#                 indice, valores = extrair_indices(celula_layout)\n",
    "                \n",
    "#                 if indice and valores:\n",
    "#                     dados_json[nome_no][chave][indice] = valores\n",
    "\n",
    "#     if verbose:\n",
    "#         logging.info(f\"Total de índices extraídos: {len(dados_json[nome_no].keys())}\")\n",
    "#         # Outros blocos de código para depuração e verbosidade\n",
    "#     return dados_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ainda sem separador de seção\n",
    "# from bs4.element import Tag\n",
    "\n",
    "# def traverse(soup, parent_dict, current_section=None, root_properties=None):\n",
    "#     \"\"\"\n",
    "#     Traverses through the soup object recursively and populates the dictionary.\n",
    "#     root_properties: the 'Properties' dictionary at the root level, where all attributes should be stored.\n",
    "#     current_section: the current section being processed, allows to set subsections.\n",
    "#     \"\"\"\n",
    "#     section = None  \n",
    "#     node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "#     node_name = node_name_element.text if node_name_element else None\n",
    "#     if node_name is not None:\n",
    "#         parent_dict['name'] = node_name\n",
    "\n",
    "#     parag_elements = soup.find_all('p')\n",
    "#     for elem in parag_elements:\n",
    "#         class_name = elem.get('class', [None])[0]\n",
    "#         text_content = elem.get_text()\n",
    "#         if class_name:\n",
    "#             parent_dict[class_name] = text_content\n",
    "\n",
    "#     if root_properties is None:\n",
    "#         root_properties = parent_dict.setdefault('Properties', {})\n",
    "\n",
    "#     for child in soup.children:\n",
    "#         if isinstance(child, Tag):\n",
    "\n",
    "#             if child.get('id') == 'artigos-completos':\n",
    "#                 section_elem = child.findChild(\"div\", class_=\"inst_back\")\n",
    "#                 if section_elem:\n",
    "#                     section = section_elem.get_text().strip()\n",
    "#                     print(section)\n",
    "#                 subsection_elem = child.findChild(\"div\", class_=\"cita-artigos\")\n",
    "#                 if subsection_elem:\n",
    "#                     subsection = subsection_elem.get_text().strip()\n",
    "#                 jcr_articles_dict = parse_jcr_articles(soup)\n",
    "#                 if jcr_articles_dict:\n",
    "#                     root_properties.update(jcr_articles_dict['Properties'])\n",
    "\n",
    "#             elif \"layout-cell-3\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "\n",
    "#                     sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-9\")\n",
    "#                     if sibling_cell:\n",
    "#                         cell_values = extract_data_from_cell(sibling_cell)\n",
    "\n",
    "#                         if current_section:\n",
    "#                             root_properties[current_section][cell_key] = cell_values\n",
    "#                         else:\n",
    "#                             root_properties[cell_key] = cell_values\n",
    "\n",
    "#             elif \"layout-cell-1\" in child.get('class', []) and \"text-align-right\" in child.get('class', []):\n",
    "#                 cell_key = extract_data_from_cell(child)\n",
    "#                 if cell_key:\n",
    "#                     cell_key = ' '.join(cell_key)\n",
    "#                     subsection_elem = child.find_previous_sibling(\"div\", class_=\"cita-artigos\")\n",
    "#                     if subsection_elem:\n",
    "#                         subsection = subsection_elem.get_text().strip()\n",
    "\n",
    "#                         current_section_dict = root_properties.get(current_section, {})\n",
    "#                         # Convert to dictionary if it is a list\n",
    "#                         if isinstance(current_section_dict, list):\n",
    "#                             current_section_dict = {i: item for i, item in enumerate(current_section_dict)}\n",
    "#                             root_properties[current_section] = current_section_dict\n",
    "#                         subsection_dict = current_section_dict.setdefault(subsection, {})\n",
    "                        \n",
    "#                         sibling_cell = child.find_next_sibling(\"div\", class_=\"layout-cell-11\")\n",
    "#                         if sibling_cell:\n",
    "#                             cell_values = extract_data_from_cell(sibling_cell)\n",
    "#                             subsection_dict[cell_key] = cell_values\n",
    "\n",
    "#             title = child.find(\"h1\")\n",
    "#             if title:\n",
    "#                 current_section = title.get_text().strip()\n",
    "#                 new_dict = {}\n",
    "#                 root_properties[current_section] = new_dict\n",
    "#                 traverse(child, new_dict, current_section, root_properties)\n",
    "#             else:\n",
    "#                 traverse(child, parent_dict, current_section, root_properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
