{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montar Lista de Pessoal para extrair currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler dados do arquivo Excel do Setor de Recursos Humanos\n",
    "pathdata = './../data/'\n",
    "file_persons = 'fioce_colaboradores-2023.xls'\n",
    "\n",
    "# Ler apenas os cabeçalhos do arquivo Excel\n",
    "headers = pd.read_excel(pathdata+file_persons, skiprows=3, header=0, nrows=0).columns\n",
    "# headers\n",
    "\n",
    "# Usar função para indicar quais colunas devem ser eliminadas na leitura\n",
    "def cols_to_keep(col_name):\n",
    "    return col_name not in ['QUANT','Unnamed: 3','Unnamed: 6','Unnamed: 9','ADICIONAL OCUPACIONAL',\n",
    "                            'EMPRESA/BOLSA/PROGRAMA','GESTOR','ADI','POSSE NA FIOCRUZ',\n",
    "                            'VIGÊNCIA BOLSA/ENCERRAMENTO DO CONTRATO','Unnamed: 17',\n",
    "                            'EMAIL INSTITUCIONAL','EMAIL PESSOAL','GENERO','DATA NASCIMENTO',\n",
    "                            'Unnamed: 22','FORMAÇÃO','ENDEREÇO RESIDENCIAL']\n",
    "\n",
    "# Filtrar cabeçalhos com base na função\n",
    "selected_columns = [col for col in headers if cols_to_keep(col)]\n",
    "\n",
    "# Ler dados do arquivo Excel do Setor de Recursos Humanos\n",
    "fioce_pessoal = pd.read_excel(pathdata+file_persons, skiprows=3, header=0, usecols=selected_columns)\n",
    "print(f'{len(fioce_pessoal.index)} nomes de colaboradores no total, todos vínculos e status')\n",
    "print(f'{len(fioce_pessoal[\"VÍNCULO\"].unique()):3} tipos de vínculos')\n",
    "print('Tipos de vínculos',list(fioce_pessoal['VÍNCULO'].unique()))\n",
    "print('  Tipos de status',list(fioce_pessoal['STATUS'].unique()))\n",
    "filtro1 = fioce_pessoal.VÍNCULO == 'SERVIDOR'\n",
    "filtro2 = fioce_pessoal.STATUS == 'ATIVO'\n",
    "lista_nomes = fioce_pessoal[(filtro1) & (filtro2)]['NOME'].tolist()\n",
    "print(f'{len(lista_nomes)} nomes para extrair currículos')\n",
    "for i,nome in enumerate(lista_nomes):\n",
    "    print(f'{i+1:2}. {nome}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classes\n",
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "# !pip install py2neo --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "# !pip install h5py\n",
    "\n",
    "import time\n",
    "import json\n",
    "import h5py\n",
    "import logging\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from typing import List, Optional, Dict, Union\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from pyjarowinkler.distance import get_jaro_distance\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from flask import render_template_string\n",
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common import exceptions\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, \n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    "    TimeoutException,\n",
    "    WebDriverException\n",
    ")\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "logging.basicConfig(filename='lattes_scraper.log', level=logging.INFO)\n",
    "\n",
    "delay = 10\n",
    "\n",
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    from string import Formatter\n",
    "    \n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    from datetime import timedelta\n",
    "        \n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57\n",
    "\n",
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('  ERRO!! Ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "\n",
    "    caminho = drive+':/'+usuario+pastaraiz+'/'\n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    import os\n",
    "\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    print()\n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout\n",
    "\n",
    "def try_folders(drives, pastas, pastasraiz):\n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    tested_path = drive + i + j\n",
    "                    if os.path.isfile(tested_path + '/chromedriver/chromedriver.exe'):\n",
    "                        logging.info(f\"Listing files in: {tested_path}\")\n",
    "                        logging.info(os.listdir(tested_path))\n",
    "                        return tested_path + '/'\n",
    "                except:\n",
    "                    logging.error('Could not locate a working folder.')\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictToHDF5:\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def create_dataset(self, filename, directory=None):\n",
    "        with h5py.File(f\"{directory or ''}{filename}\", \"w\") as f:\n",
    "            null_group = f.create_group(\"0000\")\n",
    "\n",
    "            for person_dict in self.data_list:  # Corrigido de self.data para self.data_list\n",
    "                if 'curriculo' not in person_dict:\n",
    "                    name = person_dict.get('name', 'Unknown')  # Uso de get() para evitar KeyError\n",
    "                    null_group.attrs[name] = \"No curriculum\"  # Adicionando como atributos ao grupo '0000'\n",
    "                    continue\n",
    "            \n",
    "                person_group = f.create_group(person_dict['id'])\n",
    "                for key, value in person_dict.items():\n",
    "                    if value is None:\n",
    "                        continue\n",
    "\n",
    "                    if isinstance(value, list):\n",
    "                        if not value:  # Skip empty lists\n",
    "                            continue\n",
    "\n",
    "                        dtype = type(value[0])\n",
    "                        if dtype == str:\n",
    "                            dt = h5py.string_dtype(encoding='utf-8')\n",
    "                            person_group.create_dataset(key, (len(value),), dtype=dt, data=value)\n",
    "                        else:\n",
    "                            value = np.array(value, dtype=dtype)\n",
    "                            person_group.create_dataset(key, data=value)\n",
    "                    elif isinstance(value, str):\n",
    "                        dt = h5py.string_dtype(encoding='utf-8')\n",
    "                        person_group.create_dataset(key, (1,), dtype=dt, data=value)\n",
    "                    elif isinstance(value, dict) or isinstance(value, list):\n",
    "                        json_str = json.dumps(value)\n",
    "                        dt = h5py.string_dtype(encoding='utf-8')\n",
    "                        person_group.create_dataset(key, (1,), dtype=dt, data=json_str)\n",
    "                    else:\n",
    "                        person_group.create_dataset(key, data=value)\n",
    "\n",
    "    def extract_id_lattes(self, data_dict):\n",
    "        inf_pes = data_dict.get('InfPes', [])\n",
    "        for item in inf_pes:\n",
    "            if 'ID Lattes:' in item:\n",
    "                return item.split('ID Lattes: ')[-1]\n",
    "        return \"0000\" + str(data_dict.get(\"name\"))\n",
    "\n",
    "    # Para visualização\n",
    "    def print_hdf5_structure(self, filepath):\n",
    "        def recursive_print(group, indentation=0):\n",
    "            print(\"  \" * indentation + f\"Group: {group.name}\")\n",
    "            for key in group.keys():\n",
    "                item = group[key]\n",
    "                if isinstance(item, h5py.Dataset):\n",
    "                    print(\"  \" * (indentation + 1) + f\"Dataset: {key}\")\n",
    "                elif isinstance(item, h5py.Group):\n",
    "                    recursive_print(item, indentation + 1)\n",
    "                    \n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            recursive_print(f)\n",
    "\n",
    "    # Para persisistir em N4j\n",
    "    def persist_to_neo4j(self, filepath, neo4j_url, username, password):\n",
    "        graph = Graph(neo4j_url, auth=(username, password))\n",
    "        \n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            for key in f.keys():\n",
    "                group = f[key]\n",
    "                properties = {}\n",
    "                \n",
    "                if key == '0000':  # Tratar grupo \"0000\" diferentemente\n",
    "                    for attr_name, attr_value in group.attrs.items():\n",
    "                        properties[attr_name] = attr_value\n",
    "                    node = Node(\"NoCurriculumGroup\", **properties)  # Criação de um nó específico para o grupo\n",
    "                else:\n",
    "                    for ds_key in group.keys():\n",
    "                        dataset = group[ds_key]\n",
    "                        properties[ds_key] = dataset[()]\n",
    "                    node = Node(\"Person\", **properties)  # Assumindo que o nó seja do tipo \"Person\"\n",
    "\n",
    "                graph.create(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jPersister:\n",
    "    def __init__(self, uri, username, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_primitives(input_data):\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(input_data, dict):\n",
    "            for key, value in input_data.items():\n",
    "                if isinstance(value, dict):  # Se um valor ainda é um dicionário, converte em string JSON\n",
    "                    input_data[key] = json.dumps(Neo4jPersister.convert_to_primitives(value), ensure_ascii=False)\n",
    "                else:\n",
    "                    input_data[key] = Neo4jPersister.convert_to_primitives(value)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, list):\n",
    "            return [Neo4jPersister.convert_to_primitives(item) for item in input_data]\n",
    "        \n",
    "        elif isinstance(input_data, str):\n",
    "            if 'http://' in input_data or 'https://' in input_data:\n",
    "                parts = input_data.split(\" \")\n",
    "                new_parts = [urllib.parse.quote(part) if part.startswith(('http://', 'https://')) else part for part in parts]\n",
    "                return \" \".join(new_parts)\n",
    "            return input_data\n",
    "        \n",
    "        elif isinstance(input_data, (int, float, bool)):\n",
    "            return input_data\n",
    "        \n",
    "        else:\n",
    "            return str(input_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def debug_and_convert(input_data):\n",
    "        try:\n",
    "            return Neo4jPersister.convert_to_primitives(input_data)\n",
    "        except:\n",
    "            print(\"Conversion failed for:\", input_data)\n",
    "            raise\n",
    "\n",
    "    def persist_data(self, data_dict, label):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MERGE (node:{label}) SET node = $props\"\n",
    "            session.run(query, props=data_dict_primitives)\n",
    "\n",
    "    def update_data(self, node_id, data_dict):\n",
    "        data_dict_primitives = self.convert_to_primitives(data_dict)\n",
    "        with self._driver.session() as session:\n",
    "            query = f\"MATCH (node) WHERE id(node) = {node_id} SET node += $props\"\n",
    "            session.run(query, props=data_dict_primitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Union\n",
    "\n",
    "class ParseSoup:\n",
    "    def __init__(self, driver):\n",
    "        self.base_url = 'http://buscatextual.cnpq.br'\n",
    "        self.session = requests.Session()\n",
    "        self.driver = driver\n",
    "        self.delay = 10\n",
    "        self.soup = None\n",
    "\n",
    "    def to_json(self, data_dict: Dict, filename: str) -> None:\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(data_dict, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to JSON: {e}\")\n",
    "\n",
    "    def to_hdf5(self, data_dict: Dict, filename: str, directory=None) -> None:\n",
    "        try:\n",
    "            converter = DictToHDF5(data_dict)\n",
    "            converter.create_dataset(filename, directory)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to HDF5: {e}\")\n",
    "\n",
    "    def dictlist_to_json(self, data_list: List[Dict], filename: str) -> None:\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(data_list, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to JSON: {e}\")\n",
    "\n",
    "    def dictlist_to_hdf5(self, data_list: List[Dict], filename: str, directory=None) -> None:\n",
    "        try:\n",
    "            converter = DictToHDF5(data_list)\n",
    "            converter.create_dataset(filename, directory)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while saving to HDF5: {e}\")\n",
    "    \n",
    "    def format_string(self, input_str):\n",
    "        # Verifica se a entrada é uma string de oito dígitos\n",
    "        if input_str and len(input_str) == 9:\n",
    "            return input_str\n",
    "        elif input_str and len(input_str) == 8:\n",
    "            # Divide a string em duas partes\n",
    "            part1 = input_str[:4]\n",
    "            part2 = input_str[4:]\n",
    "            \n",
    "            # Concatena as duas partes com um hífen\n",
    "            formatted_str = f\"{part1}-{part2}\"\n",
    "            \n",
    "            return formatted_str\n",
    "        else:\n",
    "            return input_str\n",
    "            \n",
    "    def extract_tit1_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        \n",
    "        # Títulos contendo subseções\n",
    "        tit1a = ['Identificação','Endereço','Formação acadêmica/titulação','Pós-doutorado','Formação Complementar',\n",
    "                'Linhas de pesquisa','Projetos de pesquisa','Projetos de extensão','Projetos de desenvolvimento', 'Revisor de periódico','Revisor de projeto de fomento','Áreas de atuação','Idiomas','Inovação']\n",
    "\n",
    "        tit1b = ['Atuação Profissional'] # dados com subseções\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            \n",
    "            # Verifique se o título está na lista 'tit1'\n",
    "            if titulo in tit1a:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                \n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    divs_layout_cell_3 = data_cell.find_all('div', class_='layout-cell-3')\n",
    "                    divs_layout_cell_9 = data_cell.find_all('div', class_='layout-cell-9')\n",
    "                    keys = []\n",
    "                    vals = []\n",
    "\n",
    "                    for i, j in zip(divs_layout_cell_3, divs_layout_cell_9):\n",
    "                        if divs_layout_cell_3 and divs_layout_cell_9:\n",
    "                            key = i.find('div', class_='layout-cell-pad-5 text-align-right')\n",
    "                            key_text = key.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            keys.append(key_text)\n",
    "                            val = j.find('div', class_='layout-cell-pad-5')\n",
    "                            val_text = val.get_text().strip().replace('\\n', ' ').replace('\\t', '')\n",
    "                            vals.append(val_text)\n",
    "                            if verbose:\n",
    "                                print(f'      {key_text:>3}: {val_text}')\n",
    "\n",
    "                    agg_dict = {key: val for key, val in zip(keys, vals)}\n",
    "                    data_dict[titulo] = Neo4jPersister.convert_to_primitives(agg_dict)\n",
    "            \n",
    "            if titulo in tit1b:\n",
    "                if verbose:\n",
    "                    print(titulo)\n",
    "                \n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")               \n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "\n",
    "                            if 'layout-cell-3' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-9' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        data_dict[titulo][section_name].append(current_data)  # Armazenamos os dados em uma lista\n",
    "\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit2_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "        \n",
    "        database = ''\n",
    "        total_trab_text = 0\n",
    "        total_cite_text = 0\n",
    "        num_fator_h = 0\n",
    "        data_wos_text = ''\n",
    "\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')    \n",
    "        \n",
    "        tit2 = ['Produções', 'Bancas', 'Orientações']\n",
    "\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            \n",
    "            # Verifique se o título está na lista 'tit2'\n",
    "            if titulo in tit2:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "\n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = {}\n",
    "                        if verbose:\n",
    "                            print(f'Seção: {section_name}')\n",
    "\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_subsection = None\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                        if section_name == 'Produção bibliográfica':\n",
    "                            subsections = section.find_next_siblings('div', class_='cita-artigos')\n",
    "                            if verbose:\n",
    "                                print(len(subsections), 'subseções')                       \n",
    "                            for subsection in subsections:                            \n",
    "                                if subsection:\n",
    "                                    subsection_name = subsection.find('b').get_text().strip()\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}') # nomes de subseção como ocorrências \n",
    "                                        print(f'    {len(subsection)} divs na subseção {subsection_name}')                                \n",
    "                                    if subsection_name == 'Citações':\n",
    "                                        current_subsection = subsection_name\n",
    "                                        data_dict[titulo][section_name]['Citações'] = {}\n",
    "                                        sub_section_list = []\n",
    "                                            \n",
    "                                        ## Extrair quantidade de citações e fator H das divs de subseção com classe lyout-cell-12\n",
    "                                        next_siblings = subsection.find_next_siblings(\"div\", class_=\"layout-cell-12\") #acha os irmãos da Subseção\n",
    "\n",
    "                                        for sibling in next_siblings:\n",
    "                                            citation_counts = sibling.findChildren(\"div\", class_=\"web_s\")  # Encontra as divs que tem os Valores de Citações\n",
    "                                            if citation_counts:\n",
    "                                                for i in citation_counts:\n",
    "                                                    database = i.get_text()\n",
    "                                                    total_trab = i.find_next_sibling(\"div\", class_=\"trab\")\n",
    "                                                    if total_trab:\n",
    "                                                        total_trab_text = total_trab.get_text().split(\"Total de trabalhos:\")[1]\n",
    "                                                    total_cite = i.find_next_sibling(\"div\", class_=\"cita\")\n",
    "                                                    if total_cite:\n",
    "                                                        total_cite_text = total_cite.get_text().split(\"Total de citações:\")[1]\n",
    "                                                    fator_h = i.find_next_sibling(\"div\", class_=\"fator\").get_text() if i.find_next_sibling(\"div\", class_=\"fator\") else None\n",
    "                                                    num_fator_h = float(fator_h.replace('Fator H:', '')) if fator_h else None\n",
    "                                                    data_wos = i.find_next_sibling(\"div\", class_=\"detalhes\")\n",
    "                                                    if data_wos:\n",
    "                                                        try:\n",
    "                                                            data_wos_text = data_wos.get_text().split(\"Data:\")[1].strip()\n",
    "                                                        except:\n",
    "                                                            data_wos_text = data_wos.get_text()\n",
    "\n",
    "                                                    # Converta os valores para tipos de dados adequados\n",
    "                                                    total_trab = int(total_trab_text)\n",
    "                                                    total_cite = int(total_cite_text)\n",
    "\n",
    "                                                    citation_numbers = {\n",
    "                                                        \"Database\": database,\n",
    "                                                        \"Total de trabalhos\": total_trab,\n",
    "                                                        \"Total de citações\": total_cite,\n",
    "                                                        \"Índice_H\": num_fator_h,\n",
    "                                                        \"Data\": data_wos_text\n",
    "                                                    }\n",
    "\n",
    "                                                    # Verifique se a subseção atual já existe no dicionário\n",
    "                                                    if 'Citações' not in data_dict[titulo][section_name]:\n",
    "                                                        data_dict[titulo][section_name]['Citações'] = {}  # Inicialize como uma lista vazia\n",
    "\n",
    "                                                    data_dict[titulo][section_name]['Citações'] = citation_numbers\n",
    "\n",
    "                                                    if verbose:\n",
    "                                                        print(f'        {database:>15}: {total_trab:>3} trabalhos, {total_cite:>3} citações, {fator_h}, {data_wos}')\n",
    "                                \n",
    "                            ## Encontrar a div irmã de div subseção com classe layout-cell-12 com artigos\n",
    "                            vals_jcr = []\n",
    "                            div_artigo_geral = data_cell.findChild(\"div\", id=\"artigos-completos\")\n",
    "                            if verbose:\n",
    "                                print(f'Encontrada {len(div_artigo_geral)} div geral de artigos')  \n",
    "                            \n",
    "                            if div_artigo_geral:\n",
    "                                divs_artigos = div_artigo_geral.find_all('div', class_='artigo-completo')\n",
    "                                if verbose:\n",
    "                                    print(len(divs_artigos), 'divs de artigos')\n",
    "                                \n",
    "                                current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "                                if divs_artigos:                              \n",
    "                                    for div_artigo in divs_artigos:\n",
    "                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = {}                                   \n",
    "                                            ## Extrair filhos da classes de artigos completos que estão à frente\n",
    "                                        sibling = div_artigo.findChild()\n",
    "\n",
    "                                        while sibling:\n",
    "                                            classes = sibling.get('class', [])\n",
    "\n",
    "                                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                                    info_dict = {\n",
    "                                                        'data-issn': 'NULL',\n",
    "                                                        'impact-factor': 'NULL',  \n",
    "                                                        'jcr-year': 'NULL',\n",
    "                                                    }\n",
    "                                                    # Remova as tags span da div\n",
    "                                                    for span in sibling.find_all('span'):\n",
    "                                                        span.extract()\n",
    "                                                    \n",
    "                                                    val_text = sibling.get_text(strip=True).strip().replace('\\n',' ').replace('\\t','')\n",
    "\n",
    "                                                    current_data[key] = val_text\n",
    "                                                    if verbose:\n",
    "                                                        print(len(current_data.values()), key, val)\n",
    "\n",
    "                                                    sup_element = sibling.find('sup')\n",
    "\n",
    "                                                    if sup_element:\n",
    "                                                        raw_jcr_data = sup_element.get_text()\n",
    "                                                        # print('sup_element:',sup_element)\n",
    "                                                        img_element = sup_element.find('img')\n",
    "                                                        # print('img_element:',img_element)                                                    \n",
    "                                                        if img_element:\n",
    "                                                            original_title = img_element.get('original-title')\n",
    "                                                            if original_title:\n",
    "                                                                info_list = original_title.split('<br />') if original_title.split('<br />') else original_title\n",
    "                                                                if info_list != 'NULL':\n",
    "                                                                    issn = self.format_string(img_element.get('data-issn'))\n",
    "                                                                    if verbose:\n",
    "                                                                        print(f'impact-factor: {info_list[1].split(\": \")[1]}')\n",
    "                                                                    info_dict = {\n",
    "                                                                        'data-issn': issn,\n",
    "                                                                        'impact-factor': info_list[1].split(': ')[1],\n",
    "                                                                        'jcr-year': info_list[1].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')',''),\n",
    "                                                                        'journal': info_list[0],\n",
    "                                                                    }\n",
    "                                                            else:\n",
    "                                                                if verbose:\n",
    "                                                                    print('Entrou no primeiro Else')\n",
    "                                                                issn = self.format_string(img_element.get('data-issn'))\n",
    "                                                                info_dict = {\n",
    "                                                                    'data-issn': issn,\n",
    "                                                                    'impact-factor': 'NULL',\n",
    "                                                                    'jcr-year': 'NULL',\n",
    "                                                                    'journal': 'NULL',\n",
    "                                                                }\n",
    "                                                    else:\n",
    "                                                        if verbose:\n",
    "                                                                    print('Entrou no segundo Else')\n",
    "                                                        info_dict = {\n",
    "                                                            'data-issn': 'NULL',\n",
    "                                                            'impact-factor': 'NULL',\n",
    "                                                            'jcr-year': 'NULL',\n",
    "                                                            'journal': 'NULL',\n",
    "                                                        }                                                                \n",
    "                                                        \n",
    "                                                    vals_jcr.append(info_dict)\n",
    "                                                    if verbose:\n",
    "                                                        print(f'         {info_dict}')\n",
    "\n",
    "                                                if 'JCR' not in data_dict:\n",
    "                                                    data_dict['JCR'] = []\n",
    "                                                \n",
    "                                                if verbose:\n",
    "                                                    print(len(vals_jcr))\n",
    "                                                data_dict['JCR'] = vals_jcr\n",
    "\n",
    "                                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                                next_sibling = sibling.find_next_sibling()\n",
    "                                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                                    sibling = None\n",
    "                                                else:\n",
    "                                                    if current_data:\n",
    "                                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                                        data_dict[titulo][section_name]['Artigos completos publicados em periódicos'] = converted_data\n",
    "\n",
    "                                            if sibling:\n",
    "                                                sibling = sibling.find_next_sibling()\n",
    "                        else:\n",
    "                            while sibling:\n",
    "                                classes = sibling.get('class', [])\n",
    "\n",
    "                                if 'cita-artigos' in classes:  # Subsection start\n",
    "                                    subsection_name = sibling.find('b').get_text().strip()\n",
    "                                    current_subsection = subsection_name\n",
    "                                    if verbose:\n",
    "                                        print(f'    Subseção: {subsection_name}')\n",
    "                                    data_dict[titulo][section_name][current_subsection] = {}\n",
    "                                    current_data = {}  # Inicializamos o dicionário de dados da subseção atual\n",
    "\n",
    "                                elif 'layout-cell-1' in classes:  # Data key\n",
    "                                    key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                    if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                        val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                        current_data[key] = val\n",
    "\n",
    "                                elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Subsection or section end\n",
    "                                    next_sibling = sibling.find_next_sibling()\n",
    "                                    if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                        sibling = None\n",
    "                                    else:\n",
    "                                        if current_subsection:\n",
    "                                            data_dict[titulo][section_name][current_subsection] = Neo4jPersister.convert_to_primitives(current_data)  # Armazenamos os dados da subseção atual\n",
    "                                if sibling:\n",
    "                                    sibling = sibling.find_next_sibling()\n",
    "        \n",
    "        # Verifique se os dados dos tooltips estão presentes no objeto soup\n",
    "        if 'tooltips' in soup.attrs:\n",
    "            tooltips_data = soup.attrs['tooltips']\n",
    "            agg = []\n",
    "            \n",
    "            for tooltip in tooltips_data:\n",
    "                agg_data = {}\n",
    "                \n",
    "                # Extração do ano JCR a partir do \"original_title\"\n",
    "                if tooltip.get(\"original_title\"):\n",
    "                    jcr_year = tooltip[\"original_title\"].split(': ')[0].replace('Fator de impacto ','').replace('(','').replace(')','')\n",
    "                    agg_data[\"jcr-ano\"] = jcr_year\n",
    "                \n",
    "                # Adicionar todas as chaves e valores do tooltip ao dicionário agg_data\n",
    "                for key, value in tooltip.items():\n",
    "                    agg_data[key] = value\n",
    "                \n",
    "                agg.append(agg_data)\n",
    "            \n",
    "            data_dict['JCR2'] = agg\n",
    "        else:\n",
    "            print('Não foram achados os dados de tooltip')\n",
    "            print(soup.attrs)  \n",
    "            \n",
    "        return data_dict\n",
    "\n",
    "    def extract_tit3_soup(self, soup, data_dict=None, verbose=False):\n",
    "        if data_dict is None:\n",
    "            data_dict = {}\n",
    "\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        divs_title_wrapper = elm_main_cell.find_all('div', class_='title-wrapper')\n",
    "        \n",
    "        # Títulos da seção 'Eventos'\n",
    "        tit3 = ['Eventos']\n",
    "\n",
    "        for div_title_wrapper in divs_title_wrapper:\n",
    "            # Encontre o título do bloco\n",
    "            try:\n",
    "                titulo = div_title_wrapper.find('h1').text.strip()\n",
    "            except:\n",
    "                titulo = 'Não disponível na tag h1 do Currículo Lattes'\n",
    "            data_cells = div_title_wrapper.find_all(\"div\", class_=\"data-cell\")\n",
    "            # Verifique se o título está na lista 'tit3'\n",
    "            if titulo in tit3:\n",
    "                if verbose:\n",
    "                    print(f'Título: {titulo}')\n",
    "                \n",
    "                data_dict[titulo] = {}  # Inicialize o dicionário para o título 'Eventos'\n",
    "                for data_cell in data_cells:\n",
    "                    sections = data_cell.find_all(\"div\", class_=\"inst_back\")\n",
    "                    if verbose:\n",
    "                        print(len(sections), 'seções')\n",
    "\n",
    "                    for section in sections:\n",
    "                        section_name = section.find('b').get_text().strip()\n",
    "                        data_dict[titulo][section_name] = []\n",
    "                        if verbose:\n",
    "                            print(section_name)\n",
    "\n",
    "                        sibling = section.find_next_sibling()\n",
    "                        current_data = {}  # Criamos um dicionário para armazenar os dados da subseção atual\n",
    "\n",
    "                        while sibling:\n",
    "                            classes = sibling.get('class', [])\n",
    "\n",
    "                            if 'layout-cell-1' in classes:  # Data key\n",
    "                                key = sibling.find(\"div\", class_=\"layout-cell-pad-5 text-align-right\").get_text().strip()\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "\n",
    "                                if sibling and 'layout-cell-11' in sibling.get('class', []):  # Check if value is present\n",
    "                                    val = sibling.find(\"div\", class_=\"layout-cell-pad-5\").get_text().strip().replace('\\n', '').replace('\\t','')\n",
    "                                    current_data[key] = val\n",
    "                                    if verbose:\n",
    "                                        print(len(current_data.values()), key, val)\n",
    "\n",
    "                            elif sibling.name == 'br' and 'clear' in sibling.get('class', []):  # Fim de seção/subseção\n",
    "                                next_sibling = sibling.find_next_sibling()\n",
    "                                if next_sibling and 'clear' in next_sibling.get('class', []):\n",
    "                                    sibling = None\n",
    "                                else:\n",
    "                                    if current_data:\n",
    "                                        converted_data = Neo4jPersister.convert_to_primitives(current_data)\n",
    "                                        data_dict[titulo][section_name] = converted_data\n",
    "\n",
    "                            if sibling:\n",
    "                                sibling = sibling.find_next_sibling()\n",
    "        return data_dict\n",
    "\n",
    "    def extract_data(self, soup):\n",
    "        \"\"\"\n",
    "        Aggregates data from various dictionary sources into a consolidated nested dictionary, \n",
    "        ensuring that all nested lists within the dictionaries are transformed into nested dictionaries.\n",
    "        \n",
    "        Parameters:\n",
    "        - soup: BeautifulSoup object, representing the parsed HTML content.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: An aggregated dictionary containing the consolidated data.\n",
    "        \"\"\"\n",
    "        self.soup = soup\n",
    "        \n",
    "        def convert_list_to_dict(lst):\n",
    "            \"\"\"\n",
    "            Converts a list into a dictionary with indices as keys.\n",
    "            \n",
    "            Parameters:\n",
    "            - lst: list, input list to be transformed.\n",
    "            \n",
    "            Returns:\n",
    "            - dict: Transformed dictionary.\n",
    "            \"\"\"\n",
    "            return {str(i): item for i, item in enumerate(lst)}\n",
    "\n",
    "        def merge_dict(d1, d2):\n",
    "            \"\"\"\n",
    "            Recursively merges two dictionaries, transforming nested lists into dictionaries.\n",
    "            \n",
    "            Parameters:\n",
    "            - d1: dict, the primary dictionary into which data is merged.\n",
    "            - d2: dict or list, the secondary dictionary or list from which data is sourced.\n",
    "            \n",
    "            Returns:\n",
    "            - None\n",
    "            \"\"\"\n",
    "            # If d2 is a list, convert it to a dictionary first\n",
    "            if isinstance(d2, list):\n",
    "                d2 = convert_list_to_dict(d2)\n",
    "            \n",
    "            for key, value in d2.items():\n",
    "                if isinstance(value, list):\n",
    "                    d2[key] = convert_list_to_dict(value)\n",
    "                if key in d1 and isinstance(d1[key], dict) and isinstance(value, dict):\n",
    "                    merge_dict(d1[key], value)\n",
    "                else:\n",
    "                    d1[key] = value\n",
    "\n",
    "        # Extract necessary information from soup\n",
    "        elm_main_cell = soup.find(\"div\", class_=\"layout-cell-pad-main\")\n",
    "        info_list = [x.strip() for x in elm_main_cell.find(\"div\", class_=\"infpessoa\").get_text().split('\\n') if x.strip() !='']\n",
    "        name = info_list[0]\n",
    "\n",
    "        # Initialization of the aggregated_data dictionary\n",
    "        aggregated_data = {\"labels\": \"Person\", \"name\": name, \"InfPes\": info_list, \"Resumo\": [elm_main_cell.find(\"p\", class_=\"resumo\").get_text().strip()]}\n",
    "\n",
    "        # Data extraction and merging\n",
    "        for data_extraction_func in [self.extract_tit1_soup, self.extract_tit2_soup, self.extract_tit3_soup]:\n",
    "            extracted_sections = data_extraction_func(soup)\n",
    "            for title, data in extracted_sections.items():\n",
    "                if title not in aggregated_data:\n",
    "                    aggregated_data[title] = {}\n",
    "                merge_dict(aggregated_data[title], data)\n",
    "\n",
    "        return aggregated_data\n",
    "        \n",
    "    def process_single_result(self, extracted_data: Dict, json_filename: str, hdf5_filename: str) -> Optional[Dict]:\n",
    "        try:\n",
    "            processed_data = {}\n",
    "            processed_data['name'] = extracted_data.get('name', 'N/A')\n",
    "            processed_data['affiliation'] = extracted_data.get('affiliation', 'N/A')\n",
    "            processed_data['publications'] = int(extracted_data.get('publications', 0))\n",
    "\n",
    "            self.to_json([processed_data], json_filename)\n",
    "            self.to_hdf5([processed_data], hdf5_filename)\n",
    "\n",
    "            return processed_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during single result processing: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_all_results(self, \n",
    "                            all_extracted_data: List[Dict], \n",
    "                            json_filename: str, \n",
    "                            hdf5_filename: str) -> List[Dict]:\n",
    "        successful_processed_data = []\n",
    "        for extracted_data in all_extracted_data:\n",
    "            processed_data = self.process_single_result(extracted_data, json_filename, hdf5_filename)\n",
    "            if processed_data is not None:\n",
    "                successful_processed_data.append(processed_data)\n",
    "            else:\n",
    "                self.failed_extractions.append(extracted_data)\n",
    "\n",
    "        if self.failed_extractions:\n",
    "            logging.info(\"Retrying failed extractions...\")\n",
    "            for failed_data in self.failed_extractions:\n",
    "                processed_data = self.process_single_result(failed_data, json_filename, hdf5_filename)\n",
    "                if processed_data is not None:\n",
    "                    successful_processed_data.append(processed_data)\n",
    "\n",
    "        self.to_json(successful_processed_data, json_filename)\n",
    "        self.to_hdf5(successful_processed_data, hdf5_filename)\n",
    "\n",
    "        return successful_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_driver(caminho):\n",
    "    '''\n",
    "    Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    \n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    # options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    # driver   = webdriver.Chrome(options=options)\n",
    "    driver_path=caminho+'chromedriver/chromedriver.exe'\n",
    "    # print(driver_path)\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service)    \n",
    "    url_busca = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_busca) # acessa a url de busca do CNPQ   \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    return driver\n",
    "\n",
    "class LattesScraper:\n",
    "    def __init__(self, driver, institution, unit, term):\n",
    "        self.base_url = 'http://buscatextual.cnpq.br'\n",
    "        self.session = requests.Session()\n",
    "        self.driver = driver\n",
    "        self.delay = 10\n",
    "\n",
    "    def wait_for_element(self, css_selector: str, ignored_exceptions=None):\n",
    "        \"\"\"\n",
    "        Waits for the element specified by the CSS selector to load.\n",
    "        :param css_selector: CSS selector of the element to wait for\n",
    "        :param ignored_exceptions: List of exceptions to ignore\n",
    "        \"\"\"\n",
    "        WebDriverWait(self.driver, self.delay, ignored_exceptions=ignored_exceptions).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))\n",
    "\n",
    "    def paginar(self, driver):\n",
    "        '''\n",
    "        Helper function to page results on the search page\n",
    "        '''\n",
    "        numpaginas = []\n",
    "        css_paginacao = \"div.paginacao:nth-child(2)\"\n",
    "        try:\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "            paginacao = self.driver.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "            paginas = paginacao.text.split(' ')\n",
    "            remover = ['', 'anterior', '...']\n",
    "            numpaginas = [x for x in paginas if x not in remover]\n",
    "        except Exception as e:\n",
    "            print('  ERRO!! Ao rodar função paginar():', e)\n",
    "        return numpaginas\n",
    "\n",
    "    def retry(self, func, expected_ex_type=Exception, limit=0, wait_ms=200,\n",
    "              wait_increase_ratio=2, on_exhaust=\"throw\"):\n",
    "        attempt = 1\n",
    "        while True:\n",
    "            try:\n",
    "                return func()\n",
    "            except Exception as ex:\n",
    "                if not isinstance(ex, expected_ex_type):\n",
    "                    raise ex\n",
    "                if 0 < limit <= attempt:\n",
    "                    if on_exhaust == \"throw\":\n",
    "                        raise ex\n",
    "                    return on_exhaust\n",
    "                attempt += 1\n",
    "                time.sleep(wait_ms / 1000)\n",
    "                wait_ms *= wait_increase_ratio\n",
    "\n",
    "    def find_terms(self, NOME, instituicao, unidade, termo, delay, limite):\n",
    "        \"\"\"\n",
    "        Função para manipular o HTML até abir a página HTML de cada currículo   \n",
    "\n",
    "        Parâmeteros:\n",
    "            - NOME: É o nome completo de cada pesquisador\n",
    "            - Instituição, unidade e termo: Strings a buscar no currículo para reduzir duplicidades\n",
    "            - driver (webdriver object): The Selenium webdriver object.\n",
    "            - limite (int): Número máximo de tentativas em casos de erro.\n",
    "            - delay (int): tempo em milisegundos a esperar nas operações de espera.\n",
    "        \n",
    "        Retorna:\n",
    "            elm_vinculo, np.NaN, np.NaN, np.NaN, driver.\n",
    "        \n",
    "        Em caso de erro retorna:\n",
    "            None, NOME, np.NaN, e, driver\n",
    "        \"\"\"\n",
    "\n",
    "        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "        \n",
    "        # Inicializando variáveis para evitar UnboundLocalError\n",
    "        elm_vinculo = None\n",
    "        qte_resultados = 0\n",
    "\n",
    "        ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "        duvidas   = []\n",
    "        force_break_loop = False\n",
    "        try:\n",
    "            # Wait and fetch the number of results\n",
    "            css_resultados = \".resultado\"\n",
    "            WebDriverWait(driver, delay, ignored_exceptions=ignored_exceptions).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "            resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "        \n",
    "            ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "            try:\n",
    "                css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                div_element = soup.find('div', {'class': 'tit_form'})\n",
    "                match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "                if match:\n",
    "                    qte_resultados = int(match.group(1))\n",
    "                    # print(f'{qte_resultados} resultados para {NOME}')\n",
    "                else:\n",
    "                    return None, NOME, np.NaN, 'Currículo não encontrado', driver\n",
    "            except Exception as e1:\n",
    "                print('  ERRO!! Currículo não disponível no Lattes')\n",
    "                return None, NOME, np.NaN, e1, driver\n",
    "            \n",
    "            ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "            ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "            numpaginas = self.paginar(driver)\n",
    "            if numpaginas == [] and qte_resultados==1:\n",
    "                # capturar link para o primeiro nome resultado da busca\n",
    "                try:\n",
    "                    css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                    nome_vinculo = elm_vinculo.text\n",
    "                except Exception as e2:\n",
    "                    print('  ERRO!! Ao encontrar o primeiro resultado da lista de nomes:', e2)\n",
    "                    \n",
    "                    # Call the handle stale file_error function\n",
    "                    if self.handle_stale_file_error(driver):\n",
    "                        # If the function returns True, it means the error was resolved.\n",
    "                        # try to get the nome_vinculo again:\n",
    "                        try:\n",
    "                            elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                            nome_vinculo = elm_vinculo.text\n",
    "                        except Exception as e3:\n",
    "                            print('  ERRO!! Servidor CNPq indisponível no momento, tentar em alguns minutos:', e3)\n",
    "                            return None, NOME, np.NaN, e3, driver\n",
    "                    else:\n",
    "                        # If the function returns False, it means the error was not resolved within the given retries.\n",
    "                        return None, NOME, np.NaN, e2, driver\n",
    "\n",
    "                    print('  Não foi possível extrair por falha no servidor do CNPq:',e)\n",
    "                    return None, NOME, np.NaN, e2, driver\n",
    "                # print('Clicar no nome único:', nome_vinculo)\n",
    "                try:\n",
    "                    self.retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                        wait_ms=20,\n",
    "                        limit=limite,\n",
    "                        on_exhaust=(f'  Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "                except Exception as e4:\n",
    "                    print('  ERRO!! Ao clicar no único nome encontrado anteriormente',e)\n",
    "                    return None, NOME, np.NaN, e4, driver\n",
    "            \n",
    "            ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "            else:\n",
    "                print(f'       {qte_resultados:>2} homônimos de: {NOME}')\n",
    "                numpaginas = self.paginar(driver)\n",
    "                numpaginas.append('próximo')\n",
    "                iteracoes=0\n",
    "                ## iterar em cada página de resultados\n",
    "                pagin = qte_resultados//10+1\n",
    "                for i in range(pagin+1):\n",
    "                    # print(i,'/',pagin)\n",
    "                    iteracoes+=1\n",
    "                    try:\n",
    "                        numpaginas = self.paginar(driver)\n",
    "                        print(f'       Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                        css_resultados = \".resultado\"\n",
    "                        WebDriverWait(driver, delay).until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                    except Exception as e:\n",
    "                        print('  ERRO!! Ao paginar:',e)\n",
    "                    ## iterar em cada resultado\n",
    "                    for n,i in enumerate(resultados):\n",
    "                        linhas = i.text.split('\\n\\n')\n",
    "                        # print(linhas)\n",
    "                        if 'Stale file handle' in str(linhas):\n",
    "                            return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                        for m,linha in enumerate(linhas):\n",
    "                            # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                            # print('Conteúdo da linha:',linha.lower())\n",
    "                            # print(linha)\n",
    "                            try:\n",
    "                                if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                    # print('Vínculo encontrado!')\n",
    "                                    count=m\n",
    "                                    # print(' NOME:', NOME, type(NOME))\n",
    "                                    # test = linhas[count].split('\\n')[0]\n",
    "                                    # print('TESTE:',test, type(test))\n",
    "                                    while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                        count-=1\n",
    "                                    print('       Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                    nome_vinculo = linhas[count].strip()\n",
    "                                    print(f'       Achado: {nome_vinculo}')\n",
    "                                    try:\n",
    "                                        css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                        # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                        css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                        WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                        elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                        nome_vinculo = elm_vinculo.text\n",
    "                                        # print('Elemento retornado:',nome_vinculo)\n",
    "                                        self.retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                            wait_ms=100,\n",
    "                                            limit=limite,\n",
    "                                            on_exhaust=(f'  Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                    except Exception as e5:\n",
    "                                        print('  ERRO!! Ao achar o link do nome com múltiplos resultados')\n",
    "                                        return np.NaN, NOME, np.NaN, e5, driver\n",
    "                                    force_break_loop = True\n",
    "                                    break\n",
    "                            except Exception as e6:\n",
    "                                traceback_str = ''.join(traceback.format_tb(e6.__traceback__))\n",
    "                                print('  ERRO!! Ao procurar vínculo com currículos achados')    \n",
    "                                print(e6,traceback_str)\n",
    "                            ## Caso percorra toda lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                            if m==(qte_resultados):\n",
    "                                print(f'Nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                                duvidas.append(NOME)\n",
    "                                # clear_output(wait=True)\n",
    "                                # driver.quit()\n",
    "                                continue\n",
    "                        if force_break_loop:\n",
    "                            break\n",
    "                    try:\n",
    "                        prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                        prox.click()\n",
    "                    except:\n",
    "                        continue\n",
    "            try:\n",
    "                elm_vinculo.text\n",
    "                # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "            except:\n",
    "                return None, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "        except exceptions.TimeoutException:\n",
    "            print(\"  ERRO!! O tempo limite de espera foi atingido.\")\n",
    "            return None, NOME, np.NaN, \"TimeoutException\", driver\n",
    "        except exceptions.WebDriverException as e7:\n",
    "            print(\"  ERRO!! Problema ao interagir com o driver.\")\n",
    "            return None, NOME, np.NaN, e7, driver\n",
    "        except Exception as e8:\n",
    "            print(\"  ERRO 8!! Um erro inesperado ocorreu.\")\n",
    "            print(f'  {e8}')\n",
    "            return None, NOME, np.NaN, e8, driver\n",
    "        # Verifica antes de retornar para garantir que elm_vinculo foi definido\n",
    "        if elm_vinculo is None:\n",
    "            print(\"Vínculo não foi definido.\")\n",
    "            return None, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "        # Retorna a saída de sucesso\n",
    "        return elm_vinculo, np.NaN, np.NaN, np.NaN, driver\n",
    "\n",
    "    def handle_stale_file_error(self, max_retries=5, retry_interval=10):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                error_div = self.driver.find_element(By.CSS_SELECTOR, 'resultado')\n",
    "                linha1 = error_div.fidChild('li')\n",
    "                if 'Stale file handle' in linha1.text:\n",
    "                    time.sleep(retry_interval)\n",
    "                else:\n",
    "                    return True\n",
    "            except NoSuchElementException:\n",
    "                return True\n",
    "        return False\n",
    "       \n",
    "    def extract_data_from_cvuri(self, element) -> dict:\n",
    "        \"\"\"\n",
    "        Extracts data from the cvuri attribute of the given element.\n",
    "        :param element: WebElement object\n",
    "        :return: Dictionary of extracted data\n",
    "        \"\"\"\n",
    "        cvuri = element.get_attribute('cvuri')\n",
    "        parsed_url = urlparse(cvuri)\n",
    "        params = parse_qs(parsed_url.query)\n",
    "        data_dict = {k: v[0] for k, v in params.items()}\n",
    "        return data_dict\n",
    "\n",
    "    def fill_name(self, NOME):\n",
    "        '''\n",
    "        Move cursor to the search field and fill in the specified name.\n",
    "        '''\n",
    "        if self.driver is None:\n",
    "            logging.error(\"O driver não foi inicializado corretamente.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            nome = lambda: self.driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "            nome().send_keys(Keys.CONTROL + \"a\")\n",
    "            nome().send_keys(NOME)\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(f'  ERRO!! Ao colar nome para buscar.') #, {traceback_str}\n",
    "        try:            \n",
    "            seletorcss = 'div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "            \n",
    "            seletorcss = \"#botaoBuscaFiltros\"\n",
    "            WebDriverWait(self.driver, self.delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        except Exception as e:\n",
    "            traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "            print(f'  ERRO!! Ao clicar no botão Buscar.\\n{e}, {traceback_str}')\n",
    "\n",
    "    def return_search_page(self):\n",
    "        url_busca = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "        driver.get(url_busca) # acessa a url de busca do CNPQ        \n",
    "\n",
    "    def check_and_click_vinculo(self, elm_vinculo):\n",
    "        if elm_vinculo is None:\n",
    "            self.return_search_page()\n",
    "            logging.error(\"Vínculo não encontrado, passando para o próximo nome...\")\n",
    "        try:\n",
    "            logging.info(f'Vínculo encontrado no currículo de nome: {elm_vinculo.text}')\n",
    "        except AttributeError:\n",
    "            self.return_search_page()\n",
    "            logging.error(\"Vínculo não encontrado, passando para o próximo nome...\")\n",
    "        try:\n",
    "            # Clicar no botão para abrir o currículo\n",
    "            btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "            time.sleep(0.2)\n",
    "            ActionChains(driver).click(btn_abrir_curriculo).perform()            \n",
    "            # logging.info('Successfully clicked on the vínculo.')\n",
    "        except WebDriverException:\n",
    "            self.return_search_page()\n",
    "            logging.error('Falha ao clicar no link do nome.')\n",
    "\n",
    "    def switch_to_new_window(self):\n",
    "        window_before = self.driver.current_window_handle\n",
    "        WebDriverWait(self.driver, self.delay).until(EC.number_of_windows_to_be(2))\n",
    "        window_after = self.driver.window_handles\n",
    "        new_window = [x for x in window_after if x != window_before][0]\n",
    "        self.driver.switch_to.window(new_window)\n",
    "\n",
    "    def switch_back_to_original_window(self):\n",
    "        current_window = self.driver.current_window_handle\n",
    "        original_window = [x for x in self.driver.window_handles if x != current_window][0]\n",
    "\n",
    "        # Close the current window\n",
    "        self.driver.close()\n",
    "\n",
    "        # Switch back to the original window\n",
    "        self.driver.switch_to.window(original_window)\n",
    "\n",
    "    def extract_tooltip_data(self) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Extracts tooltip data from articles section using Selenium.\n",
    "        :return: List of dictionaries containing the extracted tooltip data\n",
    "        \"\"\"\n",
    "        tooltip_data_list = []\n",
    "        try:\n",
    "            self.wait_for_element(\"#artigos-completos img.ajaxJCR\", [TimeoutException])\n",
    "            layout_cells = self.driver.find_elements(By.CSS_SELECTOR, '#artigos-completos .layout-cell-11 .layout-cell-pad-5')\n",
    "\n",
    "            for cell in layout_cells:\n",
    "                tooltip_data = {}\n",
    "                try:\n",
    "                    elem_citado = cell.find_element(By.CSS_SELECTOR, '.citado')\n",
    "                    tooltip_data.update(self.extract_data_from_cvuri(elem_citado))\n",
    "                except (ElementNotInteractableException, NoSuchElementException):\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    doi_elem = cell.find_element(By.CSS_SELECTOR, \"a.icone-producao.icone-doi\")\n",
    "                    tooltip_data[\"doi\"] = doi_elem.get_attribute(\"href\")\n",
    "                except NoSuchElementException:\n",
    "                    tooltip_data[\"doi\"] = None\n",
    "\n",
    "                try:\n",
    "                    self.wait_for_element(\"img.ajaxJCR\", [TimeoutException])\n",
    "                    tooltip_elem = self.driver.find_element(By.CSS_SELECTOR, \"img.ajaxJCR\")\n",
    "                    ActionChains(self.driver).move_to_element(tooltip_elem).perform()\n",
    "                    \n",
    "                    original_title = tooltip_elem.get_attribute(\"original-title\")\n",
    "                    match = re.search(r\"Fator de impacto \\(JCR \\d{4}\\): (\\d+\\.\\d+)\", original_title)\n",
    "                    tooltip_data[\"impact-factor\"] = match.group(1) if match else None\n",
    "                    tooltip_data[\"original_title\"] = original_title.split('<br />')[0].strip()\n",
    "\n",
    "                except (NoSuchElementException, TimeoutException):\n",
    "                    pass\n",
    "                \n",
    "                tooltip_data_list.append(tooltip_data)\n",
    "            print(f'       {len(tooltip_data_list):>003} artigos extraídos')\n",
    "            logging.info(f'{len(tooltip_data_list):>003} artigos extraídos')\n",
    "\n",
    "        except TimeoutException as e:\n",
    "            logging.error(f\"Sem respota antes do timeout\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro inesperado ao extrair tooltips: {e}\")\n",
    "        return tooltip_data_list\n",
    "            \n",
    "    def search_profile(self, name, instituicao, unidade, termo):\n",
    "        try:\n",
    "            # Find terms to interact with the web page and extract the profile\n",
    "            profile_element, _, _, _, _ = self.find_terms(\n",
    "                name, \n",
    "                instituicao,  \n",
    "                unidade,  \n",
    "                termo,  \n",
    "                10,  \n",
    "                3  \n",
    "            )\n",
    "            # print('Elemento encontrado:', profile_element)\n",
    "            if profile_element:\n",
    "                return profile_element\n",
    "            else:\n",
    "                self.return_search_page()\n",
    "                logging.info(f'Currículo não encontrado: {name}')\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            logging.error(f\"HTTPError occurred: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro inesperado ao buscar: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    def scrape(self, driver, name_list, instituicao, unidade, termo, json_filename, hdf5_filename):\n",
    "        dict_list=[]\n",
    "        for k, name in enumerate(name_list):\n",
    "            try:\n",
    "                print(f'{k+1:>2}/{len(name_list)}: {name}')\n",
    "                self.fill_name(name)\n",
    "                elm_vinculo = self.search_profile(name, instituicao, unidade, termo)\n",
    "                self.check_and_click_vinculo(elm_vinculo)\n",
    "                self.switch_to_new_window()\n",
    "                \n",
    "                if elm_vinculo:\n",
    "                    tooltip_data_list = self.extract_tooltip_data()\n",
    "                    page_source = driver.page_source\n",
    "                    if page_source is not None:\n",
    "                        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                        soup.attrs['tooltips'] = tooltip_data_list                 \n",
    "                        if soup:\n",
    "                            # print('Extraindo dados do objeto Soup...')\n",
    "                            parse_soup_instance = ParseSoup(driver)\n",
    "                            data = parse_soup_instance.extract_data(soup)\n",
    "                            # Chama métodos de conversão de dicionário individual\n",
    "                            # parse_soup_instance.to_json(data, json_filename)\n",
    "                            # parse_soup_instance.to_hdf5(data, hdf5_filename)\n",
    "                            dict_list.append(data)\n",
    "                    else:\n",
    "                        logging.error(f\"Could not get soup for profile: {name}\")\n",
    "                else:\n",
    "                    logging.error(f\"Currículo não encontrado para: {name}\")\n",
    "\n",
    "                # Fechar janela do currículo e voltar para página de busca\n",
    "                self.switch_back_to_original_window()\n",
    "\n",
    "                # Clicar no botão para fechar janela pop-up\n",
    "                btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnfechar\")))\n",
    "                ActionChains(driver).click(btn_abrir_curriculo).perform()    \n",
    "                # logging.info('Successfully closed pop-up.')           \n",
    "\n",
    "                # # Clicar no botão para fazer nova consulta\n",
    "                # btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "                #     EC.element_to_be_clickable((By.CSS_SELECTOR, \"#botaoBuscaFiltros\")))\n",
    "                # ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "                self.return_search_page()\n",
    "                # logging.info('Successfully restarded extraction.')\n",
    "            except TimeoutException as e:\n",
    "                logging.error(f\"Sem resposta antes do timeout para: {name}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erro inesperado ao extrair para: {name}: {str(e)}\")\n",
    "        driver.quit()\n",
    "        return dict_list\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":   \n",
    "    drives=['C:/Users/','E:/','./home/']\n",
    "    pastas=['marcos.aires/', 'marco/']\n",
    "    pastasraiz=['kgfioce','fioce']\n",
    "    pasta_dados = './../data/'\n",
    "    pastaraiz = 'fioce'\n",
    "    caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    pathzip, pathcsv, pathjson, pathfig, pathaux, pathout = preparar_pastas(caminho)\n",
    "\n",
    "    instituicao = 'Fundação Oswaldo Cruz'\n",
    "    unidade = 'Fiocruz Ceará'\n",
    "    termo = 'Ministério da Saúde'\n",
    "    driver = connect_driver(caminho)\n",
    "    t0 = time.time()\n",
    "    scraper = LattesScraper(driver, instituicao, unidade, termo)\n",
    "    dict_list = scraper.scrape(driver, lista_nomes, instituicao, unidade, termo,\n",
    "                               pasta_dados+\"output.json\", pasta_dados+\"output.hdf5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo(t0,time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lattes_scraper.log', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
