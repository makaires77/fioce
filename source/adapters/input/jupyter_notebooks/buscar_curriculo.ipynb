{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ambiente beakerx\n",
    "# !pip install py2neo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, sqlite3, asyncio\n",
    "import os, re, time, traceback, json\n",
    "import warnings, csv, sys, pip, string\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from string import Formatter\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from flask import render_template_string\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "\n",
    "## Configurar exibição dos dataframes do pandas na tela\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "pd.set_option('display.max_rows', 600)\n",
    "\n",
    "delay = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database credentials and URI\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "\n",
    "def connect_driver():\n",
    "    '''\n",
    "    Função 1: Conecta ao servidor do CNPq para busca de currículo\n",
    "    '''\n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    \n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    driver   = webdriver.Chrome(options=options)\n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    driver.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    \n",
    "    driver.set_window_position(-20, -10)\n",
    "    driver.set_window_size(170, 1896)\n",
    "    driver.mouse = webdriver.ActionChains(driver)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "\n",
    "    # return browser, url, session_id\n",
    "    return driver\n",
    "\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    try:\n",
    "        graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "    except:\n",
    "        print('Erro ao conectar ao Neo4j')\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'].split('(')[1].strip(')'), meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    print(type(header_node))\n",
    "\n",
    "    return header_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_name(driver, delay, NOME):\n",
    "    '''\n",
    "    Função 2: passa o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # driver.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def achar_busca(driver, delay):\n",
    "    '''\n",
    "    Função 3: clica no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = driver.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "               #expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "               #logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_curriculum(driver,elm_vinculo):\n",
    "    link_nome     = achar_busca(driver, delay)\n",
    "    window_before = driver.current_window_handle\n",
    "\n",
    "    if str(elm_vinculo) == 'nan':\n",
    "        print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "        falhas.append(nome_falha)\n",
    "        duvidas.append(duvida)\n",
    "        tipo_erro.append(erro)\n",
    "        # print(nome_falha)\n",
    "        # print(erro)\n",
    "        # clear_output(wait=True)\n",
    "        raise Exception\n",
    "    print('Vínculo encontrado no currículo de nome:',elm_vinculo.text)\n",
    "\n",
    "    ## Clicar no botão abrir currículo e mudar de aba\n",
    "    try:\n",
    "        ## Aguarda, encontra, clica em buscar nome\n",
    "        link_nome    = achar_busca(driver, delay)\n",
    "        nome_buscado = []\n",
    "        nome_achado  = []\n",
    "        nome_buscado.append(NOME)\n",
    "    except Exception as e:\n",
    "        print('Erro')\n",
    "        print(e)\n",
    "        \n",
    "    if link_nome.text == None:\n",
    "        xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "        # 'Stale file handle'\n",
    "        print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "        retry(WebDriverWait(driver, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "        #    expected_ex_type=ZeroDivisionError, \n",
    "        wait_ms=200,\n",
    "        limit=limite, \n",
    "        #    logger=logger, \n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "    try:\n",
    "        ActionChains(driver).click(link_nome).perform()\n",
    "        nome_achado.append(link_nome.text)\n",
    "    except:\n",
    "        print(f'Currículo não encontrado para: {NOME}.')\n",
    "\n",
    "    retry(WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "        #    expected_ex_type=ZeroDivisionError, \n",
    "        wait_ms=200,\n",
    "        limit=limite, \n",
    "        #    logger=logger, \n",
    "        on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "    # Clicar botão para abrir o currículo\n",
    "    btn_abrir_curriculo = WebDriverWait(driver, delay).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    ActionChains(driver).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "    ## Gerenciamento das janelas abertas no browser\n",
    "    WebDriverWait(driver, delay).until(EC.number_of_windows_to_be(2))\n",
    "    window_after = driver.window_handles\n",
    "    new_window   = [x for x in window_after if x != window_before][0]\n",
    "    driver.switch_to.window(new_window)\n",
    "\n",
    "    # Pega o código fonte da página\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    # Usa BeautifulSoup para analisar\n",
    "    soup = BeautifulSoup(page_source, 'html.parser') \n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_terms(NOME, instituicao, unidade, termo, driver, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    \n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(driver, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(driver)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            ## TO-DO inserir a crítica para o 'Stale file handle'\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(driver, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, driver\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(driver)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(driver)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(driver, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = driver.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', driver\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = driver.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(driver).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, driver\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # driver.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = driver.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', driver\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, driver\n",
    "    \n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERSÃO 01\n",
    "import logging\n",
    "\n",
    "def extrair_indices(layout_celula):\n",
    "    indice = layout_celula.select_one('div.layout-cell-pad-5.text-align-right')\n",
    "    valor = layout_celula.select_one('div.layout-cell.layout-cell-9 div.layout-cell-pad-5')\n",
    "    \n",
    "    if indice and valor:\n",
    "        valores_extraidos = valor.text.split('<br class=\"clear\">') if '<br class=\"clear\">' in valor.text else valor.text.split('\\n\\t\\t\\n\\t')\n",
    "        return indice.text, valores_extraidos\n",
    "\n",
    "def encontrar_subchave(title_wrapper):\n",
    "    tags_relevantes  = ['ul', 'a', 'b']\n",
    "    tags_encontradas = [(tag, title_wrapper.find(tag)) for tag in tags_relevantes]\n",
    "    tags_ordenadas   = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def extrair_secao(title_wrapper):\n",
    "    # Inicializa a lista de tags relevantes e a lista que armazenará os elementos encontrados\n",
    "    tags_relevantes = ['h1', 'ul', 'b']\n",
    "    tags_encontradas = []\n",
    "\n",
    "    # Loop para encontrar os demais elementos relevantes\n",
    "    for tag in tags_relevantes:\n",
    "        elemento_encontrado = title_wrapper.select_one(tag)\n",
    "        if elemento_encontrado:\n",
    "            tags_encontradas.append((tag, elemento_encontrado))\n",
    "\n",
    "    # Ordena os elementos encontrados com base na linha de origem no código fonte HTML\n",
    "    tags_ordenadas = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "\n",
    "    # Retorna o primeiro elemento da lista ordenada, se houver elementos\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def extrair_titulo(title_wrapper):\n",
    "    inst_back_texts = {}\n",
    "    inst_back_elements = title_wrapper.find_all('div', class_='inst_back')\n",
    "    for index, inst_back in enumerate(inst_back_elements):\n",
    "        b_tag = inst_back.find('b')\n",
    "        if b_tag:\n",
    "            inst_back_texts[index] = b_tag.text.strip()\n",
    "\n",
    "    return inst_back_texts if inst_back_texts else None\n",
    "\n",
    "def extrair_dados(soup, verbose=False):\n",
    "    nome_no = soup.select_one('div.infpessoa h2.nome').text if soup.select_one('div.infpessoa h2.nome') else None\n",
    "    \n",
    "    if not nome_no:\n",
    "        logging.error(\"Nome do nó não encontrado. Abortando.\")\n",
    "        return\n",
    "    \n",
    "    dados_json = {nome_no: {}}\n",
    "    celula_principal = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "    title_wrappers = celula_principal.select('div.title-wrapper')\n",
    "    if verbose:\n",
    "        logging.info(f'{len(title_wrappers)} seções de dados lidas com sucesso.')\n",
    "    \n",
    "    for title_wrapper in title_wrappers:\n",
    "        nome_secao = extrair_secao(title_wrapper)\n",
    "        \n",
    "        if nome_secao is None:\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            logging.info(f'Marcador de Seção: \"{nome_secao.text}\"')\n",
    "\n",
    "        titulo = extrair_titulo(title_wrapper)\n",
    "        if verbose:\n",
    "            logging.info(f'Marcador de Título: \"{titulo}\"')\n",
    "                    \n",
    "        chave = nome_secao.text\n",
    "        dados_json[nome_no][chave] = {}\n",
    "\n",
    "        # A seleção agora ocorre dentro do contexto de title_wrapper, e não de celula_principal.\n",
    "        celulas_layout = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "        \n",
    "        for celula_layout in celulas_layout:\n",
    "            if celula_layout.find_all('div'):\n",
    "                indice, valores = extrair_indices(celula_layout)\n",
    "                \n",
    "                if indice and valores:\n",
    "                    dados_json[nome_no][chave][indice] = valores\n",
    "\n",
    "    if verbose:\n",
    "        logging.info(f\"Total de índices extraídos: {len(dados_json[nome_no].keys())}\")\n",
    "        # Outros blocos de código para depuração e verbosidade\n",
    "    return dados_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERSÃO 3 com erros ainda\n",
    "import json\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Initialize Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extrair_secao(title_wrapper):\n",
    "    tags_relevantes = ['h1', 'ul', 'b']\n",
    "    tags_encontradas = []\n",
    "    for tag in tags_relevantes:\n",
    "        elemento_encontrado = title_wrapper.select_one(tag)\n",
    "        if elemento_encontrado:\n",
    "            tags_encontradas.append((tag, elemento_encontrado))\n",
    "    tags_ordenadas = sorted(tags_encontradas, key=lambda x: x[1].sourceline if x[1] else float('inf'))\n",
    "    return tags_ordenadas[0][1] if tags_ordenadas else None\n",
    "\n",
    "def extrair_titulo(title_wrapper):\n",
    "    inst_back_texts = {}\n",
    "    inst_back_elements = title_wrapper.find_all('div', class_='inst_back')\n",
    "    for index, inst_back in enumerate(inst_back_elements):\n",
    "        b_tag = inst_back.find('b')\n",
    "        if b_tag:\n",
    "            inst_back_texts[index] = b_tag.text.strip()\n",
    "    return inst_back_texts if inst_back_texts else None\n",
    "\n",
    "def extrair_indices(layout_celula):\n",
    "    indice = layout_celula.select_one('div.layout-cell-pad-5.text-align-right')\n",
    "    valor = layout_celula.select_one('div.layout-cell.layout-cell-9 div.layout-cell-pad-5')\n",
    "    if indice and valor:\n",
    "        valores_extraidos = valor.text.split('<br class=\"clear\">') if '<br class=\"clear\">' in valor.text else valor.text.split('\\n\\t\\t\\n\\t')\n",
    "        return indice.text, valores_extraidos\n",
    "    return None, None\n",
    "\n",
    "def imprimir_informacoes(dados_json, nome_no, indent=0):\n",
    "    indentation = '    ' * indent  # Calculating the current indentation level\n",
    "\n",
    "    if dados_json and nome_no and dados_json.get(nome_no):\n",
    "        if indent == 0:  # Logging node-level information only at the root\n",
    "            logging.info(f\"{indentation}Node: {nome_no}\")\n",
    "            logging.info(f\"{indentation}Total keys extracted: {len(dados_json[nome_no].keys())}\")\n",
    "        \n",
    "        for key in dados_json[nome_no].keys():\n",
    "            logging.info(f\"{indentation}{key.strip() if key else ''}\")\n",
    "\n",
    "            if isinstance(dados_json[nome_no][key], dict):  # Check for nested dictionaries\n",
    "                # Recursive call to handle nested dictionaries\n",
    "                imprimir_informacoes(dados_json[nome_no], key, indent + 1)\n",
    "            else:\n",
    "                logging.info(f\"{indentation}    Values: {dados_json[nome_no][key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in soup.select('div.layout-cell.layout-cell-12.data-cell'):\n",
    "#     print(i.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def extrair_wraper(soup, json_data):\n",
    "    title_wrappers = soup.select('div.layout-cell-pad-main div.title-wrapper')\n",
    "    for title_wrapper in title_wrappers:\n",
    "        section_name = extrair_secao(title_wrapper)\n",
    "        if section_name:\n",
    "            section_name = section_name.text.strip()\n",
    "            \n",
    "            titles = extrair_titulo(title_wrapper)\n",
    "            json_data[\"Properties\"][section_name] = {}\n",
    "            \n",
    "            if titles:\n",
    "                for index, title in titles.items():\n",
    "                    json_data[\"Properties\"][section_name][title] = {}\n",
    "            \n",
    "            layout_cells = title_wrapper.select('div.layout-cell.layout-cell-12.data-cell')\n",
    "            for layout_celula in layout_cells:\n",
    "                indice, valores_extraidos = extrair_indices(layout_celula)\n",
    "                if indice and valores_extraidos:\n",
    "                    if titles and indice in titles.values():\n",
    "                        if len(titles) > 1:\n",
    "                            for title in titles.values():\n",
    "                                if title.strip() in indice:\n",
    "                                    json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                        else:\n",
    "                            title = list(titles.values())[0]\n",
    "                            json_data[\"Properties\"][section_name][title][indice] = valores_extraidos\n",
    "                    else:\n",
    "                        json_data[\"Properties\"][section_name][indice] = valores_extraidos\n",
    "    return json_data\n",
    "\n",
    "def extrair_dados(soup, verbose=False):\n",
    "    # Step 1: Identify Node Name\n",
    "    node_name_element = soup.select_one('div.infpessoa h2.nome')\n",
    "    node_name = node_name_element.text if node_name_element else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Node name: {node_name}\")\n",
    "\n",
    "    # Step 2: Initialize JSON object\n",
    "    json_data = {\"Node Name\": node_name, \"Properties\": {}}\n",
    "    \n",
    "    # Step 3: Traverse to Find Sections\n",
    "    json_data = extrair_wraper(soup, json_data)\n",
    "\n",
    "    # Step 4: Information extraction: Bibliographic Production Section\n",
    "    producao_bibliografica_div = soup.find('div', {'id': 'artigos-completos'})\n",
    "    producoes = []\n",
    "    for artigo_div in producao_bibliografica_div.find_all('div', {'class': 'artigo-completo'}):\n",
    "        artigo_dict = {}\n",
    "\n",
    "        # Extrair os números de ordem dos artigos\n",
    "        ordens = artigo_div.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        for index, ordem in enumerate(ordens):\n",
    "            b_tag = ordem.find('b')\n",
    "            # if b_tag:\n",
    "            #     print(b_tag.text.strip())\n",
    "            \n",
    "        try:\n",
    "            ano = artigo_div.find('span', {'data-tipo-ordenacao': 'ano'}).text\n",
    "        except:\n",
    "            ano = None\n",
    "        try:\n",
    "            prim_autor = artigo_div.find('span', {'data-tipo-ordenacao': 'autor'}).text\n",
    "        except:\n",
    "            prim_autor = None\n",
    "        try:\n",
    "            jcr = artigo_div.find('span', {'data-tipo-ordenacao': 'jcr'}).text\n",
    "        except:\n",
    "            jcr = None\n",
    "        try:\n",
    "            doi = artigo_div.find('a', {'class': 'icone-doi'})['href']\n",
    "        except:\n",
    "            doi = None\n",
    "        # try:\n",
    "        #     titulo = artigo_div.find('div', {'class': 'citado'}).text\n",
    "        # except:\n",
    "        #     titulo = None\n",
    "        dados   = artigo_div.find_all('div', {'class': 'layout-cell-pad-5'})\n",
    "        list    = str(dados).split(\" . \")\n",
    "        autores = prim_autor + list[0].split(prim_autor)[-1].replace('</a>','').replace('</b>','').replace('<b>','')\n",
    "        revista = list[1].split('nomePeriodico=')[1].split('tooltip=')[0].strip('\\\" ')\n",
    "        titulo  = list[1].split('titulo=')[1].split('&amp')[0].strip('\\\" ')\n",
    "        artigo_dict['ano']     = ano\n",
    "        artigo_dict['autores'] = autores\n",
    "        artigo_dict['revista'] = revista\n",
    "        artigo_dict['titulo']  = titulo\n",
    "        artigo_dict['jcr']     = jcr\n",
    "        artigo_dict['doi']     = doi\n",
    "        producoes.append(artigo_dict)\n",
    "        json_data[\"Properties\"]['Produções'] = producoes\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração do objeto Soup do Lattes em HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = connect_driver()\n",
    "NOME = ['Antonio Marcos Aires Barbosa']\n",
    "fill_name(driver, delay, NOME)\n",
    "\n",
    "limite=3\n",
    "instituicao = 'Fundação Oswaldo Cruz'\n",
    "unidade     = 'Fiocruz Ceará'\n",
    "termo       = 'Ministerio da Saude'\n",
    "\n",
    "elm_vinculo, np.NaN, np.NaN, np.NaN, driver = find_terms(NOME, instituicao, unidade, termo, driver, delay, limite)\n",
    "soup = open_curriculum(driver,elm_vinculo)\n",
    "\n",
    "caracteres = len(soup.text)\n",
    "linhas = len(soup.text.split('\\n'))\n",
    "print(f'Total de caracteres extraídos: {caracteres:6d}')\n",
    "print(f'Quantidade extraída de linhas: {linhas:6d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = extrair_dados(soup, True)\n",
    "nome_no = 'Antonio Marcos Aires Barbosa'\n",
    "imprimir_informacoes({nome_no: json_data['Properties']}, nome_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "for title_wrapper in layout_cell_main.find_all('div.title-wrapper'):\n",
    "    index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def generate_json_from_html(soup):\n",
    "    \"\"\"\n",
    "    Generate a JSON object from an Soup object for a Neo4j integration.\n",
    "\n",
    "    Parameters:\n",
    "        soup (object): Soup object from a HTML content.\n",
    "\n",
    "    Returns:\n",
    "        json_data (dict): A dictionary representing the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract node name\n",
    "    node_name = soup.select_one('div.infpessoa h2.nome').text.strip()\n",
    "\n",
    "    # Initialize the master dictionary\n",
    "    json_data = {node_name: {}}\n",
    "\n",
    "    # Locate the main layout cell\n",
    "    layout_cell_main = soup.select_one('div.layout-cell-pad-main')\n",
    "\n",
    "    # Iterate over title-wrapper elements\n",
    "    for title_wrapper in layout_cell_main.select('div.title-wrapper'):\n",
    "        index = title_wrapper.select_one('ul, a, b').text.strip()\n",
    "        \n",
    "        # Initialize the child dictionary\n",
    "        json_data[node_name][index] = {}\n",
    "\n",
    "        # Iterate over layout cells\n",
    "        for layout_cell_3 in title_wrapper.select('div.layout-cell.layout-cell-3.text-align-right'):\n",
    "            grandchild_index = layout_cell_3.select_one('div.layout-cell-pad-5.text-align-right').text.strip()\n",
    "            \n",
    "            # Find the corresponding layout cell for values\n",
    "            layout_cell_9 = layout_cell_3.find_next_sibling('div', class_='layout-cell.layout-cell-9')\n",
    "            \n",
    "            values_text = layout_cell_9.select_one('div.layout-cell-pad-5').text\n",
    "            \n",
    "            # Create a list of values\n",
    "            values = values_text.split('<br class=\"clear\">' or '\\n\\n\\n')\n",
    "\n",
    "            # Add the grandchild dictionary\n",
    "            json_data[node_name][index][grandchild_index] = values\n",
    "\n",
    "    return json_data\n",
    "\n",
    "# Generate JSON data\n",
    "json_data = generate_json_from_html(soup)\n",
    "\n",
    "# Print or persist the generated JSON data\n",
    "print(json.dumps(json_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lattes = extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint(json_lattes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(json_data, parent_key='', separator='.'):\n",
    "    \"\"\"\n",
    "    Convert a nested dictionary into a flat dictionary, suitable for DataFrame conversion.\n",
    "    \n",
    "    Parameters:\n",
    "    - json_data (dict): The nested dictionary to flatten.\n",
    "    - parent_key (str, optional): The concatenated key used to represent nesting.\n",
    "    - separator (str, optional): The character to use for separating nested keys.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame representing the flattened dictionary.\n",
    "    \"\"\"\n",
    "    flat_dict = {}\n",
    "    \n",
    "    for k, v in json_data.items():\n",
    "        new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "        \n",
    "        if isinstance(v, dict):\n",
    "            flat_dict.update(dict_to_dataframe(v, new_key, separator=separator))\n",
    "        else:\n",
    "            flat_dict[new_key] = v\n",
    "            \n",
    "    return pd.DataFrame([flat_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming json_lattes is a JSON-formatted string\n",
    "json_lattes_dict = json.loads(json_lattes)\n",
    "\n",
    "# Then call the dict_to_dataframe function\n",
    "df = dict_to_dataframe(json_lattes_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    splpt = 'Currículo do Sistema de Currículos Lattes ('\n",
    "    string_title = soup.title.string if soup.title else \"Unknown\"\n",
    "    title = string_title.split(splpt)[1].strip(')')\n",
    "    \n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    secoes = []\n",
    "    \n",
    "    print(f'{len(h1_elements[2:])} elementos encontrados')\n",
    "    for n,i in enumerate(h1_elements):\n",
    "        if n>1:\n",
    "            secao = i.text\n",
    "            print(f'    {secao}')\n",
    "            secoes.append(secao)\n",
    "    \n",
    "    for elem in h1_elements[2:]:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        \n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_parsoninfo(soup):\n",
    "    # Localizar o elemento de link que contém o título do currículo\n",
    "    link_element = soup.find(\"a\", {\"href\": lambda x: x and \"abreDetalhe\" in x})\n",
    "\n",
    "    # Extrair o texto do link para usar como título do nó\n",
    "    node_title = link_element.text if link_element else \"Unknown\"\n",
    "    print(f'Título do Nó: {node_title}')\n",
    "\n",
    "    # Localizar o elemento div contendo as propriedades\n",
    "    properties_div = soup.find(\"div\", {\"class\": \"resultado\"})\n",
    "    if properties_div:\n",
    "        print(f'Resultado: {properties_div.text}')\n",
    "    else:\n",
    "        print('Resultados não encontrados')\n",
    "\n",
    "    # Inicializar um dicionário para armazenar as propriedades\n",
    "    properties = {}\n",
    "    \n",
    "    # Localizar o elemento li que contém as informações do idlattes\n",
    "    li_element = soup.find(\"li\")\n",
    "    for i in li_element:\n",
    "        if 'http://lattes.cnpq.br/' in i:\n",
    "            idlattes = i.split('http://lattes.cnpq.br/')[1]\n",
    "            properties['Idlattes'] = idlattes\n",
    "            print(idlattes)\n",
    "\n",
    "    # Extrair e armazenar as propriedades relevantes\n",
    "    if properties_div:\n",
    "        properties['Nacionalidade'] = 'Brasil'\n",
    "        properties['Cargo'] = properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}).text if properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}) else 'Desconhecido'\n",
    "        properties['Titulação'] = properties_div.contents[-4] if len(properties_div.contents) > 4 else 'Desconhecido'\n",
    "\n",
    "        # Extração de nome e identificador único\n",
    "        a_element = li_element.find(\"a\")\n",
    "        properties[\"Nome\"] = a_element.text\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Títulos Acadêmicos e outras informações\n",
    "\n",
    "    return node_title, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_infopessoa(soup):\n",
    "    # Localiza a seção com a classe 'infpessoa'\n",
    "    section = soup.find('div', class_='infpessoa')\n",
    "\n",
    "    # Inicializa um dicionário para armazenar os dados extraídos\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Extrai e armazena o nome\n",
    "    name = section.find('h2', class_='nome').text.strip() if section.find('h2', class_='nome') else None\n",
    "    extracted_data['Nome'] = re.sub(r'\\s+', ' ', name) if name else None\n",
    "\n",
    "    # Extrai e armazena o título ou posição\n",
    "    title = section.find('span', class_='texto').text.strip() if section.find('span', class_='texto') else None\n",
    "    extracted_data['Bolsa'] = re.sub(r'\\s+', ' ', title) if title else None\n",
    "\n",
    "    # Extrai e armazena as informações adicionais\n",
    "    info_list = section.find('ul', class_='informacoes-autor')\n",
    "    if info_list:\n",
    "        for li in info_list.find_all('li'):\n",
    "            text = re.sub(r'\\s+', ' ', li.text.strip())\n",
    "            if 'Endereço para acessar este CV:' in text:\n",
    "                extracted_data['Endereço para acessar este CV'] = text.replace('Endereço para acessar este CV:', '').strip()\n",
    "            elif 'ID Lattes:' in text:\n",
    "                extracted_data['ID Lattes'] = text.replace('ID Lattes:', '').strip()\n",
    "            elif 'Última atualização do currículo em' in text:\n",
    "                extracted_data['Última atualização do currículo em'] = text.replace('Última atualização do currículo em', '').strip()\n",
    "\n",
    "    extracted_data['Resumo'] = soup.find('p', class_='resumo').text.strip()\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def extract_academic(soup):\n",
    "    \"\"\"\n",
    "    Extrai dados da página do Lattes, nas divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Bs4 object (soup): Objeto dod Beaultiful Soap com o conteúdo bruto da página HTML.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "    \"\"\"\n",
    "    # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # Localiza todas as divs com a classe 'title-wrapper'\n",
    "    divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "    for div_key in divs_key:\n",
    "        # Extrai o conteúdo da tag para formar a chave do dicionário\n",
    "        find_div = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "        if find_div:\n",
    "            key = find_div.text.strip('\\n')\n",
    "        \n",
    "        # Encontra a div que segue imediatamente para o valor\n",
    "        div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell layout-cell-9'})\n",
    "        \n",
    "        # Extrai o conteúdo da div para o valor\n",
    "        value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "        # Armazena no dicionário se ambas chave e valor existirem\n",
    "        if key and value:\n",
    "            extracted_data[key] = value\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def mount_articles(extracted_content):\n",
    "    \"\"\"\n",
    "    Organiza os dados extraídos pelo web scraping em um DataFrame do pandas, melhorando a detecção de '\\n'.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - extracted_content (dict): Um dicionário contendo os conteúdos extraídos organizados por classe.\n",
    "    \n",
    "    Retorno:\n",
    "    - Um DataFrame do pandas contendo os dados organizados.\n",
    "    \"\"\"\n",
    "    # Localiza a lista de artigos completos\n",
    "    artigos_completos = extracted_content.get('artigo-completo', [])\n",
    "    print(f'Total de artigos detectados: {len(artigos_completos)}')\n",
    "    \n",
    "    # Concatena todos os artigos em uma única string\n",
    "    artigos_concatenados = \" \".join(artigos_completos)\n",
    "    \n",
    "    # Remove múltiplos espaços e substitui por um único espaço\n",
    "    artigos_concatenados = re.sub(r'\\s+', ' ', artigos_concatenados)\n",
    "    \n",
    "    # Divide os artigos com base em um padrão específico (exemplo: números seguidos de pontos)\n",
    "    artigos_divididos = re.split(r'\\s\\d+\\.\\s', artigos_concatenados)\n",
    "    \n",
    "    # Remove entradas vazias\n",
    "    artigos_divididos = [artigo.strip() for artigo in artigos_divididos if artigo.strip()]\n",
    "    \n",
    "    # Lista para armazenar os registros para o DataFrame\n",
    "    records = []\n",
    "    ordens = []\n",
    "    \n",
    "    for artigo in artigos_divididos:\n",
    "        # Encontra o primeiro ano mencionado no artigo\n",
    "        match_ano = re.search(r'20\\d{2}', artigo)\n",
    "        \n",
    "        if match_ano:\n",
    "            ano = match_ano.group(0)\n",
    "            indice, resto = artigo.split(ano, 1)\n",
    "            \n",
    "            # Armazena o registro em formato de dicionário\n",
    "            record = {\n",
    "                'Índice': indice.split('. ')[0],\n",
    "                'Ano': ano,\n",
    "                'Título e Autores': resto.strip()\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Cria um DataFrame do pandas com os registros\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_research_project(soup):\n",
    "    project_list = []\n",
    "    projects_section = soup.find('h1', string='Projetos de pesquisa')\n",
    "    periods=[]\n",
    "    titles=[]\n",
    "    descriptions=[]\n",
    "    if projects_section:\n",
    "        project_divs = projects_section.find_all_next('div', {'class': 'layout-cell layout-cell-3 text-align-right'})\n",
    "        \n",
    "        for div in project_divs:\n",
    "            project_dict = {}\n",
    "            period_div = div.find('b')\n",
    "            if period_div:\n",
    "                periods.append(period_div.text.strip())\n",
    "            \n",
    "            title_div_container = div.find_next_sibling('div')\n",
    "            if title_div_container:\n",
    "                title_div = title_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                if title_div:\n",
    "                    titles.append(title_div.text.strip())\n",
    "            \n",
    "            # Locate the div that contains the project description\n",
    "            parent_div = div.find_parent('div')\n",
    "            if parent_div:\n",
    "                description_div_container = parent_div.find_next_sibling('div')\n",
    "                if description_div_container:\n",
    "                    description_div_container = description_div_container.find_next_sibling('div')\n",
    "                    if description_div_container:\n",
    "                        description_div = description_div_container.find('div', {'class': 'layout-cell-pad-5'})\n",
    "                        if description_div:\n",
    "                            full_text = description_div.text\n",
    "                            description_start_index = full_text.find('Descrição:')\n",
    "                            if description_start_index != -1:\n",
    "                                descriptions.append(full_text[description_start_index + len('Descrição:'):].strip())\n",
    "\n",
    "    df =pd.DataFrame({\n",
    "        'PERIODO': pd.Series(periods),\n",
    "        'TITULO': pd.Series(titles),\n",
    "        'DESCRICAO': pd.Series(descriptions),\n",
    "            })                \n",
    "    \n",
    "    descricoes = df[df['PERIODO']==\"\"]['TITULO'].values\n",
    "    df = df[df['PERIODO']!=\"\"]\n",
    "    df = df[:len(descricoes)]\n",
    "    df['DESCRICAO']=descricoes\n",
    "    df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_annotation_in_db(uri, user, password, annot_html, cv_id):\n",
    "    \"\"\"\n",
    "    Store the annotation HTML in a Neo4j database.\n",
    "\n",
    "    Parameters:\n",
    "        uri (str): URI of the Neo4j database\n",
    "        user (str): Username for the Neo4j database\n",
    "        password (str): Password for the Neo4j database\n",
    "        annot_html (str): The HTML string containing annotations\n",
    "        cv_id (str): The unique identifier for the annotated CV\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize Neo4j driver\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    # Define Cypher query for adding an annotation\n",
    "    add_annotation_query = '''\n",
    "    MERGE (cv:CV {id: $cv_id})\n",
    "    CREATE (a:Annotation {html: $annot_html})\n",
    "    MERGE (cv)-[:HAS]->(a)\n",
    "    '''\n",
    "\n",
    "    # Execute query\n",
    "    with driver.session() as session:\n",
    "        session.run(add_annotation_query, cv_id=cv_id, annot_html=annot_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# from collections import Counter\n",
    "\n",
    "# def enumerate_tags(soup):\n",
    "#     # Extração de todos os marcadores (tags) no documento\n",
    "#     all_tags = [tag.name for tag in soup.find_all(True)]\n",
    "    \n",
    "#     # Contagem de ocorrências de cada marcador\n",
    "#     tag_count = Counter(all_tags)\n",
    "    \n",
    "#     # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "#     tag_dict = dict(tag_count)\n",
    "    \n",
    "#     return tag_dict\n",
    "\n",
    "# # Exemplo de uso\n",
    "# tag_dictionary = enumerate_tags(soup)\n",
    "# print(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div_elements = soup.find_all('div')\n",
    "# div_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_div_data(soup):\n",
    "#     \"\"\"\n",
    "#     Extrai dados das divs com classes 'layout-cell-pad-5 text-align-right' e 'layout-cell-pad-5' em um dicionário.\n",
    "    \n",
    "#     Parâmetros:\n",
    "#     - html_document (str): String contendo o documento HTML.\n",
    "    \n",
    "#     Retorno:\n",
    "#     - Um dicionário contendo os pares de chave-valor extraídos.\n",
    "#     \"\"\"\n",
    "#     # Dicionário para armazenar os pares de chave-valor extraídos\n",
    "#     extracted_data = {}\n",
    "    \n",
    "#     # Localiza todas as divs com a classe 'layout-cell-pad-5 text-align-right'\n",
    "# #     divs_key = soup.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "#     divs_key = soup.find_all('div', {'class': 'title-wrapper'})\n",
    "    \n",
    "    \n",
    "#     for div_key in divs_key:\n",
    "#         # Extrai o conteúdo da tag <b> dentro da div\n",
    "#         key = div_key.find('b').text if div_key.find('b') else None\n",
    "        \n",
    "#         # Encontra a div que segue imediatamente\n",
    "#         div_value = div_key.find_next_sibling('div').find('div', {'class': 'layout-cell-pad-5'})\n",
    "        \n",
    "#         # Extrai o conteúdo da div\n",
    "#         value = div_value.text.strip() if div_value else None\n",
    "        \n",
    "#         # Armazena no dicionário se ambas chave e valor existirem\n",
    "#         if key and value:\n",
    "#             extracted_data[key] = value\n",
    "    \n",
    "#     return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_div_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(extracted_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_divs(soup):\n",
    "    div_elements = soup.find_all('div')\n",
    "    unique_classes = set()\n",
    "\n",
    "    for div in div_elements[1:]:\n",
    "        div_id = div.get('id', 'N/A')\n",
    "        class_list = div.get('class')\n",
    "        print(f'{div_id} {class_list}')\n",
    "        \n",
    "        if class_list:\n",
    "            for i in class_list:\n",
    "                cont = div.find('div',{'class': i})\n",
    "                try:\n",
    "                    text = cont.text\n",
    "                    print(f'  {text}')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            print('-'*50)\n",
    "            unique_classes.update(class_list)\n",
    "                \n",
    "    print('\\nLista de Classes únicas:')\n",
    "    for i in unique_classes:\n",
    "        print(f'  {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(soup): \n",
    "    # Extração de todas as classes no documento, armazenadas em uma lista\n",
    "    all_classes = [value for element in soup.find_all(class_=True) for value in element[\"class\"]]\n",
    "    \n",
    "    # Contagem de ocorrências de cada classe\n",
    "    class_count = Counter(all_classes)\n",
    "    \n",
    "    # Conversão do objeto Counter para um dicionário padrão para facilitar a manipulação subsequente\n",
    "    class_dict = dict(class_count)\n",
    "    \n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_divs(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_divs_with_hierarchy(tag, indent_level=0):\n",
    "    indent = \" \" * indent_level * 4  # Four spaces for each level of indentation\n",
    "    div_elements = tag.find_all('div', recursive=False)  # Find direct children only\n",
    "    unique_classes = set()\n",
    "\n",
    "#     print(f\"{indent}Inspecting level {indent_level}, found {len(div_elements)} divs.\")  # Debugging line\n",
    "\n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        if class_list:\n",
    "            unique_classes.update(class_list)\n",
    "            print(f\"{indent} {', '.join(class_list)}\")\n",
    "        else:\n",
    "            print(f\"{indent} None\")\n",
    "\n",
    "        # Recursive call to explore the children of this div\n",
    "        list_divs_with_hierarchy(div, indent_level + 1)\n",
    "\n",
    "    if indent_level == 0:  # Print unique classes only once, at the end of the initial call\n",
    "        print(\"\\nUnique classes:\")\n",
    "        for i in unique_classes:\n",
    "            print(f'    {i}')\n",
    "\n",
    "# # Initialize BeautifulSoup with a sample HTML content\n",
    "# soup_sample = BeautifulSoup('<html><head></head><body><div class=\"test\">Hello</div></body></html>', 'html.parser')\n",
    "# print(soup_sample.prettify())\n",
    "\n",
    "# # Start from the <body> tag\n",
    "# body_tag = soup_sample.body\n",
    "\n",
    "# # Then try calling list_divs_with_hierarchy on the <body> tag\n",
    "# list_divs_with_hierarchy(body_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        \n",
    "        data_dict = {}\n",
    "        \n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "            \n",
    "            higher_order_key = None  # Initialize a variable to store the higher-order key\n",
    "            data_list = []  # Initialize a list to store year-details entries\n",
    "            \n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                \n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    data_list = []  # Reset the list for the new higher-order key\n",
    "                    data_dict[higher_order_key] = data_list  # Create a new list for this higher-order key\n",
    "                    \n",
    "                year_elems = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for year_elem, details_elem in zip(year_elems, details_elems):\n",
    "                    year_text = year_elem.text.strip() if year_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    data_entry = {'rotulos': year_text, 'conteudos': details_text}\n",
    "                    \n",
    "                    data_list.append(data_entry)  # Append each entry to the list\n",
    "                \n",
    "            if higher_order_key is None:\n",
    "                # If no higher-order key is found, associate the data list directly with the title\n",
    "                result_dict[title_text] = data_list\n",
    "            else:\n",
    "                # Otherwise, associate the data_dict containing higher-order keys with the title\n",
    "                result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = extract_data(soup)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_dataframe(data_dict):\n",
    "    frames = {}  # Store DataFrames corresponding to each section\n",
    "    for section, items in data_dict.items():\n",
    "        if type(items) is list:\n",
    "            df = pd.DataFrame(items)\n",
    "            df['Section'] = section  # Add a column to identify the section\n",
    "            frames[section] = df\n",
    "        elif type(items) is dict:\n",
    "            for subsection, subitems in items.items():\n",
    "                df = pd.DataFrame(subitems)\n",
    "                df['Subsection'] = subsection  # Add a column to identify the subsection\n",
    "                df['Section'] = section  # Add a column to identify the section\n",
    "                frames[f\"{section}_{subsection}\"] = df\n",
    "    return frames\n",
    "\n",
    "frames = generate_dataframe(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(frames.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access individual DataFrames like so:\n",
    "identificacao_df = frames['Identificação']\n",
    "identificacao_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endereco_df = frames['Endereço']\n",
    "endereco_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formacao_df = frames['Formação acadêmica/titulação']\n",
    "formacao_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formcompl_df = frames['Formação Complementar']\n",
    "formcompl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atuaprof_df = frames['Atuação Profissional_Fundação Oswaldo Cruz, FIOCRUZ, Brasil.']\n",
    "atuaprof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_df = frames['Linhas de pesquisa']\n",
    "linhas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projdesv_df = frames['Projetos de desenvolvimento']\n",
    "projdesv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projpesq_df = frames['Áreas de atuação']\n",
    "projpesq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areasatuacao_df = frames['Áreas de atuação']\n",
    "areasatuacao_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(soup):\n",
    "    result_dict = {}\n",
    "    \n",
    "    title_elements = soup.find_all('h1')\n",
    "    for title_elem in title_elements:\n",
    "        title_text = title_elem.text.strip()\n",
    "        \n",
    "        data_dict = {}\n",
    "        \n",
    "        parent_div = title_elem.find_parent('div', {'class': 'title-wrapper'})\n",
    "        if parent_div:\n",
    "            data_cells = parent_div.find_all('div', {'class': 'layout-cell layout-cell-12 data-cell'})\n",
    "            \n",
    "            current_higher_order_dict = None  # Initialize a variable to store the current higher-order dictionary\n",
    "            \n",
    "            for cell in data_cells:\n",
    "                inst_back_elem = cell.find('div', {'class': 'inst_back'})\n",
    "                \n",
    "                if inst_back_elem:\n",
    "                    higher_order_key = inst_back_elem.text.strip()\n",
    "                    current_higher_order_dict = {}  # Create a new dictionary for this higher-order key\n",
    "                    data_dict[higher_order_key] = current_higher_order_dict  # Associate the new dictionary with the higher-order key\n",
    "                    \n",
    "                year_elems = cell.find_all('div', {'class': 'layout-cell-pad-5 text-align-right'})\n",
    "                details_elems = cell.find_all('div', {'class': 'layout-cell layout-cell-9'})\n",
    "                \n",
    "                for year_elem, details_elem in zip(year_elems, details_elems):\n",
    "                    year_text = year_elem.text.strip() if year_elem else None\n",
    "                    details_text = details_elem.text.strip() if details_elem else None\n",
    "                    \n",
    "                    if current_higher_order_dict is not None:\n",
    "                        # Insert the year-details pair into the current higher-order dictionary\n",
    "                        current_higher_order_dict[year_text] = details_text\n",
    "                    else:\n",
    "                        # If no higher-order key is present, associate the year-details pair directly with the title\n",
    "                        data_dict[year_text] = details_text\n",
    "                \n",
    "            result_dict[title_text] = data_dict\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(input_dict):\n",
    "    def recursive_descent(current_dict, parent_key='', separator='.'):\n",
    "        nonlocal flattened_dict\n",
    "        for k, v in current_dict.items():\n",
    "            new_key = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                recursive_descent(v, new_key, separator=separator)\n",
    "            else:\n",
    "                flattened_dict[new_key] = v\n",
    "                \n",
    "    flattened_dict = {}\n",
    "    recursive_descent(input_dict)\n",
    "    \n",
    "    # Create DataFrame from the flattened dictionary\n",
    "    df = pd.DataFrame([flattened_dict])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dict = extract_data(soup)\n",
    "df = dict_to_dataframe(nested_dict)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag, NavigableString\n",
    "\n",
    "html_content = '''\n",
    "<div class=\"layout-cell-pad-5\">\n",
    "    Doutorado em andamento em Informática Aplicada.\n",
    "    <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "    <br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde\n",
    "    <br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho.\n",
    "</div>\n",
    "'''\n",
    "\n",
    "soup_sample = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup_sample)\n",
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def extract_text_from_selectors(soup,select_path):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        elements = title_wrapper.select(select_path)\n",
    "#         print(len(elements))\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4.element import Tag, NavigableString\n",
    "def extract_text_by_br_class(soup):\n",
    "    result_list = []\n",
    "    elements = soup.select('div.layout-cell-pad-5')\n",
    "    for element in elements:\n",
    "        text_segments = []\n",
    "        current_dict = {}\n",
    "        for content in element.contents:\n",
    "            if isinstance(content, Tag):\n",
    "                if content.get('class') == ['clear']:\n",
    "                    joined_text = ' '.join(text_segments).strip()\n",
    "                    if \"Orientador:\" in joined_text:\n",
    "                        key, value = joined_text.split(\"Orientador:\")\n",
    "                        current_dict[key.strip()] = {'name': value.strip(), 'url': current_dict.pop('url', None)}\n",
    "                    else:\n",
    "                        current_dict[joined_text] = None\n",
    "                    text_segments = []\n",
    "                elif content.name == 'a':\n",
    "                    current_dict['url'] = content.get('href')\n",
    "            elif isinstance(content, NavigableString):\n",
    "                text_segments.append(str(content).strip())\n",
    "        # Capture any remaining text\n",
    "        if text_segments:\n",
    "            joined_text = ' '.join(text_segments).strip()\n",
    "            current_dict[joined_text] = None\n",
    "        result_list.append(current_dict)\n",
    "    return result_list\n",
    "\n",
    "# Execução da função\n",
    "extracted_texts = extract_text_by_br_class(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=\"layout-cell.layout-cell-3.text-align-right\"\n",
    "vals=\"layout-cell layout-cell-9\"\n",
    "<div class=\"layout-cell layout-cell-9\">\n",
    "<div class=\"layout-cell-pad-5\">Doutorado em andamento em Informática Aplicada. <br class=\"clear\">Universidade de Fortaleza, UNIFOR, Brasil.\n",
    "\t\t\n",
    "\t<br class=\"clear\">Título: Processamento de Linguagem Natural no Desenvolvimento de Bioprodutos para Saúde<br class=\"clear\">Orientador: <a class=\"icone-lattes\" target=\"_blank\" href=\"http://lattes.cnpq.br/2607811863279622\" tabindex=\"80\"><img src=\"images/curriculo/logolattes.gif\"></a>Raimir Holanda Filho. </div>\n",
    "</div>\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "father     = 'div.title-wrapper'\n",
    "sons       = 'h1'\n",
    "grandchild = '' \n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        son_elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for grandchild in son_elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_titles(soup,father,sons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "father = 'div.title-wrapper'\n",
    "sons   = 'h1'\n",
    "def extract_text_titles(soup,father,sons):\n",
    "    result_list = []\n",
    "    \n",
    "    # Identificando os elementos que correspondem aos seletores desejados.\n",
    "    for father_element in soup.select(father):\n",
    "        elements = father_element.select(sons)\n",
    "        \n",
    "        # Extração do texto dos elementos identificados.\n",
    "        for element in elements:\n",
    "            text_content = element.text.strip()\n",
    "            result_list.append(text_content)\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-12.data-cell'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "# <div class=\"layout-cell-pad-5 text-align-right\">\n",
    "# <b>2017 - 2019</b>\n",
    "# </div>\n",
    "select_path='div.layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução da função\n",
    "select_path='layout-cell-pad-5.text-align-right'\n",
    "result_list = extract_text_from_selectors(soup,select_path)\n",
    "pprint(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_to_dict(soup):\n",
    "    result_list = []\n",
    "    \n",
    "    for title_wrapper in soup.select('div.title-wrapper'):\n",
    "        temp_dict = defaultdict(dict)\n",
    "        first_level_key = title_wrapper.select_one('div.layout-cell.layout-cell-12.data-cell').text.strip()\n",
    "        \n",
    "        for cell in title_wrapper.select('div.layout-cell.layout-cell-12.data-cell'):\n",
    "            second_level_keys = cell.select('div.layout-cell-pad-5.text-align-right')\n",
    "            values = cell.select('div.layout-cell.layout-cell-3.text-align-right')\n",
    "            \n",
    "            if len(second_level_keys) == len(values):\n",
    "                for key, value in zip(second_level_keys, values):\n",
    "                    second_level_key = key.text.strip()\n",
    "                    value_text = value.text.strip()\n",
    "                    temp_dict[first_level_key][second_level_key] = value_text\n",
    "        \n",
    "        result_list.append(temp_dict)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "# Execute the function\n",
    "result_list = extract_to_dict(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['layout-cell', 'layout-cell-3', 'text-align-right']\n",
    "\n",
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "# 'text-align-right': 152,\n",
    "# 'layout-cell-pad-5': 152,\n",
    "\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-pad-main'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'text-align-right'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-9'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'layout-cell-3'},\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the <body> tag\n",
    "body_tag = soup.body\n",
    "content_wrapper_tag = body_tag.find('div',\n",
    "                                    {'class': 'data-cell'},\n",
    "#                                     {'class': 'layout-cell'},\n",
    "#                                     {'class': 'layout-cell-9},                                    \n",
    "#                                     {'class': 'layout-cell-12'},\n",
    "#                                     {'class': 'layout-cell-pad-5'},\n",
    "#                                     {'class': 'data-cell'}\n",
    "                                   )\n",
    "\n",
    "list_divs_with_hierarchy(content_wrapper_tag, indent_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_text_from_hierarchy(tag, result_dict, parent_key=\"root\"):\n",
    "    div_elements = tag.find_all('div', recursive=False)\n",
    "    \n",
    "    for div in div_elements:\n",
    "        class_list = div.get('class')\n",
    "        class_key = ', '.join(class_list) if class_list else \"None\"\n",
    "        \n",
    "        # Criando uma chave única que incorpora o caminho da raiz até esta div\n",
    "        full_key = f\"{parent_key} -> {class_key}\"\n",
    "\n",
    "        # Coleta o texto contido no elemento div atual\n",
    "        text_content = div.get_text(strip=True)\n",
    "\n",
    "        # Armazenar o conteúdo textual sob esta chave única\n",
    "        if full_key not in result_dict:\n",
    "            result_dict[full_key] = []\n",
    "        result_dict[full_key].append(text_content)\n",
    "\n",
    "        # Chamada recursiva para extrair textos dos filhos deste div\n",
    "        extract_text_from_hierarchy(div, result_dict, full_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)\n",
    "\n",
    "extract_text_from_hierarchy(soup.body, result_dict, 'text-align-right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict.values():\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes, key_stack=None):\n",
    "    if key_stack is None:\n",
    "        key_stack = []\n",
    "    \n",
    "    popped_key = None\n",
    "    if soup_element.name == \"div\":\n",
    "        class_list = soup_element.get('class', [])\n",
    "        \n",
    "        if any(k in class_list for k in key_classes):\n",
    "            new_key = soup_element.text.strip()\n",
    "#             print(f'Key found: {new_key}')  # Debugging line\n",
    "            key_stack.append(new_key)\n",
    "        \n",
    "        elif any(v in class_list for v in value_classes) and key_stack:\n",
    "            value = soup_element.text.strip()\n",
    "#             print(f'Value found: {value}')  # Debugging line\n",
    "            current_key = key_stack[-1]\n",
    "            if current_key in result_dict:\n",
    "                result_dict[current_key].append(value)\n",
    "            else:\n",
    "                result_dict[current_key] = [value]\n",
    "                \n",
    "#     print(f\"Current key_stack: {key_stack}\")  # Debugging line\n",
    "\n",
    "    for child in soup_element.find_all(\"div\", recursive=False):\n",
    "        extract_key_value_pairs(child, result_dict, key_classes, value_classes, key_stack)\n",
    "\n",
    "    if popped_key:\n",
    "        key_stack.append(popped_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'text-align-right'})\n",
    "# key_classes   = ['data-cell']\n",
    "# value_classes = ['text-align-right']\n",
    "\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in result_dict.keys():\n",
    "    pprint(i)\n",
    "\n",
    "print()\n",
    "\n",
    "for i in result_dict.values():\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_element  = soup.find('div', {'class': 'layout-cell-pad-5'})\n",
    "key_classes   = ['layout-cell-3']\n",
    "value_classes = ['layout-cell-9']\n",
    "\n",
    "extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "\n",
    "def extract_target_classes(soup,classe):\n",
    "    soup_element  = soup.find('div', {'class': classe})\n",
    "#     key_classes   = ['data-cell']\n",
    "#     value_classes = ['text-align-right']\n",
    "    key_classes   = ['layout-cell-9']\n",
    "    value_classes = ['layout-cell-pad-5']\n",
    "    \n",
    "    target_classes = [\n",
    "        'Produção '\n",
    "        'Bibliográfica',\n",
    "        'Produção '\n",
    "        'Técnica',\n",
    "        'Produção '\n",
    "        'Artística/Cultural'\n",
    "        'nome', \n",
    "        'resumo', \n",
    "        'artigo-completo', \n",
    "        'cita', \n",
    "        'cita-artigos', \n",
    "        'citacoes', \n",
    "        'detalhes', \n",
    "        'fator', \n",
    "        'foto', \n",
    "        'informacao-artigo', \n",
    "        'informacoes-autor', \n",
    "        'infpessoa', \n",
    "        'rodape-cv', \n",
    "        'science_cont', \n",
    "        'texto', \n",
    "        'trab'\n",
    "        ]\n",
    "    try:\n",
    "        extract_key_value_pairs(soup_element, result_dict, key_classes, value_classes)\n",
    "        for x,y in zip(result_dict.keys(),result_dict.values()):\n",
    "            if x in target_classes:\n",
    "                try:\n",
    "                    print(f\"{x:>12} | {y[0]}\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        filtered_dict = {k: v for k, v in result_dict.items() if k in target_classes}\n",
    "\n",
    "    except:\n",
    "        print(f'Classe \"{classe}\" não encontrada')\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = 'infpessoa'\n",
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_target_classes(soup,classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_classes_content(soup_element, target_classes):\n",
    "    result_dict = defaultdict(list)\n",
    "    \n",
    "    for t_class in target_classes:\n",
    "        for element in soup_element.find_all(class_=t_class):\n",
    "            text_content = element.text.strip()\n",
    "            result_dict[t_class].append(text_content)\n",
    "            \n",
    "    return dict(result_dict)\n",
    "\n",
    "target_classes = [\n",
    "        'artigo-completo', 'cita', 'cita-artigos', 'citacoes', 'detalhes', 'fator', \n",
    "        'foto', 'informacao-artigo', 'informacoes-autor', 'infpessoa', 'nome', \n",
    "        'resumo', 'rodape-cv', 'science_cont', 'texto', 'trab'\n",
    "        ]\n",
    "\n",
    "result_dict = extract_classes_content(soup.body, target_classes)\n",
    "from pprint import pprint\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['informacao-artigo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_dict['artigo-completo']:\n",
    "    print(i.split('\\n\\n\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\n",
    "        'artigo-completo', 'cita', 'cita-artigos', 'citacoes', 'detalhes', 'fator', \n",
    "        'foto', 'informacao-artigo', 'informacoes-autor', 'infpessoa', 'nome', \n",
    "        'resumo', 'rodape-cv', 'science_cont', 'texto', 'trab'\n",
    "        ]\n",
    "\n",
    "def extract_selected_classes(soup, target_classes):\n",
    "    \"\"\"\n",
    "    Extrai conteúdos de classes específicas de um objeto soup extraído de documento HTML.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - Objeto soup e lista de classes a serem extraídas.\n",
    "\n",
    "    Retorno:\n",
    "    - Um dicionário que mapeia o nome da classe à lista de conteúdos extraídos.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Dicionário para armazenar os conteúdos\n",
    "    content_dict = defaultdict(list)\n",
    "    \n",
    "    # Iteração através das classes alvo para extração de conteúdo\n",
    "    for target_class in target_classes:\n",
    "        elements = soup.find_all(class_=target_class)\n",
    "        for element in elements:\n",
    "            content_dict[target_class].append(element.text.replace('\\n\\n\\n','\\n').replace('\\n\\n','\\n').strip())\n",
    "            \n",
    "    return dict(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_infopessoa(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extract_academic(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = pd.DataFrame([extract_academic(soup)])\n",
    "df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extracted_projects = extract_research_project(soup)\n",
    "extracted_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_selected_classes(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_content = extract_selected_classes(soup)\n",
    "mount_articles(extracted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência do cabeçalho\n",
    "header_data = parse_header(soup)\n",
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_node = \n",
    "# header_node = persist_to_neo4j(header_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração e Persistência dos elementos H1\n",
    "try:\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "except Exception as e:\n",
    "    print('Erro ao conectar ao Neo4j')\n",
    "    print(e)\n",
    "try:    \n",
    "    header_node = persist_to_neo4j(header_data)\n",
    "    print({type(header_node)})\n",
    "    parse_h1_elements(soup, header_node, graph)\n",
    "    cv_node, properties = parse_parsoninfo(soup)\n",
    "except Exception as e:\n",
    "    print('Erro ao persistir nó')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_h1_elements(soup, parent_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node(\"Curriculum\", \n",
    "     title=header_data['title'].split('(')[1].strip(')'), \n",
    "     meta_keywords=header_data['meta_keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_lattes_page(name_link, images_urls, recent_updates_url, qualis_data, data_source_info):\n",
    "    print(qualis_data)\n",
    "    \n",
    "    # Inicializando o WebDriver\n",
    "    driver_service = Service('path/to/chromedriver')\n",
    "    driver = webdriver.Chrome(service=driver_service)\n",
    "    driver.get(name_link['link'])\n",
    "    \n",
    "    # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "    visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "    # Limpar cache de dados de Qualis\n",
    "    qualis_data_cache = {}\n",
    "    \n",
    "    # Annotate Lattes page com informações de Qualis\n",
    "    lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "    if lattes_info:\n",
    "        await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "    # Consolidar dados de publicação a partir das informações de Lattes\n",
    "    pub_info = consolidate_qualis_data(lattes_info)\n",
    "    print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "    # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "    conn = sqlite3.connect('lattes_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "    existing_data = cursor.fetchall()\n",
    "    \n",
    "    lattes_data_array = []\n",
    "    \n",
    "    if existing_data:\n",
    "        # Filtrar dados existentes para evitar duplicatas\n",
    "        lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "    # Adicionar dados de Lattes atuais ao array\n",
    "    lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "    # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "    cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_published_articles(soup, qualis_data):\n",
    "    # Localiza elementos contendo os artigos publicados\n",
    "    articles_elements = soup.find_all('div', class_='published-article')\n",
    "    \n",
    "    # Inicializa lista para conter dados dos artigos publicados\n",
    "    annotated_articles = []\n",
    "    \n",
    "    # Itera sobre cada elemento e anotando as informações necessárias\n",
    "    for article in articles_elements:\n",
    "        title = article.find('div', class_='article-title').text\n",
    "        issn = article.find('div', class_='article-issn').text\n",
    "        qualis = qualis_data.get(issn, 'N/A')  # Buscando o Qualis correspondente\n",
    "        \n",
    "        # Adiciona ao conjunto de artigos anotados\n",
    "        annotated_articles.append({\n",
    "            'title': title,\n",
    "            'issn': issn,\n",
    "            'qualis': qualis\n",
    "        })\n",
    "    \n",
    "    # Persiste em SQLite\n",
    "    conn = sqlite3.connect(\"lattes_data.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Cria tabela de artigos publicados, se não existir\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS published_articles (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        title TEXT,\n",
    "        issn TEXT,\n",
    "        qualis TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insere artigos anotados na tabela\n",
    "    for article in annotated_articles:\n",
    "        cursor.execute(\"INSERT INTO published_articles (title, issn, qualis) VALUES (?, ?, ?)\",\n",
    "                       (article['title'], article['issn'], article['qualis']))\n",
    "    \n",
    "    # Commit e fecha a conexão\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "async def annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info):\n",
    "    print(\"Procurando por publicações de periódicos...\")\n",
    "    \n",
    "    # Inicializa BeautifulSoup para analisar o conteúdo HTML da página\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extrai e anota informações dos artigos publicados\n",
    "    qualis_info = await annotate_published_articles(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup)\n",
    "    \n",
    "    return qualis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_annotation_message(images_urls, visualization_url, recent_updates_url, pub_count):\n",
    "    annot_header_html = ''\n",
    "    annot_buttons_html = ''\n",
    "    pub_count_string = ''\n",
    "    \n",
    "    if pub_count > 0:\n",
    "        s_char = 's' if pub_count > 1 else ''\n",
    "        pub_count_string = f'anotou o Qualis de {pub_count} artigo{s_char} em periódico{s_char} neste CV.'\n",
    "        annot_buttons_html = render_template_string(\"\"\"\n",
    "            <a href=\"#artigos-completos\">\n",
    "                <button> Ver anotações </button>\n",
    "            </a>\n",
    "        \"\"\")\n",
    "    else:\n",
    "        pub_count_string = 'não anotou nenhum artigo em periódico neste CV.'\n",
    "\n",
    "    annot_header_html = render_template_string(\"\"\"\n",
    "        <a href=\"{{visualization_url}}\" target=\"_blank\" id=\"qlattes-logo\">\n",
    "            <img src=\"{{images_urls['qlattesLogoURL']}}\" width=\"70\">\n",
    "        </a>{{pub_count_string}}\n",
    "        </br>\n",
    "    \"\"\", visualization_url=visualization_url, images_urls=images_urls, pub_count_string=pub_count_string)\n",
    "\n",
    "    # Aqui a informação seria armazenada em um banco de dados em vez de ser injetada em um elemento HTML\n",
    "    store_annotation_in_db(annot_header_html + annot_buttons_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_qualis_data(qualis_info):\n",
    "    pub_data = []\n",
    "    pub_data_year = []\n",
    "    curr_year = 0\n",
    "    \n",
    "    for i in range(len(qualis_info)):\n",
    "        if curr_year != qualis_info[i]['year']:\n",
    "            if curr_year > 0:\n",
    "                pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "                pub_data_year = []\n",
    "            \n",
    "            curr_year = qualis_info[i]['year']\n",
    "        \n",
    "        pub_data_item = {\n",
    "            'issn': qualis_info[i]['issn'],\n",
    "            'title': qualis_info[i]['title'],\n",
    "            'pubName': qualis_info[i]['pubName'],\n",
    "            'qualis': qualis_info[i]['qualisLabels']['qualis'],\n",
    "            'baseYear': qualis_info[i]['qualisLabels']['baseYear'],\n",
    "            'jcr': qualis_info[i]['jcrData']['jcr'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else 0,\n",
    "            'jcrYear': qualis_info[i]['jcrData']['baseYear'] if 'jcrData' in qualis_info[i] and qualis_info[i]['jcrData'] else ''\n",
    "        }\n",
    "        \n",
    "        pub_data_year.append(pub_data_item)\n",
    "        \n",
    "    if len(qualis_info) > len(pub_data):\n",
    "        pub_data.append({'year': curr_year, 'pubList': pub_data_year})\n",
    "    \n",
    "    return pub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_link_html(text, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">{{text}}</a>\n",
    "    \"\"\", text=text, tooltip=tooltip, target_url=target_url)\n",
    "\n",
    "def create_icon_link_html(icon_url, icon_style, tooltip, target_url):\n",
    "    return render_template_string(\"\"\"\n",
    "        <a href=\"{{target_url}}\" target=\"_blank\" title=\"{{tooltip}}\">\n",
    "            <img src=\"{{icon_url}}\" style=\"{{icon_style}}\">\n",
    "        </a>\n",
    "    \"\"\", icon_url=icon_url, icon_style=icon_style, tooltip=tooltip, target_url=target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qualis_from_percentil(percentil):\n",
    "    if not percentil:\n",
    "        return 'N'\n",
    "\n",
    "    qualis_class_list     = ['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4']\n",
    "    qualis_threshold_list = [87.5, 75, 62.5, 50, 37.5, 25, 12.5, 0]\n",
    "\n",
    "    for i in range(len(qualis_threshold_list)):\n",
    "        if percentil >= qualis_threshold_list[i]:\n",
    "            return qualis_class_list[i]\n",
    "\n",
    "    return 'N'\n",
    "\n",
    "\n",
    "def get_alternative_issn(issn, capes_alt_data, scopus_data):\n",
    "    # Pesquisar ISSN nos dados complementares da CAPES\n",
    "    match = next((elem for elem in capes_alt_data if elem['issn'] == issn or elem['alt_issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        if 'alt_issn' in match and match['alt_issn'] != issn:\n",
    "            return match['alt_issn']\n",
    "        elif 'issn' in match and match['issn'] != issn:\n",
    "            return match['issn']\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        # Pesquisar ISSN nos dados do Scopus\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "        \n",
    "        if match:\n",
    "            if 'e-issn' in match and len(match['e-issn']) > 0 and match['e-issn'] != issn:\n",
    "                return match['e-issn']\n",
    "            elif 'issn' in match and len(match['issn']) > 0 and match['issn'] != issn:\n",
    "                return match['issn']\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualis_from_capes_data(issn, alt_issn, capes_data, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procurar pelo ISSN na base de dados da CAPES\n",
    "    match = next((elem for elem in capes_data if elem['issn'] == issn), None)\n",
    "    \n",
    "    if match:\n",
    "        qualis_labels['source'] = 'capes'\n",
    "    elif alt_issn != '':\n",
    "        match = next((elem for elem in capes_data if elem['issn'] == alt_issn), None)\n",
    "        \n",
    "        if match:\n",
    "            qualis_labels['source'] = 'capes_alt'\n",
    "            \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = match['qualis']\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['baseYear'] = data_source_info[qualis_labels['source']]['baseYear']\n",
    "\n",
    "        qualis_labels_scopus = get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info)\n",
    "        \n",
    "        if qualis_labels_scopus['qualis'] != 'N':\n",
    "            qualis_labels['linkScopus'] = qualis_labels_scopus['linkScopus']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_pucrs_data(issn, alt_issn, pub_name, pucrs_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    labels_map = {\n",
    "        'pubName': 'periodico',\n",
    "        'qualis': 'Qualis_Final',\n",
    "        'percentil': 'percentil',\n",
    "        'linkScopus': 'link_scopus',\n",
    "        'adjusted': 'Ajuste_SBC'\n",
    "    }\n",
    "    \n",
    "    match = next((elem for elem in pucrs_data if elem['issn'] == issn or (alt_issn and elem['issn'] == alt_issn)), None)\n",
    "    \n",
    "    if match:\n",
    "        for key in labels_map.keys():\n",
    "            if labels_map[key] in match and match[labels_map[key]] != 'nulo':\n",
    "                qualis_labels[key] = match[labels_map[key]]\n",
    "                \n",
    "        qualis_labels['source'] = 'pucrs'\n",
    "        qualis_labels['baseYear'] = data_source_info['pucrs']['baseYear']\n",
    "\n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "def get_qualis_from_scopus_data(issn, alt_issn, scopus_data, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    # Procura pelo ISSN nos dados da Scopus\n",
    "    match = next((elem for elem in scopus_data if elem['issn'] == issn or elem['e-issn'] == issn), None)\n",
    "    \n",
    "    if not match and alt_issn != '':\n",
    "        match = next((elem for elem in scopus_data if elem['issn'] == alt_issn or elem['e-issn'] == alt_issn), None)\n",
    "        \n",
    "    if match:\n",
    "        qualis_labels['qualis'] = calculate_qualis_from_percentil(match['percentil'])\n",
    "        qualis_labels['pubName'] = match['title'].upper()\n",
    "        qualis_labels['percentil'] = match['percentil']\n",
    "        qualis_labels['linkScopus'] = match['source-id-url']\n",
    "        qualis_labels['source'] = 'scopus'\n",
    "        qualis_labels['baseYear'] = data_source_info['scopus']['baseYear']\n",
    "    \n",
    "    return qualis_labels\n",
    "\n",
    "\n",
    "async def get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info):\n",
    "    qualis_labels = {\n",
    "        'pubName': '',\n",
    "        'qualis': 'N',\n",
    "        'percentil': '',\n",
    "        'linkScopus': '',\n",
    "        'adjusted': '',\n",
    "        'source': '',\n",
    "        'baseYear': ''\n",
    "    }\n",
    "\n",
    "    if not issn:\n",
    "        return qualis_labels\n",
    "\n",
    "    alt_issn = ''\n",
    "\n",
    "    # Verificar se o ISSN já está em cache\n",
    "    if issn in qualis_data_cache:\n",
    "        return qualis_data_cache[issn]\n",
    "    else:\n",
    "        # Verificar se um ISSN alternativo existe e está em cache\n",
    "        alt_issn = await get_alternative_issn(issn, qualis_data['capes_alt'], qualis_data['scopus'])\n",
    "        if alt_issn and alt_issn in qualis_data_cache:\n",
    "            return qualis_data_cache[alt_issn]\n",
    "\n",
    "    # Procurar pelo ISSN nos dados CAPES\n",
    "    qualis_labels = await get_qualis_from_capes_data(\n",
    "        issn, alt_issn, qualis_data['capes'], qualis_data['scopus'], data_source_info\n",
    "    )\n",
    "\n",
    "    # Se não encontrado\n",
    "    if qualis_labels['qualis'] == 'N':\n",
    "        # Procurar pelo ISSN nos dados PUC-RS\n",
    "        qualis_labels = await get_qualis_from_pucrs_data(\n",
    "            issn, alt_issn, pub_name, qualis_data['pucrs'], data_source_info\n",
    "        )\n",
    "\n",
    "        # Se ainda não encontrado\n",
    "        if qualis_labels['qualis'] == 'N':\n",
    "            # Procurar pelo ISSN nos dados Scopus\n",
    "            qualis_labels = await get_qualis_from_scopus_data(\n",
    "                issn, alt_issn, qualis_data['scopus'], data_source_info\n",
    "            )\n",
    "\n",
    "    # Adicionar rótulos ao cache Qualis\n",
    "    qualis_data_cache[issn] = qualis_labels\n",
    "\n",
    "    if alt_issn:\n",
    "        qualis_data_cache[alt_issn] = qualis_labels\n",
    "\n",
    "    return qualis_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_qualis_annotation(pub_info, images_URLs, data_source_info):\n",
    "    annotation_dict = {}\n",
    "    \n",
    "    # Create QLattes icon element\n",
    "    qlattes_img_elem = BeautifulSoup('<img>', 'html.parser')\n",
    "    qlattes_img_elem['src'] = images_URLs['qlattesIconURL']\n",
    "    qlattes_img_elem['style'] = 'margin-bottom:-4px'\n",
    "    \n",
    "    annotation_dict['qlattes_img_elem'] = str(qlattes_img_elem)\n",
    "    \n",
    "    # Create Qualis labels annotations\n",
    "    issn_label = f', ISSN {pub_info[\"issn\"]}' if pub_info.get('issn') else ''\n",
    "    \n",
    "    if pub_info['qualisLabels']['qualis'] == 'N':\n",
    "        qualis_annot = f' Não classificado{issn_label}'\n",
    "    else:\n",
    "        qualis_annot = f' {pub_info[\"qualisLabels\"][\"qualis\"]}{issn_label}'\n",
    "        \n",
    "        # add Data source and base year\n",
    "        source = pub_info['qualisLabels']['source']\n",
    "        source_info = data_source_info[source]\n",
    "        \n",
    "        data_source_label = f'{source_info[\"label\"]} ({source_info[\"baseYear\"]})'\n",
    "        qualis_annot += f', fonte {data_source_label}'\n",
    "    \n",
    "    annotation_dict['qualis_annot'] = qualis_annot\n",
    "    \n",
    "    # add icon with link to Google Scholar\n",
    "    base_url = 'https://scholar.google.com/scholar?q='\n",
    "    title_param = f'intitle%3A%22{pub_info[\"title\"].replace(\" \", \"+\")}%22'\n",
    "    link_scholar = f'{base_url}{title_param}'\n",
    "    \n",
    "    annotation_dict['link_scholar'] = link_scholar\n",
    "    \n",
    "    # add icon with link to Scopus (if available)\n",
    "    if pub_info['qualisLabels'].get('linkScopus'):\n",
    "        annotation_dict['link_scopus'] = pub_info['qualisLabels']['linkScopus']\n",
    "    \n",
    "    return annotation_dict\n",
    "\n",
    "def persist_annotation_div():\n",
    "    alert_div = BeautifulSoup('<div>', 'html.parser')\n",
    "    \n",
    "    alert_div['class'] = 'main-content max-width min-width'\n",
    "    alert_div['id'] = 'annot-div'\n",
    "    \n",
    "    print('Alert div persisted!')\n",
    "    \n",
    "    return str(alert_div)\n",
    "\n",
    "\n",
    "def persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo):\n",
    "    annot_dict = {}\n",
    "    \n",
    "    issnLabel = f\", ISSN {pubInfo['issn']}\" if pubInfo['issn'] else pubInfo['issn']\n",
    "    if pubInfo['qualisLabels']['qualis'] == 'N':\n",
    "        qualisAnnot = f\"Não classificado{issnLabel}\"\n",
    "    else:\n",
    "        qualisAnnot = f\"{pubInfo['qualisLabels']['qualis']}{issnLabel}\"\n",
    "        \n",
    "        dataSourceLabel = dataSourceInfo[pubInfo['qualisLabels']['source']]['label']\n",
    "        baseYear = dataSourceInfo[pubInfo['qualisLabels']['source']]['baseYear']\n",
    "        dataSourceLabel += f\" ({baseYear})\"\n",
    "        qualisAnnot += f\", fonte {dataSourceLabel}\"\n",
    "        \n",
    "    annot_dict['qualisAnnot'] = qualisAnnot\n",
    "    \n",
    "    titleParam = f\"intitle:\\\"{pubInfo['title']}\\\"\"\n",
    "    linkScholar = f\"https://scholar.google.com/scholar?q={titleParam}\"\n",
    "    annot_dict['linkScholar'] = linkScholar\n",
    "    \n",
    "    if pubInfo['qualisLabels'].get('linkScopus'):\n",
    "        annot_dict['linkScopus'] = pubInfo['qualisLabels']['linkScopus']\n",
    "    \n",
    "    elem['annotation'] = annot_dict\n",
    "\n",
    "def persist_annotation_div(imagesURLs, visualizationURL):\n",
    "    alert_div_dict = {}\n",
    "    alert_div_dict['id'] = \"annot-div\"\n",
    "    return alert_div_dict\n",
    "\n",
    "def persist_annotation_message(imagesURLs, visualizationURL, recentUpdatesURL, pubCount):\n",
    "    annot_dict = {}\n",
    "    if pubCount > 0:\n",
    "        sChar = 's' if pubCount > 1 else ''\n",
    "        pubCountString = f\"anotou o Qualis de {pubCount} artigo{sChar} em periódico{sChar}  neste CV.\"\n",
    "    else:\n",
    "        pubCountString = \"não anotou nenhum artigo em periódico neste CV.\"\n",
    "        \n",
    "    annot_dict['pubCountString'] = pubCountString\n",
    "    annot_dict['visualizationURL'] = visualizationURL\n",
    "    annot_dict['recentUpdatesURL'] = recentUpdatesURL\n",
    "    return annot_dict\n",
    "\n",
    "def set_attributes(elem, attrs):\n",
    "    for key, value in attrs.items():\n",
    "        elem[key] = value\n",
    "\n",
    "def consolidate_qualis_data(qualisInfo):\n",
    "    pubData = []\n",
    "    pubDataYear = []\n",
    "    currYear = 0\n",
    "    \n",
    "    for qInfo in qualisInfo:\n",
    "        if currYear != qInfo['year']:\n",
    "            if currYear > 0:\n",
    "                pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "                pubDataYear = []\n",
    "            currYear = qInfo['year']\n",
    "        \n",
    "        pubDataItem = {\n",
    "            'issn': qInfo['issn'],\n",
    "            'title': qInfo['title'],\n",
    "            'pubName': qInfo['pubName'],\n",
    "            'qualis': qInfo['qualisLabels']['qualis'],\n",
    "            'baseYear': qInfo['qualisLabels']['baseYear'],\n",
    "            'jcr': qInfo['jcrData']['jcr'] if 'jcr' in qInfo['jcrData'] else 0,\n",
    "            'jcrYear': qInfo['jcrData']['baseYear'] if 'baseYear' in qInfo['jcrData'] else ''\n",
    "        }\n",
    "        pubDataYear.append(pubDataItem)\n",
    "    \n",
    "    if len(qualisInfo) > len(pubData):\n",
    "        pubData.append({'year': currYear, 'pubList': pubDataYear})\n",
    "        \n",
    "    return pubData\n",
    "\n",
    "# Suponhamos que 'html_content' seja o conteúdo HTML em que as anotações serão inseridas.\n",
    "# html_content = ...\n",
    "\n",
    "# Criamos um objeto BeautifulSoup\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Aqui você pode utilizar os métodos acima para persistir as informações.\n",
    "# Exemplo:\n",
    "# elem = {}\n",
    "# persist_qualis_annotation(elem, pubInfo, imagesURLs, dataSourceInfo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML annotation and CV ID\n",
    "annot_html = \"<a href='some_url'>Annotated Data</a>\"\n",
    "cv_id = \"cv_123\"\n",
    "\n",
    "# Store annotation\n",
    "store_annotation_in_db(uri, user, password, annot_html, cv_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução na nova abordagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variações das funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def process_lattes_page_v1(name_link, images_urls, recent_updates_url, qualis_data, data_source_info): \n",
    "#     print(qualis_data)\n",
    "    \n",
    "#     # Inicializando o WebDriver\n",
    "#     driver_service = Service('path/to/chromedriver')\n",
    "#     driver = webdriver.Chrome(service=driver_service)\n",
    "#     driver.get(name_link['link'])\n",
    "    \n",
    "#     # URL para visualização (pode ser determinado conforme a necessidade)\n",
    "#     visualization_url = 'path/to/visualization.html'\n",
    "    \n",
    "#     # Limpar cache de dados de Qualis\n",
    "#     qualis_data_cache = {}\n",
    "    \n",
    "#     # Annotate Lattes page com informações de Qualis\n",
    "#     lattes_info = await annotate_lattes_page(driver, images_urls, qualis_data, qualis_data_cache, data_source_info)\n",
    "    \n",
    "#     if lattes_info:\n",
    "#         await inject_annotation_message(images_urls, visualization_url, recent_updates_url, len(lattes_info))\n",
    "    \n",
    "#     # Consolidar dados de publicação a partir das informações de Lattes\n",
    "#     pub_info = consolidate_qualis_data(lattes_info)\n",
    "#     print(f\"pubInfo: {pub_info}\")\n",
    "    \n",
    "#     # Tentativa de ler dados de Lattes do armazenamento local (neste caso, SQLite)\n",
    "#     conn = sqlite3.connect('lattes_data.db')\n",
    "#     cursor = conn.cursor()\n",
    "#     cursor.execute(\"SELECT * FROM lattes_data WHERE name_link_link = ?\", (name_link['link'],))\n",
    "#     existing_data = cursor.fetchall()\n",
    "    \n",
    "#     lattes_data_array = []\n",
    "    \n",
    "#     if existing_data:\n",
    "#         # Filtrar dados existentes para evitar duplicatas\n",
    "#         lattes_data_array = [elem for elem in existing_data if elem['name_link']['link'] != name_link['link']]\n",
    "    \n",
    "#     # Adicionar dados de Lattes atuais ao array\n",
    "#     lattes_data_array.append({'name_link': name_link, 'pub_info': pub_info})\n",
    "    \n",
    "#     # Salvar array de dados de Lattes no armazenamento local (SQLite)\n",
    "#     cursor.execute(\"INSERT INTO lattes_data (name_link, pub_info) VALUES (?, ?)\", (json.dumps(name_link), json.dumps(pub_info)))\n",
    "#     conn.commit()\n",
    "#     conn.close()\n",
    "    \n",
    "#     print(\"Informações de nome, link e publicação de Lattes salvas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def annotate_published_articles_v2(driver, images_urls, qualis_data, qualis_data_cache, data_source_info, soup):\n",
    "#     # Localizar o primeiro elemento de artigo publicado\n",
    "#     start_elem = soup.find(\"div\", id=\"artigos-completos\")\n",
    "\n",
    "#     # Retornar uma lista vazia se não houver nenhum artigo publicado no CV\n",
    "#     if start_elem is None:\n",
    "#         return []\n",
    "\n",
    "#     qualis_info = []\n",
    "\n",
    "#     # Encontrar todos os artigos publicados\n",
    "#     pub_elems = start_elem.find_all(\"div\", class_=\"artigo-completo\")\n",
    "\n",
    "#     for pub_elem in pub_elems:\n",
    "#         qualis_pub_info = {\n",
    "#             'year': None,\n",
    "#             'issn': '',\n",
    "#             'title': '',\n",
    "#             'pubName': '',\n",
    "#             'qualisLabels': '',\n",
    "#             'jcrData': {}\n",
    "#         }\n",
    "#         # Obter o ano de publicação\n",
    "#         year_span = pub_elem.find(\"span\", class_=\"informacao-artigo\", attrs={\"data-tipo-ordenacao\": \"ano\"})\n",
    "#         if year_span:\n",
    "#             qualis_pub_info['year'] = int(year_span.text)\n",
    "        \n",
    "#         # Obter dados de publicação\n",
    "#         pub_elem_data = pub_elem.find(\"div\", attrs={\"cvuri\": True})\n",
    "\n",
    "#         if pub_elem_data:\n",
    "#             # Obter informações do periódico\n",
    "#             pub_info_string = pub_elem_data['cvuri']\n",
    "#             # Para fins de simplicidade, omitimos a função escapeHtml já que não é relevante para o BeautifulSoup\n",
    "\n",
    "#             # Obter ISSN, título e nome do periódico\n",
    "#             # Detalhes de implementação podem variar, pois o exemplo original usa JavaScript para manipular atributos DOM\n",
    "#             issn = ''  # Implemente a lógica para extrair o ISSN\n",
    "#             title = ''  # Implemente a lógica para extrair o título\n",
    "#             pub_name = ''  # Implemente a lógica para extrair o nome do periódico\n",
    "            \n",
    "#             qualis_pub_info['issn'] = issn\n",
    "#             qualis_pub_info['title'] = title\n",
    "#             qualis_pub_info['pubName'] = pub_name.upper()\n",
    "\n",
    "#             # Obter classificação Qualis do periódico\n",
    "#             qualis_labels = await get_qualis(issn, pub_name, qualis_data, qualis_data_cache, data_source_info)\n",
    "#             qualis_pub_info['qualisLabels'] = qualis_labels\n",
    "\n",
    "#             # Obter dados JCR (omitido neste exemplo; pode ser implementado conforme a necessidade)\n",
    "            \n",
    "#             # Adicionar informações ao vetor qualis_info\n",
    "#             qualis_info.append(qualis_pub_info)\n",
    "            \n",
    "#     return qualis_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_sistema(pastaraiz):\n",
    "    import os\n",
    "    import sys\n",
    "    sistema_operacional =sys.platform\n",
    "\n",
    "    try:\n",
    "        if 'linux' in sistema_operacional:\n",
    "            print('Sistema operacional Linux')\n",
    "            try:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'mak/'\n",
    "                os.listdir(drive+usuario)\n",
    "            except:\n",
    "                drive   = '/home/'\n",
    "                usuario = 'marcos/'\n",
    "        elif 'win32' in sistema_operacional:\n",
    "            print('Sistema operacional Windows')\n",
    "            drive   = 'C'\n",
    "            print(f'Drive em uso {drive.upper()}')\n",
    "            # drive = 'E'\n",
    "            # drive = input('Indique qual a letra da unidade onde deseja armazenar os arquivos (Ex.: C, E...)')\n",
    "            usuario = 'Users/marco/'\n",
    "            if os.path.isdir(drive+':/'+usuario) is False:\n",
    "                usuario = 'Users/marcos.aires/'\n",
    "        else:\n",
    "            print('SO não reconhecido')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Erro ao preparar as pastas locais!')\n",
    "        print(e)\n",
    "\n",
    "    caminho = drive+':/'+usuario+pastaraiz\n",
    "    \n",
    "    print(f'Pasta armazenagem local {caminho}\\n')\n",
    "\n",
    "    return caminho, drive, usuario\n",
    "\n",
    "def preparar_pastas(caminho):\n",
    "    # caminho, drive, usuario = definir_sistema(pastaraiz)\n",
    "    # # caminho = drive+':/'+usuario+pastaraiz\n",
    "    # caminho = drive+':/'+pastaraiz\n",
    "    if os.path.isdir(caminho) is False:\n",
    "        os.mkdir(caminho)\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os arquivo xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para arquivo xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os arquivo CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para arquivo CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os arquivo JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "    else:\n",
    "        if os.path.isdir(caminho+'/xml_zip'):\n",
    "            print ('Pasta para os xml já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/xml_zip')\n",
    "            print ('Pasta para xml criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/csv'):\n",
    "            print ('Pasta para os CSV já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/csv')\n",
    "            print ('Pasta para CSV criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/json'):\n",
    "            print ('Pasta para os JSON já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/json')\n",
    "            print ('Pasta para JSON criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/fig'):\n",
    "            print ('Pasta para figuras já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/fig')\n",
    "            print ('Pasta para figuras criada com sucesso!')\n",
    "        if os.path.isdir(caminho+'/output'):\n",
    "            print ('Pasta para saídas já existe!')\n",
    "        else:\n",
    "            os.mkdir(caminho+'/output')\n",
    "            print ('Pasta para saídas criada com sucesso!')            \n",
    "\n",
    "    pathzip  = caminho+'xml_zip/'\n",
    "    pathcsv  = caminho+'csv/'\n",
    "    pathjson = caminho+'json/'\n",
    "    pathfig  = caminho+'fig/'\n",
    "    pathaux  = caminho\n",
    "    pathout  = caminho+'output/'\n",
    "\n",
    "    print('\\nCaminho da pasta raiz', pathaux)\n",
    "    print('Caminho arquivos  XML', pathzip)\n",
    "    print('Caminho arquivos JSON', pathjson)\n",
    "    print('Caminho arquivos  CSV', pathcsv)\n",
    "    print('Caminho para  figuras', pathfig)\n",
    "    print('Pasta arquivos saídas', pathout)\n",
    "    \n",
    "    return pathzip, pathcsv, pathjson, pathfig, pathaux, pathout\n",
    "\n",
    "def strfdelta(tdelta, fmt='{H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-formatted string, \n",
    "    just like the stftime() method does for datetime.datetime objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    \n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    \n",
    "    return f.format(fmt, **values)\n",
    "\n",
    "def tempo(start, end):\n",
    "    t=end-start\n",
    "\n",
    "    tempo = timedelta(\n",
    "        weeks   = t//(3600*24*7),\n",
    "        days    = t//(3600*24),\n",
    "        seconds = t,\n",
    "        minutes = t//(60),\n",
    "        hours   = t//(3600),\n",
    "        microseconds=t//1000000,\n",
    "        )\n",
    "    fmt='{H:2}:{M:02}:{S:02}'\n",
    "    return strfdelta(tempo)\n",
    "\n",
    "# https://sh-tsang.medium.com/tutorial-cuda-cudnn-anaconda-jupyter-pytorch-installation-in-windows-10-96b2a2f0ac57\n",
    "\n",
    "def try_amb():\n",
    "    ## Visualizar versões dos principais componentes\n",
    "    import os\n",
    "    import pip\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # !pip3 install shutup\n",
    "    # import shutup; shutup.please()\n",
    "    \n",
    "    pyVer      = sys.version\n",
    "    pipVer     = pip.__version__\n",
    "    \n",
    "    print('\\nVERSÕES DAS PRINCIPAIS BIBLIOTECAS INSTALADAS NO ENVIROMENT')\n",
    "    print('Interpretador em uso:', sys.executable)\n",
    "    print('    Ambiente ativado:',os.environ['CONDA_DEFAULT_ENV'])\n",
    "    print('     Python: '+pyVer, '\\n        Pip:', pipVer,'\\n'\n",
    "         )\n",
    "\n",
    "    !nvcc -V\n",
    "\n",
    "def try_gpu():\n",
    "    print('\\nVERSÕES DO PYTORCH E GPU DISPONÍVEIS')\n",
    "    try:\n",
    "        import torch\n",
    "        print('    PyTorch:',torch.__version__)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Dispositivo:',device)\n",
    "        print('Disponível :',device,torch.cuda.is_available(),' | Inicializado:',torch.cuda.is_initialized(),'| Capacidade:',torch.cuda.get_device_capability(device=None))\n",
    "        print('Nome GPU   :',torch.cuda.get_device_name(0),'         | Quantidade:',torch.cuda.device_count(),'\\n')\n",
    "    except Exception as e:\n",
    "        print('Erro ao configurar a GPU:',e,'\\n')\n",
    "\n",
    "def try_folders(drives,pastas,pastasraiz):\n",
    "    import os\n",
    "    for drive in drives:\n",
    "        for i in pastas:\n",
    "            for j in pastasraiz:\n",
    "                try:\n",
    "                    caminho_testado = drive+i+j\n",
    "                    if os.path.isfile(caminho_testado+'/chromedriver/chromedriver.exe'):\n",
    "                        print(f\"Listando arqivos em: {caminho_testado}\")\n",
    "                        print(os.listdir(caminho_testado))\n",
    "                        caminho = caminho_testado+'/'\n",
    "                except:\n",
    "                    caminho=''\n",
    "                    print('Não foi possível encontrar uma pasta de trabalho')\n",
    "    return caminho\n",
    "\n",
    "def try_browser(raiz):\n",
    "    print('\\nVERSÕES DO BROWSER E DO CHROMEDRIVER INSTALADAS')\n",
    "    from selenium import webdriver\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "    try:\n",
    "        # Caminho para o seu chromedriver\n",
    "        driver_path=raiz+'chromedriver/chromedriver.exe'\n",
    "        print(driver_path)\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        str1 = driver.capabilities['browserVersion']\n",
    "        str2 = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]\n",
    "        print(f'     Versão do browser: {str1}')\n",
    "        print(f'Versão do chromedriver: {str2}')\n",
    "        driver.quit()\n",
    "\n",
    "        if str1[0:3] != str2[0:3]: \n",
    "            print(\"Versões incompatíveis, atualizar chromedriver!\")\n",
    "            print('  Baixar versão atualizada do Chromedriver em:')\n",
    "            print('  https://googlechromelabs.github.io/chrome-for-testing/#stable')\n",
    "            print('     Ex. Versão 116 PARA WINDOWS:')\n",
    "            print('\t    https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/win64/chromedriver-win64.zip')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def try_chromedriver(caminho):\n",
    "    try:\n",
    "        import os\n",
    "        os.listdir(caminho)\n",
    "    except Exception as e:\n",
    "        raiz=caminho\n",
    "\n",
    "    finally:\n",
    "        print(raiz)\n",
    "    return raiz\n",
    "\n",
    "def conectar_busca():    \n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    import time\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    t0=time.time()\n",
    "    \n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    browser   = webdriver.Chrome(options=options)\n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    browser.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    \n",
    "    browser.set_window_position(-20, -10)\n",
    "    browser.set_window_size(170, 1896)\n",
    "    browser.mouse = webdriver.ActionChains(browser)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "         \n",
    "    t1 = time.time()\n",
    "    tcon = tempo(t0,t1)\n",
    "    print(f'{tcon} para conectar ao servidor do CNPq')\n",
    "    # print('Conectado com sucesso em:', url, session_id)   \n",
    "    time.sleep(0.00001)\n",
    "    \n",
    "    # return browser, url, session_id\n",
    "    return browser\n",
    "\n",
    "def retry(func,\n",
    "          expected_ex_type=Exception,\n",
    "          limit=0,\n",
    "          wait_ms=100,\n",
    "          wait_increase_ratio=2,\n",
    "          logger=None,\n",
    "          on_exhaust=\"throw\"\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Retry a function invocation until no exception occurs\n",
    "    :param func: function to invoke\n",
    "    :param expected_ex_type: retry only if exception is subclass of this type\n",
    "    :param limit: maximum number of invocation attempts, 0 for unlimited attempts.\n",
    "    :param wait_ms: initial wait time after each attempt in milliseconds.\n",
    "    :param wait_increase_ratio: increase wait period by multiplying this value after each attempt.\n",
    "    :param logger: if not None, retry attempts will be logged to this logging.logger\n",
    "    :param on_exhaust: return value when retry attempts exhausted. Default is \"throw\" which will rethrow the exception\n",
    "                 of the last attempt.\n",
    "    :return: result of first successful invocation\n",
    "    :raises: last invocation exception if attempts exhausted or exception is not an instance of ex_type\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as ex:\n",
    "            if not isinstance(ex, expected_ex_type):\n",
    "                raise ex\n",
    "\n",
    "            if logger:\n",
    "                logger.error(\"Failed execution attempt #%d\", attempt, exc_info=ex)\n",
    "\n",
    "            # check if attempts exhausted\n",
    "            if 0 < limit <= attempt:\n",
    "                if logger:\n",
    "                    logger.warning(\"Attempt limit (%d) reached\", limit)\n",
    "                if on_exhaust == \"throw\":\n",
    "                    raise ex\n",
    "                return on_exhaust\n",
    "\n",
    "            # prepare for next attempt\n",
    "            attempt += 1\n",
    "            if logger:\n",
    "                logger.info(\"Waiting %d ms before attempt #%d\", wait_ms, attempt)\n",
    "            time.sleep(wait_ms / 1000)\n",
    "            wait_ms *= wait_increase_ratio\n",
    "\n",
    "def paginar(browser):\n",
    "    try:\n",
    "        css_paginacao=\"div.paginacao:nth-child(2)\"  #seletorcss=\"div.paginacao:nth-child(4) > a:nth-child(2)\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_paginacao)))\n",
    "        paginacao = browser.find_element(By.CSS_SELECTOR, css_paginacao)\n",
    "        paginas=paginacao.text.split(' ')\n",
    "        remover=['','anterior','...']\n",
    "        numpaginas = [x for x in paginas if x not in remover]\n",
    "        # print('NumPáginas interno:',numpaginas)\n",
    "    except Exception as e:\n",
    "        print('Erro ao utilizar função paginar():', e)\n",
    "    return numpaginas\n",
    "\n",
    "def achar_busca(browser, delay):\n",
    "    '''\n",
    "    Função para clicar no botão Buscar Currículo\n",
    "    '''\n",
    "    delay=10\n",
    "    try:\n",
    "        limite=5\n",
    "        xpath_nome = \"/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li/b\"\n",
    "        retry(WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=20,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        link_nome  = browser.find_element(By.XPATH, xpath_nome)\n",
    "        # ActionChains(browser).move_to_element(link_nome).perform()\n",
    "        \n",
    "        # Avaliar 'Stale file handle'\n",
    "        if link_nome.text == None:\n",
    "            xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "            \n",
    "            print('Sem resposta do servidor, tentando novamente...')\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "            #    expected_ex_type=ZeroDivisionError, \n",
    "               wait_ms=200,\n",
    "               limit=limite, \n",
    "            #    logger=logger, \n",
    "               on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "\n",
    "        return link_nome\n",
    "\n",
    "    except TimeoutException as t:\n",
    "        print(f'Erro de conexão durante achar_busca(). Reiniciando a função dentro de 2 segundos...')\n",
    "        time.sleep(2)\n",
    "\n",
    "def preencher_busca(browser, delay, NOME):\n",
    "    '''\n",
    "    Função para passar o nome para campo de busca\n",
    "    '''\n",
    "    try:\n",
    "        nome = lambda: browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\"))\n",
    "        nome().send_keys(Keys.CONTROL + \"a\")\n",
    "        nome().send_keys(NOME)\n",
    "        # browser.find_element(By.CSS_SELECTOR, (\"#textoBusca\")).send_keys(Keys.ENTER)\n",
    "        seletorcss='div.layout-cell-12:nth-child(8) > div:nth-child(1) > div:nth-child(1) > a:nth-child(1)'\n",
    "        # seletorcss=\"#textoBusca\"  \n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss))).click()\n",
    "        seletorcss=\"#botaoBuscaFiltros\"  \n",
    "        \n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, seletorcss)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback_str = ''.join(traceback.format_tb(e.__traceback__))          \n",
    "        print('Erro ao preencher nome no campo de busca, pausando por 1 segundo')\n",
    "        print(e,traceback_str)\n",
    "        # print(e)\n",
    "        # time.sleep(1.5)\n",
    "\n",
    "def definir_filtros(browser, delay, mestres=True, assunto=False):\n",
    "    '''\n",
    "    Clica nos check-boxes para definir os filtros de buscas\n",
    "    Para buscar por Assuntos usar parâmetro True, caso omitido fará busca por Nome por default\n",
    "    '''\n",
    "    from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import traceback   \n",
    "    \n",
    "    ## Aguardar carregar e clicar em checkbox de Assunto\n",
    "    try:\n",
    "        if mestres == True:\n",
    "            limite=2\n",
    "            ## Aguardar opção dropdown ser carregada e clicar em sua checkbox\n",
    "            css_buscar_demais = '#buscarDemais'\n",
    "            checkbox_buscar_demais = browser.find_element(By.CSS_SELECTOR, css_buscar_demais)\n",
    "            \n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, css_buscar_demais))),\n",
    "                   wait_ms=150,\n",
    "                   limit=limite, \n",
    "                   on_exhaust=(f'Problema clicar em {checkbox_buscar_demais}, {limite} tentativas sem sucesso.'))\n",
    "            \n",
    "            time.sleep(1)\n",
    "            checkbox_buscar_demais.click()\n",
    "            print(f'Clique efetuado em {checkbox_buscar_demais}')\n",
    "\n",
    "        # css_estrangeiros  = '#buscarEstrangeiros'\n",
    "\n",
    "        if assunto == True:\n",
    "            ## Implementar número de retentativas para casos de conexão muito instável\n",
    "            limite=5\n",
    "            retry(WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"#buscaAssunto\"))).click(),\n",
    "                #    expected_ex_type=ZeroDivisionError, \n",
    "                   wait_ms=200,\n",
    "                   limit=limite, \n",
    "                #    logger=logger, \n",
    "                   on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "        \n",
    "            ## Aguardar opção Atuação Profissional ser carregada e clicar em sua checkbox\n",
    "            xpath_atuacaoprofissional = \".//*[contains(text(), 'Atuação profissional')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_atuacaoprofissional))).click()\n",
    "\n",
    "            ## Aguardar opção Ciências da Saúde ser carregada e clicar em sua checkbox\n",
    "            xpath_cienciassaude = \".//*[contains(text(), 'Ciências da Saúde')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_cienciassaude))).click()\n",
    "            #browser.find_element_by_xpath(xpath_cienciassaude).click()        \n",
    "\n",
    "            ## Aguardar opção Enfermagem ser carregada e clicar em sua checkbox\n",
    "            xpath_enfermagem    = \".//*[contains(text(), 'Enfermagem')]\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, xpath_enfermagem))).click()\n",
    "            #browser.find_element_by_xpath(xpath_enfermagem).click()\n",
    "            aplicar_link  = browser.find_element(By.LINK_TEXT, 'Aplicar')\n",
    "            aplicar_link.click()\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f'Erro na função definir_filtros()')\n",
    "        print(e)\n",
    "        # traceback_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        # print(e, traceback_str) \n",
    "\n",
    "def procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite):\n",
    "    from selenium.common import exceptions\n",
    "    from pyjarowinkler.distance import get_jaro_distance\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n",
    "    ## Receber a quantidade de opções ao ler elementos de resultados\n",
    "    duvidas   = []\n",
    "    force_break_loop = False\n",
    "    try:\n",
    "        css_resultados = \".resultado\"\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "        resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)       \n",
    "        ## Ler quantidade de resultados apresentados pela busca de nome\n",
    "        try:\n",
    "            css_qteresultados = \".tit_form > b:nth-child(1)\"\n",
    "            WebDriverWait(browser, delay).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_qteresultados)))                       \n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            div_element = soup.find('div', {'class': 'tit_form'})\n",
    "            match = re.search(r'<b>(\\d+)</b>', str(div_element))\n",
    "            if match:\n",
    "                qte_resultados = int(match.group(1))\n",
    "                print(f'{qte_resultados} resultados para {NOME}')\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print('Erro ao ler a quantidade de resultados:')\n",
    "            print(e)\n",
    "            return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Escolher função a partir da quantidade de resultados da lista apresentada na busca\n",
    "        ## Ao achar clica no elemento elm_vinculo com link do nome para abrir o currículo\n",
    "        numpaginas = paginar(browser)\n",
    "        if numpaginas == [] and qte_resultados==1:\n",
    "            # capturar link para o primeiro nome resultado da busca\n",
    "            ## TO-DO inserir a crítica para o 'Stale file handle'\n",
    "            try:\n",
    "                css_linknome = \".resultado > ol:nth-child(1) > li:nth-child(1) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_linknome)))            \n",
    "                elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_linknome)\n",
    "                nome_vinculo = elm_vinculo.text\n",
    "            except Exception as e:\n",
    "                print('Erro ao encontrar o primeiro resultado da lista de nomes:',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "            # print('Clicar no nome único:', nome_vinculo)\n",
    "            try:\n",
    "                retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                       wait_ms=20,\n",
    "                       limit=limite,\n",
    "                       on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))   \n",
    "            except Exception as e:\n",
    "                print('Erro ao clicar no único nome encontrado anteriormente',e)\n",
    "                return np.NaN, NOME, np.NaN, e, browser\n",
    "        \n",
    "        ## Quantidade de resultados até 10 currículos, acessados sem paginação\n",
    "        else:\n",
    "            print(f'{qte_resultados} currículos de homônimos em potencial...')\n",
    "            numpaginas = paginar(browser)\n",
    "            numpaginas.append('próximo')\n",
    "            iteracoes=0\n",
    "            ## iterar em cada página de resultados\n",
    "            pagin = qte_resultados//10+1\n",
    "            for i in range(pagin+1):\n",
    "                # print(i,'/',pagin)\n",
    "                iteracoes+=1\n",
    "                try:\n",
    "                    numpaginas = paginar(browser)\n",
    "                    print(f'Iteração: {iteracoes}. Páginas sendo lidas: {numpaginas}')\n",
    "                    css_resultados = \".resultado\"\n",
    "                    WebDriverWait(browser, delay).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, css_resultados)))\n",
    "                    resultados = browser.find_elements(By.CSS_SELECTOR, css_resultados)\n",
    "                except Exception as e:\n",
    "                    print('Erro ao paginar:',e)\n",
    "                ## iterar em cada resultado\n",
    "                for n,i in enumerate(resultados):\n",
    "                    linhas = i.text.split('\\n\\n')\n",
    "                    # print(linhas)\n",
    "                    if 'Stale file handle' in str(linhas):\n",
    "                        return np.NaN, NOME, np.NaN, 'Stale file handle', browser\n",
    "                    for m,linha in enumerate(linhas):\n",
    "                        # print(f'\\nOrdem da linha: {m+1}, de total de linhas {len(linhas)}')\n",
    "                        # print('Conteúdo da linha:',linha.lower())\n",
    "                        print(linha)\n",
    "                        try:\n",
    "                            if instituicao.lower() in linha.lower() or unidade.lower() in linha.lower() or termo.lower() in linha.lower():\n",
    "                                # print('Vínculo encontrado!')\n",
    "                                count=m\n",
    "                                # print(' NOME:', NOME, type(NOME))\n",
    "                                # test = linhas[count].split('\\n')[0]\n",
    "                                # print('TESTE:',test, type(test))\n",
    "                                while get_jaro_distance(linhas[count].split('\\n')[0], str(NOME)) < 0.75:\n",
    "                                    count-=1\n",
    "                                print('Identificado vínculo com o interesse de análise no resultado:', m+1)\n",
    "                                nome_vinculo = linhas[count].strip()\n",
    "                                print(f'    Achado: {nome_vinculo}')\n",
    "                                try:\n",
    "                                    css_vinculo = f\".resultado > ol:nth-child(1) > li:nth-child({m+1}) > b:nth-child(1) > a:nth-child(1)\"\n",
    "                                    # print('\\nCSS_SELECTOR usado:', css_vinculo)\n",
    "                                    css_alvo = '.resultado > ol:nth-child(1) > li:nth-child(7) > b:nth-child(1) > a:nth-child(1)'\n",
    "                                    WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_vinculo)))            \n",
    "                                    elm_vinculo  = browser.find_element(By.CSS_SELECTOR, css_vinculo)\n",
    "                                    nome_vinculo = elm_vinculo.text\n",
    "                                    # print('Elemento retornado:',nome_vinculo)\n",
    "                                    retry(ActionChains(browser).click(elm_vinculo).perform(),\n",
    "                                        wait_ms=100,\n",
    "                                        limit=limite,\n",
    "                                        on_exhaust=(f'Problema ao clicar no link do nome. {limite} tentativas sem sucesso.'))            \n",
    "                                except Exception as e:\n",
    "                                    print('Erro ao achar o link do nome com múltiplos resultados')\n",
    "                                    return np.NaN, NOME, np.NaN, e, browser\n",
    "                                force_break_loop = True\n",
    "                                break\n",
    "                        except Exception as e2:\n",
    "                            traceback_str = ''.join(traceback.format_tb(e2.__traceback__))\n",
    "                            print('Erro ao procurar vínculo com currículos achados')    \n",
    "                            print(e2,traceback_str)\n",
    "                        ## Caso percorra todos elementos da lista e não encontre vínculo adiciona à dúvidas quanto ao nome\n",
    "                        if m==(qte_resultados):\n",
    "                            print(f'Não encontrada nenhuma referência à {instituicao} ou ao {unidade} ou ao termo {termo}')\n",
    "                            duvidas.append(NOME)\n",
    "                            # clear_output(wait=True)\n",
    "                            # browser.quit()\n",
    "                            continue\n",
    "                    if force_break_loop:\n",
    "                        break\n",
    "                try:\n",
    "                    prox = browser.find_element(By.PARTIAL_LINK_TEXT, 'próximo')\n",
    "                    prox.click()\n",
    "                except:\n",
    "                    continue\n",
    "        try:\n",
    "            elm_vinculo.text\n",
    "            # print(f'Nomes: {NOME} | {elm_vinculo.text}')\n",
    "        except:\n",
    "            return np.NaN, NOME, np.NaN, 'Vínculo não encontrado', browser\n",
    "    except Exception as err:\n",
    "        print('Erro ao sair da função procurar_vinculos()')\n",
    "        print('Conteúdo do erro:',err)\n",
    "        return np.NaN, NOME, np.NaN, err, browser\n",
    "    return elm_vinculo, np.NaN, np.NaN, np.NaN, browser\n",
    "\n",
    "def listar_idlattes(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='ID Lattes:']\n",
    "    return df_idlattes\n",
    "\n",
    "def listar_artigos(df_secoes):\n",
    "    df_idlattes = df_secoes[df_secoes['ROTULOS']=='Artigos completos publicados em periódicos']\n",
    "    return df_idlattes\n",
    "\n",
    "def contar_artigos(df_secoes):\n",
    "    def count_year_occurrences(content):\n",
    "        \"\"\"Count the number of occurrences of four-digit years followed by a period in the given string.\"\"\" \n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        pattern = r'\\b\\d{4}\\.'\n",
    "        return len(re.findall(pattern, content))\n",
    "\n",
    "    def extract_citations(content):\n",
    "        if not isinstance(content, (str, bytes)):\n",
    "            content = ' '.join(map(str, content))\n",
    "        citation_pattern = r'Citações:([\\d\\|]+),'\n",
    "        all_citations = re.findall(citation_pattern, content)\n",
    "        total_citations = 0\n",
    "        for citation_group in all_citations:\n",
    "            numbers = map(int, citation_group.split('|'))\n",
    "            total_citations += sum(numbers)\n",
    "        return len(all_citations), total_citations\n",
    "\n",
    "    df_secoes_contadas = df_secoes.copy()\n",
    "    df_secoes_contadas['QTE_ARTIGOS'] = df_secoes['CONTEUDOS'].apply(count_year_occurrences)\n",
    "    df_secoes_contadas['ARTIGOS_CITADOS'], df_secoes['SOMA_CITACOES'] = zip(*df_secoes['CONTEUDOS'].apply(extract_citations))\n",
    "\n",
    "    return df_secoes_contadas\n",
    "\n",
    "# Extração com BeaultifulSoap\n",
    "def parse_header(soup):\n",
    "    title = soup.title.string if soup.title else \"Unknown\"\n",
    "    meta_keywords = soup.find(\"meta\", {\"http-equiv\": \"keywords\"})[\"content\"] if soup.find(\"meta\", {\"http-equiv\": \"keywords\"}) else \"Unknown\"\n",
    "    return {\"title\": title, \"meta_keywords\": meta_keywords}\n",
    "\n",
    "def parse_h1_elements(soup, parent_node, graph):\n",
    "    h1_elements = soup.find_all('h1', {'tabindex': '0'})\n",
    "    for elem in h1_elements:\n",
    "        h1_text = elem.text.strip()\n",
    "        h1_node = Node(\"H1Element\", text=h1_text)\n",
    "        graph.create(h1_node)\n",
    "        relation = Relationship(parent_node, \"HAS_SECTION\", h1_node)\n",
    "        graph.create(relation)\n",
    "\n",
    "def persist_to_neo4j(header_data):\n",
    "    graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # Substitua pelo seu endpoint e credenciais\n",
    "    header_node = Node(\"Curriculum\", title=header_data['title'], meta_keywords=header_data['meta_keywords'])\n",
    "    graph.create(header_node)\n",
    "    return header_node\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Exemplo de entrada HTML\n",
    "#     html_content = '''\n",
    "#     <html lang=\"pt-BR\">\n",
    "#         <head>\n",
    "#             <title>Currículo do Sistema de Currículos Lattes (Raimir Holanda Filho)</title>\n",
    "#             <meta content=\"currículo,curriculo,curriculum,cv,vitae,lattes,produção,producao,científica,cientifica,Brasil\" http-equiv=\"keywords\">\n",
    "#         </head>\n",
    "#         <body>\n",
    "#             <h1 tabindex=\"0\">Educação</h1>\n",
    "#             <h1 tabindex=\"0\">Publicações</h1>\n",
    "#             <!-- ... -->\n",
    "#         </body>\n",
    "#     </html>\n",
    "#     '''\n",
    "#     # Inicialização do BeautifulSoup\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     # Extração e Persistência do cabeçalho\n",
    "#     header_data = parse_header(soup)\n",
    "#     header_node = persist_to_neo4j(header_data)\n",
    "    \n",
    "#     # Extração e Persistência dos elementos H1\n",
    "#     graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "#     parse_h1_elements(soup, header_node, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drives=['C:/Users/','E:/','./home/']\n",
    "pastas=['marcos.aires/', 'marco/']\n",
    "pastasraiz=['kgfioce','fioce']\n",
    "caminho=try_folders(drives,pastas,pastasraiz)\n",
    "\n",
    "preparar_pastas(caminho)\n",
    "try_amb()\n",
    "try_gpu()\n",
    "try_browser(caminho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conectar_busca():    \n",
    "    print(f'Conectando com o servidor do CNPq...')\n",
    "    # print(f'Iniciada extração de {len(lista_nomes)} currículos')\n",
    "    \n",
    "    ## https://www.selenium.dev/documentation/pt-br/webdriver/browser_manipulation/\n",
    "    options   = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    browser   = webdriver.Chrome(options=options)\n",
    "    url_buscaespecialista = 'http://buscatextual.cnpq.br/buscatextual/busca.do?buscarDoutores=true&buscarDemais=true&textoBusca='\n",
    "    browser.get(url_buscaespecialista) # acessa a url de busca do CNPQ   \n",
    "    \n",
    "    browser.set_window_position(-20, -10)\n",
    "    browser.set_window_size(170, 1896)\n",
    "    browser.mouse = webdriver.ActionChains(browser)\n",
    "    # url        = browser.command_executor._url #\"http://127.0.0.1:60622/hub\"\n",
    "    # session_id = browser.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'\n",
    "\n",
    "    # return browser, url, session_id\n",
    "    return browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from py2neo import Graph, Node\n",
    "\n",
    "def parse_parsoninfo(soup):\n",
    "    # Localizar o elemento de link que contém o título do currículo\n",
    "    link_element = soup.find(\"a\", {\"href\": lambda x: x and \"abreDetalhe\" in x})\n",
    "\n",
    "    # Extrair o texto do link para usar como título do nó\n",
    "    node_title = link_element.text if link_element else \"Unknown\"\n",
    "\n",
    "    # Localizar o elemento div contendo as propriedades\n",
    "    properties_div = soup.find(\"div\", {\"class\": \"resultado\"})\n",
    "\n",
    "    # Localizar o elemento li que contém as informações\n",
    "    li_element = soup.find(\"li\")\n",
    "\n",
    "    # Inicializar um dicionário para armazenar as propriedades\n",
    "    properties = {}\n",
    "\n",
    "    # Extrair e armazenar as propriedades relevantes\n",
    "    if properties_div:\n",
    "        properties['Nacionalidade'] = 'Brasil'\n",
    "        properties['Cargo'] = properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}).text if properties_div.find(\"span\", {\"class\": \"tooltip coautor\"}) else 'Desconhecido'\n",
    "        properties['Titulação'] = properties_div.contents[-4] if len(properties_div.contents) > 4 else 'Desconhecido'\n",
    "\n",
    "        # Extração de nome e identificador único\n",
    "        a_element = li_element.find(\"a\")\n",
    "        properties[\"Nome\"] = a_element.text\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Nacionalidade\n",
    "        img_element = li_element.find(\"img\")\n",
    "        properties[\"Nacionalidade\"] = img_element.get(\"title\") or img_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Bolsa de Produtividade\n",
    "        span_element = li_element.find(\"span\", {\"class\": \"tooltip coautor\"})\n",
    "        properties[\"Bolsista_Prod_Pesquisa\"] = span_element.get(\"alt\")\n",
    "\n",
    "        # Extração de Títulos Acadêmicos e outras informações\n",
    "\n",
    "\n",
    "\n",
    "    return node_title, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_lista(lista, mestres=True, assunto=False):\n",
    "    sucesso=[]\n",
    "    falhas=[]\n",
    "    duvidas=[]\n",
    "    tipo_erro=[]\n",
    "    curriculos=[]\n",
    "    rotulos=[]\n",
    "    conteudos=[]\n",
    "\n",
    "    delay=10\n",
    "    limite=3\n",
    "    instituicao = 'Fundação Oswaldo Cruz'\n",
    "    unidade     = 'Fiocruz Ceará'\n",
    "    termo       = 'Ministerio da Saude'\n",
    "\n",
    "    t0 = time.time()\n",
    "    browser = conectar_busca()\n",
    "    for NOME in lista:\n",
    "        try:\n",
    "            preencher_busca(browser, delay, NOME)\n",
    "            elemento_achado, nome_falha, duvida, erro, browser = procurar_vinculos(NOME, instituicao, unidade, termo, browser, delay, limite)\n",
    "            link_nome     = achar_busca(browser, delay)\n",
    "            window_before = browser.current_window_handle\n",
    "            \n",
    "            if str(elemento_achado) == 'nan':\n",
    "                print('Vínculo não encontrado, passando ao próximo nome...')\n",
    "                falhas.append(nome_falha)\n",
    "                duvidas.append(duvida)\n",
    "                tipo_erro.append(erro)\n",
    "                # print(nome_falha)\n",
    "                # print(erro)\n",
    "                # clear_output(wait=True)\n",
    "                raise Exception\n",
    "            print('Vínculo encontrado no currículo de nome:',elemento_achado.text)\n",
    "\n",
    "            ## Clicar no botão abrir currículo e mudar de aba\n",
    "            try:\n",
    "                ## Aguarda, encontra, clica em buscar nome\n",
    "                link_nome    = achar_busca(browser, delay)\n",
    "                nome_buscado = []\n",
    "                nome_achado  = []\n",
    "                nome_buscado.append(NOME)\n",
    "                \n",
    "                if link_nome.text == None:\n",
    "                    xpath_nome = '/html/body/form/div/div[4]/div/div/div/div[3]/div/div[3]/ol/li'\n",
    "                    # 'Stale file handle'\n",
    "                    print('Ainda sem resposta do servidor, tentando novamente...')\n",
    "                    retry(WebDriverWait(browser, delay).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, xpath_nome))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))\n",
    "                try:\n",
    "                    ActionChains(browser).click(link_nome).perform()\n",
    "                    nome_achado.append(link_nome.text)\n",
    "                except:\n",
    "                    print(f'Currículo não encontrado para: {NOME}.')\n",
    "                    return\n",
    "                \n",
    "                retry(WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\"))),\n",
    "                    #    expected_ex_type=ZeroDivisionError, \n",
    "                    wait_ms=200,\n",
    "                    limit=limite, \n",
    "                    #    logger=logger, \n",
    "                    on_exhaust=(f'Problema ao acessar ao servidor do CNpQ função definir_filtros(). {limite} tentativas sem sucesso.'))   \n",
    "                \n",
    "                # Clicar botão para abrir o currículo\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                time.sleep(0.2)\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "\n",
    "                # Pega o código fonte da página\n",
    "                page_source = browser.page_source\n",
    "\n",
    "                # Usa BeautifulSoup para analisar\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                # Extração e Persistência do cabeçalho\n",
    "                header_data = parse_header(soup)\n",
    "                header_node = persist_to_neo4j(header_data)\n",
    "                \n",
    "                # Extração e Persistência dos elementos H1\n",
    "                graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "                parse_h1_elements(soup, header_node, graph)\n",
    "                graph, cv_node, properties = parse_parsoninfo(soup)                \n",
    "\n",
    "            except Exception as e:\n",
    "                print('Erro',e)\n",
    "                print('Tentando nova requisição ao servidor')\n",
    "                time.sleep(1)\n",
    "                btn_abrir_curriculo = WebDriverWait(browser, delay).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#idbtnabrircurriculo\")))\n",
    "                ActionChains(browser).click(btn_abrir_curriculo).perform()\n",
    "                WebDriverWait(browser, delay).until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "                ## Gerenciamento das janelas abertas no browser\n",
    "                window_after = browser.window_handles\n",
    "                new_window   = [x for x in window_after if x != window_before][0]\n",
    "                browser.switch_to.window(new_window)\n",
    "                time.sleep(1)\n",
    "\n",
    "            sucesso.append(NOME)\n",
    "\n",
    "        except:\n",
    "            print(f'Currículo não encontrado: {NOME}')\n",
    "            browser.back()\n",
    "            continue\n",
    "\n",
    "    df_dados =pd.DataFrame({\n",
    "        'CURRICULO': pd.Series(curriculos),\n",
    "        'ROTULOS': pd.Series(rotulos),\n",
    "        'CONTEUDOS': pd.Series(conteudos),\n",
    "            })\n",
    "    \n",
    "    t6=time.time()\n",
    "    print('='*95)\n",
    "    # print(f' {len(sucesso)} currículos extraídos com sucesso')\n",
    "    print(f' Tempo total para extrair {len(conteudos)} seções dos currículos: {tempo(t0,t6)}')\n",
    "    # print('='*95)\n",
    "    browser.quit()\n",
    "    \n",
    "    return df_dados, sucesso  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ['Raimir Holanda Filho', 'Carlos Jose Araujo Pinheiro','Antonio Marcos Aires Barbosa']\n",
    "df_dados, sucesso = extrair_lista(lista, mestres=True, assunto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakerx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
